{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('customer.pkl', 'rb') as f:\n",
    "    X_customer_balanced, Y_customer_balanced = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *SVM - 74%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3918, 10), (3918,))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_customer_balanced.shape, Y_customer_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', random_state=42, C=2.0)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_customer_balanced, Y_customer_balanced, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.7372449  0.75       0.75255102 0.73469388 0.72704082 0.69897959\n",
      " 0.73469388 0.78316327 0.72890026 0.76982097]\n",
      "Score médio: 0.7417088574560259\n",
      "Desvio padrão: 0.022401792602462955\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores}\")\n",
    "print(f\"Score médio: {np.mean(scores)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Artificial Neural Network - 74,5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = MLPClassifier(max_iter=1500, verbose=True, tol=0.000000, solver='adam', activation='relu', hidden_layer_sizes=(10,10))\n",
    "kf_ann = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72184529\n",
      "Iteration 2, loss = 0.69971280\n",
      "Iteration 3, loss = 0.68600046\n",
      "Iteration 4, loss = 0.67699051\n",
      "Iteration 5, loss = 0.66882852\n",
      "Iteration 6, loss = 0.66084492\n",
      "Iteration 7, loss = 0.65260820\n",
      "Iteration 8, loss = 0.64340674\n",
      "Iteration 9, loss = 0.63380350\n",
      "Iteration 10, loss = 0.62354339\n",
      "Iteration 11, loss = 0.61251169\n",
      "Iteration 12, loss = 0.60241865\n",
      "Iteration 13, loss = 0.59265956\n",
      "Iteration 14, loss = 0.58443543\n",
      "Iteration 15, loss = 0.57752761\n",
      "Iteration 16, loss = 0.57180983\n",
      "Iteration 17, loss = 0.56779704\n",
      "Iteration 18, loss = 0.56408444\n",
      "Iteration 19, loss = 0.56150211\n",
      "Iteration 20, loss = 0.55873777\n",
      "Iteration 21, loss = 0.55655085\n",
      "Iteration 22, loss = 0.55460782\n",
      "Iteration 23, loss = 0.55276857\n",
      "Iteration 24, loss = 0.55133290\n",
      "Iteration 25, loss = 0.55004115\n",
      "Iteration 26, loss = 0.54856424\n",
      "Iteration 27, loss = 0.54727116\n",
      "Iteration 28, loss = 0.54593505\n",
      "Iteration 29, loss = 0.54464335\n",
      "Iteration 30, loss = 0.54327980\n",
      "Iteration 31, loss = 0.54221217\n",
      "Iteration 32, loss = 0.54052370\n",
      "Iteration 33, loss = 0.53933395\n",
      "Iteration 34, loss = 0.53801415\n",
      "Iteration 35, loss = 0.53687516\n",
      "Iteration 36, loss = 0.53567391\n",
      "Iteration 37, loss = 0.53454224\n",
      "Iteration 38, loss = 0.53332461\n",
      "Iteration 39, loss = 0.53221353\n",
      "Iteration 40, loss = 0.53093998\n",
      "Iteration 41, loss = 0.52961081\n",
      "Iteration 42, loss = 0.52827100\n",
      "Iteration 43, loss = 0.52717704\n",
      "Iteration 44, loss = 0.52610063\n",
      "Iteration 45, loss = 0.52513457\n",
      "Iteration 46, loss = 0.52417106\n",
      "Iteration 47, loss = 0.52331396\n",
      "Iteration 48, loss = 0.52227563\n",
      "Iteration 49, loss = 0.52130815\n",
      "Iteration 50, loss = 0.52055455\n",
      "Iteration 51, loss = 0.51959402\n",
      "Iteration 52, loss = 0.51868411\n",
      "Iteration 53, loss = 0.51794987\n",
      "Iteration 54, loss = 0.51719503\n",
      "Iteration 55, loss = 0.51639159\n",
      "Iteration 56, loss = 0.51575794\n",
      "Iteration 57, loss = 0.51490686\n",
      "Iteration 58, loss = 0.51387393\n",
      "Iteration 59, loss = 0.51349931\n",
      "Iteration 60, loss = 0.51260066\n",
      "Iteration 61, loss = 0.51186339\n",
      "Iteration 62, loss = 0.51109437\n",
      "Iteration 63, loss = 0.51065011\n",
      "Iteration 64, loss = 0.51008484\n",
      "Iteration 65, loss = 0.50955838\n",
      "Iteration 66, loss = 0.50827971\n",
      "Iteration 67, loss = 0.50786099\n",
      "Iteration 68, loss = 0.50707689\n",
      "Iteration 69, loss = 0.50687319\n",
      "Iteration 70, loss = 0.50612405\n",
      "Iteration 71, loss = 0.50568722\n",
      "Iteration 72, loss = 0.50500933\n",
      "Iteration 73, loss = 0.50460084\n",
      "Iteration 74, loss = 0.50437632\n",
      "Iteration 75, loss = 0.50406855\n",
      "Iteration 76, loss = 0.50327280\n",
      "Iteration 77, loss = 0.50289079\n",
      "Iteration 78, loss = 0.50269311\n",
      "Iteration 79, loss = 0.50196680\n",
      "Iteration 80, loss = 0.50191557\n",
      "Iteration 81, loss = 0.50158475\n",
      "Iteration 82, loss = 0.50138198\n",
      "Iteration 83, loss = 0.50103864\n",
      "Iteration 84, loss = 0.50051812\n",
      "Iteration 85, loss = 0.50032484\n",
      "Iteration 86, loss = 0.49986288\n",
      "Iteration 87, loss = 0.49965145\n",
      "Iteration 88, loss = 0.49936685\n",
      "Iteration 89, loss = 0.49909767\n",
      "Iteration 90, loss = 0.49858680\n",
      "Iteration 91, loss = 0.49821484\n",
      "Iteration 92, loss = 0.49806988\n",
      "Iteration 93, loss = 0.49776489\n",
      "Iteration 94, loss = 0.49717073\n",
      "Iteration 95, loss = 0.49685301\n",
      "Iteration 96, loss = 0.49662753\n",
      "Iteration 97, loss = 0.49658404\n",
      "Iteration 98, loss = 0.49590815\n",
      "Iteration 99, loss = 0.49558021\n",
      "Iteration 100, loss = 0.49539655\n",
      "Iteration 101, loss = 0.49536447\n",
      "Iteration 102, loss = 0.49496960\n",
      "Iteration 103, loss = 0.49433639\n",
      "Iteration 104, loss = 0.49436123\n",
      "Iteration 105, loss = 0.49371583\n",
      "Iteration 106, loss = 0.49333468\n",
      "Iteration 107, loss = 0.49339474\n",
      "Iteration 108, loss = 0.49308669\n",
      "Iteration 109, loss = 0.49269529\n",
      "Iteration 110, loss = 0.49258234\n",
      "Iteration 111, loss = 0.49231532\n",
      "Iteration 112, loss = 0.49212184\n",
      "Iteration 113, loss = 0.49166692\n",
      "Iteration 114, loss = 0.49151562\n",
      "Iteration 115, loss = 0.49130289\n",
      "Iteration 116, loss = 0.49122306\n",
      "Iteration 117, loss = 0.49096232\n",
      "Iteration 118, loss = 0.49069930\n",
      "Iteration 119, loss = 0.49054397\n",
      "Iteration 120, loss = 0.49044841\n",
      "Iteration 121, loss = 0.48999027\n",
      "Iteration 122, loss = 0.49012262\n",
      "Iteration 123, loss = 0.48970376\n",
      "Iteration 124, loss = 0.48967354\n",
      "Iteration 125, loss = 0.49030731\n",
      "Iteration 126, loss = 0.48919446\n",
      "Iteration 127, loss = 0.48920657\n",
      "Iteration 128, loss = 0.48892834\n",
      "Iteration 129, loss = 0.48870141\n",
      "Iteration 130, loss = 0.48897132\n",
      "Iteration 131, loss = 0.48844840\n",
      "Iteration 132, loss = 0.48818695\n",
      "Iteration 133, loss = 0.48798055\n",
      "Iteration 134, loss = 0.48784091\n",
      "Iteration 135, loss = 0.48777494\n",
      "Iteration 136, loss = 0.48783552\n",
      "Iteration 137, loss = 0.48744847\n",
      "Iteration 138, loss = 0.48726854\n",
      "Iteration 139, loss = 0.48732097\n",
      "Iteration 140, loss = 0.48696901\n",
      "Iteration 141, loss = 0.48698235\n",
      "Iteration 142, loss = 0.48678999\n",
      "Iteration 143, loss = 0.48653584\n",
      "Iteration 144, loss = 0.48668967\n",
      "Iteration 145, loss = 0.48639218\n",
      "Iteration 146, loss = 0.48620121\n",
      "Iteration 147, loss = 0.48622417\n",
      "Iteration 148, loss = 0.48596074\n",
      "Iteration 149, loss = 0.48596786\n",
      "Iteration 150, loss = 0.48581342\n",
      "Iteration 151, loss = 0.48546615\n",
      "Iteration 152, loss = 0.48590254\n",
      "Iteration 153, loss = 0.48534965\n",
      "Iteration 154, loss = 0.48519179\n",
      "Iteration 155, loss = 0.48521424\n",
      "Iteration 156, loss = 0.48534474\n",
      "Iteration 157, loss = 0.48533473\n",
      "Iteration 158, loss = 0.48465639\n",
      "Iteration 159, loss = 0.48480432\n",
      "Iteration 160, loss = 0.48454432\n",
      "Iteration 161, loss = 0.48435840\n",
      "Iteration 162, loss = 0.48434350\n",
      "Iteration 163, loss = 0.48414834\n",
      "Iteration 164, loss = 0.48415014\n",
      "Iteration 165, loss = 0.48404628\n",
      "Iteration 166, loss = 0.48415805\n",
      "Iteration 167, loss = 0.48372032\n",
      "Iteration 168, loss = 0.48364305\n",
      "Iteration 169, loss = 0.48345801\n",
      "Iteration 170, loss = 0.48343100\n",
      "Iteration 171, loss = 0.48328124\n",
      "Iteration 172, loss = 0.48328271\n",
      "Iteration 173, loss = 0.48324352\n",
      "Iteration 174, loss = 0.48286800\n",
      "Iteration 175, loss = 0.48289191\n",
      "Iteration 176, loss = 0.48273756\n",
      "Iteration 177, loss = 0.48294140\n",
      "Iteration 178, loss = 0.48247001\n",
      "Iteration 179, loss = 0.48237613\n",
      "Iteration 180, loss = 0.48246125\n",
      "Iteration 181, loss = 0.48229394\n",
      "Iteration 182, loss = 0.48234943\n",
      "Iteration 183, loss = 0.48201134\n",
      "Iteration 184, loss = 0.48231749\n",
      "Iteration 185, loss = 0.48246444\n",
      "Iteration 186, loss = 0.48179897\n",
      "Iteration 187, loss = 0.48178512\n",
      "Iteration 188, loss = 0.48210020\n",
      "Iteration 189, loss = 0.48170750\n",
      "Iteration 190, loss = 0.48151036\n",
      "Iteration 191, loss = 0.48129047\n",
      "Iteration 192, loss = 0.48123485\n",
      "Iteration 193, loss = 0.48106781\n",
      "Iteration 194, loss = 0.48111992\n",
      "Iteration 195, loss = 0.48095363\n",
      "Iteration 196, loss = 0.48101999\n",
      "Iteration 197, loss = 0.48071371\n",
      "Iteration 198, loss = 0.48079823\n",
      "Iteration 199, loss = 0.48077233\n",
      "Iteration 200, loss = 0.48049681\n",
      "Iteration 201, loss = 0.48037894\n",
      "Iteration 202, loss = 0.48033214\n",
      "Iteration 203, loss = 0.48026087\n",
      "Iteration 204, loss = 0.48020740\n",
      "Iteration 205, loss = 0.48019153\n",
      "Iteration 206, loss = 0.48014002\n",
      "Iteration 207, loss = 0.47992862\n",
      "Iteration 208, loss = 0.47992890\n",
      "Iteration 209, loss = 0.47985117\n",
      "Iteration 210, loss = 0.47972361\n",
      "Iteration 211, loss = 0.47962858\n",
      "Iteration 212, loss = 0.47953962\n",
      "Iteration 213, loss = 0.47942834\n",
      "Iteration 214, loss = 0.47943423\n",
      "Iteration 215, loss = 0.47928485\n",
      "Iteration 216, loss = 0.47910528\n",
      "Iteration 217, loss = 0.47919452\n",
      "Iteration 218, loss = 0.47930018\n",
      "Iteration 219, loss = 0.47902031\n",
      "Iteration 220, loss = 0.47887936\n",
      "Iteration 221, loss = 0.47891964\n",
      "Iteration 222, loss = 0.47880751\n",
      "Iteration 223, loss = 0.47912016\n",
      "Iteration 224, loss = 0.47901773\n",
      "Iteration 225, loss = 0.47866465\n",
      "Iteration 226, loss = 0.47858279\n",
      "Iteration 227, loss = 0.47860198\n",
      "Iteration 228, loss = 0.47881940\n",
      "Iteration 229, loss = 0.47854322\n",
      "Iteration 230, loss = 0.47894933\n",
      "Iteration 231, loss = 0.47842125\n",
      "Iteration 232, loss = 0.47846965\n",
      "Iteration 233, loss = 0.47835153\n",
      "Iteration 234, loss = 0.47844069\n",
      "Iteration 235, loss = 0.47820813\n",
      "Iteration 236, loss = 0.47799489\n",
      "Iteration 237, loss = 0.47823924\n",
      "Iteration 238, loss = 0.47795664\n",
      "Iteration 239, loss = 0.47788936\n",
      "Iteration 240, loss = 0.47801467\n",
      "Iteration 241, loss = 0.47801347\n",
      "Iteration 242, loss = 0.47812812\n",
      "Iteration 243, loss = 0.47778396\n",
      "Iteration 244, loss = 0.47751698\n",
      "Iteration 245, loss = 0.47785880\n",
      "Iteration 246, loss = 0.47751320\n",
      "Iteration 247, loss = 0.47743688\n",
      "Iteration 248, loss = 0.47744011\n",
      "Iteration 249, loss = 0.47726464\n",
      "Iteration 250, loss = 0.47714352\n",
      "Iteration 251, loss = 0.47711491\n",
      "Iteration 252, loss = 0.47700760\n",
      "Iteration 253, loss = 0.47712557\n",
      "Iteration 254, loss = 0.47702321\n",
      "Iteration 255, loss = 0.47710669\n",
      "Iteration 256, loss = 0.47693529\n",
      "Iteration 257, loss = 0.47703085\n",
      "Iteration 258, loss = 0.47709432\n",
      "Iteration 259, loss = 0.47700194\n",
      "Iteration 260, loss = 0.47673750\n",
      "Iteration 261, loss = 0.47688986\n",
      "Iteration 262, loss = 0.47667283\n",
      "Iteration 263, loss = 0.47691135\n",
      "Iteration 264, loss = 0.47660437\n",
      "Iteration 265, loss = 0.47654663\n",
      "Iteration 266, loss = 0.47638710\n",
      "Iteration 267, loss = 0.47646075\n",
      "Iteration 268, loss = 0.47653137\n",
      "Iteration 269, loss = 0.47638679\n",
      "Iteration 270, loss = 0.47626811\n",
      "Iteration 271, loss = 0.47628289\n",
      "Iteration 272, loss = 0.47616895\n",
      "Iteration 273, loss = 0.47618970\n",
      "Iteration 274, loss = 0.47621712\n",
      "Iteration 275, loss = 0.47597086\n",
      "Iteration 276, loss = 0.47617517\n",
      "Iteration 277, loss = 0.47622502\n",
      "Iteration 278, loss = 0.47594797\n",
      "Iteration 279, loss = 0.47599024\n",
      "Iteration 280, loss = 0.47570910\n",
      "Iteration 281, loss = 0.47578288\n",
      "Iteration 282, loss = 0.47569728\n",
      "Iteration 283, loss = 0.47558365\n",
      "Iteration 284, loss = 0.47569693\n",
      "Iteration 285, loss = 0.47549639\n",
      "Iteration 286, loss = 0.47534043\n",
      "Iteration 287, loss = 0.47570787\n",
      "Iteration 288, loss = 0.47515562\n",
      "Iteration 289, loss = 0.47528813\n",
      "Iteration 290, loss = 0.47527562\n",
      "Iteration 291, loss = 0.47499538\n",
      "Iteration 292, loss = 0.47503877\n",
      "Iteration 293, loss = 0.47507471\n",
      "Iteration 294, loss = 0.47494862\n",
      "Iteration 295, loss = 0.47491126\n",
      "Iteration 296, loss = 0.47492227\n",
      "Iteration 297, loss = 0.47491529\n",
      "Iteration 298, loss = 0.47491346\n",
      "Iteration 299, loss = 0.47498280\n",
      "Iteration 300, loss = 0.47461025\n",
      "Iteration 301, loss = 0.47450598\n",
      "Iteration 302, loss = 0.47447740\n",
      "Iteration 303, loss = 0.47452028\n",
      "Iteration 304, loss = 0.47439662\n",
      "Iteration 305, loss = 0.47435659\n",
      "Iteration 306, loss = 0.47439894\n",
      "Iteration 307, loss = 0.47439141\n",
      "Iteration 308, loss = 0.47428696\n",
      "Iteration 309, loss = 0.47409452\n",
      "Iteration 310, loss = 0.47430882\n",
      "Iteration 311, loss = 0.47428530\n",
      "Iteration 312, loss = 0.47419033\n",
      "Iteration 313, loss = 0.47395214\n",
      "Iteration 314, loss = 0.47380628\n",
      "Iteration 315, loss = 0.47376621\n",
      "Iteration 316, loss = 0.47407882\n",
      "Iteration 317, loss = 0.47389688\n",
      "Iteration 318, loss = 0.47386873\n",
      "Iteration 319, loss = 0.47366358\n",
      "Iteration 320, loss = 0.47345294\n",
      "Iteration 321, loss = 0.47365200\n",
      "Iteration 322, loss = 0.47357899\n",
      "Iteration 323, loss = 0.47346800\n",
      "Iteration 324, loss = 0.47348613\n",
      "Iteration 325, loss = 0.47342246\n",
      "Iteration 326, loss = 0.47356212\n",
      "Iteration 327, loss = 0.47361618\n",
      "Iteration 328, loss = 0.47316169\n",
      "Iteration 329, loss = 0.47332818\n",
      "Iteration 330, loss = 0.47299725\n",
      "Iteration 331, loss = 0.47313788\n",
      "Iteration 332, loss = 0.47307691\n",
      "Iteration 333, loss = 0.47316750\n",
      "Iteration 334, loss = 0.47285873\n",
      "Iteration 335, loss = 0.47302329\n",
      "Iteration 336, loss = 0.47297901\n",
      "Iteration 337, loss = 0.47292292\n",
      "Iteration 338, loss = 0.47294359\n",
      "Iteration 339, loss = 0.47274436\n",
      "Iteration 340, loss = 0.47311564\n",
      "Iteration 341, loss = 0.47331421\n",
      "Iteration 342, loss = 0.47260885\n",
      "Iteration 343, loss = 0.47285071\n",
      "Iteration 344, loss = 0.47255829\n",
      "Iteration 345, loss = 0.47282355\n",
      "Iteration 346, loss = 0.47283339\n",
      "Iteration 347, loss = 0.47283776\n",
      "Iteration 348, loss = 0.47284281\n",
      "Iteration 349, loss = 0.47244242\n",
      "Iteration 350, loss = 0.47244848\n",
      "Iteration 351, loss = 0.47248856\n",
      "Iteration 352, loss = 0.47222015\n",
      "Iteration 353, loss = 0.47264366\n",
      "Iteration 354, loss = 0.47228344\n",
      "Iteration 355, loss = 0.47221146\n",
      "Iteration 356, loss = 0.47291461\n",
      "Iteration 357, loss = 0.47253404\n",
      "Iteration 358, loss = 0.47207844\n",
      "Iteration 359, loss = 0.47198618\n",
      "Iteration 360, loss = 0.47199930\n",
      "Iteration 361, loss = 0.47240835\n",
      "Iteration 362, loss = 0.47199282\n",
      "Iteration 363, loss = 0.47207589\n",
      "Iteration 364, loss = 0.47199791\n",
      "Iteration 365, loss = 0.47179103\n",
      "Iteration 366, loss = 0.47223343\n",
      "Iteration 367, loss = 0.47185697\n",
      "Iteration 368, loss = 0.47188767\n",
      "Iteration 369, loss = 0.47166129\n",
      "Iteration 370, loss = 0.47167972\n",
      "Iteration 371, loss = 0.47165774\n",
      "Iteration 372, loss = 0.47173009\n",
      "Iteration 373, loss = 0.47168890\n",
      "Iteration 374, loss = 0.47151823\n",
      "Iteration 375, loss = 0.47161621\n",
      "Iteration 376, loss = 0.47138624\n",
      "Iteration 377, loss = 0.47170875\n",
      "Iteration 378, loss = 0.47153570\n",
      "Iteration 379, loss = 0.47155569\n",
      "Iteration 380, loss = 0.47140717\n",
      "Iteration 381, loss = 0.47144558\n",
      "Iteration 382, loss = 0.47153402\n",
      "Iteration 383, loss = 0.47215509\n",
      "Iteration 384, loss = 0.47101086\n",
      "Iteration 385, loss = 0.47145877\n",
      "Iteration 386, loss = 0.47152375\n",
      "Iteration 387, loss = 0.47154467\n",
      "Iteration 388, loss = 0.47149764\n",
      "Iteration 389, loss = 0.47119549\n",
      "Iteration 390, loss = 0.47138782\n",
      "Iteration 391, loss = 0.47130073\n",
      "Iteration 392, loss = 0.47133694\n",
      "Iteration 393, loss = 0.47102068\n",
      "Iteration 394, loss = 0.47096978\n",
      "Iteration 395, loss = 0.47103629\n",
      "Iteration 396, loss = 0.47105816\n",
      "Iteration 397, loss = 0.47115447\n",
      "Iteration 398, loss = 0.47099126\n",
      "Iteration 399, loss = 0.47114031\n",
      "Iteration 400, loss = 0.47086673\n",
      "Iteration 401, loss = 0.47110829\n",
      "Iteration 402, loss = 0.47070281\n",
      "Iteration 403, loss = 0.47075565\n",
      "Iteration 404, loss = 0.47082236\n",
      "Iteration 405, loss = 0.47072286\n",
      "Iteration 406, loss = 0.47105628\n",
      "Iteration 407, loss = 0.47067590\n",
      "Iteration 408, loss = 0.47053422\n",
      "Iteration 409, loss = 0.47089242\n",
      "Iteration 410, loss = 0.47061461\n",
      "Iteration 411, loss = 0.47071406\n",
      "Iteration 412, loss = 0.47075854\n",
      "Iteration 413, loss = 0.47057760\n",
      "Iteration 414, loss = 0.47032427\n",
      "Iteration 415, loss = 0.47052288\n",
      "Iteration 416, loss = 0.47026797\n",
      "Iteration 417, loss = 0.47036575\n",
      "Iteration 418, loss = 0.47039494\n",
      "Iteration 419, loss = 0.47073308\n",
      "Iteration 420, loss = 0.47042894\n",
      "Iteration 421, loss = 0.47048692\n",
      "Iteration 422, loss = 0.47031149\n",
      "Iteration 423, loss = 0.47039363\n",
      "Iteration 424, loss = 0.47048084\n",
      "Iteration 425, loss = 0.47070381\n",
      "Iteration 426, loss = 0.47031679\n",
      "Iteration 427, loss = 0.47024336\n",
      "Iteration 428, loss = 0.47021801\n",
      "Iteration 429, loss = 0.47044216\n",
      "Iteration 430, loss = 0.47024054\n",
      "Iteration 431, loss = 0.47009146\n",
      "Iteration 432, loss = 0.47020557\n",
      "Iteration 433, loss = 0.47027055\n",
      "Iteration 434, loss = 0.47022846\n",
      "Iteration 435, loss = 0.47037864\n",
      "Iteration 436, loss = 0.47001103\n",
      "Iteration 437, loss = 0.47033727\n",
      "Iteration 438, loss = 0.46996041\n",
      "Iteration 439, loss = 0.47011124\n",
      "Iteration 440, loss = 0.47001261\n",
      "Iteration 441, loss = 0.46986895\n",
      "Iteration 442, loss = 0.47026264\n",
      "Iteration 443, loss = 0.46981575\n",
      "Iteration 444, loss = 0.46984878\n",
      "Iteration 445, loss = 0.46990472\n",
      "Iteration 446, loss = 0.47050080\n",
      "Iteration 447, loss = 0.47002608\n",
      "Iteration 448, loss = 0.46984769\n",
      "Iteration 449, loss = 0.46986259\n",
      "Iteration 450, loss = 0.46981359\n",
      "Iteration 451, loss = 0.46978287\n",
      "Iteration 452, loss = 0.46975955\n",
      "Iteration 453, loss = 0.46993764\n",
      "Iteration 454, loss = 0.46963920\n",
      "Iteration 455, loss = 0.46961365\n",
      "Iteration 456, loss = 0.46969760\n",
      "Iteration 457, loss = 0.46968779\n",
      "Iteration 458, loss = 0.46988992\n",
      "Iteration 459, loss = 0.46965097\n",
      "Iteration 460, loss = 0.46934548\n",
      "Iteration 461, loss = 0.46962343\n",
      "Iteration 462, loss = 0.46934633\n",
      "Iteration 463, loss = 0.46957675\n",
      "Iteration 464, loss = 0.46931462\n",
      "Iteration 465, loss = 0.46967768\n",
      "Iteration 466, loss = 0.46988915\n",
      "Iteration 467, loss = 0.46935865\n",
      "Iteration 468, loss = 0.46955083\n",
      "Iteration 469, loss = 0.46946780\n",
      "Iteration 470, loss = 0.46954193\n",
      "Iteration 471, loss = 0.46967603\n",
      "Iteration 472, loss = 0.46941665\n",
      "Iteration 473, loss = 0.46938290\n",
      "Iteration 474, loss = 0.46956894\n",
      "Iteration 475, loss = 0.46918355\n",
      "Iteration 476, loss = 0.46909459\n",
      "Iteration 477, loss = 0.46918200\n",
      "Iteration 478, loss = 0.46924570\n",
      "Iteration 479, loss = 0.46892523\n",
      "Iteration 480, loss = 0.46905331\n",
      "Iteration 481, loss = 0.46898914\n",
      "Iteration 482, loss = 0.46914460\n",
      "Iteration 483, loss = 0.46902565\n",
      "Iteration 484, loss = 0.46918298\n",
      "Iteration 485, loss = 0.46893552\n",
      "Iteration 486, loss = 0.46896875\n",
      "Iteration 487, loss = 0.46921299\n",
      "Iteration 488, loss = 0.46894050\n",
      "Iteration 489, loss = 0.46882439\n",
      "Iteration 490, loss = 0.46888081\n",
      "Iteration 491, loss = 0.46902750\n",
      "Iteration 492, loss = 0.46898020\n",
      "Iteration 493, loss = 0.46919743\n",
      "Iteration 494, loss = 0.46930703\n",
      "Iteration 495, loss = 0.46913401\n",
      "Iteration 496, loss = 0.46903816\n",
      "Iteration 497, loss = 0.46891787\n",
      "Iteration 498, loss = 0.46924562\n",
      "Iteration 499, loss = 0.46896182\n",
      "Iteration 500, loss = 0.46862606\n",
      "Iteration 501, loss = 0.46879496\n",
      "Iteration 502, loss = 0.46861973\n",
      "Iteration 503, loss = 0.46858571\n",
      "Iteration 504, loss = 0.46868328\n",
      "Iteration 505, loss = 0.46845646\n",
      "Iteration 506, loss = 0.46855115\n",
      "Iteration 507, loss = 0.46877920\n",
      "Iteration 508, loss = 0.46865011\n",
      "Iteration 509, loss = 0.46854493\n",
      "Iteration 510, loss = 0.46847654\n",
      "Iteration 511, loss = 0.46824435\n",
      "Iteration 512, loss = 0.46841155\n",
      "Iteration 513, loss = 0.46839061\n",
      "Iteration 514, loss = 0.46867733\n",
      "Iteration 515, loss = 0.46817113\n",
      "Iteration 516, loss = 0.46855756\n",
      "Iteration 517, loss = 0.46843527\n",
      "Iteration 518, loss = 0.46842086\n",
      "Iteration 519, loss = 0.46843870\n",
      "Iteration 520, loss = 0.46848596\n",
      "Iteration 521, loss = 0.46837277\n",
      "Iteration 522, loss = 0.46817513\n",
      "Iteration 523, loss = 0.46824340\n",
      "Iteration 524, loss = 0.46811965\n",
      "Iteration 525, loss = 0.46832634\n",
      "Iteration 526, loss = 0.46843877\n",
      "Iteration 527, loss = 0.46849945\n",
      "Iteration 528, loss = 0.46816146\n",
      "Iteration 529, loss = 0.46837981\n",
      "Iteration 530, loss = 0.46849213\n",
      "Iteration 531, loss = 0.46793778\n",
      "Iteration 532, loss = 0.46818867\n",
      "Iteration 533, loss = 0.46790050\n",
      "Iteration 534, loss = 0.46797053\n",
      "Iteration 535, loss = 0.46817693\n",
      "Iteration 536, loss = 0.46806334\n",
      "Iteration 537, loss = 0.46858904\n",
      "Iteration 538, loss = 0.46813801\n",
      "Iteration 539, loss = 0.46781948\n",
      "Iteration 540, loss = 0.46782072\n",
      "Iteration 541, loss = 0.46783277\n",
      "Iteration 542, loss = 0.46785318\n",
      "Iteration 543, loss = 0.46828672\n",
      "Iteration 544, loss = 0.46796622\n",
      "Iteration 545, loss = 0.46789424\n",
      "Iteration 546, loss = 0.46790532\n",
      "Iteration 547, loss = 0.46809002\n",
      "Iteration 548, loss = 0.46792961\n",
      "Iteration 549, loss = 0.46851949\n",
      "Iteration 550, loss = 0.46807048\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73194198\n",
      "Iteration 2, loss = 0.70944395\n",
      "Iteration 3, loss = 0.69287469\n",
      "Iteration 4, loss = 0.67881224\n",
      "Iteration 5, loss = 0.66578443\n",
      "Iteration 6, loss = 0.65263973\n",
      "Iteration 7, loss = 0.63973517\n",
      "Iteration 8, loss = 0.62688005\n",
      "Iteration 9, loss = 0.61389968\n",
      "Iteration 10, loss = 0.60210442\n",
      "Iteration 11, loss = 0.59158472\n",
      "Iteration 12, loss = 0.58289129\n",
      "Iteration 13, loss = 0.57526814\n",
      "Iteration 14, loss = 0.56953237\n",
      "Iteration 15, loss = 0.56497730\n",
      "Iteration 16, loss = 0.56136531\n",
      "Iteration 17, loss = 0.55815795\n",
      "Iteration 18, loss = 0.55577655\n",
      "Iteration 19, loss = 0.55363042\n",
      "Iteration 20, loss = 0.55174028\n",
      "Iteration 21, loss = 0.55017443\n",
      "Iteration 22, loss = 0.54810878\n",
      "Iteration 23, loss = 0.54652835\n",
      "Iteration 24, loss = 0.54509412\n",
      "Iteration 25, loss = 0.54361368\n",
      "Iteration 26, loss = 0.54221096\n",
      "Iteration 27, loss = 0.54091491\n",
      "Iteration 28, loss = 0.53964960\n",
      "Iteration 29, loss = 0.53855489\n",
      "Iteration 30, loss = 0.53753712\n",
      "Iteration 31, loss = 0.53647854\n",
      "Iteration 32, loss = 0.53571845\n",
      "Iteration 33, loss = 0.53444527\n",
      "Iteration 34, loss = 0.53344385\n",
      "Iteration 35, loss = 0.53263277\n",
      "Iteration 36, loss = 0.53168010\n",
      "Iteration 37, loss = 0.53107965\n",
      "Iteration 38, loss = 0.53019973\n",
      "Iteration 39, loss = 0.52956984\n",
      "Iteration 40, loss = 0.52870833\n",
      "Iteration 41, loss = 0.52828149\n",
      "Iteration 42, loss = 0.52728130\n",
      "Iteration 43, loss = 0.52675037\n",
      "Iteration 44, loss = 0.52598702\n",
      "Iteration 45, loss = 0.52537631\n",
      "Iteration 46, loss = 0.52472659\n",
      "Iteration 47, loss = 0.52381397\n",
      "Iteration 48, loss = 0.52330147\n",
      "Iteration 49, loss = 0.52248933\n",
      "Iteration 50, loss = 0.52166504\n",
      "Iteration 51, loss = 0.52089824\n",
      "Iteration 52, loss = 0.52012724\n",
      "Iteration 53, loss = 0.51940526\n",
      "Iteration 54, loss = 0.51879515\n",
      "Iteration 55, loss = 0.51804876\n",
      "Iteration 56, loss = 0.51738002\n",
      "Iteration 57, loss = 0.51696205\n",
      "Iteration 58, loss = 0.51576508\n",
      "Iteration 59, loss = 0.51524758\n",
      "Iteration 60, loss = 0.51457062\n",
      "Iteration 61, loss = 0.51366232\n",
      "Iteration 62, loss = 0.51314386\n",
      "Iteration 63, loss = 0.51261466\n",
      "Iteration 64, loss = 0.51216259\n",
      "Iteration 65, loss = 0.51115279\n",
      "Iteration 66, loss = 0.51085201\n",
      "Iteration 67, loss = 0.51010021\n",
      "Iteration 68, loss = 0.50970329\n",
      "Iteration 69, loss = 0.50916266\n",
      "Iteration 70, loss = 0.50847865\n",
      "Iteration 71, loss = 0.50801306\n",
      "Iteration 72, loss = 0.50752735\n",
      "Iteration 73, loss = 0.50738602\n",
      "Iteration 74, loss = 0.50682062\n",
      "Iteration 75, loss = 0.50614085\n",
      "Iteration 76, loss = 0.50570147\n",
      "Iteration 77, loss = 0.50540465\n",
      "Iteration 78, loss = 0.50498337\n",
      "Iteration 79, loss = 0.50462400\n",
      "Iteration 80, loss = 0.50414152\n",
      "Iteration 81, loss = 0.50380407\n",
      "Iteration 82, loss = 0.50368509\n",
      "Iteration 83, loss = 0.50313982\n",
      "Iteration 84, loss = 0.50297752\n",
      "Iteration 85, loss = 0.50285986\n",
      "Iteration 86, loss = 0.50222436\n",
      "Iteration 87, loss = 0.50176942\n",
      "Iteration 88, loss = 0.50158229\n",
      "Iteration 89, loss = 0.50132019\n",
      "Iteration 90, loss = 0.50117045\n",
      "Iteration 91, loss = 0.50111439\n",
      "Iteration 92, loss = 0.50053978\n",
      "Iteration 93, loss = 0.50034136\n",
      "Iteration 94, loss = 0.50016712\n",
      "Iteration 95, loss = 0.50007006\n",
      "Iteration 96, loss = 0.49952488\n",
      "Iteration 97, loss = 0.49957557\n",
      "Iteration 98, loss = 0.49946634\n",
      "Iteration 99, loss = 0.49897210\n",
      "Iteration 100, loss = 0.49882580\n",
      "Iteration 101, loss = 0.49864028\n",
      "Iteration 102, loss = 0.49879213\n",
      "Iteration 103, loss = 0.49862624\n",
      "Iteration 104, loss = 0.49810665\n",
      "Iteration 105, loss = 0.49800939\n",
      "Iteration 106, loss = 0.49789236\n",
      "Iteration 107, loss = 0.49745609\n",
      "Iteration 108, loss = 0.49746692\n",
      "Iteration 109, loss = 0.49719963\n",
      "Iteration 110, loss = 0.49717644\n",
      "Iteration 111, loss = 0.49697484\n",
      "Iteration 112, loss = 0.49660940\n",
      "Iteration 113, loss = 0.49663826\n",
      "Iteration 114, loss = 0.49659023\n",
      "Iteration 115, loss = 0.49637523\n",
      "Iteration 116, loss = 0.49605687\n",
      "Iteration 117, loss = 0.49606686\n",
      "Iteration 118, loss = 0.49580047\n",
      "Iteration 119, loss = 0.49585195\n",
      "Iteration 120, loss = 0.49560910\n",
      "Iteration 121, loss = 0.49550166\n",
      "Iteration 122, loss = 0.49535607\n",
      "Iteration 123, loss = 0.49503227\n",
      "Iteration 124, loss = 0.49492034\n",
      "Iteration 125, loss = 0.49474604\n",
      "Iteration 126, loss = 0.49486710\n",
      "Iteration 127, loss = 0.49466275\n",
      "Iteration 128, loss = 0.49439925\n",
      "Iteration 129, loss = 0.49428561\n",
      "Iteration 130, loss = 0.49424860\n",
      "Iteration 131, loss = 0.49418066\n",
      "Iteration 132, loss = 0.49397318\n",
      "Iteration 133, loss = 0.49365485\n",
      "Iteration 134, loss = 0.49372068\n",
      "Iteration 135, loss = 0.49356390\n",
      "Iteration 136, loss = 0.49332650\n",
      "Iteration 137, loss = 0.49307655\n",
      "Iteration 138, loss = 0.49303189\n",
      "Iteration 139, loss = 0.49291708\n",
      "Iteration 140, loss = 0.49284315\n",
      "Iteration 141, loss = 0.49268449\n",
      "Iteration 142, loss = 0.49259214\n",
      "Iteration 143, loss = 0.49240855\n",
      "Iteration 144, loss = 0.49230640\n",
      "Iteration 145, loss = 0.49193779\n",
      "Iteration 146, loss = 0.49209591\n",
      "Iteration 147, loss = 0.49203407\n",
      "Iteration 148, loss = 0.49192609\n",
      "Iteration 149, loss = 0.49181225\n",
      "Iteration 150, loss = 0.49158669\n",
      "Iteration 151, loss = 0.49125112\n",
      "Iteration 152, loss = 0.49121386\n",
      "Iteration 153, loss = 0.49116879\n",
      "Iteration 154, loss = 0.49083106\n",
      "Iteration 155, loss = 0.49085851\n",
      "Iteration 156, loss = 0.49053602\n",
      "Iteration 157, loss = 0.49057525\n",
      "Iteration 158, loss = 0.49065859\n",
      "Iteration 159, loss = 0.49032608\n",
      "Iteration 160, loss = 0.49034268\n",
      "Iteration 161, loss = 0.49024607\n",
      "Iteration 162, loss = 0.49008834\n",
      "Iteration 163, loss = 0.48997549\n",
      "Iteration 164, loss = 0.48983811\n",
      "Iteration 165, loss = 0.48986154\n",
      "Iteration 166, loss = 0.48979590\n",
      "Iteration 167, loss = 0.48958176\n",
      "Iteration 168, loss = 0.48930849\n",
      "Iteration 169, loss = 0.48925475\n",
      "Iteration 170, loss = 0.48907925\n",
      "Iteration 171, loss = 0.48903064\n",
      "Iteration 172, loss = 0.48878974\n",
      "Iteration 173, loss = 0.48873380\n",
      "Iteration 174, loss = 0.48882081\n",
      "Iteration 175, loss = 0.48858334\n",
      "Iteration 176, loss = 0.48838326\n",
      "Iteration 177, loss = 0.48839558\n",
      "Iteration 178, loss = 0.48842868\n",
      "Iteration 179, loss = 0.48785751\n",
      "Iteration 180, loss = 0.48780505\n",
      "Iteration 181, loss = 0.48795213\n",
      "Iteration 182, loss = 0.48765870\n",
      "Iteration 183, loss = 0.48742635\n",
      "Iteration 184, loss = 0.48741890\n",
      "Iteration 185, loss = 0.48749509\n",
      "Iteration 186, loss = 0.48744426\n",
      "Iteration 187, loss = 0.48720697\n",
      "Iteration 188, loss = 0.48681996\n",
      "Iteration 189, loss = 0.48674105\n",
      "Iteration 190, loss = 0.48679857\n",
      "Iteration 191, loss = 0.48691606\n",
      "Iteration 192, loss = 0.48672593\n",
      "Iteration 193, loss = 0.48634487\n",
      "Iteration 194, loss = 0.48666014\n",
      "Iteration 195, loss = 0.48609054\n",
      "Iteration 196, loss = 0.48590815\n",
      "Iteration 197, loss = 0.48596575\n",
      "Iteration 198, loss = 0.48583471\n",
      "Iteration 199, loss = 0.48564396\n",
      "Iteration 200, loss = 0.48553357\n",
      "Iteration 201, loss = 0.48554338\n",
      "Iteration 202, loss = 0.48543622\n",
      "Iteration 203, loss = 0.48527143\n",
      "Iteration 204, loss = 0.48504396\n",
      "Iteration 205, loss = 0.48494768\n",
      "Iteration 206, loss = 0.48491490\n",
      "Iteration 207, loss = 0.48517257\n",
      "Iteration 208, loss = 0.48479151\n",
      "Iteration 209, loss = 0.48474840\n",
      "Iteration 210, loss = 0.48465715\n",
      "Iteration 211, loss = 0.48450934\n",
      "Iteration 212, loss = 0.48428996\n",
      "Iteration 213, loss = 0.48420614\n",
      "Iteration 214, loss = 0.48408673\n",
      "Iteration 215, loss = 0.48442116\n",
      "Iteration 216, loss = 0.48418674\n",
      "Iteration 217, loss = 0.48391911\n",
      "Iteration 218, loss = 0.48443753\n",
      "Iteration 219, loss = 0.48378486\n",
      "Iteration 220, loss = 0.48364745\n",
      "Iteration 221, loss = 0.48353621\n",
      "Iteration 222, loss = 0.48348914\n",
      "Iteration 223, loss = 0.48414516\n",
      "Iteration 224, loss = 0.48342457\n",
      "Iteration 225, loss = 0.48328788\n",
      "Iteration 226, loss = 0.48309752\n",
      "Iteration 227, loss = 0.48292849\n",
      "Iteration 228, loss = 0.48304496\n",
      "Iteration 229, loss = 0.48296183\n",
      "Iteration 230, loss = 0.48273392\n",
      "Iteration 231, loss = 0.48281564\n",
      "Iteration 232, loss = 0.48260970\n",
      "Iteration 233, loss = 0.48266210\n",
      "Iteration 234, loss = 0.48241271\n",
      "Iteration 235, loss = 0.48241416\n",
      "Iteration 236, loss = 0.48226472\n",
      "Iteration 237, loss = 0.48250070\n",
      "Iteration 238, loss = 0.48213636\n",
      "Iteration 239, loss = 0.48219262\n",
      "Iteration 240, loss = 0.48217698\n",
      "Iteration 241, loss = 0.48189686\n",
      "Iteration 242, loss = 0.48171527\n",
      "Iteration 243, loss = 0.48179720\n",
      "Iteration 244, loss = 0.48158284\n",
      "Iteration 245, loss = 0.48152116\n",
      "Iteration 246, loss = 0.48147276\n",
      "Iteration 247, loss = 0.48166934\n",
      "Iteration 248, loss = 0.48143481\n",
      "Iteration 249, loss = 0.48162836\n",
      "Iteration 250, loss = 0.48130511\n",
      "Iteration 251, loss = 0.48135300\n",
      "Iteration 252, loss = 0.48110689\n",
      "Iteration 253, loss = 0.48104642\n",
      "Iteration 254, loss = 0.48108930\n",
      "Iteration 255, loss = 0.48103678\n",
      "Iteration 256, loss = 0.48108240\n",
      "Iteration 257, loss = 0.48089256\n",
      "Iteration 258, loss = 0.48092832\n",
      "Iteration 259, loss = 0.48080252\n",
      "Iteration 260, loss = 0.48057237\n",
      "Iteration 261, loss = 0.48033222\n",
      "Iteration 262, loss = 0.48047119\n",
      "Iteration 263, loss = 0.48033011\n",
      "Iteration 264, loss = 0.48022255\n",
      "Iteration 265, loss = 0.48018553\n",
      "Iteration 266, loss = 0.48016287\n",
      "Iteration 267, loss = 0.48027932\n",
      "Iteration 268, loss = 0.48042321\n",
      "Iteration 269, loss = 0.47967090\n",
      "Iteration 270, loss = 0.47976393\n",
      "Iteration 271, loss = 0.47976271\n",
      "Iteration 272, loss = 0.47993876\n",
      "Iteration 273, loss = 0.47935131\n",
      "Iteration 274, loss = 0.47984672\n",
      "Iteration 275, loss = 0.47942986\n",
      "Iteration 276, loss = 0.47958490\n",
      "Iteration 277, loss = 0.47912499\n",
      "Iteration 278, loss = 0.47919750\n",
      "Iteration 279, loss = 0.47938444\n",
      "Iteration 280, loss = 0.47901065\n",
      "Iteration 281, loss = 0.47916777\n",
      "Iteration 282, loss = 0.47879831\n",
      "Iteration 283, loss = 0.47876128\n",
      "Iteration 284, loss = 0.47923701\n",
      "Iteration 285, loss = 0.47862989\n",
      "Iteration 286, loss = 0.47866713\n",
      "Iteration 287, loss = 0.47838522\n",
      "Iteration 288, loss = 0.47835074\n",
      "Iteration 289, loss = 0.47847964\n",
      "Iteration 290, loss = 0.47821921\n",
      "Iteration 291, loss = 0.47813474\n",
      "Iteration 292, loss = 0.47820305\n",
      "Iteration 293, loss = 0.47824413\n",
      "Iteration 294, loss = 0.47811703\n",
      "Iteration 295, loss = 0.47856168\n",
      "Iteration 296, loss = 0.47801681\n",
      "Iteration 297, loss = 0.47803436\n",
      "Iteration 298, loss = 0.47794249\n",
      "Iteration 299, loss = 0.47774370\n",
      "Iteration 300, loss = 0.47771178\n",
      "Iteration 301, loss = 0.47755625\n",
      "Iteration 302, loss = 0.47757694\n",
      "Iteration 303, loss = 0.47755411\n",
      "Iteration 304, loss = 0.47772584\n",
      "Iteration 305, loss = 0.47755778\n",
      "Iteration 306, loss = 0.47728796\n",
      "Iteration 307, loss = 0.47742783\n",
      "Iteration 308, loss = 0.47737624\n",
      "Iteration 309, loss = 0.47724186\n",
      "Iteration 310, loss = 0.47783680\n",
      "Iteration 311, loss = 0.47721557\n",
      "Iteration 312, loss = 0.47730329\n",
      "Iteration 313, loss = 0.47709935\n",
      "Iteration 314, loss = 0.47709520\n",
      "Iteration 315, loss = 0.47697928\n",
      "Iteration 316, loss = 0.47697491\n",
      "Iteration 317, loss = 0.47705096\n",
      "Iteration 318, loss = 0.47692992\n",
      "Iteration 319, loss = 0.47684119\n",
      "Iteration 320, loss = 0.47695150\n",
      "Iteration 321, loss = 0.47681633\n",
      "Iteration 322, loss = 0.47682702\n",
      "Iteration 323, loss = 0.47666878\n",
      "Iteration 324, loss = 0.47721590\n",
      "Iteration 325, loss = 0.47705424\n",
      "Iteration 326, loss = 0.47693929\n",
      "Iteration 327, loss = 0.47665524\n",
      "Iteration 328, loss = 0.47640300\n",
      "Iteration 329, loss = 0.47635103\n",
      "Iteration 330, loss = 0.47627943\n",
      "Iteration 331, loss = 0.47636212\n",
      "Iteration 332, loss = 0.47611703\n",
      "Iteration 333, loss = 0.47629546\n",
      "Iteration 334, loss = 0.47602532\n",
      "Iteration 335, loss = 0.47629389\n",
      "Iteration 336, loss = 0.47618770\n",
      "Iteration 337, loss = 0.47649765\n",
      "Iteration 338, loss = 0.47598271\n",
      "Iteration 339, loss = 0.47598130\n",
      "Iteration 340, loss = 0.47615388\n",
      "Iteration 341, loss = 0.47593576\n",
      "Iteration 342, loss = 0.47590401\n",
      "Iteration 343, loss = 0.47581298\n",
      "Iteration 344, loss = 0.47572447\n",
      "Iteration 345, loss = 0.47550934\n",
      "Iteration 346, loss = 0.47577979\n",
      "Iteration 347, loss = 0.47547642\n",
      "Iteration 348, loss = 0.47563876\n",
      "Iteration 349, loss = 0.47556840\n",
      "Iteration 350, loss = 0.47563949\n",
      "Iteration 351, loss = 0.47592648\n",
      "Iteration 352, loss = 0.47539840\n",
      "Iteration 353, loss = 0.47545500\n",
      "Iteration 354, loss = 0.47544651\n",
      "Iteration 355, loss = 0.47559482\n",
      "Iteration 356, loss = 0.47535201\n",
      "Iteration 357, loss = 0.47505136\n",
      "Iteration 358, loss = 0.47502779\n",
      "Iteration 359, loss = 0.47514270\n",
      "Iteration 360, loss = 0.47497381\n",
      "Iteration 361, loss = 0.47516334\n",
      "Iteration 362, loss = 0.47474937\n",
      "Iteration 363, loss = 0.47495834\n",
      "Iteration 364, loss = 0.47482296\n",
      "Iteration 365, loss = 0.47468408\n",
      "Iteration 366, loss = 0.47469651\n",
      "Iteration 367, loss = 0.47476544\n",
      "Iteration 368, loss = 0.47472398\n",
      "Iteration 369, loss = 0.47477875\n",
      "Iteration 370, loss = 0.47443981\n",
      "Iteration 371, loss = 0.47463768\n",
      "Iteration 372, loss = 0.47458277\n",
      "Iteration 373, loss = 0.47448593\n",
      "Iteration 374, loss = 0.47430532\n",
      "Iteration 375, loss = 0.47443999\n",
      "Iteration 376, loss = 0.47438757\n",
      "Iteration 377, loss = 0.47430663\n",
      "Iteration 378, loss = 0.47423986\n",
      "Iteration 379, loss = 0.47439950\n",
      "Iteration 380, loss = 0.47416342\n",
      "Iteration 381, loss = 0.47419485\n",
      "Iteration 382, loss = 0.47405112\n",
      "Iteration 383, loss = 0.47426626\n",
      "Iteration 384, loss = 0.47413035\n",
      "Iteration 385, loss = 0.47401410\n",
      "Iteration 386, loss = 0.47411976\n",
      "Iteration 387, loss = 0.47396218\n",
      "Iteration 388, loss = 0.47378962\n",
      "Iteration 389, loss = 0.47386914\n",
      "Iteration 390, loss = 0.47405276\n",
      "Iteration 391, loss = 0.47372254\n",
      "Iteration 392, loss = 0.47380912\n",
      "Iteration 393, loss = 0.47374913\n",
      "Iteration 394, loss = 0.47380266\n",
      "Iteration 395, loss = 0.47378637\n",
      "Iteration 396, loss = 0.47392859\n",
      "Iteration 397, loss = 0.47373108\n",
      "Iteration 398, loss = 0.47357336\n",
      "Iteration 399, loss = 0.47349294\n",
      "Iteration 400, loss = 0.47347677\n",
      "Iteration 401, loss = 0.47320266\n",
      "Iteration 402, loss = 0.47335814\n",
      "Iteration 403, loss = 0.47322310\n",
      "Iteration 404, loss = 0.47327357\n",
      "Iteration 405, loss = 0.47329649\n",
      "Iteration 406, loss = 0.47335017\n",
      "Iteration 407, loss = 0.47308722\n",
      "Iteration 408, loss = 0.47312220\n",
      "Iteration 409, loss = 0.47316058\n",
      "Iteration 410, loss = 0.47321178\n",
      "Iteration 411, loss = 0.47297091\n",
      "Iteration 412, loss = 0.47303507\n",
      "Iteration 413, loss = 0.47300178\n",
      "Iteration 414, loss = 0.47294467\n",
      "Iteration 415, loss = 0.47287662\n",
      "Iteration 416, loss = 0.47320305\n",
      "Iteration 417, loss = 0.47271852\n",
      "Iteration 418, loss = 0.47264401\n",
      "Iteration 419, loss = 0.47272631\n",
      "Iteration 420, loss = 0.47271150\n",
      "Iteration 421, loss = 0.47266175\n",
      "Iteration 422, loss = 0.47275343\n",
      "Iteration 423, loss = 0.47274206\n",
      "Iteration 424, loss = 0.47251327\n",
      "Iteration 425, loss = 0.47248535\n",
      "Iteration 426, loss = 0.47241481\n",
      "Iteration 427, loss = 0.47244419\n",
      "Iteration 428, loss = 0.47248058\n",
      "Iteration 429, loss = 0.47248590\n",
      "Iteration 430, loss = 0.47240408\n",
      "Iteration 431, loss = 0.47225159\n",
      "Iteration 432, loss = 0.47236905\n",
      "Iteration 433, loss = 0.47215083\n",
      "Iteration 434, loss = 0.47231899\n",
      "Iteration 435, loss = 0.47236113\n",
      "Iteration 436, loss = 0.47227382\n",
      "Iteration 437, loss = 0.47213821\n",
      "Iteration 438, loss = 0.47208707\n",
      "Iteration 439, loss = 0.47194471\n",
      "Iteration 440, loss = 0.47195583\n",
      "Iteration 441, loss = 0.47226194\n",
      "Iteration 442, loss = 0.47198281\n",
      "Iteration 443, loss = 0.47218778\n",
      "Iteration 444, loss = 0.47204176\n",
      "Iteration 445, loss = 0.47197278\n",
      "Iteration 446, loss = 0.47172157\n",
      "Iteration 447, loss = 0.47195935\n",
      "Iteration 448, loss = 0.47178502\n",
      "Iteration 449, loss = 0.47174723\n",
      "Iteration 450, loss = 0.47193975\n",
      "Iteration 451, loss = 0.47161498\n",
      "Iteration 452, loss = 0.47161853\n",
      "Iteration 453, loss = 0.47158701\n",
      "Iteration 454, loss = 0.47180610\n",
      "Iteration 455, loss = 0.47147592\n",
      "Iteration 456, loss = 0.47158240\n",
      "Iteration 457, loss = 0.47160534\n",
      "Iteration 458, loss = 0.47142755\n",
      "Iteration 459, loss = 0.47150356\n",
      "Iteration 460, loss = 0.47145229\n",
      "Iteration 461, loss = 0.47128947\n",
      "Iteration 462, loss = 0.47182412\n",
      "Iteration 463, loss = 0.47141062\n",
      "Iteration 464, loss = 0.47162831\n",
      "Iteration 465, loss = 0.47131996\n",
      "Iteration 466, loss = 0.47132296\n",
      "Iteration 467, loss = 0.47130461\n",
      "Iteration 468, loss = 0.47133187\n",
      "Iteration 469, loss = 0.47117207\n",
      "Iteration 470, loss = 0.47108806\n",
      "Iteration 471, loss = 0.47108870\n",
      "Iteration 472, loss = 0.47107234\n",
      "Iteration 473, loss = 0.47113014\n",
      "Iteration 474, loss = 0.47122891\n",
      "Iteration 475, loss = 0.47101968\n",
      "Iteration 476, loss = 0.47101194\n",
      "Iteration 477, loss = 0.47124953\n",
      "Iteration 478, loss = 0.47102668\n",
      "Iteration 479, loss = 0.47108019\n",
      "Iteration 480, loss = 0.47125143\n",
      "Iteration 481, loss = 0.47114507\n",
      "Iteration 482, loss = 0.47110648\n",
      "Iteration 483, loss = 0.47086074\n",
      "Iteration 484, loss = 0.47072413\n",
      "Iteration 485, loss = 0.47088416\n",
      "Iteration 486, loss = 0.47083277\n",
      "Iteration 487, loss = 0.47064540\n",
      "Iteration 488, loss = 0.47085989\n",
      "Iteration 489, loss = 0.47076359\n",
      "Iteration 490, loss = 0.47062099\n",
      "Iteration 491, loss = 0.47048205\n",
      "Iteration 492, loss = 0.47046463\n",
      "Iteration 493, loss = 0.47107655\n",
      "Iteration 494, loss = 0.47095889\n",
      "Iteration 495, loss = 0.47050178\n",
      "Iteration 496, loss = 0.47041016\n",
      "Iteration 497, loss = 0.47042911\n",
      "Iteration 498, loss = 0.47047037\n",
      "Iteration 499, loss = 0.47044509\n",
      "Iteration 500, loss = 0.47041588\n",
      "Iteration 501, loss = 0.47055484\n",
      "Iteration 502, loss = 0.47040121\n",
      "Iteration 503, loss = 0.47048438\n",
      "Iteration 504, loss = 0.47042761\n",
      "Iteration 505, loss = 0.47035356\n",
      "Iteration 506, loss = 0.47050741\n",
      "Iteration 507, loss = 0.47034346\n",
      "Iteration 508, loss = 0.47028765\n",
      "Iteration 509, loss = 0.47022640\n",
      "Iteration 510, loss = 0.47034434\n",
      "Iteration 511, loss = 0.47016131\n",
      "Iteration 512, loss = 0.47024064\n",
      "Iteration 513, loss = 0.47038320\n",
      "Iteration 514, loss = 0.47061459\n",
      "Iteration 515, loss = 0.47002419\n",
      "Iteration 516, loss = 0.47003584\n",
      "Iteration 517, loss = 0.47017930\n",
      "Iteration 518, loss = 0.47001474\n",
      "Iteration 519, loss = 0.47018305\n",
      "Iteration 520, loss = 0.47007945\n",
      "Iteration 521, loss = 0.47018935\n",
      "Iteration 522, loss = 0.47015055\n",
      "Iteration 523, loss = 0.46984552\n",
      "Iteration 524, loss = 0.47014019\n",
      "Iteration 525, loss = 0.47016202\n",
      "Iteration 526, loss = 0.46980728\n",
      "Iteration 527, loss = 0.46984825\n",
      "Iteration 528, loss = 0.46987261\n",
      "Iteration 529, loss = 0.46971457\n",
      "Iteration 530, loss = 0.46976596\n",
      "Iteration 531, loss = 0.46974654\n",
      "Iteration 532, loss = 0.46981377\n",
      "Iteration 533, loss = 0.46968825\n",
      "Iteration 534, loss = 0.46953739\n",
      "Iteration 535, loss = 0.46985920\n",
      "Iteration 536, loss = 0.46987496\n",
      "Iteration 537, loss = 0.46965545\n",
      "Iteration 538, loss = 0.46965974\n",
      "Iteration 539, loss = 0.46977351\n",
      "Iteration 540, loss = 0.46990931\n",
      "Iteration 541, loss = 0.46985832\n",
      "Iteration 542, loss = 0.46965834\n",
      "Iteration 543, loss = 0.46955382\n",
      "Iteration 544, loss = 0.46954675\n",
      "Iteration 545, loss = 0.46958655\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85011620\n",
      "Iteration 2, loss = 0.77909806\n",
      "Iteration 3, loss = 0.73164220\n",
      "Iteration 4, loss = 0.69888107\n",
      "Iteration 5, loss = 0.67601815\n",
      "Iteration 6, loss = 0.65831659\n",
      "Iteration 7, loss = 0.64334451\n",
      "Iteration 8, loss = 0.63076770\n",
      "Iteration 9, loss = 0.61981937\n",
      "Iteration 10, loss = 0.61063858\n",
      "Iteration 11, loss = 0.60238008\n",
      "Iteration 12, loss = 0.59597798\n",
      "Iteration 13, loss = 0.59031763\n",
      "Iteration 14, loss = 0.58578574\n",
      "Iteration 15, loss = 0.58185306\n",
      "Iteration 16, loss = 0.57833721\n",
      "Iteration 17, loss = 0.57520612\n",
      "Iteration 18, loss = 0.57246074\n",
      "Iteration 19, loss = 0.56996564\n",
      "Iteration 20, loss = 0.56775690\n",
      "Iteration 21, loss = 0.56599616\n",
      "Iteration 22, loss = 0.56422141\n",
      "Iteration 23, loss = 0.56273221\n",
      "Iteration 24, loss = 0.56119333\n",
      "Iteration 25, loss = 0.55988051\n",
      "Iteration 26, loss = 0.55836027\n",
      "Iteration 27, loss = 0.55711129\n",
      "Iteration 28, loss = 0.55599658\n",
      "Iteration 29, loss = 0.55486363\n",
      "Iteration 30, loss = 0.55398991\n",
      "Iteration 31, loss = 0.55315392\n",
      "Iteration 32, loss = 0.55225519\n",
      "Iteration 33, loss = 0.55079716\n",
      "Iteration 34, loss = 0.55001135\n",
      "Iteration 35, loss = 0.54917216\n",
      "Iteration 36, loss = 0.54830262\n",
      "Iteration 37, loss = 0.54736706\n",
      "Iteration 38, loss = 0.54637409\n",
      "Iteration 39, loss = 0.54569320\n",
      "Iteration 40, loss = 0.54498284\n",
      "Iteration 41, loss = 0.54418129\n",
      "Iteration 42, loss = 0.54331617\n",
      "Iteration 43, loss = 0.54267038\n",
      "Iteration 44, loss = 0.54194964\n",
      "Iteration 45, loss = 0.54132286\n",
      "Iteration 46, loss = 0.54068586\n",
      "Iteration 47, loss = 0.54002352\n",
      "Iteration 48, loss = 0.53889627\n",
      "Iteration 49, loss = 0.53820539\n",
      "Iteration 50, loss = 0.53722122\n",
      "Iteration 51, loss = 0.53626287\n",
      "Iteration 52, loss = 0.53534870\n",
      "Iteration 53, loss = 0.53435280\n",
      "Iteration 54, loss = 0.53318952\n",
      "Iteration 55, loss = 0.53224641\n",
      "Iteration 56, loss = 0.53105225\n",
      "Iteration 57, loss = 0.53008764\n",
      "Iteration 58, loss = 0.52908390\n",
      "Iteration 59, loss = 0.52802591\n",
      "Iteration 60, loss = 0.52698990\n",
      "Iteration 61, loss = 0.52564653\n",
      "Iteration 62, loss = 0.52457102\n",
      "Iteration 63, loss = 0.52351508\n",
      "Iteration 64, loss = 0.52247378\n",
      "Iteration 65, loss = 0.52130797\n",
      "Iteration 66, loss = 0.52036209\n",
      "Iteration 67, loss = 0.51936351\n",
      "Iteration 68, loss = 0.51802751\n",
      "Iteration 69, loss = 0.51691085\n",
      "Iteration 70, loss = 0.51603892\n",
      "Iteration 71, loss = 0.51508675\n",
      "Iteration 72, loss = 0.51397448\n",
      "Iteration 73, loss = 0.51276192\n",
      "Iteration 74, loss = 0.51163298\n",
      "Iteration 75, loss = 0.51089893\n",
      "Iteration 76, loss = 0.50961504\n",
      "Iteration 77, loss = 0.50874463\n",
      "Iteration 78, loss = 0.50773552\n",
      "Iteration 79, loss = 0.50708684\n",
      "Iteration 80, loss = 0.50607936\n",
      "Iteration 81, loss = 0.50519741\n",
      "Iteration 82, loss = 0.50461569\n",
      "Iteration 83, loss = 0.50403129\n",
      "Iteration 84, loss = 0.50370588\n",
      "Iteration 85, loss = 0.50300994\n",
      "Iteration 86, loss = 0.50220282\n",
      "Iteration 87, loss = 0.50207157\n",
      "Iteration 88, loss = 0.50163694\n",
      "Iteration 89, loss = 0.50073557\n",
      "Iteration 90, loss = 0.50045354\n",
      "Iteration 91, loss = 0.50020179\n",
      "Iteration 92, loss = 0.49991383\n",
      "Iteration 93, loss = 0.49929990\n",
      "Iteration 94, loss = 0.49914211\n",
      "Iteration 95, loss = 0.49896176\n",
      "Iteration 96, loss = 0.49849749\n",
      "Iteration 97, loss = 0.49827111\n",
      "Iteration 98, loss = 0.49795043\n",
      "Iteration 99, loss = 0.49775016\n",
      "Iteration 100, loss = 0.49761675\n",
      "Iteration 101, loss = 0.49722678\n",
      "Iteration 102, loss = 0.49700984\n",
      "Iteration 103, loss = 0.49666625\n",
      "Iteration 104, loss = 0.49663854\n",
      "Iteration 105, loss = 0.49630138\n",
      "Iteration 106, loss = 0.49640376\n",
      "Iteration 107, loss = 0.49599435\n",
      "Iteration 108, loss = 0.49573702\n",
      "Iteration 109, loss = 0.49573789\n",
      "Iteration 110, loss = 0.49523332\n",
      "Iteration 111, loss = 0.49512087\n",
      "Iteration 112, loss = 0.49497243\n",
      "Iteration 113, loss = 0.49469340\n",
      "Iteration 114, loss = 0.49455065\n",
      "Iteration 115, loss = 0.49439235\n",
      "Iteration 116, loss = 0.49418150\n",
      "Iteration 117, loss = 0.49411489\n",
      "Iteration 118, loss = 0.49396749\n",
      "Iteration 119, loss = 0.49358622\n",
      "Iteration 120, loss = 0.49357674\n",
      "Iteration 121, loss = 0.49341125\n",
      "Iteration 122, loss = 0.49329144\n",
      "Iteration 123, loss = 0.49307357\n",
      "Iteration 124, loss = 0.49303350\n",
      "Iteration 125, loss = 0.49286043\n",
      "Iteration 126, loss = 0.49278071\n",
      "Iteration 127, loss = 0.49265256\n",
      "Iteration 128, loss = 0.49240729\n",
      "Iteration 129, loss = 0.49220539\n",
      "Iteration 130, loss = 0.49209844\n",
      "Iteration 131, loss = 0.49202810\n",
      "Iteration 132, loss = 0.49211631\n",
      "Iteration 133, loss = 0.49164831\n",
      "Iteration 134, loss = 0.49165398\n",
      "Iteration 135, loss = 0.49171833\n",
      "Iteration 136, loss = 0.49105982\n",
      "Iteration 137, loss = 0.49114146\n",
      "Iteration 138, loss = 0.49086386\n",
      "Iteration 139, loss = 0.49070410\n",
      "Iteration 140, loss = 0.49052681\n",
      "Iteration 141, loss = 0.49064300\n",
      "Iteration 142, loss = 0.49018648\n",
      "Iteration 143, loss = 0.49026465\n",
      "Iteration 144, loss = 0.49013221\n",
      "Iteration 145, loss = 0.48980565\n",
      "Iteration 146, loss = 0.48988972\n",
      "Iteration 147, loss = 0.48965301\n",
      "Iteration 148, loss = 0.48999252\n",
      "Iteration 149, loss = 0.49004564\n",
      "Iteration 150, loss = 0.48920063\n",
      "Iteration 151, loss = 0.48928103\n",
      "Iteration 152, loss = 0.48912280\n",
      "Iteration 153, loss = 0.48906018\n",
      "Iteration 154, loss = 0.48898037\n",
      "Iteration 155, loss = 0.48913355\n",
      "Iteration 156, loss = 0.48876914\n",
      "Iteration 157, loss = 0.48878321\n",
      "Iteration 158, loss = 0.48870412\n",
      "Iteration 159, loss = 0.48858734\n",
      "Iteration 160, loss = 0.48846487\n",
      "Iteration 161, loss = 0.48830926\n",
      "Iteration 162, loss = 0.48825100\n",
      "Iteration 163, loss = 0.48828655\n",
      "Iteration 164, loss = 0.48808967\n",
      "Iteration 165, loss = 0.48857015\n",
      "Iteration 166, loss = 0.48833525\n",
      "Iteration 167, loss = 0.48801864\n",
      "Iteration 168, loss = 0.48786671\n",
      "Iteration 169, loss = 0.48771283\n",
      "Iteration 170, loss = 0.48726214\n",
      "Iteration 171, loss = 0.48762747\n",
      "Iteration 172, loss = 0.48714508\n",
      "Iteration 173, loss = 0.48751175\n",
      "Iteration 174, loss = 0.48685928\n",
      "Iteration 175, loss = 0.48681235\n",
      "Iteration 176, loss = 0.48667021\n",
      "Iteration 177, loss = 0.48665050\n",
      "Iteration 178, loss = 0.48642460\n",
      "Iteration 179, loss = 0.48617507\n",
      "Iteration 180, loss = 0.48621747\n",
      "Iteration 181, loss = 0.48607332\n",
      "Iteration 182, loss = 0.48620821\n",
      "Iteration 183, loss = 0.48580189\n",
      "Iteration 184, loss = 0.48587247\n",
      "Iteration 185, loss = 0.48551143\n",
      "Iteration 186, loss = 0.48574270\n",
      "Iteration 187, loss = 0.48531194\n",
      "Iteration 188, loss = 0.48532534\n",
      "Iteration 189, loss = 0.48518066\n",
      "Iteration 190, loss = 0.48526005\n",
      "Iteration 191, loss = 0.48508258\n",
      "Iteration 192, loss = 0.48490333\n",
      "Iteration 193, loss = 0.48486101\n",
      "Iteration 194, loss = 0.48479544\n",
      "Iteration 195, loss = 0.48464621\n",
      "Iteration 196, loss = 0.48459490\n",
      "Iteration 197, loss = 0.48504390\n",
      "Iteration 198, loss = 0.48446117\n",
      "Iteration 199, loss = 0.48435625\n",
      "Iteration 200, loss = 0.48423755\n",
      "Iteration 201, loss = 0.48410696\n",
      "Iteration 202, loss = 0.48423048\n",
      "Iteration 203, loss = 0.48399601\n",
      "Iteration 204, loss = 0.48388764\n",
      "Iteration 205, loss = 0.48368263\n",
      "Iteration 206, loss = 0.48360162\n",
      "Iteration 207, loss = 0.48359314\n",
      "Iteration 208, loss = 0.48346024\n",
      "Iteration 209, loss = 0.48346680\n",
      "Iteration 210, loss = 0.48340366\n",
      "Iteration 211, loss = 0.48412514\n",
      "Iteration 212, loss = 0.48317320\n",
      "Iteration 213, loss = 0.48336430\n",
      "Iteration 214, loss = 0.48307993\n",
      "Iteration 215, loss = 0.48322464\n",
      "Iteration 216, loss = 0.48316853\n",
      "Iteration 217, loss = 0.48316950\n",
      "Iteration 218, loss = 0.48266271\n",
      "Iteration 219, loss = 0.48267870\n",
      "Iteration 220, loss = 0.48264476\n",
      "Iteration 221, loss = 0.48267072\n",
      "Iteration 222, loss = 0.48261075\n",
      "Iteration 223, loss = 0.48236896\n",
      "Iteration 224, loss = 0.48224993\n",
      "Iteration 225, loss = 0.48249731\n",
      "Iteration 226, loss = 0.48206587\n",
      "Iteration 227, loss = 0.48200207\n",
      "Iteration 228, loss = 0.48213307\n",
      "Iteration 229, loss = 0.48181881\n",
      "Iteration 230, loss = 0.48205650\n",
      "Iteration 231, loss = 0.48170421\n",
      "Iteration 232, loss = 0.48172489\n",
      "Iteration 233, loss = 0.48159844\n",
      "Iteration 234, loss = 0.48179504\n",
      "Iteration 235, loss = 0.48157712\n",
      "Iteration 236, loss = 0.48147363\n",
      "Iteration 237, loss = 0.48146992\n",
      "Iteration 238, loss = 0.48132785\n",
      "Iteration 239, loss = 0.48129388\n",
      "Iteration 240, loss = 0.48155925\n",
      "Iteration 241, loss = 0.48167063\n",
      "Iteration 242, loss = 0.48114455\n",
      "Iteration 243, loss = 0.48125605\n",
      "Iteration 244, loss = 0.48108130\n",
      "Iteration 245, loss = 0.48112711\n",
      "Iteration 246, loss = 0.48088110\n",
      "Iteration 247, loss = 0.48098023\n",
      "Iteration 248, loss = 0.48082355\n",
      "Iteration 249, loss = 0.48071894\n",
      "Iteration 250, loss = 0.48065123\n",
      "Iteration 251, loss = 0.48049868\n",
      "Iteration 252, loss = 0.48067038\n",
      "Iteration 253, loss = 0.48039395\n",
      "Iteration 254, loss = 0.48035158\n",
      "Iteration 255, loss = 0.48051823\n",
      "Iteration 256, loss = 0.48055271\n",
      "Iteration 257, loss = 0.48007446\n",
      "Iteration 258, loss = 0.48039828\n",
      "Iteration 259, loss = 0.48003032\n",
      "Iteration 260, loss = 0.48008499\n",
      "Iteration 261, loss = 0.48015811\n",
      "Iteration 262, loss = 0.47975569\n",
      "Iteration 263, loss = 0.47979735\n",
      "Iteration 264, loss = 0.47976225\n",
      "Iteration 265, loss = 0.47966543\n",
      "Iteration 266, loss = 0.47964487\n",
      "Iteration 267, loss = 0.47976242\n",
      "Iteration 268, loss = 0.47964362\n",
      "Iteration 269, loss = 0.47997877\n",
      "Iteration 270, loss = 0.47949107\n",
      "Iteration 271, loss = 0.47952411\n",
      "Iteration 272, loss = 0.47960067\n",
      "Iteration 273, loss = 0.47925854\n",
      "Iteration 274, loss = 0.47924274\n",
      "Iteration 275, loss = 0.47916983\n",
      "Iteration 276, loss = 0.47910337\n",
      "Iteration 277, loss = 0.47913124\n",
      "Iteration 278, loss = 0.47915017\n",
      "Iteration 279, loss = 0.47905360\n",
      "Iteration 280, loss = 0.47907617\n",
      "Iteration 281, loss = 0.47925573\n",
      "Iteration 282, loss = 0.47880289\n",
      "Iteration 283, loss = 0.47868707\n",
      "Iteration 284, loss = 0.47877957\n",
      "Iteration 285, loss = 0.47865428\n",
      "Iteration 286, loss = 0.47886869\n",
      "Iteration 287, loss = 0.47832507\n",
      "Iteration 288, loss = 0.47846248\n",
      "Iteration 289, loss = 0.47821989\n",
      "Iteration 290, loss = 0.47841144\n",
      "Iteration 291, loss = 0.47824777\n",
      "Iteration 292, loss = 0.47840038\n",
      "Iteration 293, loss = 0.47832958\n",
      "Iteration 294, loss = 0.47816698\n",
      "Iteration 295, loss = 0.47802627\n",
      "Iteration 296, loss = 0.47788881\n",
      "Iteration 297, loss = 0.47793272\n",
      "Iteration 298, loss = 0.47810429\n",
      "Iteration 299, loss = 0.47785902\n",
      "Iteration 300, loss = 0.47788184\n",
      "Iteration 301, loss = 0.47782815\n",
      "Iteration 302, loss = 0.47785915\n",
      "Iteration 303, loss = 0.47772412\n",
      "Iteration 304, loss = 0.47770405\n",
      "Iteration 305, loss = 0.47764629\n",
      "Iteration 306, loss = 0.47749555\n",
      "Iteration 307, loss = 0.47757933\n",
      "Iteration 308, loss = 0.47739260\n",
      "Iteration 309, loss = 0.47745534\n",
      "Iteration 310, loss = 0.47748503\n",
      "Iteration 311, loss = 0.47744442\n",
      "Iteration 312, loss = 0.47757816\n",
      "Iteration 313, loss = 0.47726365\n",
      "Iteration 314, loss = 0.47746809\n",
      "Iteration 315, loss = 0.47750933\n",
      "Iteration 316, loss = 0.47711525\n",
      "Iteration 317, loss = 0.47758418\n",
      "Iteration 318, loss = 0.47717944\n",
      "Iteration 319, loss = 0.47727909\n",
      "Iteration 320, loss = 0.47714087\n",
      "Iteration 321, loss = 0.47697221\n",
      "Iteration 322, loss = 0.47683732\n",
      "Iteration 323, loss = 0.47700409\n",
      "Iteration 324, loss = 0.47681207\n",
      "Iteration 325, loss = 0.47685602\n",
      "Iteration 326, loss = 0.47699052\n",
      "Iteration 327, loss = 0.47701124\n",
      "Iteration 328, loss = 0.47670378\n",
      "Iteration 329, loss = 0.47734699\n",
      "Iteration 330, loss = 0.47679093\n",
      "Iteration 331, loss = 0.47649627\n",
      "Iteration 332, loss = 0.47689120\n",
      "Iteration 333, loss = 0.47655279\n",
      "Iteration 334, loss = 0.47667987\n",
      "Iteration 335, loss = 0.47667649\n",
      "Iteration 336, loss = 0.47654002\n",
      "Iteration 337, loss = 0.47626402\n",
      "Iteration 338, loss = 0.47667801\n",
      "Iteration 339, loss = 0.47655943\n",
      "Iteration 340, loss = 0.47648598\n",
      "Iteration 341, loss = 0.47627080\n",
      "Iteration 342, loss = 0.47624051\n",
      "Iteration 343, loss = 0.47622363\n",
      "Iteration 344, loss = 0.47621419\n",
      "Iteration 345, loss = 0.47662917\n",
      "Iteration 346, loss = 0.47650676\n",
      "Iteration 347, loss = 0.47661688\n",
      "Iteration 348, loss = 0.47612728\n",
      "Iteration 349, loss = 0.47604077\n",
      "Iteration 350, loss = 0.47596760\n",
      "Iteration 351, loss = 0.47609855\n",
      "Iteration 352, loss = 0.47592925\n",
      "Iteration 353, loss = 0.47595715\n",
      "Iteration 354, loss = 0.47569732\n",
      "Iteration 355, loss = 0.47587760\n",
      "Iteration 356, loss = 0.47570669\n",
      "Iteration 357, loss = 0.47558111\n",
      "Iteration 358, loss = 0.47577680\n",
      "Iteration 359, loss = 0.47594128\n",
      "Iteration 360, loss = 0.47558949\n",
      "Iteration 361, loss = 0.47581106\n",
      "Iteration 362, loss = 0.47544142\n",
      "Iteration 363, loss = 0.47556982\n",
      "Iteration 364, loss = 0.47550716\n",
      "Iteration 365, loss = 0.47551947\n",
      "Iteration 366, loss = 0.47522645\n",
      "Iteration 367, loss = 0.47527746\n",
      "Iteration 368, loss = 0.47536773\n",
      "Iteration 369, loss = 0.47559706\n",
      "Iteration 370, loss = 0.47528053\n",
      "Iteration 371, loss = 0.47516631\n",
      "Iteration 372, loss = 0.47544472\n",
      "Iteration 373, loss = 0.47534811\n",
      "Iteration 374, loss = 0.47494226\n",
      "Iteration 375, loss = 0.47515483\n",
      "Iteration 376, loss = 0.47545248\n",
      "Iteration 377, loss = 0.47536943\n",
      "Iteration 378, loss = 0.47505999\n",
      "Iteration 379, loss = 0.47500515\n",
      "Iteration 380, loss = 0.47524466\n",
      "Iteration 381, loss = 0.47512869\n",
      "Iteration 382, loss = 0.47490511\n",
      "Iteration 383, loss = 0.47487637\n",
      "Iteration 384, loss = 0.47541891\n",
      "Iteration 385, loss = 0.47533393\n",
      "Iteration 386, loss = 0.47495870\n",
      "Iteration 387, loss = 0.47486479\n",
      "Iteration 388, loss = 0.47466525\n",
      "Iteration 389, loss = 0.47490001\n",
      "Iteration 390, loss = 0.47463321\n",
      "Iteration 391, loss = 0.47470787\n",
      "Iteration 392, loss = 0.47444947\n",
      "Iteration 393, loss = 0.47437292\n",
      "Iteration 394, loss = 0.47440490\n",
      "Iteration 395, loss = 0.47431215\n",
      "Iteration 396, loss = 0.47463839\n",
      "Iteration 397, loss = 0.47445583\n",
      "Iteration 398, loss = 0.47455830\n",
      "Iteration 399, loss = 0.47443041\n",
      "Iteration 400, loss = 0.47409189\n",
      "Iteration 401, loss = 0.47437088\n",
      "Iteration 402, loss = 0.47415470\n",
      "Iteration 403, loss = 0.47418649\n",
      "Iteration 404, loss = 0.47448621\n",
      "Iteration 405, loss = 0.47407627\n",
      "Iteration 406, loss = 0.47411301\n",
      "Iteration 407, loss = 0.47463312\n",
      "Iteration 408, loss = 0.47427142\n",
      "Iteration 409, loss = 0.47380667\n",
      "Iteration 410, loss = 0.47387471\n",
      "Iteration 411, loss = 0.47400870\n",
      "Iteration 412, loss = 0.47394844\n",
      "Iteration 413, loss = 0.47396872\n",
      "Iteration 414, loss = 0.47473112\n",
      "Iteration 415, loss = 0.47415582\n",
      "Iteration 416, loss = 0.47389952\n",
      "Iteration 417, loss = 0.47378711\n",
      "Iteration 418, loss = 0.47368379\n",
      "Iteration 419, loss = 0.47368984\n",
      "Iteration 420, loss = 0.47390479\n",
      "Iteration 421, loss = 0.47364956\n",
      "Iteration 422, loss = 0.47357673\n",
      "Iteration 423, loss = 0.47372075\n",
      "Iteration 424, loss = 0.47364772\n",
      "Iteration 425, loss = 0.47377940\n",
      "Iteration 426, loss = 0.47388738\n",
      "Iteration 427, loss = 0.47376778\n",
      "Iteration 428, loss = 0.47342030\n",
      "Iteration 429, loss = 0.47338965\n",
      "Iteration 430, loss = 0.47354281\n",
      "Iteration 431, loss = 0.47354809\n",
      "Iteration 432, loss = 0.47370552\n",
      "Iteration 433, loss = 0.47348709\n",
      "Iteration 434, loss = 0.47355858\n",
      "Iteration 435, loss = 0.47327027\n",
      "Iteration 436, loss = 0.47350250\n",
      "Iteration 437, loss = 0.47362603\n",
      "Iteration 438, loss = 0.47304419\n",
      "Iteration 439, loss = 0.47329470\n",
      "Iteration 440, loss = 0.47334085\n",
      "Iteration 441, loss = 0.47353442\n",
      "Iteration 442, loss = 0.47326595\n",
      "Iteration 443, loss = 0.47328383\n",
      "Iteration 444, loss = 0.47354765\n",
      "Iteration 445, loss = 0.47311221\n",
      "Iteration 446, loss = 0.47319876\n",
      "Iteration 447, loss = 0.47302314\n",
      "Iteration 448, loss = 0.47298290\n",
      "Iteration 449, loss = 0.47277700\n",
      "Iteration 450, loss = 0.47316198\n",
      "Iteration 451, loss = 0.47284862\n",
      "Iteration 452, loss = 0.47318930\n",
      "Iteration 453, loss = 0.47302485\n",
      "Iteration 454, loss = 0.47304734\n",
      "Iteration 455, loss = 0.47288701\n",
      "Iteration 456, loss = 0.47275812\n",
      "Iteration 457, loss = 0.47275347\n",
      "Iteration 458, loss = 0.47287500\n",
      "Iteration 459, loss = 0.47270948\n",
      "Iteration 460, loss = 0.47277182\n",
      "Iteration 461, loss = 0.47252455\n",
      "Iteration 462, loss = 0.47265587\n",
      "Iteration 463, loss = 0.47260500\n",
      "Iteration 464, loss = 0.47267049\n",
      "Iteration 465, loss = 0.47243399\n",
      "Iteration 466, loss = 0.47230152\n",
      "Iteration 467, loss = 0.47233326\n",
      "Iteration 468, loss = 0.47215805\n",
      "Iteration 469, loss = 0.47225632\n",
      "Iteration 470, loss = 0.47232525\n",
      "Iteration 471, loss = 0.47229846\n",
      "Iteration 472, loss = 0.47226149\n",
      "Iteration 473, loss = 0.47225941\n",
      "Iteration 474, loss = 0.47215072\n",
      "Iteration 475, loss = 0.47195160\n",
      "Iteration 476, loss = 0.47209314\n",
      "Iteration 477, loss = 0.47209606\n",
      "Iteration 478, loss = 0.47219777\n",
      "Iteration 479, loss = 0.47205739\n",
      "Iteration 480, loss = 0.47199423\n",
      "Iteration 481, loss = 0.47210468\n",
      "Iteration 482, loss = 0.47223792\n",
      "Iteration 483, loss = 0.47199828\n",
      "Iteration 484, loss = 0.47239452\n",
      "Iteration 485, loss = 0.47220220\n",
      "Iteration 486, loss = 0.47216679\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71753820\n",
      "Iteration 2, loss = 0.69776737\n",
      "Iteration 3, loss = 0.68224566\n",
      "Iteration 4, loss = 0.66907283\n",
      "Iteration 5, loss = 0.65709151\n",
      "Iteration 6, loss = 0.64557096\n",
      "Iteration 7, loss = 0.63396872\n",
      "Iteration 8, loss = 0.62226398\n",
      "Iteration 9, loss = 0.61047790\n",
      "Iteration 10, loss = 0.59941285\n",
      "Iteration 11, loss = 0.58941319\n",
      "Iteration 12, loss = 0.58110685\n",
      "Iteration 13, loss = 0.57473830\n",
      "Iteration 14, loss = 0.56980926\n",
      "Iteration 15, loss = 0.56579380\n",
      "Iteration 16, loss = 0.56276319\n",
      "Iteration 17, loss = 0.56061830\n",
      "Iteration 18, loss = 0.55791100\n",
      "Iteration 19, loss = 0.55585461\n",
      "Iteration 20, loss = 0.55416853\n",
      "Iteration 21, loss = 0.55239138\n",
      "Iteration 22, loss = 0.55080678\n",
      "Iteration 23, loss = 0.54929311\n",
      "Iteration 24, loss = 0.54771213\n",
      "Iteration 25, loss = 0.54619592\n",
      "Iteration 26, loss = 0.54468083\n",
      "Iteration 27, loss = 0.54358452\n",
      "Iteration 28, loss = 0.54189812\n",
      "Iteration 29, loss = 0.54046921\n",
      "Iteration 30, loss = 0.53938373\n",
      "Iteration 31, loss = 0.53769439\n",
      "Iteration 32, loss = 0.53635154\n",
      "Iteration 33, loss = 0.53567287\n",
      "Iteration 34, loss = 0.53342609\n",
      "Iteration 35, loss = 0.53232128\n",
      "Iteration 36, loss = 0.53056493\n",
      "Iteration 37, loss = 0.52950446\n",
      "Iteration 38, loss = 0.52801887\n",
      "Iteration 39, loss = 0.52657185\n",
      "Iteration 40, loss = 0.52569053\n",
      "Iteration 41, loss = 0.52474491\n",
      "Iteration 42, loss = 0.52325250\n",
      "Iteration 43, loss = 0.52225154\n",
      "Iteration 44, loss = 0.52110264\n",
      "Iteration 45, loss = 0.51991175\n",
      "Iteration 46, loss = 0.51917677\n",
      "Iteration 47, loss = 0.51810229\n",
      "Iteration 48, loss = 0.51688827\n",
      "Iteration 49, loss = 0.51580042\n",
      "Iteration 50, loss = 0.51468497\n",
      "Iteration 51, loss = 0.51380671\n",
      "Iteration 52, loss = 0.51296821\n",
      "Iteration 53, loss = 0.51224078\n",
      "Iteration 54, loss = 0.51137646\n",
      "Iteration 55, loss = 0.51070288\n",
      "Iteration 56, loss = 0.50938409\n",
      "Iteration 57, loss = 0.50867752\n",
      "Iteration 58, loss = 0.50825389\n",
      "Iteration 59, loss = 0.50741924\n",
      "Iteration 60, loss = 0.50672381\n",
      "Iteration 61, loss = 0.50641098\n",
      "Iteration 62, loss = 0.50591874\n",
      "Iteration 63, loss = 0.50518439\n",
      "Iteration 64, loss = 0.50457526\n",
      "Iteration 65, loss = 0.50425995\n",
      "Iteration 66, loss = 0.50365141\n",
      "Iteration 67, loss = 0.50338576\n",
      "Iteration 68, loss = 0.50298080\n",
      "Iteration 69, loss = 0.50258903\n",
      "Iteration 70, loss = 0.50223119\n",
      "Iteration 71, loss = 0.50189948\n",
      "Iteration 72, loss = 0.50160578\n",
      "Iteration 73, loss = 0.50129293\n",
      "Iteration 74, loss = 0.50090532\n",
      "Iteration 75, loss = 0.50070718\n",
      "Iteration 76, loss = 0.50082831\n",
      "Iteration 77, loss = 0.50002453\n",
      "Iteration 78, loss = 0.49977587\n",
      "Iteration 79, loss = 0.49963274\n",
      "Iteration 80, loss = 0.49966841\n",
      "Iteration 81, loss = 0.49926717\n",
      "Iteration 82, loss = 0.49866657\n",
      "Iteration 83, loss = 0.49849786\n",
      "Iteration 84, loss = 0.49859200\n",
      "Iteration 85, loss = 0.49866131\n",
      "Iteration 86, loss = 0.49810937\n",
      "Iteration 87, loss = 0.49775929\n",
      "Iteration 88, loss = 0.49751633\n",
      "Iteration 89, loss = 0.49737157\n",
      "Iteration 90, loss = 0.49733389\n",
      "Iteration 91, loss = 0.49697917\n",
      "Iteration 92, loss = 0.49682937\n",
      "Iteration 93, loss = 0.49654922\n",
      "Iteration 94, loss = 0.49665355\n",
      "Iteration 95, loss = 0.49616138\n",
      "Iteration 96, loss = 0.49614445\n",
      "Iteration 97, loss = 0.49611407\n",
      "Iteration 98, loss = 0.49575376\n",
      "Iteration 99, loss = 0.49593388\n",
      "Iteration 100, loss = 0.49514583\n",
      "Iteration 101, loss = 0.49531142\n",
      "Iteration 102, loss = 0.49487650\n",
      "Iteration 103, loss = 0.49459691\n",
      "Iteration 104, loss = 0.49456138\n",
      "Iteration 105, loss = 0.49459727\n",
      "Iteration 106, loss = 0.49439506\n",
      "Iteration 107, loss = 0.49392326\n",
      "Iteration 108, loss = 0.49397734\n",
      "Iteration 109, loss = 0.49384384\n",
      "Iteration 110, loss = 0.49369690\n",
      "Iteration 111, loss = 0.49332751\n",
      "Iteration 112, loss = 0.49341006\n",
      "Iteration 113, loss = 0.49306315\n",
      "Iteration 114, loss = 0.49289093\n",
      "Iteration 115, loss = 0.49273361\n",
      "Iteration 116, loss = 0.49273523\n",
      "Iteration 117, loss = 0.49287571\n",
      "Iteration 118, loss = 0.49259151\n",
      "Iteration 119, loss = 0.49209849\n",
      "Iteration 120, loss = 0.49215662\n",
      "Iteration 121, loss = 0.49212822\n",
      "Iteration 122, loss = 0.49192809\n",
      "Iteration 123, loss = 0.49208171\n",
      "Iteration 124, loss = 0.49169976\n",
      "Iteration 125, loss = 0.49169381\n",
      "Iteration 126, loss = 0.49189280\n",
      "Iteration 127, loss = 0.49154274\n",
      "Iteration 128, loss = 0.49125455\n",
      "Iteration 129, loss = 0.49121112\n",
      "Iteration 130, loss = 0.49113570\n",
      "Iteration 131, loss = 0.49074677\n",
      "Iteration 132, loss = 0.49095574\n",
      "Iteration 133, loss = 0.49071259\n",
      "Iteration 134, loss = 0.49106339\n",
      "Iteration 135, loss = 0.49064452\n",
      "Iteration 136, loss = 0.49062640\n",
      "Iteration 137, loss = 0.49035024\n",
      "Iteration 138, loss = 0.49039641\n",
      "Iteration 139, loss = 0.49016236\n",
      "Iteration 140, loss = 0.49007974\n",
      "Iteration 141, loss = 0.49004229\n",
      "Iteration 142, loss = 0.48962446\n",
      "Iteration 143, loss = 0.48949217\n",
      "Iteration 144, loss = 0.48958261\n",
      "Iteration 145, loss = 0.48950036\n",
      "Iteration 146, loss = 0.48937168\n",
      "Iteration 147, loss = 0.48928562\n",
      "Iteration 148, loss = 0.48904146\n",
      "Iteration 149, loss = 0.48911634\n",
      "Iteration 150, loss = 0.48958981\n",
      "Iteration 151, loss = 0.48901912\n",
      "Iteration 152, loss = 0.48899007\n",
      "Iteration 153, loss = 0.48872018\n",
      "Iteration 154, loss = 0.48854012\n",
      "Iteration 155, loss = 0.48824574\n",
      "Iteration 156, loss = 0.48842691\n",
      "Iteration 157, loss = 0.48816431\n",
      "Iteration 158, loss = 0.48801802\n",
      "Iteration 159, loss = 0.48822468\n",
      "Iteration 160, loss = 0.48848895\n",
      "Iteration 161, loss = 0.48799160\n",
      "Iteration 162, loss = 0.48781128\n",
      "Iteration 163, loss = 0.48780364\n",
      "Iteration 164, loss = 0.48779618\n",
      "Iteration 165, loss = 0.48742436\n",
      "Iteration 166, loss = 0.48761678\n",
      "Iteration 167, loss = 0.48712418\n",
      "Iteration 168, loss = 0.48716334\n",
      "Iteration 169, loss = 0.48732514\n",
      "Iteration 170, loss = 0.48704222\n",
      "Iteration 171, loss = 0.48679781\n",
      "Iteration 172, loss = 0.48670757\n",
      "Iteration 173, loss = 0.48668349\n",
      "Iteration 174, loss = 0.48648483\n",
      "Iteration 175, loss = 0.48639975\n",
      "Iteration 176, loss = 0.48642714\n",
      "Iteration 177, loss = 0.48649673\n",
      "Iteration 178, loss = 0.48628370\n",
      "Iteration 179, loss = 0.48636173\n",
      "Iteration 180, loss = 0.48606408\n",
      "Iteration 181, loss = 0.48586298\n",
      "Iteration 182, loss = 0.48611046\n",
      "Iteration 183, loss = 0.48555194\n",
      "Iteration 184, loss = 0.48545975\n",
      "Iteration 185, loss = 0.48533561\n",
      "Iteration 186, loss = 0.48535834\n",
      "Iteration 187, loss = 0.48521766\n",
      "Iteration 188, loss = 0.48506872\n",
      "Iteration 189, loss = 0.48487707\n",
      "Iteration 190, loss = 0.48485926\n",
      "Iteration 191, loss = 0.48464485\n",
      "Iteration 192, loss = 0.48461039\n",
      "Iteration 193, loss = 0.48496442\n",
      "Iteration 194, loss = 0.48447936\n",
      "Iteration 195, loss = 0.48430197\n",
      "Iteration 196, loss = 0.48450432\n",
      "Iteration 197, loss = 0.48413472\n",
      "Iteration 198, loss = 0.48516626\n",
      "Iteration 199, loss = 0.48444930\n",
      "Iteration 200, loss = 0.48408364\n",
      "Iteration 201, loss = 0.48368448\n",
      "Iteration 202, loss = 0.48362827\n",
      "Iteration 203, loss = 0.48362925\n",
      "Iteration 204, loss = 0.48331135\n",
      "Iteration 205, loss = 0.48352669\n",
      "Iteration 206, loss = 0.48337057\n",
      "Iteration 207, loss = 0.48335883\n",
      "Iteration 208, loss = 0.48332824\n",
      "Iteration 209, loss = 0.48293052\n",
      "Iteration 210, loss = 0.48298860\n",
      "Iteration 211, loss = 0.48282978\n",
      "Iteration 212, loss = 0.48280345\n",
      "Iteration 213, loss = 0.48295266\n",
      "Iteration 214, loss = 0.48267206\n",
      "Iteration 215, loss = 0.48250767\n",
      "Iteration 216, loss = 0.48283076\n",
      "Iteration 217, loss = 0.48241015\n",
      "Iteration 218, loss = 0.48223880\n",
      "Iteration 219, loss = 0.48227053\n",
      "Iteration 220, loss = 0.48241436\n",
      "Iteration 221, loss = 0.48186784\n",
      "Iteration 222, loss = 0.48196339\n",
      "Iteration 223, loss = 0.48227382\n",
      "Iteration 224, loss = 0.48186912\n",
      "Iteration 225, loss = 0.48206016\n",
      "Iteration 226, loss = 0.48196460\n",
      "Iteration 227, loss = 0.48169003\n",
      "Iteration 228, loss = 0.48149883\n",
      "Iteration 229, loss = 0.48168392\n",
      "Iteration 230, loss = 0.48184045\n",
      "Iteration 231, loss = 0.48166927\n",
      "Iteration 232, loss = 0.48131767\n",
      "Iteration 233, loss = 0.48107346\n",
      "Iteration 234, loss = 0.48116870\n",
      "Iteration 235, loss = 0.48132443\n",
      "Iteration 236, loss = 0.48169479\n",
      "Iteration 237, loss = 0.48101360\n",
      "Iteration 238, loss = 0.48107880\n",
      "Iteration 239, loss = 0.48125707\n",
      "Iteration 240, loss = 0.48098010\n",
      "Iteration 241, loss = 0.48083646\n",
      "Iteration 242, loss = 0.48100000\n",
      "Iteration 243, loss = 0.48094452\n",
      "Iteration 244, loss = 0.48063827\n",
      "Iteration 245, loss = 0.48081557\n",
      "Iteration 246, loss = 0.48063240\n",
      "Iteration 247, loss = 0.48098575\n",
      "Iteration 248, loss = 0.48053771\n",
      "Iteration 249, loss = 0.48067882\n",
      "Iteration 250, loss = 0.48040500\n",
      "Iteration 251, loss = 0.48065799\n",
      "Iteration 252, loss = 0.48064490\n",
      "Iteration 253, loss = 0.48061479\n",
      "Iteration 254, loss = 0.48029178\n",
      "Iteration 255, loss = 0.48022283\n",
      "Iteration 256, loss = 0.48056650\n",
      "Iteration 257, loss = 0.48027775\n",
      "Iteration 258, loss = 0.48026389\n",
      "Iteration 259, loss = 0.48065881\n",
      "Iteration 260, loss = 0.48017492\n",
      "Iteration 261, loss = 0.48028997\n",
      "Iteration 262, loss = 0.48021049\n",
      "Iteration 263, loss = 0.47993662\n",
      "Iteration 264, loss = 0.48047375\n",
      "Iteration 265, loss = 0.47994118\n",
      "Iteration 266, loss = 0.48015738\n",
      "Iteration 267, loss = 0.47982771\n",
      "Iteration 268, loss = 0.47975047\n",
      "Iteration 269, loss = 0.47992001\n",
      "Iteration 270, loss = 0.48002038\n",
      "Iteration 271, loss = 0.47958992\n",
      "Iteration 272, loss = 0.47990830\n",
      "Iteration 273, loss = 0.47959614\n",
      "Iteration 274, loss = 0.47919370\n",
      "Iteration 275, loss = 0.47927892\n",
      "Iteration 276, loss = 0.47950061\n",
      "Iteration 277, loss = 0.47972746\n",
      "Iteration 278, loss = 0.47916793\n",
      "Iteration 279, loss = 0.47947460\n",
      "Iteration 280, loss = 0.47907782\n",
      "Iteration 281, loss = 0.47917906\n",
      "Iteration 282, loss = 0.47902313\n",
      "Iteration 283, loss = 0.47940394\n",
      "Iteration 284, loss = 0.47928691\n",
      "Iteration 285, loss = 0.47931461\n",
      "Iteration 286, loss = 0.47903396\n",
      "Iteration 287, loss = 0.47877368\n",
      "Iteration 288, loss = 0.47895032\n",
      "Iteration 289, loss = 0.47868332\n",
      "Iteration 290, loss = 0.47864262\n",
      "Iteration 291, loss = 0.47895930\n",
      "Iteration 292, loss = 0.47866762\n",
      "Iteration 293, loss = 0.47853408\n",
      "Iteration 294, loss = 0.47893088\n",
      "Iteration 295, loss = 0.47868429\n",
      "Iteration 296, loss = 0.47859634\n",
      "Iteration 297, loss = 0.47845990\n",
      "Iteration 298, loss = 0.47850961\n",
      "Iteration 299, loss = 0.47828171\n",
      "Iteration 300, loss = 0.47818210\n",
      "Iteration 301, loss = 0.47816196\n",
      "Iteration 302, loss = 0.47834448\n",
      "Iteration 303, loss = 0.47850056\n",
      "Iteration 304, loss = 0.47811489\n",
      "Iteration 305, loss = 0.47821461\n",
      "Iteration 306, loss = 0.47893198\n",
      "Iteration 307, loss = 0.47798255\n",
      "Iteration 308, loss = 0.47787893\n",
      "Iteration 309, loss = 0.47792749\n",
      "Iteration 310, loss = 0.47794327\n",
      "Iteration 311, loss = 0.47814885\n",
      "Iteration 312, loss = 0.47773375\n",
      "Iteration 313, loss = 0.47779988\n",
      "Iteration 314, loss = 0.47762818\n",
      "Iteration 315, loss = 0.47755239\n",
      "Iteration 316, loss = 0.47771235\n",
      "Iteration 317, loss = 0.47795524\n",
      "Iteration 318, loss = 0.47781164\n",
      "Iteration 319, loss = 0.47752680\n",
      "Iteration 320, loss = 0.47792612\n",
      "Iteration 321, loss = 0.47761331\n",
      "Iteration 322, loss = 0.47736715\n",
      "Iteration 323, loss = 0.47735282\n",
      "Iteration 324, loss = 0.47740627\n",
      "Iteration 325, loss = 0.47738007\n",
      "Iteration 326, loss = 0.47713970\n",
      "Iteration 327, loss = 0.47750877\n",
      "Iteration 328, loss = 0.47711564\n",
      "Iteration 329, loss = 0.47718457\n",
      "Iteration 330, loss = 0.47717397\n",
      "Iteration 331, loss = 0.47745981\n",
      "Iteration 332, loss = 0.47774875\n",
      "Iteration 333, loss = 0.47762644\n",
      "Iteration 334, loss = 0.47704053\n",
      "Iteration 335, loss = 0.47682531\n",
      "Iteration 336, loss = 0.47728749\n",
      "Iteration 337, loss = 0.47703714\n",
      "Iteration 338, loss = 0.47684430\n",
      "Iteration 339, loss = 0.47693300\n",
      "Iteration 340, loss = 0.47686164\n",
      "Iteration 341, loss = 0.47694020\n",
      "Iteration 342, loss = 0.47669669\n",
      "Iteration 343, loss = 0.47683801\n",
      "Iteration 344, loss = 0.47698197\n",
      "Iteration 345, loss = 0.47685348\n",
      "Iteration 346, loss = 0.47704499\n",
      "Iteration 347, loss = 0.47674435\n",
      "Iteration 348, loss = 0.47673227\n",
      "Iteration 349, loss = 0.47665574\n",
      "Iteration 350, loss = 0.47667910\n",
      "Iteration 351, loss = 0.47674398\n",
      "Iteration 352, loss = 0.47665728\n",
      "Iteration 353, loss = 0.47638498\n",
      "Iteration 354, loss = 0.47657718\n",
      "Iteration 355, loss = 0.47650020\n",
      "Iteration 356, loss = 0.47608718\n",
      "Iteration 357, loss = 0.47632449\n",
      "Iteration 358, loss = 0.47647284\n",
      "Iteration 359, loss = 0.47614625\n",
      "Iteration 360, loss = 0.47640137\n",
      "Iteration 361, loss = 0.47625386\n",
      "Iteration 362, loss = 0.47623658\n",
      "Iteration 363, loss = 0.47608938\n",
      "Iteration 364, loss = 0.47616549\n",
      "Iteration 365, loss = 0.47617284\n",
      "Iteration 366, loss = 0.47618229\n",
      "Iteration 367, loss = 0.47621119\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81098132\n",
      "Iteration 2, loss = 0.76918410\n",
      "Iteration 3, loss = 0.74068941\n",
      "Iteration 4, loss = 0.72160787\n",
      "Iteration 5, loss = 0.70843803\n",
      "Iteration 6, loss = 0.69753335\n",
      "Iteration 7, loss = 0.68807440\n",
      "Iteration 8, loss = 0.67972063\n",
      "Iteration 9, loss = 0.67173437\n",
      "Iteration 10, loss = 0.66375701\n",
      "Iteration 11, loss = 0.65540229\n",
      "Iteration 12, loss = 0.64711819\n",
      "Iteration 13, loss = 0.63881139\n",
      "Iteration 14, loss = 0.63057664\n",
      "Iteration 15, loss = 0.62254020\n",
      "Iteration 16, loss = 0.61467429\n",
      "Iteration 17, loss = 0.60765796\n",
      "Iteration 18, loss = 0.60061524\n",
      "Iteration 19, loss = 0.59453376\n",
      "Iteration 20, loss = 0.58795167\n",
      "Iteration 21, loss = 0.58223896\n",
      "Iteration 22, loss = 0.57677620\n",
      "Iteration 23, loss = 0.57202650\n",
      "Iteration 24, loss = 0.56784700\n",
      "Iteration 25, loss = 0.56415486\n",
      "Iteration 26, loss = 0.56081898\n",
      "Iteration 27, loss = 0.55767432\n",
      "Iteration 28, loss = 0.55479771\n",
      "Iteration 29, loss = 0.55236379\n",
      "Iteration 30, loss = 0.55039441\n",
      "Iteration 31, loss = 0.54852124\n",
      "Iteration 32, loss = 0.54705123\n",
      "Iteration 33, loss = 0.54570118\n",
      "Iteration 34, loss = 0.54429199\n",
      "Iteration 35, loss = 0.54303913\n",
      "Iteration 36, loss = 0.54207399\n",
      "Iteration 37, loss = 0.54091824\n",
      "Iteration 38, loss = 0.54004327\n",
      "Iteration 39, loss = 0.53894794\n",
      "Iteration 40, loss = 0.53800760\n",
      "Iteration 41, loss = 0.53668998\n",
      "Iteration 42, loss = 0.53573256\n",
      "Iteration 43, loss = 0.53453944\n",
      "Iteration 44, loss = 0.53355553\n",
      "Iteration 45, loss = 0.53256339\n",
      "Iteration 46, loss = 0.53137638\n",
      "Iteration 47, loss = 0.53027795\n",
      "Iteration 48, loss = 0.52943528\n",
      "Iteration 49, loss = 0.52832013\n",
      "Iteration 50, loss = 0.52728425\n",
      "Iteration 51, loss = 0.52663179\n",
      "Iteration 52, loss = 0.52514745\n",
      "Iteration 53, loss = 0.52440988\n",
      "Iteration 54, loss = 0.52380233\n",
      "Iteration 55, loss = 0.52240884\n",
      "Iteration 56, loss = 0.52177457\n",
      "Iteration 57, loss = 0.52085019\n",
      "Iteration 58, loss = 0.52000984\n",
      "Iteration 59, loss = 0.51924083\n",
      "Iteration 60, loss = 0.51824743\n",
      "Iteration 61, loss = 0.51769008\n",
      "Iteration 62, loss = 0.51680730\n",
      "Iteration 63, loss = 0.51594704\n",
      "Iteration 64, loss = 0.51547145\n",
      "Iteration 65, loss = 0.51455979\n",
      "Iteration 66, loss = 0.51408744\n",
      "Iteration 67, loss = 0.51304622\n",
      "Iteration 68, loss = 0.51265425\n",
      "Iteration 69, loss = 0.51175930\n",
      "Iteration 70, loss = 0.51123941\n",
      "Iteration 71, loss = 0.51062462\n",
      "Iteration 72, loss = 0.51007780\n",
      "Iteration 73, loss = 0.50925307\n",
      "Iteration 74, loss = 0.50873697\n",
      "Iteration 75, loss = 0.50839784\n",
      "Iteration 76, loss = 0.50769784\n",
      "Iteration 77, loss = 0.50715435\n",
      "Iteration 78, loss = 0.50675240\n",
      "Iteration 79, loss = 0.50622144\n",
      "Iteration 80, loss = 0.50577400\n",
      "Iteration 81, loss = 0.50529953\n",
      "Iteration 82, loss = 0.50489544\n",
      "Iteration 83, loss = 0.50417830\n",
      "Iteration 84, loss = 0.50373486\n",
      "Iteration 85, loss = 0.50330726\n",
      "Iteration 86, loss = 0.50287745\n",
      "Iteration 87, loss = 0.50263017\n",
      "Iteration 88, loss = 0.50272794\n",
      "Iteration 89, loss = 0.50182423\n",
      "Iteration 90, loss = 0.50177684\n",
      "Iteration 91, loss = 0.50073846\n",
      "Iteration 92, loss = 0.50034068\n",
      "Iteration 93, loss = 0.49978339\n",
      "Iteration 94, loss = 0.49939175\n",
      "Iteration 95, loss = 0.49902699\n",
      "Iteration 96, loss = 0.49878071\n",
      "Iteration 97, loss = 0.49817845\n",
      "Iteration 98, loss = 0.49782142\n",
      "Iteration 99, loss = 0.49745351\n",
      "Iteration 100, loss = 0.49721227\n",
      "Iteration 101, loss = 0.49695938\n",
      "Iteration 102, loss = 0.49626335\n",
      "Iteration 103, loss = 0.49613371\n",
      "Iteration 104, loss = 0.49583547\n",
      "Iteration 105, loss = 0.49526268\n",
      "Iteration 106, loss = 0.49526649\n",
      "Iteration 107, loss = 0.49496416\n",
      "Iteration 108, loss = 0.49443636\n",
      "Iteration 109, loss = 0.49422848\n",
      "Iteration 110, loss = 0.49376732\n",
      "Iteration 111, loss = 0.49351998\n",
      "Iteration 112, loss = 0.49337488\n",
      "Iteration 113, loss = 0.49284114\n",
      "Iteration 114, loss = 0.49285666\n",
      "Iteration 115, loss = 0.49231622\n",
      "Iteration 116, loss = 0.49205485\n",
      "Iteration 117, loss = 0.49198132\n",
      "Iteration 118, loss = 0.49139163\n",
      "Iteration 119, loss = 0.49106521\n",
      "Iteration 120, loss = 0.49120721\n",
      "Iteration 121, loss = 0.49068499\n",
      "Iteration 122, loss = 0.49035120\n",
      "Iteration 123, loss = 0.49024908\n",
      "Iteration 124, loss = 0.49008309\n",
      "Iteration 125, loss = 0.48989857\n",
      "Iteration 126, loss = 0.48970961\n",
      "Iteration 127, loss = 0.48921395\n",
      "Iteration 128, loss = 0.48942486\n",
      "Iteration 129, loss = 0.48931204\n",
      "Iteration 130, loss = 0.48863596\n",
      "Iteration 131, loss = 0.48855961\n",
      "Iteration 132, loss = 0.48829031\n",
      "Iteration 133, loss = 0.48803154\n",
      "Iteration 134, loss = 0.48804426\n",
      "Iteration 135, loss = 0.48785591\n",
      "Iteration 136, loss = 0.48748688\n",
      "Iteration 137, loss = 0.48729877\n",
      "Iteration 138, loss = 0.48744141\n",
      "Iteration 139, loss = 0.48707241\n",
      "Iteration 140, loss = 0.48707430\n",
      "Iteration 141, loss = 0.48678682\n",
      "Iteration 142, loss = 0.48668067\n",
      "Iteration 143, loss = 0.48629464\n",
      "Iteration 144, loss = 0.48643953\n",
      "Iteration 145, loss = 0.48608626\n",
      "Iteration 146, loss = 0.48601081\n",
      "Iteration 147, loss = 0.48591907\n",
      "Iteration 148, loss = 0.48581957\n",
      "Iteration 149, loss = 0.48577845\n",
      "Iteration 150, loss = 0.48586825\n",
      "Iteration 151, loss = 0.48527749\n",
      "Iteration 152, loss = 0.48535387\n",
      "Iteration 153, loss = 0.48517842\n",
      "Iteration 154, loss = 0.48502851\n",
      "Iteration 155, loss = 0.48490743\n",
      "Iteration 156, loss = 0.48497375\n",
      "Iteration 157, loss = 0.48491321\n",
      "Iteration 158, loss = 0.48459718\n",
      "Iteration 159, loss = 0.48455290\n",
      "Iteration 160, loss = 0.48423354\n",
      "Iteration 161, loss = 0.48435834\n",
      "Iteration 162, loss = 0.48404795\n",
      "Iteration 163, loss = 0.48403959\n",
      "Iteration 164, loss = 0.48387824\n",
      "Iteration 165, loss = 0.48362060\n",
      "Iteration 166, loss = 0.48364001\n",
      "Iteration 167, loss = 0.48363578\n",
      "Iteration 168, loss = 0.48336063\n",
      "Iteration 169, loss = 0.48346636\n",
      "Iteration 170, loss = 0.48306935\n",
      "Iteration 171, loss = 0.48321221\n",
      "Iteration 172, loss = 0.48302027\n",
      "Iteration 173, loss = 0.48357028\n",
      "Iteration 174, loss = 0.48279882\n",
      "Iteration 175, loss = 0.48304432\n",
      "Iteration 176, loss = 0.48254954\n",
      "Iteration 177, loss = 0.48277589\n",
      "Iteration 178, loss = 0.48256584\n",
      "Iteration 179, loss = 0.48277901\n",
      "Iteration 180, loss = 0.48318636\n",
      "Iteration 181, loss = 0.48245986\n",
      "Iteration 182, loss = 0.48253963\n",
      "Iteration 183, loss = 0.48225277\n",
      "Iteration 184, loss = 0.48250200\n",
      "Iteration 185, loss = 0.48182192\n",
      "Iteration 186, loss = 0.48207611\n",
      "Iteration 187, loss = 0.48176791\n",
      "Iteration 188, loss = 0.48186975\n",
      "Iteration 189, loss = 0.48195991\n",
      "Iteration 190, loss = 0.48164561\n",
      "Iteration 191, loss = 0.48171399\n",
      "Iteration 192, loss = 0.48184936\n",
      "Iteration 193, loss = 0.48183230\n",
      "Iteration 194, loss = 0.48161611\n",
      "Iteration 195, loss = 0.48162851\n",
      "Iteration 196, loss = 0.48126815\n",
      "Iteration 197, loss = 0.48142893\n",
      "Iteration 198, loss = 0.48131405\n",
      "Iteration 199, loss = 0.48117979\n",
      "Iteration 200, loss = 0.48109999\n",
      "Iteration 201, loss = 0.48090944\n",
      "Iteration 202, loss = 0.48079923\n",
      "Iteration 203, loss = 0.48080765\n",
      "Iteration 204, loss = 0.48068403\n",
      "Iteration 205, loss = 0.48070593\n",
      "Iteration 206, loss = 0.48050565\n",
      "Iteration 207, loss = 0.48075775\n",
      "Iteration 208, loss = 0.48044250\n",
      "Iteration 209, loss = 0.48062731\n",
      "Iteration 210, loss = 0.48036900\n",
      "Iteration 211, loss = 0.48020778\n",
      "Iteration 212, loss = 0.48018518\n",
      "Iteration 213, loss = 0.47997982\n",
      "Iteration 214, loss = 0.48063794\n",
      "Iteration 215, loss = 0.47982843\n",
      "Iteration 216, loss = 0.47993358\n",
      "Iteration 217, loss = 0.47987156\n",
      "Iteration 218, loss = 0.47974916\n",
      "Iteration 219, loss = 0.48004402\n",
      "Iteration 220, loss = 0.47966970\n",
      "Iteration 221, loss = 0.47952126\n",
      "Iteration 222, loss = 0.47967075\n",
      "Iteration 223, loss = 0.47952916\n",
      "Iteration 224, loss = 0.47973779\n",
      "Iteration 225, loss = 0.47982912\n",
      "Iteration 226, loss = 0.47944392\n",
      "Iteration 227, loss = 0.47909899\n",
      "Iteration 228, loss = 0.47903517\n",
      "Iteration 229, loss = 0.47919003\n",
      "Iteration 230, loss = 0.47960438\n",
      "Iteration 231, loss = 0.47899785\n",
      "Iteration 232, loss = 0.47889033\n",
      "Iteration 233, loss = 0.47887638\n",
      "Iteration 234, loss = 0.47880105\n",
      "Iteration 235, loss = 0.47909895\n",
      "Iteration 236, loss = 0.47882371\n",
      "Iteration 237, loss = 0.47899210\n",
      "Iteration 238, loss = 0.47863579\n",
      "Iteration 239, loss = 0.47865737\n",
      "Iteration 240, loss = 0.47862898\n",
      "Iteration 241, loss = 0.47846375\n",
      "Iteration 242, loss = 0.47855668\n",
      "Iteration 243, loss = 0.47856292\n",
      "Iteration 244, loss = 0.47842167\n",
      "Iteration 245, loss = 0.47844669\n",
      "Iteration 246, loss = 0.47834078\n",
      "Iteration 247, loss = 0.47809082\n",
      "Iteration 248, loss = 0.47808766\n",
      "Iteration 249, loss = 0.47796910\n",
      "Iteration 250, loss = 0.47831659\n",
      "Iteration 251, loss = 0.47857107\n",
      "Iteration 252, loss = 0.47779126\n",
      "Iteration 253, loss = 0.47805456\n",
      "Iteration 254, loss = 0.47802755\n",
      "Iteration 255, loss = 0.47786707\n",
      "Iteration 256, loss = 0.47762216\n",
      "Iteration 257, loss = 0.47754148\n",
      "Iteration 258, loss = 0.47749860\n",
      "Iteration 259, loss = 0.47754340\n",
      "Iteration 260, loss = 0.47739056\n",
      "Iteration 261, loss = 0.47756063\n",
      "Iteration 262, loss = 0.47727196\n",
      "Iteration 263, loss = 0.47768687\n",
      "Iteration 264, loss = 0.47725093\n",
      "Iteration 265, loss = 0.47731899\n",
      "Iteration 266, loss = 0.47719494\n",
      "Iteration 267, loss = 0.47708027\n",
      "Iteration 268, loss = 0.47698818\n",
      "Iteration 269, loss = 0.47713250\n",
      "Iteration 270, loss = 0.47771750\n",
      "Iteration 271, loss = 0.47676912\n",
      "Iteration 272, loss = 0.47721715\n",
      "Iteration 273, loss = 0.47706418\n",
      "Iteration 274, loss = 0.47723221\n",
      "Iteration 275, loss = 0.47688825\n",
      "Iteration 276, loss = 0.47673511\n",
      "Iteration 277, loss = 0.47684893\n",
      "Iteration 278, loss = 0.47666838\n",
      "Iteration 279, loss = 0.47662742\n",
      "Iteration 280, loss = 0.47665979\n",
      "Iteration 281, loss = 0.47654036\n",
      "Iteration 282, loss = 0.47682823\n",
      "Iteration 283, loss = 0.47623839\n",
      "Iteration 284, loss = 0.47700107\n",
      "Iteration 285, loss = 0.47637881\n",
      "Iteration 286, loss = 0.47609294\n",
      "Iteration 287, loss = 0.47625289\n",
      "Iteration 288, loss = 0.47596862\n",
      "Iteration 289, loss = 0.47586603\n",
      "Iteration 290, loss = 0.47595614\n",
      "Iteration 291, loss = 0.47591158\n",
      "Iteration 292, loss = 0.47579637\n",
      "Iteration 293, loss = 0.47620713\n",
      "Iteration 294, loss = 0.47620603\n",
      "Iteration 295, loss = 0.47572446\n",
      "Iteration 296, loss = 0.47565566\n",
      "Iteration 297, loss = 0.47554737\n",
      "Iteration 298, loss = 0.47550472\n",
      "Iteration 299, loss = 0.47549869\n",
      "Iteration 300, loss = 0.47551504\n",
      "Iteration 301, loss = 0.47546840\n",
      "Iteration 302, loss = 0.47513914\n",
      "Iteration 303, loss = 0.47498639\n",
      "Iteration 304, loss = 0.47522595\n",
      "Iteration 305, loss = 0.47500345\n",
      "Iteration 306, loss = 0.47488850\n",
      "Iteration 307, loss = 0.47484557\n",
      "Iteration 308, loss = 0.47477784\n",
      "Iteration 309, loss = 0.47457101\n",
      "Iteration 310, loss = 0.47485987\n",
      "Iteration 311, loss = 0.47474167\n",
      "Iteration 312, loss = 0.47437716\n",
      "Iteration 313, loss = 0.47445837\n",
      "Iteration 314, loss = 0.47429937\n",
      "Iteration 315, loss = 0.47484477\n",
      "Iteration 316, loss = 0.47391786\n",
      "Iteration 317, loss = 0.47410935\n",
      "Iteration 318, loss = 0.47414033\n",
      "Iteration 319, loss = 0.47400028\n",
      "Iteration 320, loss = 0.47431421\n",
      "Iteration 321, loss = 0.47379579\n",
      "Iteration 322, loss = 0.47369110\n",
      "Iteration 323, loss = 0.47360476\n",
      "Iteration 324, loss = 0.47351653\n",
      "Iteration 325, loss = 0.47353928\n",
      "Iteration 326, loss = 0.47340307\n",
      "Iteration 327, loss = 0.47356321\n",
      "Iteration 328, loss = 0.47362249\n",
      "Iteration 329, loss = 0.47324982\n",
      "Iteration 330, loss = 0.47327158\n",
      "Iteration 331, loss = 0.47352197\n",
      "Iteration 332, loss = 0.47337537\n",
      "Iteration 333, loss = 0.47308202\n",
      "Iteration 334, loss = 0.47314007\n",
      "Iteration 335, loss = 0.47280699\n",
      "Iteration 336, loss = 0.47291042\n",
      "Iteration 337, loss = 0.47273938\n",
      "Iteration 338, loss = 0.47268416\n",
      "Iteration 339, loss = 0.47250399\n",
      "Iteration 340, loss = 0.47288316\n",
      "Iteration 341, loss = 0.47233078\n",
      "Iteration 342, loss = 0.47231231\n",
      "Iteration 343, loss = 0.47222430\n",
      "Iteration 344, loss = 0.47213867\n",
      "Iteration 345, loss = 0.47218614\n",
      "Iteration 346, loss = 0.47204173\n",
      "Iteration 347, loss = 0.47197470\n",
      "Iteration 348, loss = 0.47185545\n",
      "Iteration 349, loss = 0.47229294\n",
      "Iteration 350, loss = 0.47171879\n",
      "Iteration 351, loss = 0.47162204\n",
      "Iteration 352, loss = 0.47160556\n",
      "Iteration 353, loss = 0.47161329\n",
      "Iteration 354, loss = 0.47160510\n",
      "Iteration 355, loss = 0.47136533\n",
      "Iteration 356, loss = 0.47162534\n",
      "Iteration 357, loss = 0.47133136\n",
      "Iteration 358, loss = 0.47119988\n",
      "Iteration 359, loss = 0.47126530\n",
      "Iteration 360, loss = 0.47105243\n",
      "Iteration 361, loss = 0.47115579\n",
      "Iteration 362, loss = 0.47104612\n",
      "Iteration 363, loss = 0.47099152\n",
      "Iteration 364, loss = 0.47068701\n",
      "Iteration 365, loss = 0.47068291\n",
      "Iteration 366, loss = 0.47084134\n",
      "Iteration 367, loss = 0.47063786\n",
      "Iteration 368, loss = 0.47060222\n",
      "Iteration 369, loss = 0.47064534\n",
      "Iteration 370, loss = 0.47071030\n",
      "Iteration 371, loss = 0.47060813\n",
      "Iteration 372, loss = 0.47041729\n",
      "Iteration 373, loss = 0.47026812\n",
      "Iteration 374, loss = 0.47072735\n",
      "Iteration 375, loss = 0.47007580\n",
      "Iteration 376, loss = 0.47024531\n",
      "Iteration 377, loss = 0.46998921\n",
      "Iteration 378, loss = 0.46999623\n",
      "Iteration 379, loss = 0.47000007\n",
      "Iteration 380, loss = 0.46977709\n",
      "Iteration 381, loss = 0.47015511\n",
      "Iteration 382, loss = 0.46967727\n",
      "Iteration 383, loss = 0.46983587\n",
      "Iteration 384, loss = 0.46964258\n",
      "Iteration 385, loss = 0.46960252\n",
      "Iteration 386, loss = 0.46932759\n",
      "Iteration 387, loss = 0.46950124\n",
      "Iteration 388, loss = 0.46958760\n",
      "Iteration 389, loss = 0.46918295\n",
      "Iteration 390, loss = 0.46920730\n",
      "Iteration 391, loss = 0.46918980\n",
      "Iteration 392, loss = 0.46917881\n",
      "Iteration 393, loss = 0.46905080\n",
      "Iteration 394, loss = 0.46906866\n",
      "Iteration 395, loss = 0.46899247\n",
      "Iteration 396, loss = 0.46920623\n",
      "Iteration 397, loss = 0.46891631\n",
      "Iteration 398, loss = 0.46888638\n",
      "Iteration 399, loss = 0.46889496\n",
      "Iteration 400, loss = 0.46901210\n",
      "Iteration 401, loss = 0.46892299\n",
      "Iteration 402, loss = 0.46885182\n",
      "Iteration 403, loss = 0.46869641\n",
      "Iteration 404, loss = 0.46873984\n",
      "Iteration 405, loss = 0.46848833\n",
      "Iteration 406, loss = 0.46861612\n",
      "Iteration 407, loss = 0.46835040\n",
      "Iteration 408, loss = 0.46819507\n",
      "Iteration 409, loss = 0.46849652\n",
      "Iteration 410, loss = 0.46830565\n",
      "Iteration 411, loss = 0.46818299\n",
      "Iteration 412, loss = 0.46804697\n",
      "Iteration 413, loss = 0.46794439\n",
      "Iteration 414, loss = 0.46803062\n",
      "Iteration 415, loss = 0.46785451\n",
      "Iteration 416, loss = 0.46794208\n",
      "Iteration 417, loss = 0.46804826\n",
      "Iteration 418, loss = 0.46786067\n",
      "Iteration 419, loss = 0.46773856\n",
      "Iteration 420, loss = 0.46758262\n",
      "Iteration 421, loss = 0.46771358\n",
      "Iteration 422, loss = 0.46751042\n",
      "Iteration 423, loss = 0.46744877\n",
      "Iteration 424, loss = 0.46738767\n",
      "Iteration 425, loss = 0.46724059\n",
      "Iteration 426, loss = 0.46721808\n",
      "Iteration 427, loss = 0.46732041\n",
      "Iteration 428, loss = 0.46721912\n",
      "Iteration 429, loss = 0.46717710\n",
      "Iteration 430, loss = 0.46700517\n",
      "Iteration 431, loss = 0.46702615\n",
      "Iteration 432, loss = 0.46714121\n",
      "Iteration 433, loss = 0.46703244\n",
      "Iteration 434, loss = 0.46671610\n",
      "Iteration 435, loss = 0.46670996\n",
      "Iteration 436, loss = 0.46672219\n",
      "Iteration 437, loss = 0.46692602\n",
      "Iteration 438, loss = 0.46659877\n",
      "Iteration 439, loss = 0.46675147\n",
      "Iteration 440, loss = 0.46685563\n",
      "Iteration 441, loss = 0.46643015\n",
      "Iteration 442, loss = 0.46639177\n",
      "Iteration 443, loss = 0.46705204\n",
      "Iteration 444, loss = 0.46676710\n",
      "Iteration 445, loss = 0.46660389\n",
      "Iteration 446, loss = 0.46622593\n",
      "Iteration 447, loss = 0.46622596\n",
      "Iteration 448, loss = 0.46659852\n",
      "Iteration 449, loss = 0.46616681\n",
      "Iteration 450, loss = 0.46599357\n",
      "Iteration 451, loss = 0.46608788\n",
      "Iteration 452, loss = 0.46611721\n",
      "Iteration 453, loss = 0.46596357\n",
      "Iteration 454, loss = 0.46587137\n",
      "Iteration 455, loss = 0.46598264\n",
      "Iteration 456, loss = 0.46580424\n",
      "Iteration 457, loss = 0.46582729\n",
      "Iteration 458, loss = 0.46581678\n",
      "Iteration 459, loss = 0.46582512\n",
      "Iteration 460, loss = 0.46578930\n",
      "Iteration 461, loss = 0.46576253\n",
      "Iteration 462, loss = 0.46566624\n",
      "Iteration 463, loss = 0.46550620\n",
      "Iteration 464, loss = 0.46558634\n",
      "Iteration 465, loss = 0.46541364\n",
      "Iteration 466, loss = 0.46578275\n",
      "Iteration 467, loss = 0.46526953\n",
      "Iteration 468, loss = 0.46535199\n",
      "Iteration 469, loss = 0.46508111\n",
      "Iteration 470, loss = 0.46509291\n",
      "Iteration 471, loss = 0.46509216\n",
      "Iteration 472, loss = 0.46500046\n",
      "Iteration 473, loss = 0.46494753\n",
      "Iteration 474, loss = 0.46500852\n",
      "Iteration 475, loss = 0.46507249\n",
      "Iteration 476, loss = 0.46502419\n",
      "Iteration 477, loss = 0.46504284\n",
      "Iteration 478, loss = 0.46475185\n",
      "Iteration 479, loss = 0.46516243\n",
      "Iteration 480, loss = 0.46469529\n",
      "Iteration 481, loss = 0.46456056\n",
      "Iteration 482, loss = 0.46469020\n",
      "Iteration 483, loss = 0.46456241\n",
      "Iteration 484, loss = 0.46460984\n",
      "Iteration 485, loss = 0.46451128\n",
      "Iteration 486, loss = 0.46461542\n",
      "Iteration 487, loss = 0.46470382\n",
      "Iteration 488, loss = 0.46434810\n",
      "Iteration 489, loss = 0.46453280\n",
      "Iteration 490, loss = 0.46411967\n",
      "Iteration 491, loss = 0.46434619\n",
      "Iteration 492, loss = 0.46419523\n",
      "Iteration 493, loss = 0.46423019\n",
      "Iteration 494, loss = 0.46405678\n",
      "Iteration 495, loss = 0.46443620\n",
      "Iteration 496, loss = 0.46452996\n",
      "Iteration 497, loss = 0.46435273\n",
      "Iteration 498, loss = 0.46413180\n",
      "Iteration 499, loss = 0.46404167\n",
      "Iteration 500, loss = 0.46370588\n",
      "Iteration 501, loss = 0.46398195\n",
      "Iteration 502, loss = 0.46377503\n",
      "Iteration 503, loss = 0.46381152\n",
      "Iteration 504, loss = 0.46391498\n",
      "Iteration 505, loss = 0.46384951\n",
      "Iteration 506, loss = 0.46409410\n",
      "Iteration 507, loss = 0.46375126\n",
      "Iteration 508, loss = 0.46381479\n",
      "Iteration 509, loss = 0.46387067\n",
      "Iteration 510, loss = 0.46344310\n",
      "Iteration 511, loss = 0.46352834\n",
      "Iteration 512, loss = 0.46338989\n",
      "Iteration 513, loss = 0.46332622\n",
      "Iteration 514, loss = 0.46416003\n",
      "Iteration 515, loss = 0.46383064\n",
      "Iteration 516, loss = 0.46343845\n",
      "Iteration 517, loss = 0.46324825\n",
      "Iteration 518, loss = 0.46330390\n",
      "Iteration 519, loss = 0.46323092\n",
      "Iteration 520, loss = 0.46324685\n",
      "Iteration 521, loss = 0.46310762\n",
      "Iteration 522, loss = 0.46341900\n",
      "Iteration 523, loss = 0.46308269\n",
      "Iteration 524, loss = 0.46318626\n",
      "Iteration 525, loss = 0.46321830\n",
      "Iteration 526, loss = 0.46308269\n",
      "Iteration 527, loss = 0.46319718\n",
      "Iteration 528, loss = 0.46316718\n",
      "Iteration 529, loss = 0.46279931\n",
      "Iteration 530, loss = 0.46302175\n",
      "Iteration 531, loss = 0.46325079\n",
      "Iteration 532, loss = 0.46287433\n",
      "Iteration 533, loss = 0.46270670\n",
      "Iteration 534, loss = 0.46315089\n",
      "Iteration 535, loss = 0.46296548\n",
      "Iteration 536, loss = 0.46287388\n",
      "Iteration 537, loss = 0.46298967\n",
      "Iteration 538, loss = 0.46264082\n",
      "Iteration 539, loss = 0.46285330\n",
      "Iteration 540, loss = 0.46281311\n",
      "Iteration 541, loss = 0.46259564\n",
      "Iteration 542, loss = 0.46254358\n",
      "Iteration 543, loss = 0.46269957\n",
      "Iteration 544, loss = 0.46262428\n",
      "Iteration 545, loss = 0.46270394\n",
      "Iteration 546, loss = 0.46288008\n",
      "Iteration 547, loss = 0.46277518\n",
      "Iteration 548, loss = 0.46312355\n",
      "Iteration 549, loss = 0.46277828\n",
      "Iteration 550, loss = 0.46235910\n",
      "Iteration 551, loss = 0.46256813\n",
      "Iteration 552, loss = 0.46234782\n",
      "Iteration 553, loss = 0.46256660\n",
      "Iteration 554, loss = 0.46220706\n",
      "Iteration 555, loss = 0.46232222\n",
      "Iteration 556, loss = 0.46200877\n",
      "Iteration 557, loss = 0.46205410\n",
      "Iteration 558, loss = 0.46232421\n",
      "Iteration 559, loss = 0.46270949\n",
      "Iteration 560, loss = 0.46209407\n",
      "Iteration 561, loss = 0.46199716\n",
      "Iteration 562, loss = 0.46246448\n",
      "Iteration 563, loss = 0.46221373\n",
      "Iteration 564, loss = 0.46213113\n",
      "Iteration 565, loss = 0.46189759\n",
      "Iteration 566, loss = 0.46167046\n",
      "Iteration 567, loss = 0.46222249\n",
      "Iteration 568, loss = 0.46204314\n",
      "Iteration 569, loss = 0.46158362\n",
      "Iteration 570, loss = 0.46178226\n",
      "Iteration 571, loss = 0.46154915\n",
      "Iteration 572, loss = 0.46177920\n",
      "Iteration 573, loss = 0.46145669\n",
      "Iteration 574, loss = 0.46182922\n",
      "Iteration 575, loss = 0.46144221\n",
      "Iteration 576, loss = 0.46154782\n",
      "Iteration 577, loss = 0.46146343\n",
      "Iteration 578, loss = 0.46153334\n",
      "Iteration 579, loss = 0.46147746\n",
      "Iteration 580, loss = 0.46153580\n",
      "Iteration 581, loss = 0.46109936\n",
      "Iteration 582, loss = 0.46149868\n",
      "Iteration 583, loss = 0.46125421\n",
      "Iteration 584, loss = 0.46180072\n",
      "Iteration 585, loss = 0.46148217\n",
      "Iteration 586, loss = 0.46114538\n",
      "Iteration 587, loss = 0.46104252\n",
      "Iteration 588, loss = 0.46120237\n",
      "Iteration 589, loss = 0.46111163\n",
      "Iteration 590, loss = 0.46104307\n",
      "Iteration 591, loss = 0.46089817\n",
      "Iteration 592, loss = 0.46083345\n",
      "Iteration 593, loss = 0.46111264\n",
      "Iteration 594, loss = 0.46089062\n",
      "Iteration 595, loss = 0.46078782\n",
      "Iteration 596, loss = 0.46102310\n",
      "Iteration 597, loss = 0.46069285\n",
      "Iteration 598, loss = 0.46075062\n",
      "Iteration 599, loss = 0.46080425\n",
      "Iteration 600, loss = 0.46080451\n",
      "Iteration 601, loss = 0.46107011\n",
      "Iteration 602, loss = 0.46078577\n",
      "Iteration 603, loss = 0.46054562\n",
      "Iteration 604, loss = 0.46096192\n",
      "Iteration 605, loss = 0.46065977\n",
      "Iteration 606, loss = 0.46080443\n",
      "Iteration 607, loss = 0.46056690\n",
      "Iteration 608, loss = 0.46042392\n",
      "Iteration 609, loss = 0.46066647\n",
      "Iteration 610, loss = 0.46055326\n",
      "Iteration 611, loss = 0.46100822\n",
      "Iteration 612, loss = 0.46040956\n",
      "Iteration 613, loss = 0.46047233\n",
      "Iteration 614, loss = 0.46062488\n",
      "Iteration 615, loss = 0.46029217\n",
      "Iteration 616, loss = 0.46024404\n",
      "Iteration 617, loss = 0.46043680\n",
      "Iteration 618, loss = 0.46020310\n",
      "Iteration 619, loss = 0.46031878\n",
      "Iteration 620, loss = 0.46022668\n",
      "Iteration 621, loss = 0.46032661\n",
      "Iteration 622, loss = 0.46010176\n",
      "Iteration 623, loss = 0.46024503\n",
      "Iteration 624, loss = 0.46021075\n",
      "Iteration 625, loss = 0.46020310\n",
      "Iteration 626, loss = 0.46024700\n",
      "Iteration 627, loss = 0.46009437\n",
      "Iteration 628, loss = 0.46001513\n",
      "Iteration 629, loss = 0.46012262\n",
      "Iteration 630, loss = 0.46038545\n",
      "Iteration 631, loss = 0.46033907\n",
      "Iteration 632, loss = 0.46008918\n",
      "Iteration 633, loss = 0.46016881\n",
      "Iteration 634, loss = 0.45992197\n",
      "Iteration 635, loss = 0.45985106\n",
      "Iteration 636, loss = 0.46010231\n",
      "Iteration 637, loss = 0.46026238\n",
      "Iteration 638, loss = 0.45986686\n",
      "Iteration 639, loss = 0.45999394\n",
      "Iteration 640, loss = 0.45978913\n",
      "Iteration 641, loss = 0.45973512\n",
      "Iteration 642, loss = 0.45988854\n",
      "Iteration 643, loss = 0.45963240\n",
      "Iteration 644, loss = 0.45971849\n",
      "Iteration 645, loss = 0.45978453\n",
      "Iteration 646, loss = 0.46013434\n",
      "Iteration 647, loss = 0.46000399\n",
      "Iteration 648, loss = 0.45978032\n",
      "Iteration 649, loss = 0.45949557\n",
      "Iteration 650, loss = 0.45945022\n",
      "Iteration 651, loss = 0.45976319\n",
      "Iteration 652, loss = 0.45989417\n",
      "Iteration 653, loss = 0.45948677\n",
      "Iteration 654, loss = 0.45977742\n",
      "Iteration 655, loss = 0.45955532\n",
      "Iteration 656, loss = 0.45951690\n",
      "Iteration 657, loss = 0.45957494\n",
      "Iteration 658, loss = 0.45966122\n",
      "Iteration 659, loss = 0.45954838\n",
      "Iteration 660, loss = 0.45938809\n",
      "Iteration 661, loss = 0.45939407\n",
      "Iteration 662, loss = 0.45938105\n",
      "Iteration 663, loss = 0.45929036\n",
      "Iteration 664, loss = 0.45976043\n",
      "Iteration 665, loss = 0.45927420\n",
      "Iteration 666, loss = 0.45930496\n",
      "Iteration 667, loss = 0.45944216\n",
      "Iteration 668, loss = 0.45926240\n",
      "Iteration 669, loss = 0.45946353\n",
      "Iteration 670, loss = 0.45958638\n",
      "Iteration 671, loss = 0.45930825\n",
      "Iteration 672, loss = 0.45934987\n",
      "Iteration 673, loss = 0.45941482\n",
      "Iteration 674, loss = 0.45904284\n",
      "Iteration 675, loss = 0.45907631\n",
      "Iteration 676, loss = 0.45899389\n",
      "Iteration 677, loss = 0.45950111\n",
      "Iteration 678, loss = 0.45980770\n",
      "Iteration 679, loss = 0.45903925\n",
      "Iteration 680, loss = 0.45905723\n",
      "Iteration 681, loss = 0.45956105\n",
      "Iteration 682, loss = 0.45900948\n",
      "Iteration 683, loss = 0.45907254\n",
      "Iteration 684, loss = 0.45893672\n",
      "Iteration 685, loss = 0.45902282\n",
      "Iteration 686, loss = 0.45884191\n",
      "Iteration 687, loss = 0.45887864\n",
      "Iteration 688, loss = 0.45913096\n",
      "Iteration 689, loss = 0.45892494\n",
      "Iteration 690, loss = 0.45884210\n",
      "Iteration 691, loss = 0.45897935\n",
      "Iteration 692, loss = 0.45869013\n",
      "Iteration 693, loss = 0.45904364\n",
      "Iteration 694, loss = 0.45906981\n",
      "Iteration 695, loss = 0.45863225\n",
      "Iteration 696, loss = 0.45871576\n",
      "Iteration 697, loss = 0.45870907\n",
      "Iteration 698, loss = 0.45878909\n",
      "Iteration 699, loss = 0.45868622\n",
      "Iteration 700, loss = 0.45891042\n",
      "Iteration 701, loss = 0.45876137\n",
      "Iteration 702, loss = 0.45873485\n",
      "Iteration 703, loss = 0.45856587\n",
      "Iteration 704, loss = 0.45868104\n",
      "Iteration 705, loss = 0.45886032\n",
      "Iteration 706, loss = 0.45855227\n",
      "Iteration 707, loss = 0.45864568\n",
      "Iteration 708, loss = 0.45848647\n",
      "Iteration 709, loss = 0.45879598\n",
      "Iteration 710, loss = 0.45837141\n",
      "Iteration 711, loss = 0.45864061\n",
      "Iteration 712, loss = 0.45879238\n",
      "Iteration 713, loss = 0.45861303\n",
      "Iteration 714, loss = 0.45838423\n",
      "Iteration 715, loss = 0.45849799\n",
      "Iteration 716, loss = 0.45850405\n",
      "Iteration 717, loss = 0.45848527\n",
      "Iteration 718, loss = 0.45831779\n",
      "Iteration 719, loss = 0.45845856\n",
      "Iteration 720, loss = 0.45823243\n",
      "Iteration 721, loss = 0.45856326\n",
      "Iteration 722, loss = 0.45829138\n",
      "Iteration 723, loss = 0.45839137\n",
      "Iteration 724, loss = 0.45822604\n",
      "Iteration 725, loss = 0.45842084\n",
      "Iteration 726, loss = 0.45837148\n",
      "Iteration 727, loss = 0.45812359\n",
      "Iteration 728, loss = 0.45823963\n",
      "Iteration 729, loss = 0.45816922\n",
      "Iteration 730, loss = 0.45834145\n",
      "Iteration 731, loss = 0.45814687\n",
      "Iteration 732, loss = 0.45852585\n",
      "Iteration 733, loss = 0.45806022\n",
      "Iteration 734, loss = 0.45806144\n",
      "Iteration 735, loss = 0.45820876\n",
      "Iteration 736, loss = 0.45792455\n",
      "Iteration 737, loss = 0.45801127\n",
      "Iteration 738, loss = 0.45788575\n",
      "Iteration 739, loss = 0.45823484\n",
      "Iteration 740, loss = 0.45768611\n",
      "Iteration 741, loss = 0.45827853\n",
      "Iteration 742, loss = 0.45814466\n",
      "Iteration 743, loss = 0.45821119\n",
      "Iteration 744, loss = 0.45805143\n",
      "Iteration 745, loss = 0.45798336\n",
      "Iteration 746, loss = 0.45792377\n",
      "Iteration 747, loss = 0.45811597\n",
      "Iteration 748, loss = 0.45785938\n",
      "Iteration 749, loss = 0.45786727\n",
      "Iteration 750, loss = 0.45769518\n",
      "Iteration 751, loss = 0.45781230\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84100402\n",
      "Iteration 2, loss = 0.77956210\n",
      "Iteration 3, loss = 0.73964474\n",
      "Iteration 4, loss = 0.71318432\n",
      "Iteration 5, loss = 0.69310176\n",
      "Iteration 6, loss = 0.67620641\n",
      "Iteration 7, loss = 0.65985884\n",
      "Iteration 8, loss = 0.64360981\n",
      "Iteration 9, loss = 0.62735867\n",
      "Iteration 10, loss = 0.61225212\n",
      "Iteration 11, loss = 0.59824802\n",
      "Iteration 12, loss = 0.58605938\n",
      "Iteration 13, loss = 0.57562584\n",
      "Iteration 14, loss = 0.56756002\n",
      "Iteration 15, loss = 0.56188757\n",
      "Iteration 16, loss = 0.55691653\n",
      "Iteration 17, loss = 0.55356544\n",
      "Iteration 18, loss = 0.55087259\n",
      "Iteration 19, loss = 0.54911251\n",
      "Iteration 20, loss = 0.54713823\n",
      "Iteration 21, loss = 0.54581200\n",
      "Iteration 22, loss = 0.54465225\n",
      "Iteration 23, loss = 0.54354498\n",
      "Iteration 24, loss = 0.54271111\n",
      "Iteration 25, loss = 0.54184078\n",
      "Iteration 26, loss = 0.54092213\n",
      "Iteration 27, loss = 0.53991964\n",
      "Iteration 28, loss = 0.53917385\n",
      "Iteration 29, loss = 0.53822887\n",
      "Iteration 30, loss = 0.53750492\n",
      "Iteration 31, loss = 0.53668550\n",
      "Iteration 32, loss = 0.53597293\n",
      "Iteration 33, loss = 0.53518251\n",
      "Iteration 34, loss = 0.53441823\n",
      "Iteration 35, loss = 0.53362674\n",
      "Iteration 36, loss = 0.53282823\n",
      "Iteration 37, loss = 0.53205788\n",
      "Iteration 38, loss = 0.53133479\n",
      "Iteration 39, loss = 0.53044208\n",
      "Iteration 40, loss = 0.52956708\n",
      "Iteration 41, loss = 0.52878031\n",
      "Iteration 42, loss = 0.52813970\n",
      "Iteration 43, loss = 0.52707758\n",
      "Iteration 44, loss = 0.52622278\n",
      "Iteration 45, loss = 0.52533291\n",
      "Iteration 46, loss = 0.52437779\n",
      "Iteration 47, loss = 0.52364195\n",
      "Iteration 48, loss = 0.52273740\n",
      "Iteration 49, loss = 0.52178833\n",
      "Iteration 50, loss = 0.52126144\n",
      "Iteration 51, loss = 0.52019841\n",
      "Iteration 52, loss = 0.51948102\n",
      "Iteration 53, loss = 0.51877938\n",
      "Iteration 54, loss = 0.51806502\n",
      "Iteration 55, loss = 0.51707797\n",
      "Iteration 56, loss = 0.51652162\n",
      "Iteration 57, loss = 0.51556357\n",
      "Iteration 58, loss = 0.51480333\n",
      "Iteration 59, loss = 0.51419380\n",
      "Iteration 60, loss = 0.51311196\n",
      "Iteration 61, loss = 0.51259226\n",
      "Iteration 62, loss = 0.51163338\n",
      "Iteration 63, loss = 0.51082286\n",
      "Iteration 64, loss = 0.51002903\n",
      "Iteration 65, loss = 0.50973127\n",
      "Iteration 66, loss = 0.50891162\n",
      "Iteration 67, loss = 0.50814430\n",
      "Iteration 68, loss = 0.50714765\n",
      "Iteration 69, loss = 0.50655647\n",
      "Iteration 70, loss = 0.50580187\n",
      "Iteration 71, loss = 0.50525386\n",
      "Iteration 72, loss = 0.50463044\n",
      "Iteration 73, loss = 0.50397486\n",
      "Iteration 74, loss = 0.50349883\n",
      "Iteration 75, loss = 0.50274944\n",
      "Iteration 76, loss = 0.50246082\n",
      "Iteration 77, loss = 0.50165968\n",
      "Iteration 78, loss = 0.50116400\n",
      "Iteration 79, loss = 0.50086084\n",
      "Iteration 80, loss = 0.50028751\n",
      "Iteration 81, loss = 0.50014799\n",
      "Iteration 82, loss = 0.49942959\n",
      "Iteration 83, loss = 0.49886841\n",
      "Iteration 84, loss = 0.49836915\n",
      "Iteration 85, loss = 0.49807656\n",
      "Iteration 86, loss = 0.49750099\n",
      "Iteration 87, loss = 0.49720669\n",
      "Iteration 88, loss = 0.49662311\n",
      "Iteration 89, loss = 0.49630121\n",
      "Iteration 90, loss = 0.49634370\n",
      "Iteration 91, loss = 0.49532513\n",
      "Iteration 92, loss = 0.49502217\n",
      "Iteration 93, loss = 0.49459352\n",
      "Iteration 94, loss = 0.49436755\n",
      "Iteration 95, loss = 0.49391709\n",
      "Iteration 96, loss = 0.49353952\n",
      "Iteration 97, loss = 0.49312896\n",
      "Iteration 98, loss = 0.49302510\n",
      "Iteration 99, loss = 0.49237912\n",
      "Iteration 100, loss = 0.49208705\n",
      "Iteration 101, loss = 0.49177332\n",
      "Iteration 102, loss = 0.49191833\n",
      "Iteration 103, loss = 0.49130337\n",
      "Iteration 104, loss = 0.49094443\n",
      "Iteration 105, loss = 0.49053594\n",
      "Iteration 106, loss = 0.49024764\n",
      "Iteration 107, loss = 0.49009921\n",
      "Iteration 108, loss = 0.48989577\n",
      "Iteration 109, loss = 0.48960486\n",
      "Iteration 110, loss = 0.48920619\n",
      "Iteration 111, loss = 0.48903441\n",
      "Iteration 112, loss = 0.48896813\n",
      "Iteration 113, loss = 0.48870914\n",
      "Iteration 114, loss = 0.48829801\n",
      "Iteration 115, loss = 0.48818599\n",
      "Iteration 116, loss = 0.48810704\n",
      "Iteration 117, loss = 0.48783596\n",
      "Iteration 118, loss = 0.48759187\n",
      "Iteration 119, loss = 0.48749796\n",
      "Iteration 120, loss = 0.48725597\n",
      "Iteration 121, loss = 0.48740543\n",
      "Iteration 122, loss = 0.48705931\n",
      "Iteration 123, loss = 0.48668682\n",
      "Iteration 124, loss = 0.48679715\n",
      "Iteration 125, loss = 0.48633413\n",
      "Iteration 126, loss = 0.48621577\n",
      "Iteration 127, loss = 0.48591966\n",
      "Iteration 128, loss = 0.48607347\n",
      "Iteration 129, loss = 0.48577704\n",
      "Iteration 130, loss = 0.48565198\n",
      "Iteration 131, loss = 0.48578884\n",
      "Iteration 132, loss = 0.48534009\n",
      "Iteration 133, loss = 0.48510317\n",
      "Iteration 134, loss = 0.48510219\n",
      "Iteration 135, loss = 0.48497358\n",
      "Iteration 136, loss = 0.48483044\n",
      "Iteration 137, loss = 0.48466864\n",
      "Iteration 138, loss = 0.48452714\n",
      "Iteration 139, loss = 0.48460132\n",
      "Iteration 140, loss = 0.48432355\n",
      "Iteration 141, loss = 0.48424232\n",
      "Iteration 142, loss = 0.48402212\n",
      "Iteration 143, loss = 0.48392184\n",
      "Iteration 144, loss = 0.48376113\n",
      "Iteration 145, loss = 0.48372281\n",
      "Iteration 146, loss = 0.48394792\n",
      "Iteration 147, loss = 0.48420498\n",
      "Iteration 148, loss = 0.48353426\n",
      "Iteration 149, loss = 0.48330575\n",
      "Iteration 150, loss = 0.48305260\n",
      "Iteration 151, loss = 0.48312748\n",
      "Iteration 152, loss = 0.48266348\n",
      "Iteration 153, loss = 0.48269717\n",
      "Iteration 154, loss = 0.48243172\n",
      "Iteration 155, loss = 0.48241193\n",
      "Iteration 156, loss = 0.48225073\n",
      "Iteration 157, loss = 0.48274544\n",
      "Iteration 158, loss = 0.48242208\n",
      "Iteration 159, loss = 0.48195136\n",
      "Iteration 160, loss = 0.48174333\n",
      "Iteration 161, loss = 0.48194155\n",
      "Iteration 162, loss = 0.48144083\n",
      "Iteration 163, loss = 0.48149890\n",
      "Iteration 164, loss = 0.48148733\n",
      "Iteration 165, loss = 0.48129807\n",
      "Iteration 166, loss = 0.48119244\n",
      "Iteration 167, loss = 0.48097334\n",
      "Iteration 168, loss = 0.48092046\n",
      "Iteration 169, loss = 0.48069100\n",
      "Iteration 170, loss = 0.48058126\n",
      "Iteration 171, loss = 0.48053477\n",
      "Iteration 172, loss = 0.48026865\n",
      "Iteration 173, loss = 0.48024879\n",
      "Iteration 174, loss = 0.47992306\n",
      "Iteration 175, loss = 0.47990124\n",
      "Iteration 176, loss = 0.47997698\n",
      "Iteration 177, loss = 0.47965957\n",
      "Iteration 178, loss = 0.47939949\n",
      "Iteration 179, loss = 0.47954804\n",
      "Iteration 180, loss = 0.47904818\n",
      "Iteration 181, loss = 0.47937223\n",
      "Iteration 182, loss = 0.47904140\n",
      "Iteration 183, loss = 0.47874581\n",
      "Iteration 184, loss = 0.47876192\n",
      "Iteration 185, loss = 0.47858605\n",
      "Iteration 186, loss = 0.47867016\n",
      "Iteration 187, loss = 0.47852844\n",
      "Iteration 188, loss = 0.47824651\n",
      "Iteration 189, loss = 0.47802136\n",
      "Iteration 190, loss = 0.47804103\n",
      "Iteration 191, loss = 0.47804076\n",
      "Iteration 192, loss = 0.47808559\n",
      "Iteration 193, loss = 0.47806764\n",
      "Iteration 194, loss = 0.47733335\n",
      "Iteration 195, loss = 0.47759329\n",
      "Iteration 196, loss = 0.47737491\n",
      "Iteration 197, loss = 0.47725982\n",
      "Iteration 198, loss = 0.47709735\n",
      "Iteration 199, loss = 0.47699720\n",
      "Iteration 200, loss = 0.47678645\n",
      "Iteration 201, loss = 0.47680930\n",
      "Iteration 202, loss = 0.47670029\n",
      "Iteration 203, loss = 0.47641371\n",
      "Iteration 204, loss = 0.47644077\n",
      "Iteration 205, loss = 0.47621642\n",
      "Iteration 206, loss = 0.47611252\n",
      "Iteration 207, loss = 0.47595653\n",
      "Iteration 208, loss = 0.47584429\n",
      "Iteration 209, loss = 0.47570848\n",
      "Iteration 210, loss = 0.47558575\n",
      "Iteration 211, loss = 0.47570171\n",
      "Iteration 212, loss = 0.47515567\n",
      "Iteration 213, loss = 0.47528448\n",
      "Iteration 214, loss = 0.47499424\n",
      "Iteration 215, loss = 0.47543525\n",
      "Iteration 216, loss = 0.47484376\n",
      "Iteration 217, loss = 0.47492565\n",
      "Iteration 218, loss = 0.47452977\n",
      "Iteration 219, loss = 0.47460082\n",
      "Iteration 220, loss = 0.47442753\n",
      "Iteration 221, loss = 0.47412534\n",
      "Iteration 222, loss = 0.47406869\n",
      "Iteration 223, loss = 0.47427241\n",
      "Iteration 224, loss = 0.47385972\n",
      "Iteration 225, loss = 0.47366856\n",
      "Iteration 226, loss = 0.47359636\n",
      "Iteration 227, loss = 0.47362027\n",
      "Iteration 228, loss = 0.47361783\n",
      "Iteration 229, loss = 0.47348906\n",
      "Iteration 230, loss = 0.47328571\n",
      "Iteration 231, loss = 0.47351621\n",
      "Iteration 232, loss = 0.47293595\n",
      "Iteration 233, loss = 0.47310563\n",
      "Iteration 234, loss = 0.47287577\n",
      "Iteration 235, loss = 0.47268290\n",
      "Iteration 236, loss = 0.47272016\n",
      "Iteration 237, loss = 0.47279522\n",
      "Iteration 238, loss = 0.47281191\n",
      "Iteration 239, loss = 0.47229151\n",
      "Iteration 240, loss = 0.47248585\n",
      "Iteration 241, loss = 0.47232149\n",
      "Iteration 242, loss = 0.47211718\n",
      "Iteration 243, loss = 0.47219269\n",
      "Iteration 244, loss = 0.47204247\n",
      "Iteration 245, loss = 0.47189801\n",
      "Iteration 246, loss = 0.47182090\n",
      "Iteration 247, loss = 0.47174931\n",
      "Iteration 248, loss = 0.47164233\n",
      "Iteration 249, loss = 0.47166818\n",
      "Iteration 250, loss = 0.47160660\n",
      "Iteration 251, loss = 0.47176484\n",
      "Iteration 252, loss = 0.47133226\n",
      "Iteration 253, loss = 0.47115930\n",
      "Iteration 254, loss = 0.47102763\n",
      "Iteration 255, loss = 0.47088315\n",
      "Iteration 256, loss = 0.47094346\n",
      "Iteration 257, loss = 0.47093801\n",
      "Iteration 258, loss = 0.47098612\n",
      "Iteration 259, loss = 0.47083284\n",
      "Iteration 260, loss = 0.47051361\n",
      "Iteration 261, loss = 0.47053360\n",
      "Iteration 262, loss = 0.47032862\n",
      "Iteration 263, loss = 0.47024723\n",
      "Iteration 264, loss = 0.47037232\n",
      "Iteration 265, loss = 0.47022097\n",
      "Iteration 266, loss = 0.47015802\n",
      "Iteration 267, loss = 0.47010383\n",
      "Iteration 268, loss = 0.47009072\n",
      "Iteration 269, loss = 0.47000592\n",
      "Iteration 270, loss = 0.47007562\n",
      "Iteration 271, loss = 0.46976052\n",
      "Iteration 272, loss = 0.46967212\n",
      "Iteration 273, loss = 0.46947344\n",
      "Iteration 274, loss = 0.46971130\n",
      "Iteration 275, loss = 0.46934593\n",
      "Iteration 276, loss = 0.46963450\n",
      "Iteration 277, loss = 0.46930766\n",
      "Iteration 278, loss = 0.46927764\n",
      "Iteration 279, loss = 0.46940540\n",
      "Iteration 280, loss = 0.46930738\n",
      "Iteration 281, loss = 0.46897845\n",
      "Iteration 282, loss = 0.46914450\n",
      "Iteration 283, loss = 0.46879210\n",
      "Iteration 284, loss = 0.46885315\n",
      "Iteration 285, loss = 0.46881948\n",
      "Iteration 286, loss = 0.46869525\n",
      "Iteration 287, loss = 0.46868096\n",
      "Iteration 288, loss = 0.46875809\n",
      "Iteration 289, loss = 0.46870756\n",
      "Iteration 290, loss = 0.46841034\n",
      "Iteration 291, loss = 0.46847455\n",
      "Iteration 292, loss = 0.46841704\n",
      "Iteration 293, loss = 0.46840782\n",
      "Iteration 294, loss = 0.46870493\n",
      "Iteration 295, loss = 0.46813918\n",
      "Iteration 296, loss = 0.46810514\n",
      "Iteration 297, loss = 0.46791628\n",
      "Iteration 298, loss = 0.46803026\n",
      "Iteration 299, loss = 0.46805892\n",
      "Iteration 300, loss = 0.46786072\n",
      "Iteration 301, loss = 0.46768634\n",
      "Iteration 302, loss = 0.46780791\n",
      "Iteration 303, loss = 0.46746000\n",
      "Iteration 304, loss = 0.46750986\n",
      "Iteration 305, loss = 0.46758711\n",
      "Iteration 306, loss = 0.46762249\n",
      "Iteration 307, loss = 0.46722802\n",
      "Iteration 308, loss = 0.46748767\n",
      "Iteration 309, loss = 0.46755623\n",
      "Iteration 310, loss = 0.46753057\n",
      "Iteration 311, loss = 0.46722588\n",
      "Iteration 312, loss = 0.46718109\n",
      "Iteration 313, loss = 0.46704633\n",
      "Iteration 314, loss = 0.46698513\n",
      "Iteration 315, loss = 0.46706626\n",
      "Iteration 316, loss = 0.46673173\n",
      "Iteration 317, loss = 0.46672555\n",
      "Iteration 318, loss = 0.46674524\n",
      "Iteration 319, loss = 0.46697889\n",
      "Iteration 320, loss = 0.46639232\n",
      "Iteration 321, loss = 0.46667750\n",
      "Iteration 322, loss = 0.46645256\n",
      "Iteration 323, loss = 0.46639861\n",
      "Iteration 324, loss = 0.46652353\n",
      "Iteration 325, loss = 0.46633662\n",
      "Iteration 326, loss = 0.46644931\n",
      "Iteration 327, loss = 0.46640102\n",
      "Iteration 328, loss = 0.46630738\n",
      "Iteration 329, loss = 0.46603887\n",
      "Iteration 330, loss = 0.46631790\n",
      "Iteration 331, loss = 0.46595843\n",
      "Iteration 332, loss = 0.46591998\n",
      "Iteration 333, loss = 0.46591082\n",
      "Iteration 334, loss = 0.46583951\n",
      "Iteration 335, loss = 0.46620913\n",
      "Iteration 336, loss = 0.46593345\n",
      "Iteration 337, loss = 0.46576469\n",
      "Iteration 338, loss = 0.46566615\n",
      "Iteration 339, loss = 0.46593914\n",
      "Iteration 340, loss = 0.46558407\n",
      "Iteration 341, loss = 0.46574214\n",
      "Iteration 342, loss = 0.46572132\n",
      "Iteration 343, loss = 0.46554347\n",
      "Iteration 344, loss = 0.46539941\n",
      "Iteration 345, loss = 0.46543943\n",
      "Iteration 346, loss = 0.46540624\n",
      "Iteration 347, loss = 0.46535148\n",
      "Iteration 348, loss = 0.46545312\n",
      "Iteration 349, loss = 0.46573025\n",
      "Iteration 350, loss = 0.46541871\n",
      "Iteration 351, loss = 0.46544733\n",
      "Iteration 352, loss = 0.46528773\n",
      "Iteration 353, loss = 0.46526574\n",
      "Iteration 354, loss = 0.46496910\n",
      "Iteration 355, loss = 0.46503933\n",
      "Iteration 356, loss = 0.46510058\n",
      "Iteration 357, loss = 0.46551276\n",
      "Iteration 358, loss = 0.46514564\n",
      "Iteration 359, loss = 0.46516798\n",
      "Iteration 360, loss = 0.46487134\n",
      "Iteration 361, loss = 0.46496153\n",
      "Iteration 362, loss = 0.46531238\n",
      "Iteration 363, loss = 0.46471717\n",
      "Iteration 364, loss = 0.46460901\n",
      "Iteration 365, loss = 0.46474422\n",
      "Iteration 366, loss = 0.46473482\n",
      "Iteration 367, loss = 0.46488357\n",
      "Iteration 368, loss = 0.46460433\n",
      "Iteration 369, loss = 0.46438649\n",
      "Iteration 370, loss = 0.46449658\n",
      "Iteration 371, loss = 0.46440916\n",
      "Iteration 372, loss = 0.46453476\n",
      "Iteration 373, loss = 0.46450716\n",
      "Iteration 374, loss = 0.46442311\n",
      "Iteration 375, loss = 0.46425380\n",
      "Iteration 376, loss = 0.46473025\n",
      "Iteration 377, loss = 0.46415479\n",
      "Iteration 378, loss = 0.46434612\n",
      "Iteration 379, loss = 0.46411479\n",
      "Iteration 380, loss = 0.46400262\n",
      "Iteration 381, loss = 0.46426300\n",
      "Iteration 382, loss = 0.46400730\n",
      "Iteration 383, loss = 0.46404510\n",
      "Iteration 384, loss = 0.46421442\n",
      "Iteration 385, loss = 0.46423267\n",
      "Iteration 386, loss = 0.46370667\n",
      "Iteration 387, loss = 0.46385243\n",
      "Iteration 388, loss = 0.46374447\n",
      "Iteration 389, loss = 0.46374948\n",
      "Iteration 390, loss = 0.46405885\n",
      "Iteration 391, loss = 0.46374252\n",
      "Iteration 392, loss = 0.46384161\n",
      "Iteration 393, loss = 0.46362121\n",
      "Iteration 394, loss = 0.46361883\n",
      "Iteration 395, loss = 0.46365376\n",
      "Iteration 396, loss = 0.46365205\n",
      "Iteration 397, loss = 0.46356307\n",
      "Iteration 398, loss = 0.46368000\n",
      "Iteration 399, loss = 0.46343423\n",
      "Iteration 400, loss = 0.46336423\n",
      "Iteration 401, loss = 0.46341133\n",
      "Iteration 402, loss = 0.46326975\n",
      "Iteration 403, loss = 0.46333588\n",
      "Iteration 404, loss = 0.46314507\n",
      "Iteration 405, loss = 0.46330804\n",
      "Iteration 406, loss = 0.46319919\n",
      "Iteration 407, loss = 0.46298357\n",
      "Iteration 408, loss = 0.46299727\n",
      "Iteration 409, loss = 0.46302959\n",
      "Iteration 410, loss = 0.46317841\n",
      "Iteration 411, loss = 0.46299633\n",
      "Iteration 412, loss = 0.46306296\n",
      "Iteration 413, loss = 0.46308814\n",
      "Iteration 414, loss = 0.46281316\n",
      "Iteration 415, loss = 0.46287096\n",
      "Iteration 416, loss = 0.46312674\n",
      "Iteration 417, loss = 0.46318961\n",
      "Iteration 418, loss = 0.46282557\n",
      "Iteration 419, loss = 0.46273772\n",
      "Iteration 420, loss = 0.46267354\n",
      "Iteration 421, loss = 0.46270974\n",
      "Iteration 422, loss = 0.46304971\n",
      "Iteration 423, loss = 0.46273234\n",
      "Iteration 424, loss = 0.46271670\n",
      "Iteration 425, loss = 0.46262934\n",
      "Iteration 426, loss = 0.46288968\n",
      "Iteration 427, loss = 0.46278014\n",
      "Iteration 428, loss = 0.46249385\n",
      "Iteration 429, loss = 0.46245558\n",
      "Iteration 430, loss = 0.46245061\n",
      "Iteration 431, loss = 0.46260810\n",
      "Iteration 432, loss = 0.46251294\n",
      "Iteration 433, loss = 0.46248827\n",
      "Iteration 434, loss = 0.46215381\n",
      "Iteration 435, loss = 0.46239860\n",
      "Iteration 436, loss = 0.46230642\n",
      "Iteration 437, loss = 0.46224977\n",
      "Iteration 438, loss = 0.46215125\n",
      "Iteration 439, loss = 0.46205969\n",
      "Iteration 440, loss = 0.46228316\n",
      "Iteration 441, loss = 0.46202661\n",
      "Iteration 442, loss = 0.46218578\n",
      "Iteration 443, loss = 0.46183520\n",
      "Iteration 444, loss = 0.46249648\n",
      "Iteration 445, loss = 0.46215594\n",
      "Iteration 446, loss = 0.46181341\n",
      "Iteration 447, loss = 0.46197278\n",
      "Iteration 448, loss = 0.46187614\n",
      "Iteration 449, loss = 0.46188004\n",
      "Iteration 450, loss = 0.46161225\n",
      "Iteration 451, loss = 0.46179178\n",
      "Iteration 452, loss = 0.46169860\n",
      "Iteration 453, loss = 0.46159027\n",
      "Iteration 454, loss = 0.46162671\n",
      "Iteration 455, loss = 0.46179239\n",
      "Iteration 456, loss = 0.46158566\n",
      "Iteration 457, loss = 0.46158228\n",
      "Iteration 458, loss = 0.46152538\n",
      "Iteration 459, loss = 0.46149661\n",
      "Iteration 460, loss = 0.46146749\n",
      "Iteration 461, loss = 0.46155351\n",
      "Iteration 462, loss = 0.46139691\n",
      "Iteration 463, loss = 0.46122602\n",
      "Iteration 464, loss = 0.46143175\n",
      "Iteration 465, loss = 0.46129992\n",
      "Iteration 466, loss = 0.46119429\n",
      "Iteration 467, loss = 0.46114376\n",
      "Iteration 468, loss = 0.46101667\n",
      "Iteration 469, loss = 0.46114941\n",
      "Iteration 470, loss = 0.46107496\n",
      "Iteration 471, loss = 0.46098986\n",
      "Iteration 472, loss = 0.46091772\n",
      "Iteration 473, loss = 0.46104198\n",
      "Iteration 474, loss = 0.46097375\n",
      "Iteration 475, loss = 0.46077956\n",
      "Iteration 476, loss = 0.46078091\n",
      "Iteration 477, loss = 0.46112239\n",
      "Iteration 478, loss = 0.46071934\n",
      "Iteration 479, loss = 0.46072189\n",
      "Iteration 480, loss = 0.46076419\n",
      "Iteration 481, loss = 0.46079982\n",
      "Iteration 482, loss = 0.46082521\n",
      "Iteration 483, loss = 0.46080373\n",
      "Iteration 484, loss = 0.46059939\n",
      "Iteration 485, loss = 0.46054767\n",
      "Iteration 486, loss = 0.46110553\n",
      "Iteration 487, loss = 0.46075233\n",
      "Iteration 488, loss = 0.46035204\n",
      "Iteration 489, loss = 0.46040130\n",
      "Iteration 490, loss = 0.46033411\n",
      "Iteration 491, loss = 0.46041901\n",
      "Iteration 492, loss = 0.46024584\n",
      "Iteration 493, loss = 0.46044899\n",
      "Iteration 494, loss = 0.46025007\n",
      "Iteration 495, loss = 0.46082899\n",
      "Iteration 496, loss = 0.46024447\n",
      "Iteration 497, loss = 0.46068313\n",
      "Iteration 498, loss = 0.46024014\n",
      "Iteration 499, loss = 0.46055986\n",
      "Iteration 500, loss = 0.45993917\n",
      "Iteration 501, loss = 0.46020985\n",
      "Iteration 502, loss = 0.46027299\n",
      "Iteration 503, loss = 0.45995866\n",
      "Iteration 504, loss = 0.45996936\n",
      "Iteration 505, loss = 0.46002678\n",
      "Iteration 506, loss = 0.46003577\n",
      "Iteration 507, loss = 0.45997396\n",
      "Iteration 508, loss = 0.45991221\n",
      "Iteration 509, loss = 0.46013378\n",
      "Iteration 510, loss = 0.45995213\n",
      "Iteration 511, loss = 0.45972318\n",
      "Iteration 512, loss = 0.45974949\n",
      "Iteration 513, loss = 0.45988280\n",
      "Iteration 514, loss = 0.45982909\n",
      "Iteration 515, loss = 0.45965851\n",
      "Iteration 516, loss = 0.45962645\n",
      "Iteration 517, loss = 0.45976486\n",
      "Iteration 518, loss = 0.45962961\n",
      "Iteration 519, loss = 0.45954877\n",
      "Iteration 520, loss = 0.45963626\n",
      "Iteration 521, loss = 0.45951391\n",
      "Iteration 522, loss = 0.45938167\n",
      "Iteration 523, loss = 0.45939012\n",
      "Iteration 524, loss = 0.45941075\n",
      "Iteration 525, loss = 0.45934304\n",
      "Iteration 526, loss = 0.45925272\n",
      "Iteration 527, loss = 0.45941834\n",
      "Iteration 528, loss = 0.45961730\n",
      "Iteration 529, loss = 0.45950006\n",
      "Iteration 530, loss = 0.45934158\n",
      "Iteration 531, loss = 0.45926901\n",
      "Iteration 532, loss = 0.45934071\n",
      "Iteration 533, loss = 0.45924623\n",
      "Iteration 534, loss = 0.45896978\n",
      "Iteration 535, loss = 0.45905504\n",
      "Iteration 536, loss = 0.45904420\n",
      "Iteration 537, loss = 0.45928247\n",
      "Iteration 538, loss = 0.45890109\n",
      "Iteration 539, loss = 0.45894472\n",
      "Iteration 540, loss = 0.45899490\n",
      "Iteration 541, loss = 0.45883780\n",
      "Iteration 542, loss = 0.45888165\n",
      "Iteration 543, loss = 0.45881811\n",
      "Iteration 544, loss = 0.45911380\n",
      "Iteration 545, loss = 0.45876679\n",
      "Iteration 546, loss = 0.45857004\n",
      "Iteration 547, loss = 0.45860570\n",
      "Iteration 548, loss = 0.45889520\n",
      "Iteration 549, loss = 0.45865185\n",
      "Iteration 550, loss = 0.45883714\n",
      "Iteration 551, loss = 0.45858352\n",
      "Iteration 552, loss = 0.45872562\n",
      "Iteration 553, loss = 0.45839592\n",
      "Iteration 554, loss = 0.45845211\n",
      "Iteration 555, loss = 0.45854685\n",
      "Iteration 556, loss = 0.45838075\n",
      "Iteration 557, loss = 0.45839482\n",
      "Iteration 558, loss = 0.45858756\n",
      "Iteration 559, loss = 0.45884797\n",
      "Iteration 560, loss = 0.45832827\n",
      "Iteration 561, loss = 0.45835055\n",
      "Iteration 562, loss = 0.45857174\n",
      "Iteration 563, loss = 0.45844059\n",
      "Iteration 564, loss = 0.45822513\n",
      "Iteration 565, loss = 0.45827856\n",
      "Iteration 566, loss = 0.45858364\n",
      "Iteration 567, loss = 0.45812368\n",
      "Iteration 568, loss = 0.45804521\n",
      "Iteration 569, loss = 0.45807702\n",
      "Iteration 570, loss = 0.45814050\n",
      "Iteration 571, loss = 0.45852174\n",
      "Iteration 572, loss = 0.45780472\n",
      "Iteration 573, loss = 0.45799287\n",
      "Iteration 574, loss = 0.45809871\n",
      "Iteration 575, loss = 0.45840073\n",
      "Iteration 576, loss = 0.45794551\n",
      "Iteration 577, loss = 0.45775806\n",
      "Iteration 578, loss = 0.45792825\n",
      "Iteration 579, loss = 0.45785550\n",
      "Iteration 580, loss = 0.45778638\n",
      "Iteration 581, loss = 0.45827114\n",
      "Iteration 582, loss = 0.45766731\n",
      "Iteration 583, loss = 0.45769175\n",
      "Iteration 584, loss = 0.45785517\n",
      "Iteration 585, loss = 0.45779421\n",
      "Iteration 586, loss = 0.45783783\n",
      "Iteration 587, loss = 0.45787851\n",
      "Iteration 588, loss = 0.45759022\n",
      "Iteration 589, loss = 0.45754236\n",
      "Iteration 590, loss = 0.45763062\n",
      "Iteration 591, loss = 0.45738175\n",
      "Iteration 592, loss = 0.45771716\n",
      "Iteration 593, loss = 0.45752532\n",
      "Iteration 594, loss = 0.45756922\n",
      "Iteration 595, loss = 0.45743129\n",
      "Iteration 596, loss = 0.45728447\n",
      "Iteration 597, loss = 0.45766053\n",
      "Iteration 598, loss = 0.45717854\n",
      "Iteration 599, loss = 0.45726114\n",
      "Iteration 600, loss = 0.45717832\n",
      "Iteration 601, loss = 0.45725541\n",
      "Iteration 602, loss = 0.45724176\n",
      "Iteration 603, loss = 0.45701628\n",
      "Iteration 604, loss = 0.45705834\n",
      "Iteration 605, loss = 0.45716193\n",
      "Iteration 606, loss = 0.45758899\n",
      "Iteration 607, loss = 0.45719081\n",
      "Iteration 608, loss = 0.45741659\n",
      "Iteration 609, loss = 0.45717710\n",
      "Iteration 610, loss = 0.45696518\n",
      "Iteration 611, loss = 0.45710315\n",
      "Iteration 612, loss = 0.45727064\n",
      "Iteration 613, loss = 0.45686409\n",
      "Iteration 614, loss = 0.45692273\n",
      "Iteration 615, loss = 0.45698558\n",
      "Iteration 616, loss = 0.45680871\n",
      "Iteration 617, loss = 0.45682530\n",
      "Iteration 618, loss = 0.45680820\n",
      "Iteration 619, loss = 0.45696512\n",
      "Iteration 620, loss = 0.45719841\n",
      "Iteration 621, loss = 0.45685012\n",
      "Iteration 622, loss = 0.45694468\n",
      "Iteration 623, loss = 0.45661821\n",
      "Iteration 624, loss = 0.45669082\n",
      "Iteration 625, loss = 0.45649597\n",
      "Iteration 626, loss = 0.45649745\n",
      "Iteration 627, loss = 0.45646534\n",
      "Iteration 628, loss = 0.45637838\n",
      "Iteration 629, loss = 0.45645585\n",
      "Iteration 630, loss = 0.45639968\n",
      "Iteration 631, loss = 0.45646149\n",
      "Iteration 632, loss = 0.45634038\n",
      "Iteration 633, loss = 0.45644135\n",
      "Iteration 634, loss = 0.45634055\n",
      "Iteration 635, loss = 0.45639819\n",
      "Iteration 636, loss = 0.45652227\n",
      "Iteration 637, loss = 0.45635594\n",
      "Iteration 638, loss = 0.45626527\n",
      "Iteration 639, loss = 0.45601142\n",
      "Iteration 640, loss = 0.45633473\n",
      "Iteration 641, loss = 0.45613952\n",
      "Iteration 642, loss = 0.45610689\n",
      "Iteration 643, loss = 0.45601327\n",
      "Iteration 644, loss = 0.45646281\n",
      "Iteration 645, loss = 0.45601608\n",
      "Iteration 646, loss = 0.45647152\n",
      "Iteration 647, loss = 0.45613088\n",
      "Iteration 648, loss = 0.45606292\n",
      "Iteration 649, loss = 0.45598635\n",
      "Iteration 650, loss = 0.45592866\n",
      "Iteration 651, loss = 0.45607158\n",
      "Iteration 652, loss = 0.45572046\n",
      "Iteration 653, loss = 0.45584494\n",
      "Iteration 654, loss = 0.45603789\n",
      "Iteration 655, loss = 0.45609037\n",
      "Iteration 656, loss = 0.45570519\n",
      "Iteration 657, loss = 0.45595303\n",
      "Iteration 658, loss = 0.45555600\n",
      "Iteration 659, loss = 0.45584321\n",
      "Iteration 660, loss = 0.45569723\n",
      "Iteration 661, loss = 0.45564178\n",
      "Iteration 662, loss = 0.45571941\n",
      "Iteration 663, loss = 0.45575380\n",
      "Iteration 664, loss = 0.45560085\n",
      "Iteration 665, loss = 0.45555163\n",
      "Iteration 666, loss = 0.45558944\n",
      "Iteration 667, loss = 0.45579032\n",
      "Iteration 668, loss = 0.45556972\n",
      "Iteration 669, loss = 0.45560764\n",
      "Iteration 670, loss = 0.45549103\n",
      "Iteration 671, loss = 0.45543573\n",
      "Iteration 672, loss = 0.45550425\n",
      "Iteration 673, loss = 0.45567164\n",
      "Iteration 674, loss = 0.45543682\n",
      "Iteration 675, loss = 0.45540614\n",
      "Iteration 676, loss = 0.45548154\n",
      "Iteration 677, loss = 0.45545242\n",
      "Iteration 678, loss = 0.45535050\n",
      "Iteration 679, loss = 0.45524228\n",
      "Iteration 680, loss = 0.45526952\n",
      "Iteration 681, loss = 0.45523818\n",
      "Iteration 682, loss = 0.45530753\n",
      "Iteration 683, loss = 0.45518730\n",
      "Iteration 684, loss = 0.45520048\n",
      "Iteration 685, loss = 0.45505253\n",
      "Iteration 686, loss = 0.45516932\n",
      "Iteration 687, loss = 0.45538639\n",
      "Iteration 688, loss = 0.45501863\n",
      "Iteration 689, loss = 0.45503406\n",
      "Iteration 690, loss = 0.45477808\n",
      "Iteration 691, loss = 0.45512801\n",
      "Iteration 692, loss = 0.45500819\n",
      "Iteration 693, loss = 0.45503547\n",
      "Iteration 694, loss = 0.45476632\n",
      "Iteration 695, loss = 0.45507125\n",
      "Iteration 696, loss = 0.45467661\n",
      "Iteration 697, loss = 0.45479928\n",
      "Iteration 698, loss = 0.45477970\n",
      "Iteration 699, loss = 0.45463422\n",
      "Iteration 700, loss = 0.45486603\n",
      "Iteration 701, loss = 0.45468515\n",
      "Iteration 702, loss = 0.45458649\n",
      "Iteration 703, loss = 0.45450066\n",
      "Iteration 704, loss = 0.45441123\n",
      "Iteration 705, loss = 0.45443001\n",
      "Iteration 706, loss = 0.45452631\n",
      "Iteration 707, loss = 0.45459662\n",
      "Iteration 708, loss = 0.45450066\n",
      "Iteration 709, loss = 0.45458417\n",
      "Iteration 710, loss = 0.45445442\n",
      "Iteration 711, loss = 0.45429444\n",
      "Iteration 712, loss = 0.45428495\n",
      "Iteration 713, loss = 0.45414935\n",
      "Iteration 714, loss = 0.45427431\n",
      "Iteration 715, loss = 0.45396440\n",
      "Iteration 716, loss = 0.45394182\n",
      "Iteration 717, loss = 0.45402964\n",
      "Iteration 718, loss = 0.45419354\n",
      "Iteration 719, loss = 0.45405262\n",
      "Iteration 720, loss = 0.45402900\n",
      "Iteration 721, loss = 0.45398174\n",
      "Iteration 722, loss = 0.45378962\n",
      "Iteration 723, loss = 0.45395120\n",
      "Iteration 724, loss = 0.45391828\n",
      "Iteration 725, loss = 0.45373077\n",
      "Iteration 726, loss = 0.45404817\n",
      "Iteration 727, loss = 0.45378352\n",
      "Iteration 728, loss = 0.45375467\n",
      "Iteration 729, loss = 0.45351318\n",
      "Iteration 730, loss = 0.45351385\n",
      "Iteration 731, loss = 0.45359981\n",
      "Iteration 732, loss = 0.45351626\n",
      "Iteration 733, loss = 0.45349654\n",
      "Iteration 734, loss = 0.45359982\n",
      "Iteration 735, loss = 0.45334797\n",
      "Iteration 736, loss = 0.45330366\n",
      "Iteration 737, loss = 0.45339679\n",
      "Iteration 738, loss = 0.45329641\n",
      "Iteration 739, loss = 0.45310700\n",
      "Iteration 740, loss = 0.45310183\n",
      "Iteration 741, loss = 0.45312838\n",
      "Iteration 742, loss = 0.45342700\n",
      "Iteration 743, loss = 0.45319521\n",
      "Iteration 744, loss = 0.45309871\n",
      "Iteration 745, loss = 0.45289770\n",
      "Iteration 746, loss = 0.45331569\n",
      "Iteration 747, loss = 0.45294046\n",
      "Iteration 748, loss = 0.45309399\n",
      "Iteration 749, loss = 0.45315758\n",
      "Iteration 750, loss = 0.45316775\n",
      "Iteration 751, loss = 0.45279769\n",
      "Iteration 752, loss = 0.45314750\n",
      "Iteration 753, loss = 0.45290079\n",
      "Iteration 754, loss = 0.45283969\n",
      "Iteration 755, loss = 0.45289061\n",
      "Iteration 756, loss = 0.45273514\n",
      "Iteration 757, loss = 0.45275251\n",
      "Iteration 758, loss = 0.45274416\n",
      "Iteration 759, loss = 0.45292484\n",
      "Iteration 760, loss = 0.45274241\n",
      "Iteration 761, loss = 0.45286413\n",
      "Iteration 762, loss = 0.45290161\n",
      "Iteration 763, loss = 0.45265727\n",
      "Iteration 764, loss = 0.45263933\n",
      "Iteration 765, loss = 0.45264238\n",
      "Iteration 766, loss = 0.45256938\n",
      "Iteration 767, loss = 0.45260273\n",
      "Iteration 768, loss = 0.45276107\n",
      "Iteration 769, loss = 0.45255922\n",
      "Iteration 770, loss = 0.45251413\n",
      "Iteration 771, loss = 0.45249632\n",
      "Iteration 772, loss = 0.45266531\n",
      "Iteration 773, loss = 0.45230898\n",
      "Iteration 774, loss = 0.45251142\n",
      "Iteration 775, loss = 0.45268699\n",
      "Iteration 776, loss = 0.45229939\n",
      "Iteration 777, loss = 0.45260993\n",
      "Iteration 778, loss = 0.45237139\n",
      "Iteration 779, loss = 0.45237994\n",
      "Iteration 780, loss = 0.45238162\n",
      "Iteration 781, loss = 0.45245573\n",
      "Iteration 782, loss = 0.45233398\n",
      "Iteration 783, loss = 0.45264484\n",
      "Iteration 784, loss = 0.45217300\n",
      "Iteration 785, loss = 0.45216810\n",
      "Iteration 786, loss = 0.45234124\n",
      "Iteration 787, loss = 0.45206533\n",
      "Iteration 788, loss = 0.45222911\n",
      "Iteration 789, loss = 0.45259421\n",
      "Iteration 790, loss = 0.45206613\n",
      "Iteration 791, loss = 0.45189692\n",
      "Iteration 792, loss = 0.45202270\n",
      "Iteration 793, loss = 0.45229124\n",
      "Iteration 794, loss = 0.45204676\n",
      "Iteration 795, loss = 0.45211178\n",
      "Iteration 796, loss = 0.45206872\n",
      "Iteration 797, loss = 0.45210309\n",
      "Iteration 798, loss = 0.45210259\n",
      "Iteration 799, loss = 0.45176704\n",
      "Iteration 800, loss = 0.45210727\n",
      "Iteration 801, loss = 0.45174586\n",
      "Iteration 802, loss = 0.45190730\n",
      "Iteration 803, loss = 0.45251103\n",
      "Iteration 804, loss = 0.45227900\n",
      "Iteration 805, loss = 0.45224773\n",
      "Iteration 806, loss = 0.45156639\n",
      "Iteration 807, loss = 0.45178860\n",
      "Iteration 808, loss = 0.45178011\n",
      "Iteration 809, loss = 0.45173528\n",
      "Iteration 810, loss = 0.45175756\n",
      "Iteration 811, loss = 0.45167152\n",
      "Iteration 812, loss = 0.45176254\n",
      "Iteration 813, loss = 0.45168326\n",
      "Iteration 814, loss = 0.45149581\n",
      "Iteration 815, loss = 0.45155987\n",
      "Iteration 816, loss = 0.45170031\n",
      "Iteration 817, loss = 0.45157907\n",
      "Iteration 818, loss = 0.45175537\n",
      "Iteration 819, loss = 0.45178807\n",
      "Iteration 820, loss = 0.45136666\n",
      "Iteration 821, loss = 0.45199996\n",
      "Iteration 822, loss = 0.45141900\n",
      "Iteration 823, loss = 0.45144409\n",
      "Iteration 824, loss = 0.45157095\n",
      "Iteration 825, loss = 0.45174253\n",
      "Iteration 826, loss = 0.45141308\n",
      "Iteration 827, loss = 0.45130804\n",
      "Iteration 828, loss = 0.45139527\n",
      "Iteration 829, loss = 0.45135881\n",
      "Iteration 830, loss = 0.45131219\n",
      "Iteration 831, loss = 0.45132083\n",
      "Iteration 832, loss = 0.45132000\n",
      "Iteration 833, loss = 0.45156763\n",
      "Iteration 834, loss = 0.45116620\n",
      "Iteration 835, loss = 0.45123785\n",
      "Iteration 836, loss = 0.45150706\n",
      "Iteration 837, loss = 0.45162053\n",
      "Iteration 838, loss = 0.45101338\n",
      "Iteration 839, loss = 0.45150762\n",
      "Iteration 840, loss = 0.45121274\n",
      "Iteration 841, loss = 0.45111125\n",
      "Iteration 842, loss = 0.45115048\n",
      "Iteration 843, loss = 0.45101713\n",
      "Iteration 844, loss = 0.45115677\n",
      "Iteration 845, loss = 0.45124187\n",
      "Iteration 846, loss = 0.45121156\n",
      "Iteration 847, loss = 0.45103807\n",
      "Iteration 848, loss = 0.45166293\n",
      "Iteration 849, loss = 0.45098002\n",
      "Iteration 850, loss = 0.45159911\n",
      "Iteration 851, loss = 0.45108008\n",
      "Iteration 852, loss = 0.45120236\n",
      "Iteration 853, loss = 0.45102278\n",
      "Iteration 854, loss = 0.45082656\n",
      "Iteration 855, loss = 0.45120754\n",
      "Iteration 856, loss = 0.45090934\n",
      "Iteration 857, loss = 0.45124781\n",
      "Iteration 858, loss = 0.45079695\n",
      "Iteration 859, loss = 0.45096997\n",
      "Iteration 860, loss = 0.45078377\n",
      "Iteration 861, loss = 0.45133941\n",
      "Iteration 862, loss = 0.45057674\n",
      "Iteration 863, loss = 0.45078254\n",
      "Iteration 864, loss = 0.45087990\n",
      "Iteration 865, loss = 0.45094894\n",
      "Iteration 866, loss = 0.45085670\n",
      "Iteration 867, loss = 0.45079648\n",
      "Iteration 868, loss = 0.45061727\n",
      "Iteration 869, loss = 0.45077248\n",
      "Iteration 870, loss = 0.45074752\n",
      "Iteration 871, loss = 0.45121629\n",
      "Iteration 872, loss = 0.45109958\n",
      "Iteration 873, loss = 0.45066853\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70024705\n",
      "Iteration 2, loss = 0.68322216\n",
      "Iteration 3, loss = 0.66887863\n",
      "Iteration 4, loss = 0.65593522\n",
      "Iteration 5, loss = 0.64338822\n",
      "Iteration 6, loss = 0.63102967\n",
      "Iteration 7, loss = 0.61875926\n",
      "Iteration 8, loss = 0.60673478\n",
      "Iteration 9, loss = 0.59561258\n",
      "Iteration 10, loss = 0.58511034\n",
      "Iteration 11, loss = 0.57585897\n",
      "Iteration 12, loss = 0.56801226\n",
      "Iteration 13, loss = 0.56146747\n",
      "Iteration 14, loss = 0.55582164\n",
      "Iteration 15, loss = 0.55131427\n",
      "Iteration 16, loss = 0.54769891\n",
      "Iteration 17, loss = 0.54467376\n",
      "Iteration 18, loss = 0.54189385\n",
      "Iteration 19, loss = 0.53963503\n",
      "Iteration 20, loss = 0.53747798\n",
      "Iteration 21, loss = 0.53533976\n",
      "Iteration 22, loss = 0.53351096\n",
      "Iteration 23, loss = 0.53166398\n",
      "Iteration 24, loss = 0.53025369\n",
      "Iteration 25, loss = 0.52859946\n",
      "Iteration 26, loss = 0.52724307\n",
      "Iteration 27, loss = 0.52566962\n",
      "Iteration 28, loss = 0.52455898\n",
      "Iteration 29, loss = 0.52323870\n",
      "Iteration 30, loss = 0.52206756\n",
      "Iteration 31, loss = 0.52084035\n",
      "Iteration 32, loss = 0.51962229\n",
      "Iteration 33, loss = 0.51868308\n",
      "Iteration 34, loss = 0.51770141\n",
      "Iteration 35, loss = 0.51688731\n",
      "Iteration 36, loss = 0.51607945\n",
      "Iteration 37, loss = 0.51526793\n",
      "Iteration 38, loss = 0.51449574\n",
      "Iteration 39, loss = 0.51351911\n",
      "Iteration 40, loss = 0.51296965\n",
      "Iteration 41, loss = 0.51243771\n",
      "Iteration 42, loss = 0.51161091\n",
      "Iteration 43, loss = 0.51129072\n",
      "Iteration 44, loss = 0.51075416\n",
      "Iteration 45, loss = 0.51002905\n",
      "Iteration 46, loss = 0.50960789\n",
      "Iteration 47, loss = 0.50918701\n",
      "Iteration 48, loss = 0.50848779\n",
      "Iteration 49, loss = 0.50813165\n",
      "Iteration 50, loss = 0.50783021\n",
      "Iteration 51, loss = 0.50730952\n",
      "Iteration 52, loss = 0.50684692\n",
      "Iteration 53, loss = 0.50657182\n",
      "Iteration 54, loss = 0.50636704\n",
      "Iteration 55, loss = 0.50600690\n",
      "Iteration 56, loss = 0.50603163\n",
      "Iteration 57, loss = 0.50540726\n",
      "Iteration 58, loss = 0.50558003\n",
      "Iteration 59, loss = 0.50495920\n",
      "Iteration 60, loss = 0.50472806\n",
      "Iteration 61, loss = 0.50458817\n",
      "Iteration 62, loss = 0.50411464\n",
      "Iteration 63, loss = 0.50393160\n",
      "Iteration 64, loss = 0.50363392\n",
      "Iteration 65, loss = 0.50334749\n",
      "Iteration 66, loss = 0.50305426\n",
      "Iteration 67, loss = 0.50273108\n",
      "Iteration 68, loss = 0.50267926\n",
      "Iteration 69, loss = 0.50225524\n",
      "Iteration 70, loss = 0.50218051\n",
      "Iteration 71, loss = 0.50187222\n",
      "Iteration 72, loss = 0.50174458\n",
      "Iteration 73, loss = 0.50175753\n",
      "Iteration 74, loss = 0.50163255\n",
      "Iteration 75, loss = 0.50114073\n",
      "Iteration 76, loss = 0.50087546\n",
      "Iteration 77, loss = 0.50081336\n",
      "Iteration 78, loss = 0.50084960\n",
      "Iteration 79, loss = 0.50057499\n",
      "Iteration 80, loss = 0.50032071\n",
      "Iteration 81, loss = 0.50047353\n",
      "Iteration 82, loss = 0.49981420\n",
      "Iteration 83, loss = 0.49964341\n",
      "Iteration 84, loss = 0.49988099\n",
      "Iteration 85, loss = 0.49937060\n",
      "Iteration 86, loss = 0.49939214\n",
      "Iteration 87, loss = 0.49922913\n",
      "Iteration 88, loss = 0.49902646\n",
      "Iteration 89, loss = 0.49886996\n",
      "Iteration 90, loss = 0.49858943\n",
      "Iteration 91, loss = 0.49855384\n",
      "Iteration 92, loss = 0.49839996\n",
      "Iteration 93, loss = 0.49813511\n",
      "Iteration 94, loss = 0.49801480\n",
      "Iteration 95, loss = 0.49782247\n",
      "Iteration 96, loss = 0.49764232\n",
      "Iteration 97, loss = 0.49776818\n",
      "Iteration 98, loss = 0.49764930\n",
      "Iteration 99, loss = 0.49721504\n",
      "Iteration 100, loss = 0.49734193\n",
      "Iteration 101, loss = 0.49712368\n",
      "Iteration 102, loss = 0.49725838\n",
      "Iteration 103, loss = 0.49669574\n",
      "Iteration 104, loss = 0.49680619\n",
      "Iteration 105, loss = 0.49651621\n",
      "Iteration 106, loss = 0.49636044\n",
      "Iteration 107, loss = 0.49623473\n",
      "Iteration 108, loss = 0.49607953\n",
      "Iteration 109, loss = 0.49590385\n",
      "Iteration 110, loss = 0.49590166\n",
      "Iteration 111, loss = 0.49579154\n",
      "Iteration 112, loss = 0.49567477\n",
      "Iteration 113, loss = 0.49559115\n",
      "Iteration 114, loss = 0.49528995\n",
      "Iteration 115, loss = 0.49540898\n",
      "Iteration 116, loss = 0.49511438\n",
      "Iteration 117, loss = 0.49512966\n",
      "Iteration 118, loss = 0.49493611\n",
      "Iteration 119, loss = 0.49475950\n",
      "Iteration 120, loss = 0.49487446\n",
      "Iteration 121, loss = 0.49460008\n",
      "Iteration 122, loss = 0.49467561\n",
      "Iteration 123, loss = 0.49466232\n",
      "Iteration 124, loss = 0.49442646\n",
      "Iteration 125, loss = 0.49445690\n",
      "Iteration 126, loss = 0.49414622\n",
      "Iteration 127, loss = 0.49416136\n",
      "Iteration 128, loss = 0.49431325\n",
      "Iteration 129, loss = 0.49398541\n",
      "Iteration 130, loss = 0.49409598\n",
      "Iteration 131, loss = 0.49383950\n",
      "Iteration 132, loss = 0.49397266\n",
      "Iteration 133, loss = 0.49367019\n",
      "Iteration 134, loss = 0.49363898\n",
      "Iteration 135, loss = 0.49366296\n",
      "Iteration 136, loss = 0.49355913\n",
      "Iteration 137, loss = 0.49332800\n",
      "Iteration 138, loss = 0.49354356\n",
      "Iteration 139, loss = 0.49324523\n",
      "Iteration 140, loss = 0.49301648\n",
      "Iteration 141, loss = 0.49311757\n",
      "Iteration 142, loss = 0.49294954\n",
      "Iteration 143, loss = 0.49298237\n",
      "Iteration 144, loss = 0.49284820\n",
      "Iteration 145, loss = 0.49292319\n",
      "Iteration 146, loss = 0.49303097\n",
      "Iteration 147, loss = 0.49283280\n",
      "Iteration 148, loss = 0.49278610\n",
      "Iteration 149, loss = 0.49254067\n",
      "Iteration 150, loss = 0.49258809\n",
      "Iteration 151, loss = 0.49249030\n",
      "Iteration 152, loss = 0.49254143\n",
      "Iteration 153, loss = 0.49234937\n",
      "Iteration 154, loss = 0.49233367\n",
      "Iteration 155, loss = 0.49242593\n",
      "Iteration 156, loss = 0.49271565\n",
      "Iteration 157, loss = 0.49215654\n",
      "Iteration 158, loss = 0.49209021\n",
      "Iteration 159, loss = 0.49206809\n",
      "Iteration 160, loss = 0.49189907\n",
      "Iteration 161, loss = 0.49207823\n",
      "Iteration 162, loss = 0.49172272\n",
      "Iteration 163, loss = 0.49170798\n",
      "Iteration 164, loss = 0.49179781\n",
      "Iteration 165, loss = 0.49168615\n",
      "Iteration 166, loss = 0.49154055\n",
      "Iteration 167, loss = 0.49143988\n",
      "Iteration 168, loss = 0.49158600\n",
      "Iteration 169, loss = 0.49119131\n",
      "Iteration 170, loss = 0.49107029\n",
      "Iteration 171, loss = 0.49102249\n",
      "Iteration 172, loss = 0.49105144\n",
      "Iteration 173, loss = 0.49109834\n",
      "Iteration 174, loss = 0.49114020\n",
      "Iteration 175, loss = 0.49109347\n",
      "Iteration 176, loss = 0.49073865\n",
      "Iteration 177, loss = 0.49074693\n",
      "Iteration 178, loss = 0.49090732\n",
      "Iteration 179, loss = 0.49088759\n",
      "Iteration 180, loss = 0.49027291\n",
      "Iteration 181, loss = 0.49017116\n",
      "Iteration 182, loss = 0.49008305\n",
      "Iteration 183, loss = 0.49002214\n",
      "Iteration 184, loss = 0.49007314\n",
      "Iteration 185, loss = 0.48987005\n",
      "Iteration 186, loss = 0.48971570\n",
      "Iteration 187, loss = 0.48966529\n",
      "Iteration 188, loss = 0.48951292\n",
      "Iteration 189, loss = 0.48965771\n",
      "Iteration 190, loss = 0.48983684\n",
      "Iteration 191, loss = 0.48939001\n",
      "Iteration 192, loss = 0.48943138\n",
      "Iteration 193, loss = 0.48937488\n",
      "Iteration 194, loss = 0.48917600\n",
      "Iteration 195, loss = 0.48892994\n",
      "Iteration 196, loss = 0.48901803\n",
      "Iteration 197, loss = 0.48907641\n",
      "Iteration 198, loss = 0.48883493\n",
      "Iteration 199, loss = 0.48885821\n",
      "Iteration 200, loss = 0.48881289\n",
      "Iteration 201, loss = 0.48891263\n",
      "Iteration 202, loss = 0.48862017\n",
      "Iteration 203, loss = 0.48893231\n",
      "Iteration 204, loss = 0.48851381\n",
      "Iteration 205, loss = 0.48834637\n",
      "Iteration 206, loss = 0.48857616\n",
      "Iteration 207, loss = 0.48845066\n",
      "Iteration 208, loss = 0.48834693\n",
      "Iteration 209, loss = 0.48821107\n",
      "Iteration 210, loss = 0.48828254\n",
      "Iteration 211, loss = 0.48779811\n",
      "Iteration 212, loss = 0.48838351\n",
      "Iteration 213, loss = 0.48794696\n",
      "Iteration 214, loss = 0.48782626\n",
      "Iteration 215, loss = 0.48755200\n",
      "Iteration 216, loss = 0.48767434\n",
      "Iteration 217, loss = 0.48738816\n",
      "Iteration 218, loss = 0.48745655\n",
      "Iteration 219, loss = 0.48744632\n",
      "Iteration 220, loss = 0.48764910\n",
      "Iteration 221, loss = 0.48724869\n",
      "Iteration 222, loss = 0.48701646\n",
      "Iteration 223, loss = 0.48701267\n",
      "Iteration 224, loss = 0.48718695\n",
      "Iteration 225, loss = 0.48756165\n",
      "Iteration 226, loss = 0.48747821\n",
      "Iteration 227, loss = 0.48664789\n",
      "Iteration 228, loss = 0.48675981\n",
      "Iteration 229, loss = 0.48698239\n",
      "Iteration 230, loss = 0.48678941\n",
      "Iteration 231, loss = 0.48647782\n",
      "Iteration 232, loss = 0.48634506\n",
      "Iteration 233, loss = 0.48633882\n",
      "Iteration 234, loss = 0.48620576\n",
      "Iteration 235, loss = 0.48621980\n",
      "Iteration 236, loss = 0.48600099\n",
      "Iteration 237, loss = 0.48625041\n",
      "Iteration 238, loss = 0.48590994\n",
      "Iteration 239, loss = 0.48593357\n",
      "Iteration 240, loss = 0.48585189\n",
      "Iteration 241, loss = 0.48591690\n",
      "Iteration 242, loss = 0.48555534\n",
      "Iteration 243, loss = 0.48564266\n",
      "Iteration 244, loss = 0.48597708\n",
      "Iteration 245, loss = 0.48533683\n",
      "Iteration 246, loss = 0.48560028\n",
      "Iteration 247, loss = 0.48541788\n",
      "Iteration 248, loss = 0.48540811\n",
      "Iteration 249, loss = 0.48516644\n",
      "Iteration 250, loss = 0.48514699\n",
      "Iteration 251, loss = 0.48507399\n",
      "Iteration 252, loss = 0.48475480\n",
      "Iteration 253, loss = 0.48481738\n",
      "Iteration 254, loss = 0.48485031\n",
      "Iteration 255, loss = 0.48461009\n",
      "Iteration 256, loss = 0.48497374\n",
      "Iteration 257, loss = 0.48445804\n",
      "Iteration 258, loss = 0.48450473\n",
      "Iteration 259, loss = 0.48426181\n",
      "Iteration 260, loss = 0.48430239\n",
      "Iteration 261, loss = 0.48448831\n",
      "Iteration 262, loss = 0.48465387\n",
      "Iteration 263, loss = 0.48405985\n",
      "Iteration 264, loss = 0.48405171\n",
      "Iteration 265, loss = 0.48397669\n",
      "Iteration 266, loss = 0.48402492\n",
      "Iteration 267, loss = 0.48383198\n",
      "Iteration 268, loss = 0.48380067\n",
      "Iteration 269, loss = 0.48392575\n",
      "Iteration 270, loss = 0.48382785\n",
      "Iteration 271, loss = 0.48346686\n",
      "Iteration 272, loss = 0.48358014\n",
      "Iteration 273, loss = 0.48344426\n",
      "Iteration 274, loss = 0.48335025\n",
      "Iteration 275, loss = 0.48332648\n",
      "Iteration 276, loss = 0.48334192\n",
      "Iteration 277, loss = 0.48314062\n",
      "Iteration 278, loss = 0.48314243\n",
      "Iteration 279, loss = 0.48309069\n",
      "Iteration 280, loss = 0.48303326\n",
      "Iteration 281, loss = 0.48302892\n",
      "Iteration 282, loss = 0.48302480\n",
      "Iteration 283, loss = 0.48365550\n",
      "Iteration 284, loss = 0.48351433\n",
      "Iteration 285, loss = 0.48363358\n",
      "Iteration 286, loss = 0.48272683\n",
      "Iteration 287, loss = 0.48292226\n",
      "Iteration 288, loss = 0.48257350\n",
      "Iteration 289, loss = 0.48267748\n",
      "Iteration 290, loss = 0.48252063\n",
      "Iteration 291, loss = 0.48255324\n",
      "Iteration 292, loss = 0.48258184\n",
      "Iteration 293, loss = 0.48269209\n",
      "Iteration 294, loss = 0.48234245\n",
      "Iteration 295, loss = 0.48218320\n",
      "Iteration 296, loss = 0.48226569\n",
      "Iteration 297, loss = 0.48384632\n",
      "Iteration 298, loss = 0.48231050\n",
      "Iteration 299, loss = 0.48235414\n",
      "Iteration 300, loss = 0.48201891\n",
      "Iteration 301, loss = 0.48207833\n",
      "Iteration 302, loss = 0.48195383\n",
      "Iteration 303, loss = 0.48186009\n",
      "Iteration 304, loss = 0.48163965\n",
      "Iteration 305, loss = 0.48181214\n",
      "Iteration 306, loss = 0.48187108\n",
      "Iteration 307, loss = 0.48195903\n",
      "Iteration 308, loss = 0.48151992\n",
      "Iteration 309, loss = 0.48150417\n",
      "Iteration 310, loss = 0.48154694\n",
      "Iteration 311, loss = 0.48153852\n",
      "Iteration 312, loss = 0.48141371\n",
      "Iteration 313, loss = 0.48153670\n",
      "Iteration 314, loss = 0.48168072\n",
      "Iteration 315, loss = 0.48162651\n",
      "Iteration 316, loss = 0.48158858\n",
      "Iteration 317, loss = 0.48134520\n",
      "Iteration 318, loss = 0.48111311\n",
      "Iteration 319, loss = 0.48108482\n",
      "Iteration 320, loss = 0.48108954\n",
      "Iteration 321, loss = 0.48102067\n",
      "Iteration 322, loss = 0.48091663\n",
      "Iteration 323, loss = 0.48119222\n",
      "Iteration 324, loss = 0.48081683\n",
      "Iteration 325, loss = 0.48092775\n",
      "Iteration 326, loss = 0.48101553\n",
      "Iteration 327, loss = 0.48072062\n",
      "Iteration 328, loss = 0.48082134\n",
      "Iteration 329, loss = 0.48132922\n",
      "Iteration 330, loss = 0.48107243\n",
      "Iteration 331, loss = 0.48146552\n",
      "Iteration 332, loss = 0.48093971\n",
      "Iteration 333, loss = 0.48126956\n",
      "Iteration 334, loss = 0.48077636\n",
      "Iteration 335, loss = 0.48055909\n",
      "Iteration 336, loss = 0.48045335\n",
      "Iteration 337, loss = 0.48056320\n",
      "Iteration 338, loss = 0.48028517\n",
      "Iteration 339, loss = 0.48058804\n",
      "Iteration 340, loss = 0.48019002\n",
      "Iteration 341, loss = 0.48040840\n",
      "Iteration 342, loss = 0.48053947\n",
      "Iteration 343, loss = 0.48022838\n",
      "Iteration 344, loss = 0.48051235\n",
      "Iteration 345, loss = 0.48060982\n",
      "Iteration 346, loss = 0.48048537\n",
      "Iteration 347, loss = 0.48094414\n",
      "Iteration 348, loss = 0.48009919\n",
      "Iteration 349, loss = 0.48045742\n",
      "Iteration 350, loss = 0.48006682\n",
      "Iteration 351, loss = 0.47997110\n",
      "Iteration 352, loss = 0.48012378\n",
      "Iteration 353, loss = 0.48004856\n",
      "Iteration 354, loss = 0.47988078\n",
      "Iteration 355, loss = 0.47988432\n",
      "Iteration 356, loss = 0.47973960\n",
      "Iteration 357, loss = 0.48026404\n",
      "Iteration 358, loss = 0.47972557\n",
      "Iteration 359, loss = 0.47966149\n",
      "Iteration 360, loss = 0.47960620\n",
      "Iteration 361, loss = 0.47968621\n",
      "Iteration 362, loss = 0.47951510\n",
      "Iteration 363, loss = 0.47934438\n",
      "Iteration 364, loss = 0.47957312\n",
      "Iteration 365, loss = 0.47962794\n",
      "Iteration 366, loss = 0.47941160\n",
      "Iteration 367, loss = 0.47937741\n",
      "Iteration 368, loss = 0.47954718\n",
      "Iteration 369, loss = 0.47923597\n",
      "Iteration 370, loss = 0.47934257\n",
      "Iteration 371, loss = 0.47935670\n",
      "Iteration 372, loss = 0.47943013\n",
      "Iteration 373, loss = 0.47931284\n",
      "Iteration 374, loss = 0.47903821\n",
      "Iteration 375, loss = 0.47904425\n",
      "Iteration 376, loss = 0.47909521\n",
      "Iteration 377, loss = 0.47923313\n",
      "Iteration 378, loss = 0.47901970\n",
      "Iteration 379, loss = 0.47878497\n",
      "Iteration 380, loss = 0.47898665\n",
      "Iteration 381, loss = 0.47894987\n",
      "Iteration 382, loss = 0.47877812\n",
      "Iteration 383, loss = 0.47871249\n",
      "Iteration 384, loss = 0.47868854\n",
      "Iteration 385, loss = 0.47865588\n",
      "Iteration 386, loss = 0.47892976\n",
      "Iteration 387, loss = 0.47846874\n",
      "Iteration 388, loss = 0.47878814\n",
      "Iteration 389, loss = 0.47895811\n",
      "Iteration 390, loss = 0.47864860\n",
      "Iteration 391, loss = 0.47842047\n",
      "Iteration 392, loss = 0.47814461\n",
      "Iteration 393, loss = 0.47818535\n",
      "Iteration 394, loss = 0.47823169\n",
      "Iteration 395, loss = 0.47855322\n",
      "Iteration 396, loss = 0.47819532\n",
      "Iteration 397, loss = 0.47809286\n",
      "Iteration 398, loss = 0.47806003\n",
      "Iteration 399, loss = 0.47803348\n",
      "Iteration 400, loss = 0.47801778\n",
      "Iteration 401, loss = 0.47837738\n",
      "Iteration 402, loss = 0.47817034\n",
      "Iteration 403, loss = 0.47806169\n",
      "Iteration 404, loss = 0.47762857\n",
      "Iteration 405, loss = 0.47790429\n",
      "Iteration 406, loss = 0.47779077\n",
      "Iteration 407, loss = 0.47762628\n",
      "Iteration 408, loss = 0.47786077\n",
      "Iteration 409, loss = 0.47780677\n",
      "Iteration 410, loss = 0.47766254\n",
      "Iteration 411, loss = 0.47754499\n",
      "Iteration 412, loss = 0.47750969\n",
      "Iteration 413, loss = 0.47743799\n",
      "Iteration 414, loss = 0.47737247\n",
      "Iteration 415, loss = 0.47734621\n",
      "Iteration 416, loss = 0.47733343\n",
      "Iteration 417, loss = 0.47732302\n",
      "Iteration 418, loss = 0.47717590\n",
      "Iteration 419, loss = 0.47715169\n",
      "Iteration 420, loss = 0.47749574\n",
      "Iteration 421, loss = 0.47758543\n",
      "Iteration 422, loss = 0.47761917\n",
      "Iteration 423, loss = 0.47696852\n",
      "Iteration 424, loss = 0.47716088\n",
      "Iteration 425, loss = 0.47723492\n",
      "Iteration 426, loss = 0.47723955\n",
      "Iteration 427, loss = 0.47703338\n",
      "Iteration 428, loss = 0.47723584\n",
      "Iteration 429, loss = 0.47693305\n",
      "Iteration 430, loss = 0.47701367\n",
      "Iteration 431, loss = 0.47695656\n",
      "Iteration 432, loss = 0.47684646\n",
      "Iteration 433, loss = 0.47675387\n",
      "Iteration 434, loss = 0.47722119\n",
      "Iteration 435, loss = 0.47682422\n",
      "Iteration 436, loss = 0.47681618\n",
      "Iteration 437, loss = 0.47687507\n",
      "Iteration 438, loss = 0.47667116\n",
      "Iteration 439, loss = 0.47682790\n",
      "Iteration 440, loss = 0.47671916\n",
      "Iteration 441, loss = 0.47644550\n",
      "Iteration 442, loss = 0.47651490\n",
      "Iteration 443, loss = 0.47665411\n",
      "Iteration 444, loss = 0.47631766\n",
      "Iteration 445, loss = 0.47691118\n",
      "Iteration 446, loss = 0.47663984\n",
      "Iteration 447, loss = 0.47634941\n",
      "Iteration 448, loss = 0.47665364\n",
      "Iteration 449, loss = 0.47655403\n",
      "Iteration 450, loss = 0.47626154\n",
      "Iteration 451, loss = 0.47644505\n",
      "Iteration 452, loss = 0.47664849\n",
      "Iteration 453, loss = 0.47622890\n",
      "Iteration 454, loss = 0.47624991\n",
      "Iteration 455, loss = 0.47622884\n",
      "Iteration 456, loss = 0.47623295\n",
      "Iteration 457, loss = 0.47635390\n",
      "Iteration 458, loss = 0.47636699\n",
      "Iteration 459, loss = 0.47611651\n",
      "Iteration 460, loss = 0.47628828\n",
      "Iteration 461, loss = 0.47599535\n",
      "Iteration 462, loss = 0.47624208\n",
      "Iteration 463, loss = 0.47599211\n",
      "Iteration 464, loss = 0.47605940\n",
      "Iteration 465, loss = 0.47602605\n",
      "Iteration 466, loss = 0.47599022\n",
      "Iteration 467, loss = 0.47593180\n",
      "Iteration 468, loss = 0.47573561\n",
      "Iteration 469, loss = 0.47588626\n",
      "Iteration 470, loss = 0.47577256\n",
      "Iteration 471, loss = 0.47581604\n",
      "Iteration 472, loss = 0.47603281\n",
      "Iteration 473, loss = 0.47564661\n",
      "Iteration 474, loss = 0.47591351\n",
      "Iteration 475, loss = 0.47568074\n",
      "Iteration 476, loss = 0.47580615\n",
      "Iteration 477, loss = 0.47605450\n",
      "Iteration 478, loss = 0.47595221\n",
      "Iteration 479, loss = 0.47586546\n",
      "Iteration 480, loss = 0.47588621\n",
      "Iteration 481, loss = 0.47554716\n",
      "Iteration 482, loss = 0.47603450\n",
      "Iteration 483, loss = 0.47613584\n",
      "Iteration 484, loss = 0.47549939\n",
      "Iteration 485, loss = 0.47578202\n",
      "Iteration 486, loss = 0.47553480\n",
      "Iteration 487, loss = 0.47558658\n",
      "Iteration 488, loss = 0.47547332\n",
      "Iteration 489, loss = 0.47561928\n",
      "Iteration 490, loss = 0.47556149\n",
      "Iteration 491, loss = 0.47556336\n",
      "Iteration 492, loss = 0.47539945\n",
      "Iteration 493, loss = 0.47551458\n",
      "Iteration 494, loss = 0.47572374\n",
      "Iteration 495, loss = 0.47534072\n",
      "Iteration 496, loss = 0.47531364\n",
      "Iteration 497, loss = 0.47532362\n",
      "Iteration 498, loss = 0.47537232\n",
      "Iteration 499, loss = 0.47541270\n",
      "Iteration 500, loss = 0.47571397\n",
      "Iteration 501, loss = 0.47525584\n",
      "Iteration 502, loss = 0.47509609\n",
      "Iteration 503, loss = 0.47529321\n",
      "Iteration 504, loss = 0.47539827\n",
      "Iteration 505, loss = 0.47538908\n",
      "Iteration 506, loss = 0.47488439\n",
      "Iteration 507, loss = 0.47530931\n",
      "Iteration 508, loss = 0.47496984\n",
      "Iteration 509, loss = 0.47545552\n",
      "Iteration 510, loss = 0.47515502\n",
      "Iteration 511, loss = 0.47611629\n",
      "Iteration 512, loss = 0.47539710\n",
      "Iteration 513, loss = 0.47489758\n",
      "Iteration 514, loss = 0.47489872\n",
      "Iteration 515, loss = 0.47515145\n",
      "Iteration 516, loss = 0.47495802\n",
      "Iteration 517, loss = 0.47480969\n",
      "Iteration 518, loss = 0.47485215\n",
      "Iteration 519, loss = 0.47538220\n",
      "Iteration 520, loss = 0.47481052\n",
      "Iteration 521, loss = 0.47464686\n",
      "Iteration 522, loss = 0.47503937\n",
      "Iteration 523, loss = 0.47479771\n",
      "Iteration 524, loss = 0.47488612\n",
      "Iteration 525, loss = 0.47462164\n",
      "Iteration 526, loss = 0.47504033\n",
      "Iteration 527, loss = 0.47456243\n",
      "Iteration 528, loss = 0.47473650\n",
      "Iteration 529, loss = 0.47464197\n",
      "Iteration 530, loss = 0.47455113\n",
      "Iteration 531, loss = 0.47458869\n",
      "Iteration 532, loss = 0.47451650\n",
      "Iteration 533, loss = 0.47457687\n",
      "Iteration 534, loss = 0.47452536\n",
      "Iteration 535, loss = 0.47461757\n",
      "Iteration 536, loss = 0.47484099\n",
      "Iteration 537, loss = 0.47458465\n",
      "Iteration 538, loss = 0.47495052\n",
      "Iteration 539, loss = 0.47474709\n",
      "Iteration 540, loss = 0.47465803\n",
      "Iteration 541, loss = 0.47434950\n",
      "Iteration 542, loss = 0.47461615\n",
      "Iteration 543, loss = 0.47445872\n",
      "Iteration 544, loss = 0.47441204\n",
      "Iteration 545, loss = 0.47445234\n",
      "Iteration 546, loss = 0.47454823\n",
      "Iteration 547, loss = 0.47444909\n",
      "Iteration 548, loss = 0.47441376\n",
      "Iteration 549, loss = 0.47471876\n",
      "Iteration 550, loss = 0.47443918\n",
      "Iteration 551, loss = 0.47438418\n",
      "Iteration 552, loss = 0.47431071\n",
      "Iteration 553, loss = 0.47441593\n",
      "Iteration 554, loss = 0.47429416\n",
      "Iteration 555, loss = 0.47432708\n",
      "Iteration 556, loss = 0.47422841\n",
      "Iteration 557, loss = 0.47414724\n",
      "Iteration 558, loss = 0.47452367\n",
      "Iteration 559, loss = 0.47433374\n",
      "Iteration 560, loss = 0.47452441\n",
      "Iteration 561, loss = 0.47440044\n",
      "Iteration 562, loss = 0.47449701\n",
      "Iteration 563, loss = 0.47449586\n",
      "Iteration 564, loss = 0.47412723\n",
      "Iteration 565, loss = 0.47408757\n",
      "Iteration 566, loss = 0.47430563\n",
      "Iteration 567, loss = 0.47420533\n",
      "Iteration 568, loss = 0.47421660\n",
      "Iteration 569, loss = 0.47386643\n",
      "Iteration 570, loss = 0.47419426\n",
      "Iteration 571, loss = 0.47407529\n",
      "Iteration 572, loss = 0.47399604\n",
      "Iteration 573, loss = 0.47445616\n",
      "Iteration 574, loss = 0.47429576\n",
      "Iteration 575, loss = 0.47421687\n",
      "Iteration 576, loss = 0.47401281\n",
      "Iteration 577, loss = 0.47413686\n",
      "Iteration 578, loss = 0.47380447\n",
      "Iteration 579, loss = 0.47403623\n",
      "Iteration 580, loss = 0.47391119\n",
      "Iteration 581, loss = 0.47387660\n",
      "Iteration 582, loss = 0.47383484\n",
      "Iteration 583, loss = 0.47387505\n",
      "Iteration 584, loss = 0.47394809\n",
      "Iteration 585, loss = 0.47384726\n",
      "Iteration 586, loss = 0.47386786\n",
      "Iteration 587, loss = 0.47371021\n",
      "Iteration 588, loss = 0.47404425\n",
      "Iteration 589, loss = 0.47411319\n",
      "Iteration 590, loss = 0.47370306\n",
      "Iteration 591, loss = 0.47382511\n",
      "Iteration 592, loss = 0.47370408\n",
      "Iteration 593, loss = 0.47400446\n",
      "Iteration 594, loss = 0.47390843\n",
      "Iteration 595, loss = 0.47384071\n",
      "Iteration 596, loss = 0.47391407\n",
      "Iteration 597, loss = 0.47421947\n",
      "Iteration 598, loss = 0.47367109\n",
      "Iteration 599, loss = 0.47367247\n",
      "Iteration 600, loss = 0.47378836\n",
      "Iteration 601, loss = 0.47363111\n",
      "Iteration 602, loss = 0.47387243\n",
      "Iteration 603, loss = 0.47363889\n",
      "Iteration 604, loss = 0.47368320\n",
      "Iteration 605, loss = 0.47360440\n",
      "Iteration 606, loss = 0.47383959\n",
      "Iteration 607, loss = 0.47358537\n",
      "Iteration 608, loss = 0.47358638\n",
      "Iteration 609, loss = 0.47358087\n",
      "Iteration 610, loss = 0.47391245\n",
      "Iteration 611, loss = 0.47348374\n",
      "Iteration 612, loss = 0.47392264\n",
      "Iteration 613, loss = 0.47384924\n",
      "Iteration 614, loss = 0.47371660\n",
      "Iteration 615, loss = 0.47399679\n",
      "Iteration 616, loss = 0.47357636\n",
      "Iteration 617, loss = 0.47350606\n",
      "Iteration 618, loss = 0.47391443\n",
      "Iteration 619, loss = 0.47364483\n",
      "Iteration 620, loss = 0.47384970\n",
      "Iteration 621, loss = 0.47349715\n",
      "Iteration 622, loss = 0.47397505\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71258691\n",
      "Iteration 2, loss = 0.68513831\n",
      "Iteration 3, loss = 0.66389003\n",
      "Iteration 4, loss = 0.64753207\n",
      "Iteration 5, loss = 0.63336362\n",
      "Iteration 6, loss = 0.62060398\n",
      "Iteration 7, loss = 0.60986222\n",
      "Iteration 8, loss = 0.60024009\n",
      "Iteration 9, loss = 0.59211260\n",
      "Iteration 10, loss = 0.58548415\n",
      "Iteration 11, loss = 0.58011639\n",
      "Iteration 12, loss = 0.57579292\n",
      "Iteration 13, loss = 0.57235386\n",
      "Iteration 14, loss = 0.56970627\n",
      "Iteration 15, loss = 0.56751428\n",
      "Iteration 16, loss = 0.56557397\n",
      "Iteration 17, loss = 0.56409744\n",
      "Iteration 18, loss = 0.56273404\n",
      "Iteration 19, loss = 0.56134769\n",
      "Iteration 20, loss = 0.56030032\n",
      "Iteration 21, loss = 0.55911108\n",
      "Iteration 22, loss = 0.55815910\n",
      "Iteration 23, loss = 0.55732822\n",
      "Iteration 24, loss = 0.55640445\n",
      "Iteration 25, loss = 0.55564209\n",
      "Iteration 26, loss = 0.55483239\n",
      "Iteration 27, loss = 0.55369196\n",
      "Iteration 28, loss = 0.55286789\n",
      "Iteration 29, loss = 0.55214236\n",
      "Iteration 30, loss = 0.55122786\n",
      "Iteration 31, loss = 0.55046709\n",
      "Iteration 32, loss = 0.54958832\n",
      "Iteration 33, loss = 0.54875353\n",
      "Iteration 34, loss = 0.54777472\n",
      "Iteration 35, loss = 0.54696994\n",
      "Iteration 36, loss = 0.54580895\n",
      "Iteration 37, loss = 0.54520448\n",
      "Iteration 38, loss = 0.54404960\n",
      "Iteration 39, loss = 0.54305955\n",
      "Iteration 40, loss = 0.54201476\n",
      "Iteration 41, loss = 0.54137850\n",
      "Iteration 42, loss = 0.54014891\n",
      "Iteration 43, loss = 0.53961598\n",
      "Iteration 44, loss = 0.53866547\n",
      "Iteration 45, loss = 0.53784210\n",
      "Iteration 46, loss = 0.53709423\n",
      "Iteration 47, loss = 0.53645974\n",
      "Iteration 48, loss = 0.53569005\n",
      "Iteration 49, loss = 0.53487529\n",
      "Iteration 50, loss = 0.53415432\n",
      "Iteration 51, loss = 0.53343060\n",
      "Iteration 52, loss = 0.53291194\n",
      "Iteration 53, loss = 0.53220328\n",
      "Iteration 54, loss = 0.53112189\n",
      "Iteration 55, loss = 0.53061838\n",
      "Iteration 56, loss = 0.52967165\n",
      "Iteration 57, loss = 0.52896780\n",
      "Iteration 58, loss = 0.52821202\n",
      "Iteration 59, loss = 0.52748293\n",
      "Iteration 60, loss = 0.52632930\n",
      "Iteration 61, loss = 0.52564930\n",
      "Iteration 62, loss = 0.52474295\n",
      "Iteration 63, loss = 0.52421997\n",
      "Iteration 64, loss = 0.52319904\n",
      "Iteration 65, loss = 0.52230718\n",
      "Iteration 66, loss = 0.52127637\n",
      "Iteration 67, loss = 0.52084590\n",
      "Iteration 68, loss = 0.51992023\n",
      "Iteration 69, loss = 0.51893879\n",
      "Iteration 70, loss = 0.51842983\n",
      "Iteration 71, loss = 0.51733592\n",
      "Iteration 72, loss = 0.51636049\n",
      "Iteration 73, loss = 0.51565769\n",
      "Iteration 74, loss = 0.51454481\n",
      "Iteration 75, loss = 0.51385781\n",
      "Iteration 76, loss = 0.51320276\n",
      "Iteration 77, loss = 0.51231619\n",
      "Iteration 78, loss = 0.51159860\n",
      "Iteration 79, loss = 0.51103443\n",
      "Iteration 80, loss = 0.51055065\n",
      "Iteration 81, loss = 0.50971591\n",
      "Iteration 82, loss = 0.50887151\n",
      "Iteration 83, loss = 0.50803160\n",
      "Iteration 84, loss = 0.50777876\n",
      "Iteration 85, loss = 0.50708672\n",
      "Iteration 86, loss = 0.50638172\n",
      "Iteration 87, loss = 0.50584240\n",
      "Iteration 88, loss = 0.50518501\n",
      "Iteration 89, loss = 0.50454771\n",
      "Iteration 90, loss = 0.50416765\n",
      "Iteration 91, loss = 0.50353910\n",
      "Iteration 92, loss = 0.50305217\n",
      "Iteration 93, loss = 0.50242199\n",
      "Iteration 94, loss = 0.50234677\n",
      "Iteration 95, loss = 0.50172429\n",
      "Iteration 96, loss = 0.50114272\n",
      "Iteration 97, loss = 0.50080441\n",
      "Iteration 98, loss = 0.50036588\n",
      "Iteration 99, loss = 0.49982524\n",
      "Iteration 100, loss = 0.49951595\n",
      "Iteration 101, loss = 0.49910816\n",
      "Iteration 102, loss = 0.49885244\n",
      "Iteration 103, loss = 0.49829384\n",
      "Iteration 104, loss = 0.49810053\n",
      "Iteration 105, loss = 0.49746221\n",
      "Iteration 106, loss = 0.49730518\n",
      "Iteration 107, loss = 0.49690033\n",
      "Iteration 108, loss = 0.49671715\n",
      "Iteration 109, loss = 0.49640798\n",
      "Iteration 110, loss = 0.49608003\n",
      "Iteration 111, loss = 0.49594739\n",
      "Iteration 112, loss = 0.49572369\n",
      "Iteration 113, loss = 0.49589165\n",
      "Iteration 114, loss = 0.49507371\n",
      "Iteration 115, loss = 0.49498245\n",
      "Iteration 116, loss = 0.49440380\n",
      "Iteration 117, loss = 0.49425766\n",
      "Iteration 118, loss = 0.49427722\n",
      "Iteration 119, loss = 0.49370476\n",
      "Iteration 120, loss = 0.49357000\n",
      "Iteration 121, loss = 0.49312521\n",
      "Iteration 122, loss = 0.49316939\n",
      "Iteration 123, loss = 0.49307668\n",
      "Iteration 124, loss = 0.49236445\n",
      "Iteration 125, loss = 0.49229591\n",
      "Iteration 126, loss = 0.49224727\n",
      "Iteration 127, loss = 0.49203363\n",
      "Iteration 128, loss = 0.49155620\n",
      "Iteration 129, loss = 0.49151830\n",
      "Iteration 130, loss = 0.49132283\n",
      "Iteration 131, loss = 0.49103020\n",
      "Iteration 132, loss = 0.49091337\n",
      "Iteration 133, loss = 0.49074879\n",
      "Iteration 134, loss = 0.49041288\n",
      "Iteration 135, loss = 0.49018019\n",
      "Iteration 136, loss = 0.49017000\n",
      "Iteration 137, loss = 0.49002470\n",
      "Iteration 138, loss = 0.48964008\n",
      "Iteration 139, loss = 0.48989286\n",
      "Iteration 140, loss = 0.49006706\n",
      "Iteration 141, loss = 0.48961686\n",
      "Iteration 142, loss = 0.48938859\n",
      "Iteration 143, loss = 0.48902104\n",
      "Iteration 144, loss = 0.48923085\n",
      "Iteration 145, loss = 0.48880449\n",
      "Iteration 146, loss = 0.48886264\n",
      "Iteration 147, loss = 0.48854839\n",
      "Iteration 148, loss = 0.48862183\n",
      "Iteration 149, loss = 0.48841046\n",
      "Iteration 150, loss = 0.48823989\n",
      "Iteration 151, loss = 0.48856732\n",
      "Iteration 152, loss = 0.48840256\n",
      "Iteration 153, loss = 0.48785206\n",
      "Iteration 154, loss = 0.48843336\n",
      "Iteration 155, loss = 0.48779761\n",
      "Iteration 156, loss = 0.48831192\n",
      "Iteration 157, loss = 0.48752676\n",
      "Iteration 158, loss = 0.48762536\n",
      "Iteration 159, loss = 0.48727541\n",
      "Iteration 160, loss = 0.48732842\n",
      "Iteration 161, loss = 0.48729382\n",
      "Iteration 162, loss = 0.48715866\n",
      "Iteration 163, loss = 0.48709848\n",
      "Iteration 164, loss = 0.48698368\n",
      "Iteration 165, loss = 0.48695728\n",
      "Iteration 166, loss = 0.48690285\n",
      "Iteration 167, loss = 0.48729886\n",
      "Iteration 168, loss = 0.48683300\n",
      "Iteration 169, loss = 0.48661229\n",
      "Iteration 170, loss = 0.48690088\n",
      "Iteration 171, loss = 0.48622738\n",
      "Iteration 172, loss = 0.48654243\n",
      "Iteration 173, loss = 0.48609488\n",
      "Iteration 174, loss = 0.48608901\n",
      "Iteration 175, loss = 0.48617610\n",
      "Iteration 176, loss = 0.48598041\n",
      "Iteration 177, loss = 0.48620695\n",
      "Iteration 178, loss = 0.48637231\n",
      "Iteration 179, loss = 0.48605115\n",
      "Iteration 180, loss = 0.48610417\n",
      "Iteration 181, loss = 0.48585321\n",
      "Iteration 182, loss = 0.48610645\n",
      "Iteration 183, loss = 0.48558624\n",
      "Iteration 184, loss = 0.48574244\n",
      "Iteration 185, loss = 0.48555747\n",
      "Iteration 186, loss = 0.48524754\n",
      "Iteration 187, loss = 0.48538439\n",
      "Iteration 188, loss = 0.48527219\n",
      "Iteration 189, loss = 0.48525587\n",
      "Iteration 190, loss = 0.48519610\n",
      "Iteration 191, loss = 0.48511907\n",
      "Iteration 192, loss = 0.48525250\n",
      "Iteration 193, loss = 0.48494940\n",
      "Iteration 194, loss = 0.48514360\n",
      "Iteration 195, loss = 0.48487120\n",
      "Iteration 196, loss = 0.48495012\n",
      "Iteration 197, loss = 0.48491319\n",
      "Iteration 198, loss = 0.48478859\n",
      "Iteration 199, loss = 0.48492385\n",
      "Iteration 200, loss = 0.48483954\n",
      "Iteration 201, loss = 0.48473695\n",
      "Iteration 202, loss = 0.48467310\n",
      "Iteration 203, loss = 0.48451419\n",
      "Iteration 204, loss = 0.48501237\n",
      "Iteration 205, loss = 0.48450700\n",
      "Iteration 206, loss = 0.48445015\n",
      "Iteration 207, loss = 0.48454612\n",
      "Iteration 208, loss = 0.48429460\n",
      "Iteration 209, loss = 0.48489020\n",
      "Iteration 210, loss = 0.48429271\n",
      "Iteration 211, loss = 0.48426725\n",
      "Iteration 212, loss = 0.48470559\n",
      "Iteration 213, loss = 0.48410208\n",
      "Iteration 214, loss = 0.48433445\n",
      "Iteration 215, loss = 0.48391677\n",
      "Iteration 216, loss = 0.48408790\n",
      "Iteration 217, loss = 0.48374152\n",
      "Iteration 218, loss = 0.48409236\n",
      "Iteration 219, loss = 0.48393650\n",
      "Iteration 220, loss = 0.48397903\n",
      "Iteration 221, loss = 0.48386821\n",
      "Iteration 222, loss = 0.48340429\n",
      "Iteration 223, loss = 0.48357579\n",
      "Iteration 224, loss = 0.48383084\n",
      "Iteration 225, loss = 0.48350274\n",
      "Iteration 226, loss = 0.48373415\n",
      "Iteration 227, loss = 0.48360074\n",
      "Iteration 228, loss = 0.48335457\n",
      "Iteration 229, loss = 0.48333434\n",
      "Iteration 230, loss = 0.48304735\n",
      "Iteration 231, loss = 0.48311069\n",
      "Iteration 232, loss = 0.48330872\n",
      "Iteration 233, loss = 0.48329377\n",
      "Iteration 234, loss = 0.48298067\n",
      "Iteration 235, loss = 0.48304815\n",
      "Iteration 236, loss = 0.48302322\n",
      "Iteration 237, loss = 0.48284502\n",
      "Iteration 238, loss = 0.48304083\n",
      "Iteration 239, loss = 0.48285427\n",
      "Iteration 240, loss = 0.48288569\n",
      "Iteration 241, loss = 0.48276462\n",
      "Iteration 242, loss = 0.48254034\n",
      "Iteration 243, loss = 0.48248919\n",
      "Iteration 244, loss = 0.48248423\n",
      "Iteration 245, loss = 0.48250403\n",
      "Iteration 246, loss = 0.48234033\n",
      "Iteration 247, loss = 0.48284129\n",
      "Iteration 248, loss = 0.48263607\n",
      "Iteration 249, loss = 0.48242517\n",
      "Iteration 250, loss = 0.48196357\n",
      "Iteration 251, loss = 0.48248704\n",
      "Iteration 252, loss = 0.48219487\n",
      "Iteration 253, loss = 0.48193627\n",
      "Iteration 254, loss = 0.48200751\n",
      "Iteration 255, loss = 0.48195413\n",
      "Iteration 256, loss = 0.48173990\n",
      "Iteration 257, loss = 0.48173027\n",
      "Iteration 258, loss = 0.48163376\n",
      "Iteration 259, loss = 0.48172027\n",
      "Iteration 260, loss = 0.48229065\n",
      "Iteration 261, loss = 0.48156822\n",
      "Iteration 262, loss = 0.48168147\n",
      "Iteration 263, loss = 0.48163384\n",
      "Iteration 264, loss = 0.48142251\n",
      "Iteration 265, loss = 0.48153824\n",
      "Iteration 266, loss = 0.48133885\n",
      "Iteration 267, loss = 0.48134750\n",
      "Iteration 268, loss = 0.48104882\n",
      "Iteration 269, loss = 0.48170295\n",
      "Iteration 270, loss = 0.48106906\n",
      "Iteration 271, loss = 0.48098727\n",
      "Iteration 272, loss = 0.48118667\n",
      "Iteration 273, loss = 0.48098398\n",
      "Iteration 274, loss = 0.48136021\n",
      "Iteration 275, loss = 0.48092242\n",
      "Iteration 276, loss = 0.48093232\n",
      "Iteration 277, loss = 0.48122893\n",
      "Iteration 278, loss = 0.48095031\n",
      "Iteration 279, loss = 0.48084927\n",
      "Iteration 280, loss = 0.48078862\n",
      "Iteration 281, loss = 0.48053599\n",
      "Iteration 282, loss = 0.48059435\n",
      "Iteration 283, loss = 0.48044919\n",
      "Iteration 284, loss = 0.48080121\n",
      "Iteration 285, loss = 0.48047179\n",
      "Iteration 286, loss = 0.48057491\n",
      "Iteration 287, loss = 0.48040410\n",
      "Iteration 288, loss = 0.48039040\n",
      "Iteration 289, loss = 0.48028681\n",
      "Iteration 290, loss = 0.48026134\n",
      "Iteration 291, loss = 0.48042217\n",
      "Iteration 292, loss = 0.48012858\n",
      "Iteration 293, loss = 0.48023994\n",
      "Iteration 294, loss = 0.48010168\n",
      "Iteration 295, loss = 0.48009584\n",
      "Iteration 296, loss = 0.48013535\n",
      "Iteration 297, loss = 0.48008473\n",
      "Iteration 298, loss = 0.47973559\n",
      "Iteration 299, loss = 0.48018713\n",
      "Iteration 300, loss = 0.48008548\n",
      "Iteration 301, loss = 0.48001657\n",
      "Iteration 302, loss = 0.47986782\n",
      "Iteration 303, loss = 0.48016539\n",
      "Iteration 304, loss = 0.47982525\n",
      "Iteration 305, loss = 0.47998529\n",
      "Iteration 306, loss = 0.47954219\n",
      "Iteration 307, loss = 0.47967166\n",
      "Iteration 308, loss = 0.47947216\n",
      "Iteration 309, loss = 0.47943706\n",
      "Iteration 310, loss = 0.47950674\n",
      "Iteration 311, loss = 0.47979203\n",
      "Iteration 312, loss = 0.47940296\n",
      "Iteration 313, loss = 0.47924436\n",
      "Iteration 314, loss = 0.47994825\n",
      "Iteration 315, loss = 0.47932369\n",
      "Iteration 316, loss = 0.47935153\n",
      "Iteration 317, loss = 0.47932151\n",
      "Iteration 318, loss = 0.47915037\n",
      "Iteration 319, loss = 0.47910374\n",
      "Iteration 320, loss = 0.47954820\n",
      "Iteration 321, loss = 0.47900058\n",
      "Iteration 322, loss = 0.47905383\n",
      "Iteration 323, loss = 0.47899279\n",
      "Iteration 324, loss = 0.47896540\n",
      "Iteration 325, loss = 0.47880575\n",
      "Iteration 326, loss = 0.47870130\n",
      "Iteration 327, loss = 0.47875503\n",
      "Iteration 328, loss = 0.47882274\n",
      "Iteration 329, loss = 0.47876652\n",
      "Iteration 330, loss = 0.47879772\n",
      "Iteration 331, loss = 0.47862427\n",
      "Iteration 332, loss = 0.47876880\n",
      "Iteration 333, loss = 0.47883912\n",
      "Iteration 334, loss = 0.47865774\n",
      "Iteration 335, loss = 0.47850419\n",
      "Iteration 336, loss = 0.47843299\n",
      "Iteration 337, loss = 0.47832013\n",
      "Iteration 338, loss = 0.47832713\n",
      "Iteration 339, loss = 0.47829245\n",
      "Iteration 340, loss = 0.47843832\n",
      "Iteration 341, loss = 0.47814570\n",
      "Iteration 342, loss = 0.47813564\n",
      "Iteration 343, loss = 0.47816373\n",
      "Iteration 344, loss = 0.47834456\n",
      "Iteration 345, loss = 0.47796685\n",
      "Iteration 346, loss = 0.47840975\n",
      "Iteration 347, loss = 0.47842247\n",
      "Iteration 348, loss = 0.47822427\n",
      "Iteration 349, loss = 0.47804368\n",
      "Iteration 350, loss = 0.47798377\n",
      "Iteration 351, loss = 0.47797486\n",
      "Iteration 352, loss = 0.47811742\n",
      "Iteration 353, loss = 0.47795672\n",
      "Iteration 354, loss = 0.47791973\n",
      "Iteration 355, loss = 0.47779698\n",
      "Iteration 356, loss = 0.47804224\n",
      "Iteration 357, loss = 0.47781921\n",
      "Iteration 358, loss = 0.47793980\n",
      "Iteration 359, loss = 0.47793125\n",
      "Iteration 360, loss = 0.47763309\n",
      "Iteration 361, loss = 0.47767536\n",
      "Iteration 362, loss = 0.47818126\n",
      "Iteration 363, loss = 0.47768776\n",
      "Iteration 364, loss = 0.47735356\n",
      "Iteration 365, loss = 0.47755816\n",
      "Iteration 366, loss = 0.47765046\n",
      "Iteration 367, loss = 0.47739677\n",
      "Iteration 368, loss = 0.47775833\n",
      "Iteration 369, loss = 0.47745057\n",
      "Iteration 370, loss = 0.47755001\n",
      "Iteration 371, loss = 0.47751820\n",
      "Iteration 372, loss = 0.47726988\n",
      "Iteration 373, loss = 0.47735448\n",
      "Iteration 374, loss = 0.47713588\n",
      "Iteration 375, loss = 0.47745319\n",
      "Iteration 376, loss = 0.47719060\n",
      "Iteration 377, loss = 0.47721890\n",
      "Iteration 378, loss = 0.47698508\n",
      "Iteration 379, loss = 0.47709053\n",
      "Iteration 380, loss = 0.47684319\n",
      "Iteration 381, loss = 0.47728052\n",
      "Iteration 382, loss = 0.47719096\n",
      "Iteration 383, loss = 0.47718370\n",
      "Iteration 384, loss = 0.47664990\n",
      "Iteration 385, loss = 0.47690197\n",
      "Iteration 386, loss = 0.47689372\n",
      "Iteration 387, loss = 0.47693014\n",
      "Iteration 388, loss = 0.47673578\n",
      "Iteration 389, loss = 0.47669225\n",
      "Iteration 390, loss = 0.47736919\n",
      "Iteration 391, loss = 0.47663442\n",
      "Iteration 392, loss = 0.47692680\n",
      "Iteration 393, loss = 0.47666223\n",
      "Iteration 394, loss = 0.47690877\n",
      "Iteration 395, loss = 0.47667471\n",
      "Iteration 396, loss = 0.47665917\n",
      "Iteration 397, loss = 0.47647094\n",
      "Iteration 398, loss = 0.47672791\n",
      "Iteration 399, loss = 0.47700759\n",
      "Iteration 400, loss = 0.47628629\n",
      "Iteration 401, loss = 0.47664497\n",
      "Iteration 402, loss = 0.47650260\n",
      "Iteration 403, loss = 0.47642919\n",
      "Iteration 404, loss = 0.47641614\n",
      "Iteration 405, loss = 0.47640018\n",
      "Iteration 406, loss = 0.47634717\n",
      "Iteration 407, loss = 0.47612729\n",
      "Iteration 408, loss = 0.47640762\n",
      "Iteration 409, loss = 0.47624719\n",
      "Iteration 410, loss = 0.47641835\n",
      "Iteration 411, loss = 0.47615761\n",
      "Iteration 412, loss = 0.47652204\n",
      "Iteration 413, loss = 0.47616406\n",
      "Iteration 414, loss = 0.47603655\n",
      "Iteration 415, loss = 0.47618136\n",
      "Iteration 416, loss = 0.47584618\n",
      "Iteration 417, loss = 0.47590368\n",
      "Iteration 418, loss = 0.47623856\n",
      "Iteration 419, loss = 0.47609060\n",
      "Iteration 420, loss = 0.47596163\n",
      "Iteration 421, loss = 0.47559223\n",
      "Iteration 422, loss = 0.47601303\n",
      "Iteration 423, loss = 0.47606339\n",
      "Iteration 424, loss = 0.47629023\n",
      "Iteration 425, loss = 0.47581005\n",
      "Iteration 426, loss = 0.47565020\n",
      "Iteration 427, loss = 0.47564437\n",
      "Iteration 428, loss = 0.47555054\n",
      "Iteration 429, loss = 0.47568295\n",
      "Iteration 430, loss = 0.47556127\n",
      "Iteration 431, loss = 0.47533748\n",
      "Iteration 432, loss = 0.47531506\n",
      "Iteration 433, loss = 0.47540484\n",
      "Iteration 434, loss = 0.47541797\n",
      "Iteration 435, loss = 0.47574893\n",
      "Iteration 436, loss = 0.47602050\n",
      "Iteration 437, loss = 0.47517864\n",
      "Iteration 438, loss = 0.47523610\n",
      "Iteration 439, loss = 0.47521361\n",
      "Iteration 440, loss = 0.47540670\n",
      "Iteration 441, loss = 0.47527149\n",
      "Iteration 442, loss = 0.47529919\n",
      "Iteration 443, loss = 0.47526754\n",
      "Iteration 444, loss = 0.47583611\n",
      "Iteration 445, loss = 0.47581068\n",
      "Iteration 446, loss = 0.47515114\n",
      "Iteration 447, loss = 0.47505227\n",
      "Iteration 448, loss = 0.47522211\n",
      "Iteration 449, loss = 0.47507683\n",
      "Iteration 450, loss = 0.47490678\n",
      "Iteration 451, loss = 0.47457416\n",
      "Iteration 452, loss = 0.47473547\n",
      "Iteration 453, loss = 0.47484359\n",
      "Iteration 454, loss = 0.47485681\n",
      "Iteration 455, loss = 0.47474974\n",
      "Iteration 456, loss = 0.47462578\n",
      "Iteration 457, loss = 0.47480898\n",
      "Iteration 458, loss = 0.47460030\n",
      "Iteration 459, loss = 0.47491145\n",
      "Iteration 460, loss = 0.47535706\n",
      "Iteration 461, loss = 0.47474501\n",
      "Iteration 462, loss = 0.47481486\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74462544\n",
      "Iteration 2, loss = 0.71476577\n",
      "Iteration 3, loss = 0.69223310\n",
      "Iteration 4, loss = 0.67349301\n",
      "Iteration 5, loss = 0.65671962\n",
      "Iteration 6, loss = 0.64114317\n",
      "Iteration 7, loss = 0.62659646\n",
      "Iteration 8, loss = 0.61337843\n",
      "Iteration 9, loss = 0.60073221\n",
      "Iteration 10, loss = 0.58928779\n",
      "Iteration 11, loss = 0.57868867\n",
      "Iteration 12, loss = 0.56999327\n",
      "Iteration 13, loss = 0.56216883\n",
      "Iteration 14, loss = 0.55581022\n",
      "Iteration 15, loss = 0.55056171\n",
      "Iteration 16, loss = 0.54652356\n",
      "Iteration 17, loss = 0.54348064\n",
      "Iteration 18, loss = 0.54092251\n",
      "Iteration 19, loss = 0.53890208\n",
      "Iteration 20, loss = 0.53705870\n",
      "Iteration 21, loss = 0.53544540\n",
      "Iteration 22, loss = 0.53405604\n",
      "Iteration 23, loss = 0.53259059\n",
      "Iteration 24, loss = 0.53127270\n",
      "Iteration 25, loss = 0.52997628\n",
      "Iteration 26, loss = 0.52864996\n",
      "Iteration 27, loss = 0.52768777\n",
      "Iteration 28, loss = 0.52653187\n",
      "Iteration 29, loss = 0.52505835\n",
      "Iteration 30, loss = 0.52398814\n",
      "Iteration 31, loss = 0.52281355\n",
      "Iteration 32, loss = 0.52202800\n",
      "Iteration 33, loss = 0.52072081\n",
      "Iteration 34, loss = 0.51974656\n",
      "Iteration 35, loss = 0.51874661\n",
      "Iteration 36, loss = 0.51772638\n",
      "Iteration 37, loss = 0.51687252\n",
      "Iteration 38, loss = 0.51584467\n",
      "Iteration 39, loss = 0.51511840\n",
      "Iteration 40, loss = 0.51422345\n",
      "Iteration 41, loss = 0.51349973\n",
      "Iteration 42, loss = 0.51253002\n",
      "Iteration 43, loss = 0.51186961\n",
      "Iteration 44, loss = 0.51109002\n",
      "Iteration 45, loss = 0.51078782\n",
      "Iteration 46, loss = 0.50983993\n",
      "Iteration 47, loss = 0.50924792\n",
      "Iteration 48, loss = 0.50872788\n",
      "Iteration 49, loss = 0.50844099\n",
      "Iteration 50, loss = 0.50771670\n",
      "Iteration 51, loss = 0.50723384\n",
      "Iteration 52, loss = 0.50683745\n",
      "Iteration 53, loss = 0.50652997\n",
      "Iteration 54, loss = 0.50615600\n",
      "Iteration 55, loss = 0.50556475\n",
      "Iteration 56, loss = 0.50536955\n",
      "Iteration 57, loss = 0.50504254\n",
      "Iteration 58, loss = 0.50450114\n",
      "Iteration 59, loss = 0.50450368\n",
      "Iteration 60, loss = 0.50409852\n",
      "Iteration 61, loss = 0.50392006\n",
      "Iteration 62, loss = 0.50372482\n",
      "Iteration 63, loss = 0.50332539\n",
      "Iteration 64, loss = 0.50316191\n",
      "Iteration 65, loss = 0.50273792\n",
      "Iteration 66, loss = 0.50245985\n",
      "Iteration 67, loss = 0.50235640\n",
      "Iteration 68, loss = 0.50225659\n",
      "Iteration 69, loss = 0.50173339\n",
      "Iteration 70, loss = 0.50154535\n",
      "Iteration 71, loss = 0.50129250\n",
      "Iteration 72, loss = 0.50109436\n",
      "Iteration 73, loss = 0.50077850\n",
      "Iteration 74, loss = 0.50076494\n",
      "Iteration 75, loss = 0.50046624\n",
      "Iteration 76, loss = 0.50048718\n",
      "Iteration 77, loss = 0.49987984\n",
      "Iteration 78, loss = 0.49973643\n",
      "Iteration 79, loss = 0.49932153\n",
      "Iteration 80, loss = 0.49900639\n",
      "Iteration 81, loss = 0.49884105\n",
      "Iteration 82, loss = 0.49857758\n",
      "Iteration 83, loss = 0.49900750\n",
      "Iteration 84, loss = 0.49837256\n",
      "Iteration 85, loss = 0.49789278\n",
      "Iteration 86, loss = 0.49773018\n",
      "Iteration 87, loss = 0.49756500\n",
      "Iteration 88, loss = 0.49765610\n",
      "Iteration 89, loss = 0.49704966\n",
      "Iteration 90, loss = 0.49700578\n",
      "Iteration 91, loss = 0.49675628\n",
      "Iteration 92, loss = 0.49632942\n",
      "Iteration 93, loss = 0.49619313\n",
      "Iteration 94, loss = 0.49617416\n",
      "Iteration 95, loss = 0.49584515\n",
      "Iteration 96, loss = 0.49560825\n",
      "Iteration 97, loss = 0.49537553\n",
      "Iteration 98, loss = 0.49541584\n",
      "Iteration 99, loss = 0.49542570\n",
      "Iteration 100, loss = 0.49507288\n",
      "Iteration 101, loss = 0.49476799\n",
      "Iteration 102, loss = 0.49462426\n",
      "Iteration 103, loss = 0.49435094\n",
      "Iteration 104, loss = 0.49451978\n",
      "Iteration 105, loss = 0.49420229\n",
      "Iteration 106, loss = 0.49393482\n",
      "Iteration 107, loss = 0.49384443\n",
      "Iteration 108, loss = 0.49360382\n",
      "Iteration 109, loss = 0.49356405\n",
      "Iteration 110, loss = 0.49331675\n",
      "Iteration 111, loss = 0.49314021\n",
      "Iteration 112, loss = 0.49295367\n",
      "Iteration 113, loss = 0.49259829\n",
      "Iteration 114, loss = 0.49269975\n",
      "Iteration 115, loss = 0.49276932\n",
      "Iteration 116, loss = 0.49248797\n",
      "Iteration 117, loss = 0.49213564\n",
      "Iteration 118, loss = 0.49196263\n",
      "Iteration 119, loss = 0.49181009\n",
      "Iteration 120, loss = 0.49181600\n",
      "Iteration 121, loss = 0.49150771\n",
      "Iteration 122, loss = 0.49149910\n",
      "Iteration 123, loss = 0.49134878\n",
      "Iteration 124, loss = 0.49172034\n",
      "Iteration 125, loss = 0.49079682\n",
      "Iteration 126, loss = 0.49084910\n",
      "Iteration 127, loss = 0.49062086\n",
      "Iteration 128, loss = 0.49054599\n",
      "Iteration 129, loss = 0.49041697\n",
      "Iteration 130, loss = 0.49067238\n",
      "Iteration 131, loss = 0.49005662\n",
      "Iteration 132, loss = 0.49029473\n",
      "Iteration 133, loss = 0.48975441\n",
      "Iteration 134, loss = 0.48981663\n",
      "Iteration 135, loss = 0.48978754\n",
      "Iteration 136, loss = 0.48953318\n",
      "Iteration 137, loss = 0.48973893\n",
      "Iteration 138, loss = 0.48933607\n",
      "Iteration 139, loss = 0.48928261\n",
      "Iteration 140, loss = 0.48900928\n",
      "Iteration 141, loss = 0.48884636\n",
      "Iteration 142, loss = 0.48898749\n",
      "Iteration 143, loss = 0.48892904\n",
      "Iteration 144, loss = 0.48836437\n",
      "Iteration 145, loss = 0.48826370\n",
      "Iteration 146, loss = 0.48842694\n",
      "Iteration 147, loss = 0.48803690\n",
      "Iteration 148, loss = 0.48797380\n",
      "Iteration 149, loss = 0.48803609\n",
      "Iteration 150, loss = 0.48819251\n",
      "Iteration 151, loss = 0.48792451\n",
      "Iteration 152, loss = 0.48759571\n",
      "Iteration 153, loss = 0.48773835\n",
      "Iteration 154, loss = 0.48754503\n",
      "Iteration 155, loss = 0.48769060\n",
      "Iteration 156, loss = 0.48735822\n",
      "Iteration 157, loss = 0.48784286\n",
      "Iteration 158, loss = 0.48719193\n",
      "Iteration 159, loss = 0.48706628\n",
      "Iteration 160, loss = 0.48715929\n",
      "Iteration 161, loss = 0.48711467\n",
      "Iteration 162, loss = 0.48685523\n",
      "Iteration 163, loss = 0.48705893\n",
      "Iteration 164, loss = 0.48666571\n",
      "Iteration 165, loss = 0.48664775\n",
      "Iteration 166, loss = 0.48637424\n",
      "Iteration 167, loss = 0.48636264\n",
      "Iteration 168, loss = 0.48610930\n",
      "Iteration 169, loss = 0.48619381\n",
      "Iteration 170, loss = 0.48625150\n",
      "Iteration 171, loss = 0.48591946\n",
      "Iteration 172, loss = 0.48620478\n",
      "Iteration 173, loss = 0.48602035\n",
      "Iteration 174, loss = 0.48559937\n",
      "Iteration 175, loss = 0.48658462\n",
      "Iteration 176, loss = 0.48544246\n",
      "Iteration 177, loss = 0.48558046\n",
      "Iteration 178, loss = 0.48543421\n",
      "Iteration 179, loss = 0.48527913\n",
      "Iteration 180, loss = 0.48533917\n",
      "Iteration 181, loss = 0.48513123\n",
      "Iteration 182, loss = 0.48520780\n",
      "Iteration 183, loss = 0.48501696\n",
      "Iteration 184, loss = 0.48499048\n",
      "Iteration 185, loss = 0.48490586\n",
      "Iteration 186, loss = 0.48478782\n",
      "Iteration 187, loss = 0.48460318\n",
      "Iteration 188, loss = 0.48459631\n",
      "Iteration 189, loss = 0.48449345\n",
      "Iteration 190, loss = 0.48459114\n",
      "Iteration 191, loss = 0.48419631\n",
      "Iteration 192, loss = 0.48424869\n",
      "Iteration 193, loss = 0.48425295\n",
      "Iteration 194, loss = 0.48433199\n",
      "Iteration 195, loss = 0.48392213\n",
      "Iteration 196, loss = 0.48400554\n",
      "Iteration 197, loss = 0.48391663\n",
      "Iteration 198, loss = 0.48378845\n",
      "Iteration 199, loss = 0.48405136\n",
      "Iteration 200, loss = 0.48379814\n",
      "Iteration 201, loss = 0.48384489\n",
      "Iteration 202, loss = 0.48366581\n",
      "Iteration 203, loss = 0.48379354\n",
      "Iteration 204, loss = 0.48355808\n",
      "Iteration 205, loss = 0.48363731\n",
      "Iteration 206, loss = 0.48334958\n",
      "Iteration 207, loss = 0.48335603\n",
      "Iteration 208, loss = 0.48329555\n",
      "Iteration 209, loss = 0.48317204\n",
      "Iteration 210, loss = 0.48314749\n",
      "Iteration 211, loss = 0.48309508\n",
      "Iteration 212, loss = 0.48298115\n",
      "Iteration 213, loss = 0.48342118\n",
      "Iteration 214, loss = 0.48306228\n",
      "Iteration 215, loss = 0.48326454\n",
      "Iteration 216, loss = 0.48323180\n",
      "Iteration 217, loss = 0.48283952\n",
      "Iteration 218, loss = 0.48294368\n",
      "Iteration 219, loss = 0.48306838\n",
      "Iteration 220, loss = 0.48290544\n",
      "Iteration 221, loss = 0.48278315\n",
      "Iteration 222, loss = 0.48247924\n",
      "Iteration 223, loss = 0.48256139\n",
      "Iteration 224, loss = 0.48245267\n",
      "Iteration 225, loss = 0.48255667\n",
      "Iteration 226, loss = 0.48256218\n",
      "Iteration 227, loss = 0.48240380\n",
      "Iteration 228, loss = 0.48239634\n",
      "Iteration 229, loss = 0.48221323\n",
      "Iteration 230, loss = 0.48227378\n",
      "Iteration 231, loss = 0.48216044\n",
      "Iteration 232, loss = 0.48219970\n",
      "Iteration 233, loss = 0.48232322\n",
      "Iteration 234, loss = 0.48214604\n",
      "Iteration 235, loss = 0.48252449\n",
      "Iteration 236, loss = 0.48242704\n",
      "Iteration 237, loss = 0.48190716\n",
      "Iteration 238, loss = 0.48195281\n",
      "Iteration 239, loss = 0.48187738\n",
      "Iteration 240, loss = 0.48180108\n",
      "Iteration 241, loss = 0.48182693\n",
      "Iteration 242, loss = 0.48180135\n",
      "Iteration 243, loss = 0.48191470\n",
      "Iteration 244, loss = 0.48162338\n",
      "Iteration 245, loss = 0.48162112\n",
      "Iteration 246, loss = 0.48151379\n",
      "Iteration 247, loss = 0.48158728\n",
      "Iteration 248, loss = 0.48153493\n",
      "Iteration 249, loss = 0.48150369\n",
      "Iteration 250, loss = 0.48140919\n",
      "Iteration 251, loss = 0.48138941\n",
      "Iteration 252, loss = 0.48143380\n",
      "Iteration 253, loss = 0.48133918\n",
      "Iteration 254, loss = 0.48161497\n",
      "Iteration 255, loss = 0.48131127\n",
      "Iteration 256, loss = 0.48122631\n",
      "Iteration 257, loss = 0.48131566\n",
      "Iteration 258, loss = 0.48134801\n",
      "Iteration 259, loss = 0.48120449\n",
      "Iteration 260, loss = 0.48095541\n",
      "Iteration 261, loss = 0.48120812\n",
      "Iteration 262, loss = 0.48110391\n",
      "Iteration 263, loss = 0.48107401\n",
      "Iteration 264, loss = 0.48092002\n",
      "Iteration 265, loss = 0.48095421\n",
      "Iteration 266, loss = 0.48069234\n",
      "Iteration 267, loss = 0.48072582\n",
      "Iteration 268, loss = 0.48098385\n",
      "Iteration 269, loss = 0.48095420\n",
      "Iteration 270, loss = 0.48082717\n",
      "Iteration 271, loss = 0.48068931\n",
      "Iteration 272, loss = 0.48074422\n",
      "Iteration 273, loss = 0.48047907\n",
      "Iteration 274, loss = 0.48058112\n",
      "Iteration 275, loss = 0.48067901\n",
      "Iteration 276, loss = 0.48058783\n",
      "Iteration 277, loss = 0.48028631\n",
      "Iteration 278, loss = 0.48039648\n",
      "Iteration 279, loss = 0.48037559\n",
      "Iteration 280, loss = 0.48036570\n",
      "Iteration 281, loss = 0.48014188\n",
      "Iteration 282, loss = 0.48052365\n",
      "Iteration 283, loss = 0.48020071\n",
      "Iteration 284, loss = 0.48018281\n",
      "Iteration 285, loss = 0.48035663\n",
      "Iteration 286, loss = 0.48004803\n",
      "Iteration 287, loss = 0.48013295\n",
      "Iteration 288, loss = 0.48031153\n",
      "Iteration 289, loss = 0.48009879\n",
      "Iteration 290, loss = 0.48034550\n",
      "Iteration 291, loss = 0.47982720\n",
      "Iteration 292, loss = 0.47997983\n",
      "Iteration 293, loss = 0.47982542\n",
      "Iteration 294, loss = 0.47996647\n",
      "Iteration 295, loss = 0.48001820\n",
      "Iteration 296, loss = 0.47967530\n",
      "Iteration 297, loss = 0.48028081\n",
      "Iteration 298, loss = 0.47960690\n",
      "Iteration 299, loss = 0.47975561\n",
      "Iteration 300, loss = 0.47955467\n",
      "Iteration 301, loss = 0.47963974\n",
      "Iteration 302, loss = 0.47940707\n",
      "Iteration 303, loss = 0.47930494\n",
      "Iteration 304, loss = 0.47940870\n",
      "Iteration 305, loss = 0.47927626\n",
      "Iteration 306, loss = 0.47907927\n",
      "Iteration 307, loss = 0.47927803\n",
      "Iteration 308, loss = 0.47925853\n",
      "Iteration 309, loss = 0.47957303\n",
      "Iteration 310, loss = 0.47924202\n",
      "Iteration 311, loss = 0.47930405\n",
      "Iteration 312, loss = 0.47896923\n",
      "Iteration 313, loss = 0.47904777\n",
      "Iteration 314, loss = 0.47895148\n",
      "Iteration 315, loss = 0.47927187\n",
      "Iteration 316, loss = 0.47885578\n",
      "Iteration 317, loss = 0.47904524\n",
      "Iteration 318, loss = 0.47868666\n",
      "Iteration 319, loss = 0.47871578\n",
      "Iteration 320, loss = 0.47887717\n",
      "Iteration 321, loss = 0.47860089\n",
      "Iteration 322, loss = 0.47886255\n",
      "Iteration 323, loss = 0.47845882\n",
      "Iteration 324, loss = 0.47852964\n",
      "Iteration 325, loss = 0.47860918\n",
      "Iteration 326, loss = 0.47846010\n",
      "Iteration 327, loss = 0.47861894\n",
      "Iteration 328, loss = 0.47836338\n",
      "Iteration 329, loss = 0.47850030\n",
      "Iteration 330, loss = 0.47849055\n",
      "Iteration 331, loss = 0.47845198\n",
      "Iteration 332, loss = 0.47819699\n",
      "Iteration 333, loss = 0.47809431\n",
      "Iteration 334, loss = 0.47794674\n",
      "Iteration 335, loss = 0.47803829\n",
      "Iteration 336, loss = 0.47786507\n",
      "Iteration 337, loss = 0.47789267\n",
      "Iteration 338, loss = 0.47784927\n",
      "Iteration 339, loss = 0.47812058\n",
      "Iteration 340, loss = 0.47785624\n",
      "Iteration 341, loss = 0.47777025\n",
      "Iteration 342, loss = 0.47777864\n",
      "Iteration 343, loss = 0.47788495\n",
      "Iteration 344, loss = 0.47730910\n",
      "Iteration 345, loss = 0.47808539\n",
      "Iteration 346, loss = 0.47759577\n",
      "Iteration 347, loss = 0.47736510\n",
      "Iteration 348, loss = 0.47753003\n",
      "Iteration 349, loss = 0.47742857\n",
      "Iteration 350, loss = 0.47722447\n",
      "Iteration 351, loss = 0.47735764\n",
      "Iteration 352, loss = 0.47724493\n",
      "Iteration 353, loss = 0.47730946\n",
      "Iteration 354, loss = 0.47746280\n",
      "Iteration 355, loss = 0.47680459\n",
      "Iteration 356, loss = 0.47699414\n",
      "Iteration 357, loss = 0.47681871\n",
      "Iteration 358, loss = 0.47690608\n",
      "Iteration 359, loss = 0.47697302\n",
      "Iteration 360, loss = 0.47676306\n",
      "Iteration 361, loss = 0.47663785\n",
      "Iteration 362, loss = 0.47665858\n",
      "Iteration 363, loss = 0.47653015\n",
      "Iteration 364, loss = 0.47639275\n",
      "Iteration 365, loss = 0.47666653\n",
      "Iteration 366, loss = 0.47690443\n",
      "Iteration 367, loss = 0.47642790\n",
      "Iteration 368, loss = 0.47642214\n",
      "Iteration 369, loss = 0.47620505\n",
      "Iteration 370, loss = 0.47641434\n",
      "Iteration 371, loss = 0.47647129\n",
      "Iteration 372, loss = 0.47621435\n",
      "Iteration 373, loss = 0.47588131\n",
      "Iteration 374, loss = 0.47647408\n",
      "Iteration 375, loss = 0.47631953\n",
      "Iteration 376, loss = 0.47620396\n",
      "Iteration 377, loss = 0.47617230\n",
      "Iteration 378, loss = 0.47622355\n",
      "Iteration 379, loss = 0.47579557\n",
      "Iteration 380, loss = 0.47592676\n",
      "Iteration 381, loss = 0.47562982\n",
      "Iteration 382, loss = 0.47585954\n",
      "Iteration 383, loss = 0.47576944\n",
      "Iteration 384, loss = 0.47566587\n",
      "Iteration 385, loss = 0.47557513\n",
      "Iteration 386, loss = 0.47552613\n",
      "Iteration 387, loss = 0.47563887\n",
      "Iteration 388, loss = 0.47568585\n",
      "Iteration 389, loss = 0.47553102\n",
      "Iteration 390, loss = 0.47557062\n",
      "Iteration 391, loss = 0.47556145\n",
      "Iteration 392, loss = 0.47514935\n",
      "Iteration 393, loss = 0.47525588\n",
      "Iteration 394, loss = 0.47540153\n",
      "Iteration 395, loss = 0.47520229\n",
      "Iteration 396, loss = 0.47526557\n",
      "Iteration 397, loss = 0.47504549\n",
      "Iteration 398, loss = 0.47492193\n",
      "Iteration 399, loss = 0.47499834\n",
      "Iteration 400, loss = 0.47499738\n",
      "Iteration 401, loss = 0.47513102\n",
      "Iteration 402, loss = 0.47518882\n",
      "Iteration 403, loss = 0.47523229\n",
      "Iteration 404, loss = 0.47491699\n",
      "Iteration 405, loss = 0.47500364\n",
      "Iteration 406, loss = 0.47503639\n",
      "Iteration 407, loss = 0.47465464\n",
      "Iteration 408, loss = 0.47495074\n",
      "Iteration 409, loss = 0.47467168\n",
      "Iteration 410, loss = 0.47455616\n",
      "Iteration 411, loss = 0.47450007\n",
      "Iteration 412, loss = 0.47470642\n",
      "Iteration 413, loss = 0.47457869\n",
      "Iteration 414, loss = 0.47457582\n",
      "Iteration 415, loss = 0.47474791\n",
      "Iteration 416, loss = 0.47453523\n",
      "Iteration 417, loss = 0.47438716\n",
      "Iteration 418, loss = 0.47462793\n",
      "Iteration 419, loss = 0.47450590\n",
      "Iteration 420, loss = 0.47450755\n",
      "Iteration 421, loss = 0.47414649\n",
      "Iteration 422, loss = 0.47447991\n",
      "Iteration 423, loss = 0.47529714\n",
      "Iteration 424, loss = 0.47428987\n",
      "Iteration 425, loss = 0.47424591\n",
      "Iteration 426, loss = 0.47414559\n",
      "Iteration 427, loss = 0.47474612\n",
      "Iteration 428, loss = 0.47419434\n",
      "Iteration 429, loss = 0.47426565\n",
      "Iteration 430, loss = 0.47439141\n",
      "Iteration 431, loss = 0.47392690\n",
      "Iteration 432, loss = 0.47404378\n",
      "Iteration 433, loss = 0.47402632\n",
      "Iteration 434, loss = 0.47386986\n",
      "Iteration 435, loss = 0.47420619\n",
      "Iteration 436, loss = 0.47411562\n",
      "Iteration 437, loss = 0.47425268\n",
      "Iteration 438, loss = 0.47383576\n",
      "Iteration 439, loss = 0.47416820\n",
      "Iteration 440, loss = 0.47390610\n",
      "Iteration 441, loss = 0.47384414\n",
      "Iteration 442, loss = 0.47378159\n",
      "Iteration 443, loss = 0.47369810\n",
      "Iteration 444, loss = 0.47384182\n",
      "Iteration 445, loss = 0.47349694\n",
      "Iteration 446, loss = 0.47348331\n",
      "Iteration 447, loss = 0.47363512\n",
      "Iteration 448, loss = 0.47355454\n",
      "Iteration 449, loss = 0.47360453\n",
      "Iteration 450, loss = 0.47346365\n",
      "Iteration 451, loss = 0.47334565\n",
      "Iteration 452, loss = 0.47361817\n",
      "Iteration 453, loss = 0.47335437\n",
      "Iteration 454, loss = 0.47323323\n",
      "Iteration 455, loss = 0.47326455\n",
      "Iteration 456, loss = 0.47332218\n",
      "Iteration 457, loss = 0.47343400\n",
      "Iteration 458, loss = 0.47327565\n",
      "Iteration 459, loss = 0.47317659\n",
      "Iteration 460, loss = 0.47299195\n",
      "Iteration 461, loss = 0.47309328\n",
      "Iteration 462, loss = 0.47312666\n",
      "Iteration 463, loss = 0.47339572\n",
      "Iteration 464, loss = 0.47323618\n",
      "Iteration 465, loss = 0.47286243\n",
      "Iteration 466, loss = 0.47295994\n",
      "Iteration 467, loss = 0.47271748\n",
      "Iteration 468, loss = 0.47272961\n",
      "Iteration 469, loss = 0.47251862\n",
      "Iteration 470, loss = 0.47286397\n",
      "Iteration 471, loss = 0.47283520\n",
      "Iteration 472, loss = 0.47246560\n",
      "Iteration 473, loss = 0.47251738\n",
      "Iteration 474, loss = 0.47247503\n",
      "Iteration 475, loss = 0.47251033\n",
      "Iteration 476, loss = 0.47238030\n",
      "Iteration 477, loss = 0.47240251\n",
      "Iteration 478, loss = 0.47246719\n",
      "Iteration 479, loss = 0.47229901\n",
      "Iteration 480, loss = 0.47209688\n",
      "Iteration 481, loss = 0.47218796\n",
      "Iteration 482, loss = 0.47208886\n",
      "Iteration 483, loss = 0.47225390\n",
      "Iteration 484, loss = 0.47209907\n",
      "Iteration 485, loss = 0.47204151\n",
      "Iteration 486, loss = 0.47210901\n",
      "Iteration 487, loss = 0.47174350\n",
      "Iteration 488, loss = 0.47186714\n",
      "Iteration 489, loss = 0.47191720\n",
      "Iteration 490, loss = 0.47163556\n",
      "Iteration 491, loss = 0.47214613\n",
      "Iteration 492, loss = 0.47194379\n",
      "Iteration 493, loss = 0.47171361\n",
      "Iteration 494, loss = 0.47155971\n",
      "Iteration 495, loss = 0.47189270\n",
      "Iteration 496, loss = 0.47149262\n",
      "Iteration 497, loss = 0.47148281\n",
      "Iteration 498, loss = 0.47143808\n",
      "Iteration 499, loss = 0.47134275\n",
      "Iteration 500, loss = 0.47145520\n",
      "Iteration 501, loss = 0.47141332\n",
      "Iteration 502, loss = 0.47125229\n",
      "Iteration 503, loss = 0.47125994\n",
      "Iteration 504, loss = 0.47176654\n",
      "Iteration 505, loss = 0.47104504\n",
      "Iteration 506, loss = 0.47110066\n",
      "Iteration 507, loss = 0.47112684\n",
      "Iteration 508, loss = 0.47117927\n",
      "Iteration 509, loss = 0.47091364\n",
      "Iteration 510, loss = 0.47107159\n",
      "Iteration 511, loss = 0.47110751\n",
      "Iteration 512, loss = 0.47076404\n",
      "Iteration 513, loss = 0.47108501\n",
      "Iteration 514, loss = 0.47089024\n",
      "Iteration 515, loss = 0.47092711\n",
      "Iteration 516, loss = 0.47101345\n",
      "Iteration 517, loss = 0.47064962\n",
      "Iteration 518, loss = 0.47135341\n",
      "Iteration 519, loss = 0.47053924\n",
      "Iteration 520, loss = 0.47047245\n",
      "Iteration 521, loss = 0.47046210\n",
      "Iteration 522, loss = 0.47073695\n",
      "Iteration 523, loss = 0.47044267\n",
      "Iteration 524, loss = 0.47047241\n",
      "Iteration 525, loss = 0.47057437\n",
      "Iteration 526, loss = 0.47022623\n",
      "Iteration 527, loss = 0.47040991\n",
      "Iteration 528, loss = 0.47037495\n",
      "Iteration 529, loss = 0.47024931\n",
      "Iteration 530, loss = 0.47037330\n",
      "Iteration 531, loss = 0.47048756\n",
      "Iteration 532, loss = 0.47021011\n",
      "Iteration 533, loss = 0.47023596\n",
      "Iteration 534, loss = 0.47004970\n",
      "Iteration 535, loss = 0.47009254\n",
      "Iteration 536, loss = 0.46988912\n",
      "Iteration 537, loss = 0.47002586\n",
      "Iteration 538, loss = 0.47006443\n",
      "Iteration 539, loss = 0.47033361\n",
      "Iteration 540, loss = 0.46997570\n",
      "Iteration 541, loss = 0.46992149\n",
      "Iteration 542, loss = 0.46986845\n",
      "Iteration 543, loss = 0.47006749\n",
      "Iteration 544, loss = 0.46977611\n",
      "Iteration 545, loss = 0.46982151\n",
      "Iteration 546, loss = 0.46960948\n",
      "Iteration 547, loss = 0.46988651\n",
      "Iteration 548, loss = 0.46974346\n",
      "Iteration 549, loss = 0.46978891\n",
      "Iteration 550, loss = 0.46982387\n",
      "Iteration 551, loss = 0.46975735\n",
      "Iteration 552, loss = 0.46984352\n",
      "Iteration 553, loss = 0.46978965\n",
      "Iteration 554, loss = 0.46990871\n",
      "Iteration 555, loss = 0.46956924\n",
      "Iteration 556, loss = 0.46938462\n",
      "Iteration 557, loss = 0.46943449\n",
      "Iteration 558, loss = 0.46956364\n",
      "Iteration 559, loss = 0.46933728\n",
      "Iteration 560, loss = 0.46930992\n",
      "Iteration 561, loss = 0.46910552\n",
      "Iteration 562, loss = 0.46938072\n",
      "Iteration 563, loss = 0.46910999\n",
      "Iteration 564, loss = 0.46905191\n",
      "Iteration 565, loss = 0.46914502\n",
      "Iteration 566, loss = 0.46924334\n",
      "Iteration 567, loss = 0.46906789\n",
      "Iteration 568, loss = 0.46918401\n",
      "Iteration 569, loss = 0.46938492\n",
      "Iteration 570, loss = 0.46918855\n",
      "Iteration 571, loss = 0.46950439\n",
      "Iteration 572, loss = 0.46924300\n",
      "Iteration 573, loss = 0.46888548\n",
      "Iteration 574, loss = 0.46920798\n",
      "Iteration 575, loss = 0.46946410\n",
      "Iteration 576, loss = 0.46915805\n",
      "Iteration 577, loss = 0.46892347\n",
      "Iteration 578, loss = 0.46882138\n",
      "Iteration 579, loss = 0.46873819\n",
      "Iteration 580, loss = 0.46903347\n",
      "Iteration 581, loss = 0.46916988\n",
      "Iteration 582, loss = 0.46904393\n",
      "Iteration 583, loss = 0.46861519\n",
      "Iteration 584, loss = 0.46881940\n",
      "Iteration 585, loss = 0.46870267\n",
      "Iteration 586, loss = 0.46878287\n",
      "Iteration 587, loss = 0.46870751\n",
      "Iteration 588, loss = 0.46861913\n",
      "Iteration 589, loss = 0.46858489\n",
      "Iteration 590, loss = 0.46879927\n",
      "Iteration 591, loss = 0.46911278\n",
      "Iteration 592, loss = 0.46884026\n",
      "Iteration 593, loss = 0.46901826\n",
      "Iteration 594, loss = 0.46851793\n",
      "Iteration 595, loss = 0.46858684\n",
      "Iteration 596, loss = 0.46858598\n",
      "Iteration 597, loss = 0.46843310\n",
      "Iteration 598, loss = 0.46895629\n",
      "Iteration 599, loss = 0.46842470\n",
      "Iteration 600, loss = 0.46861840\n",
      "Iteration 601, loss = 0.46827677\n",
      "Iteration 602, loss = 0.46828909\n",
      "Iteration 603, loss = 0.46825014\n",
      "Iteration 604, loss = 0.46811253\n",
      "Iteration 605, loss = 0.46816401\n",
      "Iteration 606, loss = 0.46807988\n",
      "Iteration 607, loss = 0.46813301\n",
      "Iteration 608, loss = 0.46871554\n",
      "Iteration 609, loss = 0.46790925\n",
      "Iteration 610, loss = 0.46808286\n",
      "Iteration 611, loss = 0.46788793\n",
      "Iteration 612, loss = 0.46782920\n",
      "Iteration 613, loss = 0.46833277\n",
      "Iteration 614, loss = 0.46798504\n",
      "Iteration 615, loss = 0.46781640\n",
      "Iteration 616, loss = 0.46781557\n",
      "Iteration 617, loss = 0.46783141\n",
      "Iteration 618, loss = 0.46771369\n",
      "Iteration 619, loss = 0.46776790\n",
      "Iteration 620, loss = 0.46808658\n",
      "Iteration 621, loss = 0.46774366\n",
      "Iteration 622, loss = 0.46756790\n",
      "Iteration 623, loss = 0.46772701\n",
      "Iteration 624, loss = 0.46743784\n",
      "Iteration 625, loss = 0.46757730\n",
      "Iteration 626, loss = 0.46774152\n",
      "Iteration 627, loss = 0.46755044\n",
      "Iteration 628, loss = 0.46757409\n",
      "Iteration 629, loss = 0.46754951\n",
      "Iteration 630, loss = 0.46738699\n",
      "Iteration 631, loss = 0.46734487\n",
      "Iteration 632, loss = 0.46757471\n",
      "Iteration 633, loss = 0.46760934\n",
      "Iteration 634, loss = 0.46751644\n",
      "Iteration 635, loss = 0.46745058\n",
      "Iteration 636, loss = 0.46741620\n",
      "Iteration 637, loss = 0.46739228\n",
      "Iteration 638, loss = 0.46741346\n",
      "Iteration 639, loss = 0.46761511\n",
      "Iteration 640, loss = 0.46730538\n",
      "Iteration 641, loss = 0.46743080\n",
      "Iteration 642, loss = 0.46722694\n",
      "Iteration 643, loss = 0.46724576\n",
      "Iteration 644, loss = 0.46726767\n",
      "Iteration 645, loss = 0.46734896\n",
      "Iteration 646, loss = 0.46770850\n",
      "Iteration 647, loss = 0.46733928\n",
      "Iteration 648, loss = 0.46728972\n",
      "Iteration 649, loss = 0.46724442\n",
      "Iteration 650, loss = 0.46699489\n",
      "Iteration 651, loss = 0.46718797\n",
      "Iteration 652, loss = 0.46676603\n",
      "Iteration 653, loss = 0.46695918\n",
      "Iteration 654, loss = 0.46692168\n",
      "Iteration 655, loss = 0.46705013\n",
      "Iteration 656, loss = 0.46674725\n",
      "Iteration 657, loss = 0.46681171\n",
      "Iteration 658, loss = 0.46684171\n",
      "Iteration 659, loss = 0.46678976\n",
      "Iteration 660, loss = 0.46663698\n",
      "Iteration 661, loss = 0.46673402\n",
      "Iteration 662, loss = 0.46683320\n",
      "Iteration 663, loss = 0.46687232\n",
      "Iteration 664, loss = 0.46668165\n",
      "Iteration 665, loss = 0.46640324\n",
      "Iteration 666, loss = 0.46657397\n",
      "Iteration 667, loss = 0.46666927\n",
      "Iteration 668, loss = 0.46645885\n",
      "Iteration 669, loss = 0.46653136\n",
      "Iteration 670, loss = 0.46638679\n",
      "Iteration 671, loss = 0.46656944\n",
      "Iteration 672, loss = 0.46646787\n",
      "Iteration 673, loss = 0.46661597\n",
      "Iteration 674, loss = 0.46628681\n",
      "Iteration 675, loss = 0.46623007\n",
      "Iteration 676, loss = 0.46621752\n",
      "Iteration 677, loss = 0.46634469\n",
      "Iteration 678, loss = 0.46598505\n",
      "Iteration 679, loss = 0.46627230\n",
      "Iteration 680, loss = 0.46633126\n",
      "Iteration 681, loss = 0.46604205\n",
      "Iteration 682, loss = 0.46609161\n",
      "Iteration 683, loss = 0.46586512\n",
      "Iteration 684, loss = 0.46586248\n",
      "Iteration 685, loss = 0.46577570\n",
      "Iteration 686, loss = 0.46588455\n",
      "Iteration 687, loss = 0.46576644\n",
      "Iteration 688, loss = 0.46574054\n",
      "Iteration 689, loss = 0.46622750\n",
      "Iteration 690, loss = 0.46619744\n",
      "Iteration 691, loss = 0.46584495\n",
      "Iteration 692, loss = 0.46557275\n",
      "Iteration 693, loss = 0.46576789\n",
      "Iteration 694, loss = 0.46547191\n",
      "Iteration 695, loss = 0.46580664\n",
      "Iteration 696, loss = 0.46543490\n",
      "Iteration 697, loss = 0.46547181\n",
      "Iteration 698, loss = 0.46552097\n",
      "Iteration 699, loss = 0.46553012\n",
      "Iteration 700, loss = 0.46561967\n",
      "Iteration 701, loss = 0.46507463\n",
      "Iteration 702, loss = 0.46543247\n",
      "Iteration 703, loss = 0.46606722\n",
      "Iteration 704, loss = 0.46506389\n",
      "Iteration 705, loss = 0.46538014\n",
      "Iteration 706, loss = 0.46518876\n",
      "Iteration 707, loss = 0.46511310\n",
      "Iteration 708, loss = 0.46534159\n",
      "Iteration 709, loss = 0.46521389\n",
      "Iteration 710, loss = 0.46511962\n",
      "Iteration 711, loss = 0.46534811\n",
      "Iteration 712, loss = 0.46534021\n",
      "Iteration 713, loss = 0.46559847\n",
      "Iteration 714, loss = 0.46509535\n",
      "Iteration 715, loss = 0.46510060\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74660771\n",
      "Iteration 2, loss = 0.71851119\n",
      "Iteration 3, loss = 0.69663326\n",
      "Iteration 4, loss = 0.67880410\n",
      "Iteration 5, loss = 0.66396280\n",
      "Iteration 6, loss = 0.65109906\n",
      "Iteration 7, loss = 0.63966761\n",
      "Iteration 8, loss = 0.62909033\n",
      "Iteration 9, loss = 0.61954343\n",
      "Iteration 10, loss = 0.61132206\n",
      "Iteration 11, loss = 0.60391408\n",
      "Iteration 12, loss = 0.59741112\n",
      "Iteration 13, loss = 0.59208589\n",
      "Iteration 14, loss = 0.58719205\n",
      "Iteration 15, loss = 0.58305392\n",
      "Iteration 16, loss = 0.57945873\n",
      "Iteration 17, loss = 0.57644045\n",
      "Iteration 18, loss = 0.57336335\n",
      "Iteration 19, loss = 0.57100563\n",
      "Iteration 20, loss = 0.56857034\n",
      "Iteration 21, loss = 0.56640204\n",
      "Iteration 22, loss = 0.56427642\n",
      "Iteration 23, loss = 0.56229900\n",
      "Iteration 24, loss = 0.56036378\n",
      "Iteration 25, loss = 0.55896808\n",
      "Iteration 26, loss = 0.55759555\n",
      "Iteration 27, loss = 0.55525353\n",
      "Iteration 28, loss = 0.55391467\n",
      "Iteration 29, loss = 0.55253892\n",
      "Iteration 30, loss = 0.55095900\n",
      "Iteration 31, loss = 0.54984753\n",
      "Iteration 32, loss = 0.54846954\n",
      "Iteration 33, loss = 0.54709845\n",
      "Iteration 34, loss = 0.54579043\n",
      "Iteration 35, loss = 0.54468432\n",
      "Iteration 36, loss = 0.54341648\n",
      "Iteration 37, loss = 0.54226224\n",
      "Iteration 38, loss = 0.54107603\n",
      "Iteration 39, loss = 0.54002968\n",
      "Iteration 40, loss = 0.53859531\n",
      "Iteration 41, loss = 0.53736477\n",
      "Iteration 42, loss = 0.53640335\n",
      "Iteration 43, loss = 0.53474574\n",
      "Iteration 44, loss = 0.53367953\n",
      "Iteration 45, loss = 0.53252844\n",
      "Iteration 46, loss = 0.53127625\n",
      "Iteration 47, loss = 0.53017992\n",
      "Iteration 48, loss = 0.52904691\n",
      "Iteration 49, loss = 0.52809782\n",
      "Iteration 50, loss = 0.52656559\n",
      "Iteration 51, loss = 0.52542282\n",
      "Iteration 52, loss = 0.52422008\n",
      "Iteration 53, loss = 0.52344208\n",
      "Iteration 54, loss = 0.52215629\n",
      "Iteration 55, loss = 0.52112734\n",
      "Iteration 56, loss = 0.52014695\n",
      "Iteration 57, loss = 0.51902861\n",
      "Iteration 58, loss = 0.51817988\n",
      "Iteration 59, loss = 0.51748103\n",
      "Iteration 60, loss = 0.51680678\n",
      "Iteration 61, loss = 0.51567299\n",
      "Iteration 62, loss = 0.51493321\n",
      "Iteration 63, loss = 0.51435247\n",
      "Iteration 64, loss = 0.51358749\n",
      "Iteration 65, loss = 0.51296637\n",
      "Iteration 66, loss = 0.51235607\n",
      "Iteration 67, loss = 0.51163832\n",
      "Iteration 68, loss = 0.51116085\n",
      "Iteration 69, loss = 0.51082962\n",
      "Iteration 70, loss = 0.51016474\n",
      "Iteration 71, loss = 0.50962266\n",
      "Iteration 72, loss = 0.50932057\n",
      "Iteration 73, loss = 0.50881275\n",
      "Iteration 74, loss = 0.50830126\n",
      "Iteration 75, loss = 0.50801505\n",
      "Iteration 76, loss = 0.50754004\n",
      "Iteration 77, loss = 0.50765195\n",
      "Iteration 78, loss = 0.50714629\n",
      "Iteration 79, loss = 0.50710580\n",
      "Iteration 80, loss = 0.50677828\n",
      "Iteration 81, loss = 0.50620087\n",
      "Iteration 82, loss = 0.50597592\n",
      "Iteration 83, loss = 0.50583259\n",
      "Iteration 84, loss = 0.50553333\n",
      "Iteration 85, loss = 0.50527882\n",
      "Iteration 86, loss = 0.50496497\n",
      "Iteration 87, loss = 0.50498036\n",
      "Iteration 88, loss = 0.50450456\n",
      "Iteration 89, loss = 0.50438949\n",
      "Iteration 90, loss = 0.50441836\n",
      "Iteration 91, loss = 0.50434851\n",
      "Iteration 92, loss = 0.50398771\n",
      "Iteration 93, loss = 0.50385417\n",
      "Iteration 94, loss = 0.50368725\n",
      "Iteration 95, loss = 0.50361844\n",
      "Iteration 96, loss = 0.50338849\n",
      "Iteration 97, loss = 0.50314015\n",
      "Iteration 98, loss = 0.50330768\n",
      "Iteration 99, loss = 0.50315686\n",
      "Iteration 100, loss = 0.50278817\n",
      "Iteration 101, loss = 0.50282918\n",
      "Iteration 102, loss = 0.50247091\n",
      "Iteration 103, loss = 0.50269601\n",
      "Iteration 104, loss = 0.50247625\n",
      "Iteration 105, loss = 0.50223562\n",
      "Iteration 106, loss = 0.50210745\n",
      "Iteration 107, loss = 0.50225807\n",
      "Iteration 108, loss = 0.50198957\n",
      "Iteration 109, loss = 0.50186352\n",
      "Iteration 110, loss = 0.50175381\n",
      "Iteration 111, loss = 0.50148036\n",
      "Iteration 112, loss = 0.50159696\n",
      "Iteration 113, loss = 0.50157067\n",
      "Iteration 114, loss = 0.50097065\n",
      "Iteration 115, loss = 0.50119712\n",
      "Iteration 116, loss = 0.50086091\n",
      "Iteration 117, loss = 0.50128688\n",
      "Iteration 118, loss = 0.50072833\n",
      "Iteration 119, loss = 0.50064404\n",
      "Iteration 120, loss = 0.50053351\n",
      "Iteration 121, loss = 0.50045738\n",
      "Iteration 122, loss = 0.50025010\n",
      "Iteration 123, loss = 0.50043661\n",
      "Iteration 124, loss = 0.50023627\n",
      "Iteration 125, loss = 0.50007719\n",
      "Iteration 126, loss = 0.50003029\n",
      "Iteration 127, loss = 0.50055984\n",
      "Iteration 128, loss = 0.49994170\n",
      "Iteration 129, loss = 0.49991264\n",
      "Iteration 130, loss = 0.49942453\n",
      "Iteration 131, loss = 0.49963727\n",
      "Iteration 132, loss = 0.49957595\n",
      "Iteration 133, loss = 0.49927357\n",
      "Iteration 134, loss = 0.49966290\n",
      "Iteration 135, loss = 0.49943962\n",
      "Iteration 136, loss = 0.49937223\n",
      "Iteration 137, loss = 0.49913033\n",
      "Iteration 138, loss = 0.49901655\n",
      "Iteration 139, loss = 0.49897714\n",
      "Iteration 140, loss = 0.49893055\n",
      "Iteration 141, loss = 0.49873769\n",
      "Iteration 142, loss = 0.49879073\n",
      "Iteration 143, loss = 0.49846691\n",
      "Iteration 144, loss = 0.49855281\n",
      "Iteration 145, loss = 0.49843167\n",
      "Iteration 146, loss = 0.49842260\n",
      "Iteration 147, loss = 0.49827090\n",
      "Iteration 148, loss = 0.49814944\n",
      "Iteration 149, loss = 0.49820297\n",
      "Iteration 150, loss = 0.49805227\n",
      "Iteration 151, loss = 0.49835137\n",
      "Iteration 152, loss = 0.49806105\n",
      "Iteration 153, loss = 0.49795093\n",
      "Iteration 154, loss = 0.49774574\n",
      "Iteration 155, loss = 0.49778941\n",
      "Iteration 156, loss = 0.49803553\n",
      "Iteration 157, loss = 0.49743142\n",
      "Iteration 158, loss = 0.49755361\n",
      "Iteration 159, loss = 0.49757918\n",
      "Iteration 160, loss = 0.49740485\n",
      "Iteration 161, loss = 0.49747925\n",
      "Iteration 162, loss = 0.49759570\n",
      "Iteration 163, loss = 0.49725364\n",
      "Iteration 164, loss = 0.49719834\n",
      "Iteration 165, loss = 0.49709619\n",
      "Iteration 166, loss = 0.49741328\n",
      "Iteration 167, loss = 0.49695886\n",
      "Iteration 168, loss = 0.49698289\n",
      "Iteration 169, loss = 0.49699517\n",
      "Iteration 170, loss = 0.49678676\n",
      "Iteration 171, loss = 0.49708963\n",
      "Iteration 172, loss = 0.49689066\n",
      "Iteration 173, loss = 0.49685506\n",
      "Iteration 174, loss = 0.49680311\n",
      "Iteration 175, loss = 0.49691576\n",
      "Iteration 176, loss = 0.49675828\n",
      "Iteration 177, loss = 0.49641295\n",
      "Iteration 178, loss = 0.49644699\n",
      "Iteration 179, loss = 0.49659209\n",
      "Iteration 180, loss = 0.49627703\n",
      "Iteration 181, loss = 0.49624651\n",
      "Iteration 182, loss = 0.49601433\n",
      "Iteration 183, loss = 0.49601216\n",
      "Iteration 184, loss = 0.49603949\n",
      "Iteration 185, loss = 0.49621717\n",
      "Iteration 186, loss = 0.49603550\n",
      "Iteration 187, loss = 0.49579229\n",
      "Iteration 188, loss = 0.49576659\n",
      "Iteration 189, loss = 0.49581532\n",
      "Iteration 190, loss = 0.49589318\n",
      "Iteration 191, loss = 0.49595762\n",
      "Iteration 192, loss = 0.49555115\n",
      "Iteration 193, loss = 0.49582176\n",
      "Iteration 194, loss = 0.49540110\n",
      "Iteration 195, loss = 0.49542748\n",
      "Iteration 196, loss = 0.49546170\n",
      "Iteration 197, loss = 0.49532485\n",
      "Iteration 198, loss = 0.49507367\n",
      "Iteration 199, loss = 0.49510015\n",
      "Iteration 200, loss = 0.49530966\n",
      "Iteration 201, loss = 0.49523110\n",
      "Iteration 202, loss = 0.49492800\n",
      "Iteration 203, loss = 0.49471605\n",
      "Iteration 204, loss = 0.49499242\n",
      "Iteration 205, loss = 0.49491258\n",
      "Iteration 206, loss = 0.49451978\n",
      "Iteration 207, loss = 0.49496885\n",
      "Iteration 208, loss = 0.49444198\n",
      "Iteration 209, loss = 0.49439409\n",
      "Iteration 210, loss = 0.49415682\n",
      "Iteration 211, loss = 0.49412160\n",
      "Iteration 212, loss = 0.49424868\n",
      "Iteration 213, loss = 0.49443853\n",
      "Iteration 214, loss = 0.49400058\n",
      "Iteration 215, loss = 0.49411024\n",
      "Iteration 216, loss = 0.49381407\n",
      "Iteration 217, loss = 0.49391015\n",
      "Iteration 218, loss = 0.49370054\n",
      "Iteration 219, loss = 0.49394773\n",
      "Iteration 220, loss = 0.49370480\n",
      "Iteration 221, loss = 0.49382327\n",
      "Iteration 222, loss = 0.49346321\n",
      "Iteration 223, loss = 0.49331858\n",
      "Iteration 224, loss = 0.49344769\n",
      "Iteration 225, loss = 0.49351121\n",
      "Iteration 226, loss = 0.49325038\n",
      "Iteration 227, loss = 0.49337238\n",
      "Iteration 228, loss = 0.49328532\n",
      "Iteration 229, loss = 0.49310026\n",
      "Iteration 230, loss = 0.49318500\n",
      "Iteration 231, loss = 0.49307901\n",
      "Iteration 232, loss = 0.49284688\n",
      "Iteration 233, loss = 0.49289861\n",
      "Iteration 234, loss = 0.49287734\n",
      "Iteration 235, loss = 0.49262651\n",
      "Iteration 236, loss = 0.49264104\n",
      "Iteration 237, loss = 0.49285647\n",
      "Iteration 238, loss = 0.49266953\n",
      "Iteration 239, loss = 0.49253021\n",
      "Iteration 240, loss = 0.49255539\n",
      "Iteration 241, loss = 0.49263987\n",
      "Iteration 242, loss = 0.49211043\n",
      "Iteration 243, loss = 0.49224373\n",
      "Iteration 244, loss = 0.49222767\n",
      "Iteration 245, loss = 0.49219225\n",
      "Iteration 246, loss = 0.49198337\n",
      "Iteration 247, loss = 0.49210549\n",
      "Iteration 248, loss = 0.49188139\n",
      "Iteration 249, loss = 0.49187641\n",
      "Iteration 250, loss = 0.49196055\n",
      "Iteration 251, loss = 0.49185670\n",
      "Iteration 252, loss = 0.49195861\n",
      "Iteration 253, loss = 0.49148982\n",
      "Iteration 254, loss = 0.49171607\n",
      "Iteration 255, loss = 0.49201950\n",
      "Iteration 256, loss = 0.49172733\n",
      "Iteration 257, loss = 0.49137586\n",
      "Iteration 258, loss = 0.49138744\n",
      "Iteration 259, loss = 0.49113868\n",
      "Iteration 260, loss = 0.49110435\n",
      "Iteration 261, loss = 0.49173232\n",
      "Iteration 262, loss = 0.49138242\n",
      "Iteration 263, loss = 0.49098243\n",
      "Iteration 264, loss = 0.49109985\n",
      "Iteration 265, loss = 0.49102707\n",
      "Iteration 266, loss = 0.49135111\n",
      "Iteration 267, loss = 0.49082893\n",
      "Iteration 268, loss = 0.49166473\n",
      "Iteration 269, loss = 0.49060326\n",
      "Iteration 270, loss = 0.49151681\n",
      "Iteration 271, loss = 0.49148475\n",
      "Iteration 272, loss = 0.49073761\n",
      "Iteration 273, loss = 0.49024157\n",
      "Iteration 274, loss = 0.49097982\n",
      "Iteration 275, loss = 0.49047851\n",
      "Iteration 276, loss = 0.49004504\n",
      "Iteration 277, loss = 0.49007672\n",
      "Iteration 278, loss = 0.49018605\n",
      "Iteration 279, loss = 0.48995964\n",
      "Iteration 280, loss = 0.49001088\n",
      "Iteration 281, loss = 0.48976039\n",
      "Iteration 282, loss = 0.48970266\n",
      "Iteration 283, loss = 0.48987250\n",
      "Iteration 284, loss = 0.48969360\n",
      "Iteration 285, loss = 0.48962023\n",
      "Iteration 286, loss = 0.48959725\n",
      "Iteration 287, loss = 0.48957420\n",
      "Iteration 288, loss = 0.48993504\n",
      "Iteration 289, loss = 0.48985762\n",
      "Iteration 290, loss = 0.48936641\n",
      "Iteration 291, loss = 0.48918706\n",
      "Iteration 292, loss = 0.48913346\n",
      "Iteration 293, loss = 0.48940688\n",
      "Iteration 294, loss = 0.48923846\n",
      "Iteration 295, loss = 0.48909250\n",
      "Iteration 296, loss = 0.48901544\n",
      "Iteration 297, loss = 0.48878483\n",
      "Iteration 298, loss = 0.48890772\n",
      "Iteration 299, loss = 0.48897559\n",
      "Iteration 300, loss = 0.48915850\n",
      "Iteration 301, loss = 0.48890759\n",
      "Iteration 302, loss = 0.48869841\n",
      "Iteration 303, loss = 0.48882469\n",
      "Iteration 304, loss = 0.48874373\n",
      "Iteration 305, loss = 0.48843760\n",
      "Iteration 306, loss = 0.48863454\n",
      "Iteration 307, loss = 0.48844884\n",
      "Iteration 308, loss = 0.48855532\n",
      "Iteration 309, loss = 0.48848461\n",
      "Iteration 310, loss = 0.48837791\n",
      "Iteration 311, loss = 0.48820902\n",
      "Iteration 312, loss = 0.48855906\n",
      "Iteration 313, loss = 0.48833690\n",
      "Iteration 314, loss = 0.48805652\n",
      "Iteration 315, loss = 0.48810989\n",
      "Iteration 316, loss = 0.48805439\n",
      "Iteration 317, loss = 0.48800612\n",
      "Iteration 318, loss = 0.48790361\n",
      "Iteration 319, loss = 0.48821113\n",
      "Iteration 320, loss = 0.48805421\n",
      "Iteration 321, loss = 0.48765039\n",
      "Iteration 322, loss = 0.48781221\n",
      "Iteration 323, loss = 0.48761291\n",
      "Iteration 324, loss = 0.48757376\n",
      "Iteration 325, loss = 0.48756183\n",
      "Iteration 326, loss = 0.48753183\n",
      "Iteration 327, loss = 0.48747611\n",
      "Iteration 328, loss = 0.48779578\n",
      "Iteration 329, loss = 0.48765216\n",
      "Iteration 330, loss = 0.48800165\n",
      "Iteration 331, loss = 0.48752176\n",
      "Iteration 332, loss = 0.48784799\n",
      "Iteration 333, loss = 0.48701223\n",
      "Iteration 334, loss = 0.48715990\n",
      "Iteration 335, loss = 0.48708869\n",
      "Iteration 336, loss = 0.48715939\n",
      "Iteration 337, loss = 0.48706019\n",
      "Iteration 338, loss = 0.48699943\n",
      "Iteration 339, loss = 0.48677726\n",
      "Iteration 340, loss = 0.48673606\n",
      "Iteration 341, loss = 0.48700976\n",
      "Iteration 342, loss = 0.48711448\n",
      "Iteration 343, loss = 0.48670112\n",
      "Iteration 344, loss = 0.48665689\n",
      "Iteration 345, loss = 0.48657338\n",
      "Iteration 346, loss = 0.48637751\n",
      "Iteration 347, loss = 0.48660746\n",
      "Iteration 348, loss = 0.48632246\n",
      "Iteration 349, loss = 0.48673611\n",
      "Iteration 350, loss = 0.48646463\n",
      "Iteration 351, loss = 0.48630409\n",
      "Iteration 352, loss = 0.48645710\n",
      "Iteration 353, loss = 0.48654999\n",
      "Iteration 354, loss = 0.48614642\n",
      "Iteration 355, loss = 0.48621911\n",
      "Iteration 356, loss = 0.48611233\n",
      "Iteration 357, loss = 0.48607743\n",
      "Iteration 358, loss = 0.48713844\n",
      "Iteration 359, loss = 0.48650055\n",
      "Iteration 360, loss = 0.48646508\n",
      "Iteration 361, loss = 0.48620065\n",
      "Iteration 362, loss = 0.48568963\n",
      "Iteration 363, loss = 0.48616881\n",
      "Iteration 364, loss = 0.48644474\n",
      "Iteration 365, loss = 0.48582719\n",
      "Iteration 366, loss = 0.48601719\n",
      "Iteration 367, loss = 0.48592814\n",
      "Iteration 368, loss = 0.48566794\n",
      "Iteration 369, loss = 0.48556645\n",
      "Iteration 370, loss = 0.48584032\n",
      "Iteration 371, loss = 0.48567109\n",
      "Iteration 372, loss = 0.48576494\n",
      "Iteration 373, loss = 0.48562690\n",
      "Iteration 374, loss = 0.48592053\n",
      "Iteration 375, loss = 0.48542576\n",
      "Iteration 376, loss = 0.48541475\n",
      "Iteration 377, loss = 0.48652940\n",
      "Iteration 378, loss = 0.48557742\n",
      "Iteration 379, loss = 0.48548634\n",
      "Iteration 380, loss = 0.48623064\n",
      "Iteration 381, loss = 0.48557323\n",
      "Iteration 382, loss = 0.48525966\n",
      "Iteration 383, loss = 0.48526592\n",
      "Iteration 384, loss = 0.48535224\n",
      "Iteration 385, loss = 0.48515137\n",
      "Iteration 386, loss = 0.48525325\n",
      "Iteration 387, loss = 0.48521154\n",
      "Iteration 388, loss = 0.48533710\n",
      "Iteration 389, loss = 0.48551969\n",
      "Iteration 390, loss = 0.48531075\n",
      "Iteration 391, loss = 0.48508358\n",
      "Iteration 392, loss = 0.48517891\n",
      "Iteration 393, loss = 0.48511385\n",
      "Iteration 394, loss = 0.48495207\n",
      "Iteration 395, loss = 0.48523982\n",
      "Iteration 396, loss = 0.48510680\n",
      "Iteration 397, loss = 0.48473327\n",
      "Iteration 398, loss = 0.48513939\n",
      "Iteration 399, loss = 0.48479533\n",
      "Iteration 400, loss = 0.48543003\n",
      "Iteration 401, loss = 0.48489336\n",
      "Iteration 402, loss = 0.48508000\n",
      "Iteration 403, loss = 0.48469487\n",
      "Iteration 404, loss = 0.48450661\n",
      "Iteration 405, loss = 0.48487675\n",
      "Iteration 406, loss = 0.48491876\n",
      "Iteration 407, loss = 0.48472189\n",
      "Iteration 408, loss = 0.48483199\n",
      "Iteration 409, loss = 0.48463735\n",
      "Iteration 410, loss = 0.48551212\n",
      "Iteration 411, loss = 0.48455620\n",
      "Iteration 412, loss = 0.48453294\n",
      "Iteration 413, loss = 0.48440022\n",
      "Iteration 414, loss = 0.48424295\n",
      "Iteration 415, loss = 0.48429403\n",
      "Iteration 416, loss = 0.48446655\n",
      "Iteration 417, loss = 0.48424828\n",
      "Iteration 418, loss = 0.48434869\n",
      "Iteration 419, loss = 0.48442297\n",
      "Iteration 420, loss = 0.48447621\n",
      "Iteration 421, loss = 0.48391625\n",
      "Iteration 422, loss = 0.48400607\n",
      "Iteration 423, loss = 0.48390912\n",
      "Iteration 424, loss = 0.48399435\n",
      "Iteration 425, loss = 0.48419647\n",
      "Iteration 426, loss = 0.48388446\n",
      "Iteration 427, loss = 0.48407199\n",
      "Iteration 428, loss = 0.48409912\n",
      "Iteration 429, loss = 0.48383263\n",
      "Iteration 430, loss = 0.48371445\n",
      "Iteration 431, loss = 0.48386960\n",
      "Iteration 432, loss = 0.48399936\n",
      "Iteration 433, loss = 0.48420192\n",
      "Iteration 434, loss = 0.48413161\n",
      "Iteration 435, loss = 0.48408993\n",
      "Iteration 436, loss = 0.48391202\n",
      "Iteration 437, loss = 0.48394903\n",
      "Iteration 438, loss = 0.48342010\n",
      "Iteration 439, loss = 0.48411426\n",
      "Iteration 440, loss = 0.48362151\n",
      "Iteration 441, loss = 0.48361070\n",
      "Iteration 442, loss = 0.48344480\n",
      "Iteration 443, loss = 0.48377774\n",
      "Iteration 444, loss = 0.48325498\n",
      "Iteration 445, loss = 0.48343099\n",
      "Iteration 446, loss = 0.48366836\n",
      "Iteration 447, loss = 0.48335742\n",
      "Iteration 448, loss = 0.48369039\n",
      "Iteration 449, loss = 0.48332166\n",
      "Iteration 450, loss = 0.48342191\n",
      "Iteration 451, loss = 0.48359123\n",
      "Iteration 452, loss = 0.48350843\n",
      "Iteration 453, loss = 0.48304311\n",
      "Iteration 454, loss = 0.48351796\n",
      "Iteration 455, loss = 0.48288471\n",
      "Iteration 456, loss = 0.48343454\n",
      "Iteration 457, loss = 0.48330584\n",
      "Iteration 458, loss = 0.48358532\n",
      "Iteration 459, loss = 0.48312697\n",
      "Iteration 460, loss = 0.48283890\n",
      "Iteration 461, loss = 0.48308819\n",
      "Iteration 462, loss = 0.48309810\n",
      "Iteration 463, loss = 0.48287657\n",
      "Iteration 464, loss = 0.48312412\n",
      "Iteration 465, loss = 0.48339330\n",
      "Iteration 466, loss = 0.48286284\n",
      "Iteration 467, loss = 0.48286992\n",
      "Iteration 468, loss = 0.48267171\n",
      "Iteration 469, loss = 0.48255888\n",
      "Iteration 470, loss = 0.48292803\n",
      "Iteration 471, loss = 0.48298945\n",
      "Iteration 472, loss = 0.48280969\n",
      "Iteration 473, loss = 0.48255007\n",
      "Iteration 474, loss = 0.48280467\n",
      "Iteration 475, loss = 0.48237867\n",
      "Iteration 476, loss = 0.48261016\n",
      "Iteration 477, loss = 0.48259012\n",
      "Iteration 478, loss = 0.48241666\n",
      "Iteration 479, loss = 0.48281311\n",
      "Iteration 480, loss = 0.48276290\n",
      "Iteration 481, loss = 0.48233478\n",
      "Iteration 482, loss = 0.48242763\n",
      "Iteration 483, loss = 0.48251721\n",
      "Iteration 484, loss = 0.48270508\n",
      "Iteration 485, loss = 0.48308289\n",
      "Iteration 486, loss = 0.48233939\n",
      "Iteration 487, loss = 0.48241844\n",
      "Iteration 488, loss = 0.48243516\n",
      "Iteration 489, loss = 0.48230522\n",
      "Iteration 490, loss = 0.48222598\n",
      "Iteration 491, loss = 0.48233364\n",
      "Iteration 492, loss = 0.48277907\n",
      "Iteration 493, loss = 0.48264953\n",
      "Iteration 494, loss = 0.48220186\n",
      "Iteration 495, loss = 0.48238719\n",
      "Iteration 496, loss = 0.48220249\n",
      "Iteration 497, loss = 0.48198882\n",
      "Iteration 498, loss = 0.48192669\n",
      "Iteration 499, loss = 0.48263455\n",
      "Iteration 500, loss = 0.48294820\n",
      "Iteration 501, loss = 0.48177901\n",
      "Iteration 502, loss = 0.48188555\n",
      "Iteration 503, loss = 0.48253367\n",
      "Iteration 504, loss = 0.48189553\n",
      "Iteration 505, loss = 0.48178767\n",
      "Iteration 506, loss = 0.48228442\n",
      "Iteration 507, loss = 0.48206084\n",
      "Iteration 508, loss = 0.48201774\n",
      "Iteration 509, loss = 0.48202652\n",
      "Iteration 510, loss = 0.48183663\n",
      "Iteration 511, loss = 0.48211983\n",
      "Iteration 512, loss = 0.48171402\n",
      "Iteration 513, loss = 0.48166510\n",
      "Iteration 514, loss = 0.48197331\n",
      "Iteration 515, loss = 0.48185363\n",
      "Iteration 516, loss = 0.48177451\n",
      "Iteration 517, loss = 0.48243162\n",
      "Iteration 518, loss = 0.48180931\n",
      "Iteration 519, loss = 0.48182375\n",
      "Iteration 520, loss = 0.48168300\n",
      "Iteration 521, loss = 0.48151327\n",
      "Iteration 522, loss = 0.48175734\n",
      "Iteration 523, loss = 0.48198074\n",
      "Iteration 524, loss = 0.48156440\n",
      "Iteration 525, loss = 0.48188777\n",
      "Iteration 526, loss = 0.48192086\n",
      "Iteration 527, loss = 0.48174355\n",
      "Iteration 528, loss = 0.48173131\n",
      "Iteration 529, loss = 0.48156508\n",
      "Iteration 530, loss = 0.48233599\n",
      "Iteration 531, loss = 0.48154928\n",
      "Iteration 532, loss = 0.48164159\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "scores_ann = cross_val_score(model_ann, X_customer_balanced, Y_customer_balanced, cv=kf_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.74744898 0.73469388 0.77295918 0.72959184 0.72704082 0.72704082\n",
      " 0.71428571 0.7755102  0.73657289 0.78516624]\n",
      "Score médio: 0.7450310559006212\n",
      "Desvio padrão: 0.023097573402394073\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_ann}\")\n",
    "print(f\"Score médio: {np.mean(scores_ann)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_ann)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - 66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "kf_tree = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree = cross_val_score(model_tree, X_customer_balanced, Y_customer_balanced, cv=kf_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.6505102  0.65306122 0.66071429 0.67091837 0.62755102 0.63010204\n",
      " 0.68367347 0.69642857 0.69309463 0.68030691]\n",
      "Score médio: 0.6646360718200324\n",
      "Desvio padrão: 0.023205420177937815\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p = 2)\n",
    "kf_knn = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn = cross_val_score(model_knn, X_customer_balanced, Y_customer_balanced, cv=kf_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70153061 0.70408163 0.72704082 0.69132653 0.68877551 0.66071429\n",
      " 0.68367347 0.75510204 0.70588235 0.71611253]\n",
      "Score médio: 0.703423978286967\n",
      "Desvio padrão: 0.02444292091659416\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_knn}\")\n",
    "print(f\"Score médio: {np.mean(scores_knn)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(random_state=42, max_iter=150)\n",
    "kf_logistic = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic = cross_val_score(model_logistic, X_customer_balanced, Y_customer_balanced, cv=kf_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.67602041 0.70663265 0.71938776 0.71683673 0.66326531 0.67091837\n",
      " 0.72959184 0.75255102 0.71355499 0.74680307]\n",
      "Score médio: 0.7095562137898638\n",
      "Desvio padrão: 0.029277793572685756\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_logistic}\")\n",
    "print(f\"Score médio: {np.mean(scores_logistic)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_logistic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = GaussianNB()\n",
    "kf_naive = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = cross_val_score(model_naive, X_customer_balanced, Y_customer_balanced, cv=kf_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.68877551 0.7244898  0.70918367 0.73214286 0.68877551 0.68112245\n",
      " 0.72193878 0.77295918 0.72634271 0.75959079]\n",
      "Score médio: 0.7205321258938358\n",
      "Desvio padrão: 0.028564427059807447\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_naive}\")\n",
    "print(f\"Score médio: {np.mean(scores_naive)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_naive)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(n_estimators=80, criterion='entropy', random_state=42)\n",
    "kf_forest = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_forest = cross_val_score(model_forest, X_customer_balanced, Y_customer_balanced, cv=kf_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.75       0.75765306 0.7627551  0.75       0.7372449  0.72193878\n",
      " 0.73469388 0.78316327 0.76470588 0.76982097]\n",
      "Score médio: 0.7531975833811785\n",
      "Desvio padrão: 0.01735613718267566\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_forest}\")\n",
    "print(f\"Score médio: {np.mean(scores_forest)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_forest)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
