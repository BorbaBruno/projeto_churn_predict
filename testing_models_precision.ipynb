{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('customer.pkl', 'rb') as f:\n",
    "    X_customer_balanced, Y_customer_balanced = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *SVM - 74,17%(Normal) 74,96%(Boosted)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3918, 10), (3918,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_customer_balanced.shape, Y_customer_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', random_state=42, C=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boosted = SVC(kernel='rbf', random_state=42, C=10.0, tol=0.001, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_customer_balanced, Y_customer_balanced, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_boosted = cross_val_score(model_boosted, X_customer_balanced, Y_customer_balanced, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.7372449  0.75       0.75255102 0.73469388 0.72704082 0.69897959\n",
      " 0.73469388 0.78316327 0.72890026 0.76982097]\n",
      "Score médio: 0.7417088574560259\n",
      "Desvio padrão: 0.022401792602462955\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores}\")\n",
    "print(f\"Score médio: {np.mean(scores)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.73979592 0.75       0.76785714 0.75255102 0.71938776 0.70918367\n",
      " 0.75       0.78316327 0.75191816 0.77237852]\n",
      "Score médio: 0.749623545070202\n",
      "Desvio padrão: 0.02153010197017537\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularização\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],  # Kernels mais comuns\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.001],  # Para 'rbf' kernel\n",
    "    'tol': [0.001, 0.0001]  # Tolerância de otimização\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf', 'tol': 0.001}\n",
      "0.7468110615893867\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Artificial Neural Network - 74,5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = MLPClassifier(max_iter=1500, verbose=True, tol=0.000000, solver='adam', activation='relu', hidden_layer_sizes=(10,10))\n",
    "kf_ann = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73863469\n",
      "Iteration 2, loss = 0.71626398\n",
      "Iteration 3, loss = 0.69898458\n",
      "Iteration 4, loss = 0.68418740\n",
      "Iteration 5, loss = 0.67120495\n",
      "Iteration 6, loss = 0.65907086\n",
      "Iteration 7, loss = 0.64748423\n",
      "Iteration 8, loss = 0.63641259\n",
      "Iteration 9, loss = 0.62541587\n",
      "Iteration 10, loss = 0.61502725\n",
      "Iteration 11, loss = 0.60497337\n",
      "Iteration 12, loss = 0.59606421\n",
      "Iteration 13, loss = 0.58805333\n",
      "Iteration 14, loss = 0.58129999\n",
      "Iteration 15, loss = 0.57499346\n",
      "Iteration 16, loss = 0.56989449\n",
      "Iteration 17, loss = 0.56546591\n",
      "Iteration 18, loss = 0.56181457\n",
      "Iteration 19, loss = 0.55838251\n",
      "Iteration 20, loss = 0.55552057\n",
      "Iteration 21, loss = 0.55296423\n",
      "Iteration 22, loss = 0.55061383\n",
      "Iteration 23, loss = 0.54860011\n",
      "Iteration 24, loss = 0.54653116\n",
      "Iteration 25, loss = 0.54468255\n",
      "Iteration 26, loss = 0.54284336\n",
      "Iteration 27, loss = 0.54066644\n",
      "Iteration 28, loss = 0.53881459\n",
      "Iteration 29, loss = 0.53694524\n",
      "Iteration 30, loss = 0.53503057\n",
      "Iteration 31, loss = 0.53364282\n",
      "Iteration 32, loss = 0.53155326\n",
      "Iteration 33, loss = 0.52999288\n",
      "Iteration 34, loss = 0.52851929\n",
      "Iteration 35, loss = 0.52694586\n",
      "Iteration 36, loss = 0.52521763\n",
      "Iteration 37, loss = 0.52337585\n",
      "Iteration 38, loss = 0.52188382\n",
      "Iteration 39, loss = 0.52049747\n",
      "Iteration 40, loss = 0.51904764\n",
      "Iteration 41, loss = 0.51764110\n",
      "Iteration 42, loss = 0.51643234\n",
      "Iteration 43, loss = 0.51527381\n",
      "Iteration 44, loss = 0.51366234\n",
      "Iteration 45, loss = 0.51269123\n",
      "Iteration 46, loss = 0.51146826\n",
      "Iteration 47, loss = 0.51037211\n",
      "Iteration 48, loss = 0.50922122\n",
      "Iteration 49, loss = 0.50844878\n",
      "Iteration 50, loss = 0.50702749\n",
      "Iteration 51, loss = 0.50615990\n",
      "Iteration 52, loss = 0.50543189\n",
      "Iteration 53, loss = 0.50452415\n",
      "Iteration 54, loss = 0.50361903\n",
      "Iteration 55, loss = 0.50270790\n",
      "Iteration 56, loss = 0.50206388\n",
      "Iteration 57, loss = 0.50139266\n",
      "Iteration 58, loss = 0.50067607\n",
      "Iteration 59, loss = 0.49989222\n",
      "Iteration 60, loss = 0.49952779\n",
      "Iteration 61, loss = 0.49870192\n",
      "Iteration 62, loss = 0.49814354\n",
      "Iteration 63, loss = 0.49756373\n",
      "Iteration 64, loss = 0.49720474\n",
      "Iteration 65, loss = 0.49672588\n",
      "Iteration 66, loss = 0.49635845\n",
      "Iteration 67, loss = 0.49574618\n",
      "Iteration 68, loss = 0.49542515\n",
      "Iteration 69, loss = 0.49525859\n",
      "Iteration 70, loss = 0.49474089\n",
      "Iteration 71, loss = 0.49461353\n",
      "Iteration 72, loss = 0.49409402\n",
      "Iteration 73, loss = 0.49390095\n",
      "Iteration 74, loss = 0.49373793\n",
      "Iteration 75, loss = 0.49341704\n",
      "Iteration 76, loss = 0.49312912\n",
      "Iteration 77, loss = 0.49275968\n",
      "Iteration 78, loss = 0.49243910\n",
      "Iteration 79, loss = 0.49249381\n",
      "Iteration 80, loss = 0.49240446\n",
      "Iteration 81, loss = 0.49160378\n",
      "Iteration 82, loss = 0.49138986\n",
      "Iteration 83, loss = 0.49113357\n",
      "Iteration 84, loss = 0.49117227\n",
      "Iteration 85, loss = 0.49042753\n",
      "Iteration 86, loss = 0.49014164\n",
      "Iteration 87, loss = 0.48996237\n",
      "Iteration 88, loss = 0.48964238\n",
      "Iteration 89, loss = 0.48936706\n",
      "Iteration 90, loss = 0.48912152\n",
      "Iteration 91, loss = 0.48868973\n",
      "Iteration 92, loss = 0.48850758\n",
      "Iteration 93, loss = 0.48818133\n",
      "Iteration 94, loss = 0.48810569\n",
      "Iteration 95, loss = 0.48793488\n",
      "Iteration 96, loss = 0.48814071\n",
      "Iteration 97, loss = 0.48739850\n",
      "Iteration 98, loss = 0.48728940\n",
      "Iteration 99, loss = 0.48711745\n",
      "Iteration 100, loss = 0.48683734\n",
      "Iteration 101, loss = 0.48671713\n",
      "Iteration 102, loss = 0.48669436\n",
      "Iteration 103, loss = 0.48663910\n",
      "Iteration 104, loss = 0.48624456\n",
      "Iteration 105, loss = 0.48625769\n",
      "Iteration 106, loss = 0.48604570\n",
      "Iteration 107, loss = 0.48568334\n",
      "Iteration 108, loss = 0.48550108\n",
      "Iteration 109, loss = 0.48554002\n",
      "Iteration 110, loss = 0.48521032\n",
      "Iteration 111, loss = 0.48521785\n",
      "Iteration 112, loss = 0.48512033\n",
      "Iteration 113, loss = 0.48487364\n",
      "Iteration 114, loss = 0.48449711\n",
      "Iteration 115, loss = 0.48463472\n",
      "Iteration 116, loss = 0.48429611\n",
      "Iteration 117, loss = 0.48413666\n",
      "Iteration 118, loss = 0.48398197\n",
      "Iteration 119, loss = 0.48403685\n",
      "Iteration 120, loss = 0.48379440\n",
      "Iteration 121, loss = 0.48353353\n",
      "Iteration 122, loss = 0.48337101\n",
      "Iteration 123, loss = 0.48317377\n",
      "Iteration 124, loss = 0.48311415\n",
      "Iteration 125, loss = 0.48309722\n",
      "Iteration 126, loss = 0.48288183\n",
      "Iteration 127, loss = 0.48289912\n",
      "Iteration 128, loss = 0.48266001\n",
      "Iteration 129, loss = 0.48245271\n",
      "Iteration 130, loss = 0.48294230\n",
      "Iteration 131, loss = 0.48237107\n",
      "Iteration 132, loss = 0.48234013\n",
      "Iteration 133, loss = 0.48228262\n",
      "Iteration 134, loss = 0.48194219\n",
      "Iteration 135, loss = 0.48195990\n",
      "Iteration 136, loss = 0.48190300\n",
      "Iteration 137, loss = 0.48193794\n",
      "Iteration 138, loss = 0.48190245\n",
      "Iteration 139, loss = 0.48182137\n",
      "Iteration 140, loss = 0.48157019\n",
      "Iteration 141, loss = 0.48138590\n",
      "Iteration 142, loss = 0.48147219\n",
      "Iteration 143, loss = 0.48119609\n",
      "Iteration 144, loss = 0.48136671\n",
      "Iteration 145, loss = 0.48102905\n",
      "Iteration 146, loss = 0.48101587\n",
      "Iteration 147, loss = 0.48087684\n",
      "Iteration 148, loss = 0.48083588\n",
      "Iteration 149, loss = 0.48075893\n",
      "Iteration 150, loss = 0.48069906\n",
      "Iteration 151, loss = 0.48063038\n",
      "Iteration 152, loss = 0.48073904\n",
      "Iteration 153, loss = 0.48054527\n",
      "Iteration 154, loss = 0.48047809\n",
      "Iteration 155, loss = 0.48045115\n",
      "Iteration 156, loss = 0.48026058\n",
      "Iteration 157, loss = 0.48019285\n",
      "Iteration 158, loss = 0.48024350\n",
      "Iteration 159, loss = 0.48007045\n",
      "Iteration 160, loss = 0.48021924\n",
      "Iteration 161, loss = 0.48067309\n",
      "Iteration 162, loss = 0.47997064\n",
      "Iteration 163, loss = 0.47990670\n",
      "Iteration 164, loss = 0.47986719\n",
      "Iteration 165, loss = 0.47994689\n",
      "Iteration 166, loss = 0.47978347\n",
      "Iteration 167, loss = 0.47956426\n",
      "Iteration 168, loss = 0.47947936\n",
      "Iteration 169, loss = 0.47948359\n",
      "Iteration 170, loss = 0.47947867\n",
      "Iteration 171, loss = 0.47939277\n",
      "Iteration 172, loss = 0.47935752\n",
      "Iteration 173, loss = 0.47919601\n",
      "Iteration 174, loss = 0.47922856\n",
      "Iteration 175, loss = 0.47914400\n",
      "Iteration 176, loss = 0.47885090\n",
      "Iteration 177, loss = 0.47883585\n",
      "Iteration 178, loss = 0.47877936\n",
      "Iteration 179, loss = 0.47894075\n",
      "Iteration 180, loss = 0.47880265\n",
      "Iteration 181, loss = 0.47864383\n",
      "Iteration 182, loss = 0.47855758\n",
      "Iteration 183, loss = 0.47837792\n",
      "Iteration 184, loss = 0.47871756\n",
      "Iteration 185, loss = 0.47820480\n",
      "Iteration 186, loss = 0.47834395\n",
      "Iteration 187, loss = 0.47827998\n",
      "Iteration 188, loss = 0.47821888\n",
      "Iteration 189, loss = 0.47828743\n",
      "Iteration 190, loss = 0.47830079\n",
      "Iteration 191, loss = 0.47802169\n",
      "Iteration 192, loss = 0.47799544\n",
      "Iteration 193, loss = 0.47785251\n",
      "Iteration 194, loss = 0.47792773\n",
      "Iteration 195, loss = 0.47769366\n",
      "Iteration 196, loss = 0.47778946\n",
      "Iteration 197, loss = 0.47754305\n",
      "Iteration 198, loss = 0.47752456\n",
      "Iteration 199, loss = 0.47782269\n",
      "Iteration 200, loss = 0.47742947\n",
      "Iteration 201, loss = 0.47766574\n",
      "Iteration 202, loss = 0.47739474\n",
      "Iteration 203, loss = 0.47755690\n",
      "Iteration 204, loss = 0.47739117\n",
      "Iteration 205, loss = 0.47720427\n",
      "Iteration 206, loss = 0.47724705\n",
      "Iteration 207, loss = 0.47712525\n",
      "Iteration 208, loss = 0.47723543\n",
      "Iteration 209, loss = 0.47712984\n",
      "Iteration 210, loss = 0.47707929\n",
      "Iteration 211, loss = 0.47717925\n",
      "Iteration 212, loss = 0.47696063\n",
      "Iteration 213, loss = 0.47690290\n",
      "Iteration 214, loss = 0.47687244\n",
      "Iteration 215, loss = 0.47680348\n",
      "Iteration 216, loss = 0.47693878\n",
      "Iteration 217, loss = 0.47667952\n",
      "Iteration 218, loss = 0.47677968\n",
      "Iteration 219, loss = 0.47656547\n",
      "Iteration 220, loss = 0.47650068\n",
      "Iteration 221, loss = 0.47650115\n",
      "Iteration 222, loss = 0.47645577\n",
      "Iteration 223, loss = 0.47656720\n",
      "Iteration 224, loss = 0.47629841\n",
      "Iteration 225, loss = 0.47635057\n",
      "Iteration 226, loss = 0.47638969\n",
      "Iteration 227, loss = 0.47634399\n",
      "Iteration 228, loss = 0.47621840\n",
      "Iteration 229, loss = 0.47603742\n",
      "Iteration 230, loss = 0.47600070\n",
      "Iteration 231, loss = 0.47597869\n",
      "Iteration 232, loss = 0.47616544\n",
      "Iteration 233, loss = 0.47594994\n",
      "Iteration 234, loss = 0.47593097\n",
      "Iteration 235, loss = 0.47595587\n",
      "Iteration 236, loss = 0.47576382\n",
      "Iteration 237, loss = 0.47584849\n",
      "Iteration 238, loss = 0.47566889\n",
      "Iteration 239, loss = 0.47586645\n",
      "Iteration 240, loss = 0.47575692\n",
      "Iteration 241, loss = 0.47564825\n",
      "Iteration 242, loss = 0.47558529\n",
      "Iteration 243, loss = 0.47543938\n",
      "Iteration 244, loss = 0.47537028\n",
      "Iteration 245, loss = 0.47535297\n",
      "Iteration 246, loss = 0.47533961\n",
      "Iteration 247, loss = 0.47516999\n",
      "Iteration 248, loss = 0.47531928\n",
      "Iteration 249, loss = 0.47520044\n",
      "Iteration 250, loss = 0.47509911\n",
      "Iteration 251, loss = 0.47506711\n",
      "Iteration 252, loss = 0.47481679\n",
      "Iteration 253, loss = 0.47497535\n",
      "Iteration 254, loss = 0.47506969\n",
      "Iteration 255, loss = 0.47463725\n",
      "Iteration 256, loss = 0.47468498\n",
      "Iteration 257, loss = 0.47463380\n",
      "Iteration 258, loss = 0.47485267\n",
      "Iteration 259, loss = 0.47475380\n",
      "Iteration 260, loss = 0.47454361\n",
      "Iteration 261, loss = 0.47486072\n",
      "Iteration 262, loss = 0.47428039\n",
      "Iteration 263, loss = 0.47436253\n",
      "Iteration 264, loss = 0.47427134\n",
      "Iteration 265, loss = 0.47419725\n",
      "Iteration 266, loss = 0.47412100\n",
      "Iteration 267, loss = 0.47411877\n",
      "Iteration 268, loss = 0.47408280\n",
      "Iteration 269, loss = 0.47394871\n",
      "Iteration 270, loss = 0.47392079\n",
      "Iteration 271, loss = 0.47383655\n",
      "Iteration 272, loss = 0.47424685\n",
      "Iteration 273, loss = 0.47365887\n",
      "Iteration 274, loss = 0.47390071\n",
      "Iteration 275, loss = 0.47366991\n",
      "Iteration 276, loss = 0.47374543\n",
      "Iteration 277, loss = 0.47341548\n",
      "Iteration 278, loss = 0.47348534\n",
      "Iteration 279, loss = 0.47356667\n",
      "Iteration 280, loss = 0.47351020\n",
      "Iteration 281, loss = 0.47337548\n",
      "Iteration 282, loss = 0.47325726\n",
      "Iteration 283, loss = 0.47332324\n",
      "Iteration 284, loss = 0.47307052\n",
      "Iteration 285, loss = 0.47335911\n",
      "Iteration 286, loss = 0.47310001\n",
      "Iteration 287, loss = 0.47295241\n",
      "Iteration 288, loss = 0.47310082\n",
      "Iteration 289, loss = 0.47308724\n",
      "Iteration 290, loss = 0.47276217\n",
      "Iteration 291, loss = 0.47310849\n",
      "Iteration 292, loss = 0.47277414\n",
      "Iteration 293, loss = 0.47294506\n",
      "Iteration 294, loss = 0.47255476\n",
      "Iteration 295, loss = 0.47272232\n",
      "Iteration 296, loss = 0.47256626\n",
      "Iteration 297, loss = 0.47253084\n",
      "Iteration 298, loss = 0.47245612\n",
      "Iteration 299, loss = 0.47259260\n",
      "Iteration 300, loss = 0.47250300\n",
      "Iteration 301, loss = 0.47217965\n",
      "Iteration 302, loss = 0.47224023\n",
      "Iteration 303, loss = 0.47222951\n",
      "Iteration 304, loss = 0.47229483\n",
      "Iteration 305, loss = 0.47200197\n",
      "Iteration 306, loss = 0.47190252\n",
      "Iteration 307, loss = 0.47210107\n",
      "Iteration 308, loss = 0.47215715\n",
      "Iteration 309, loss = 0.47201045\n",
      "Iteration 310, loss = 0.47190425\n",
      "Iteration 311, loss = 0.47202413\n",
      "Iteration 312, loss = 0.47169737\n",
      "Iteration 313, loss = 0.47173223\n",
      "Iteration 314, loss = 0.47143310\n",
      "Iteration 315, loss = 0.47147875\n",
      "Iteration 316, loss = 0.47145312\n",
      "Iteration 317, loss = 0.47140219\n",
      "Iteration 318, loss = 0.47149037\n",
      "Iteration 319, loss = 0.47125318\n",
      "Iteration 320, loss = 0.47144616\n",
      "Iteration 321, loss = 0.47111421\n",
      "Iteration 322, loss = 0.47107162\n",
      "Iteration 323, loss = 0.47096372\n",
      "Iteration 324, loss = 0.47099143\n",
      "Iteration 325, loss = 0.47082773\n",
      "Iteration 326, loss = 0.47100688\n",
      "Iteration 327, loss = 0.47089514\n",
      "Iteration 328, loss = 0.47076877\n",
      "Iteration 329, loss = 0.47072764\n",
      "Iteration 330, loss = 0.47070985\n",
      "Iteration 331, loss = 0.47061648\n",
      "Iteration 332, loss = 0.47034148\n",
      "Iteration 333, loss = 0.47067065\n",
      "Iteration 334, loss = 0.47036483\n",
      "Iteration 335, loss = 0.47026395\n",
      "Iteration 336, loss = 0.47030219\n",
      "Iteration 337, loss = 0.47024201\n",
      "Iteration 338, loss = 0.47007834\n",
      "Iteration 339, loss = 0.47002714\n",
      "Iteration 340, loss = 0.46992275\n",
      "Iteration 341, loss = 0.47008373\n",
      "Iteration 342, loss = 0.47002819\n",
      "Iteration 343, loss = 0.46994284\n",
      "Iteration 344, loss = 0.46963944\n",
      "Iteration 345, loss = 0.46966448\n",
      "Iteration 346, loss = 0.46966804\n",
      "Iteration 347, loss = 0.46947004\n",
      "Iteration 348, loss = 0.46960373\n",
      "Iteration 349, loss = 0.46982567\n",
      "Iteration 350, loss = 0.46944667\n",
      "Iteration 351, loss = 0.46938761\n",
      "Iteration 352, loss = 0.46932928\n",
      "Iteration 353, loss = 0.46924567\n",
      "Iteration 354, loss = 0.46934707\n",
      "Iteration 355, loss = 0.46941521\n",
      "Iteration 356, loss = 0.46923193\n",
      "Iteration 357, loss = 0.46910307\n",
      "Iteration 358, loss = 0.46910534\n",
      "Iteration 359, loss = 0.46896536\n",
      "Iteration 360, loss = 0.46890089\n",
      "Iteration 361, loss = 0.46898081\n",
      "Iteration 362, loss = 0.46890206\n",
      "Iteration 363, loss = 0.46864295\n",
      "Iteration 364, loss = 0.46886788\n",
      "Iteration 365, loss = 0.46867835\n",
      "Iteration 366, loss = 0.46870766\n",
      "Iteration 367, loss = 0.46856194\n",
      "Iteration 368, loss = 0.46876667\n",
      "Iteration 369, loss = 0.46854259\n",
      "Iteration 370, loss = 0.46844166\n",
      "Iteration 371, loss = 0.46853630\n",
      "Iteration 372, loss = 0.46878479\n",
      "Iteration 373, loss = 0.46854410\n",
      "Iteration 374, loss = 0.46827536\n",
      "Iteration 375, loss = 0.46847956\n",
      "Iteration 376, loss = 0.46836867\n",
      "Iteration 377, loss = 0.46835069\n",
      "Iteration 378, loss = 0.46827478\n",
      "Iteration 379, loss = 0.46800995\n",
      "Iteration 380, loss = 0.46820862\n",
      "Iteration 381, loss = 0.46823883\n",
      "Iteration 382, loss = 0.46790074\n",
      "Iteration 383, loss = 0.46792799\n",
      "Iteration 384, loss = 0.46814145\n",
      "Iteration 385, loss = 0.46813405\n",
      "Iteration 386, loss = 0.46822967\n",
      "Iteration 387, loss = 0.46832900\n",
      "Iteration 388, loss = 0.46781075\n",
      "Iteration 389, loss = 0.46785847\n",
      "Iteration 390, loss = 0.46772628\n",
      "Iteration 391, loss = 0.46778739\n",
      "Iteration 392, loss = 0.46778516\n",
      "Iteration 393, loss = 0.46769739\n",
      "Iteration 394, loss = 0.46778336\n",
      "Iteration 395, loss = 0.46765117\n",
      "Iteration 396, loss = 0.46788277\n",
      "Iteration 397, loss = 0.46766008\n",
      "Iteration 398, loss = 0.46751193\n",
      "Iteration 399, loss = 0.46747353\n",
      "Iteration 400, loss = 0.46791628\n",
      "Iteration 401, loss = 0.46755938\n",
      "Iteration 402, loss = 0.46768061\n",
      "Iteration 403, loss = 0.46765554\n",
      "Iteration 404, loss = 0.46736056\n",
      "Iteration 405, loss = 0.46744584\n",
      "Iteration 406, loss = 0.46763048\n",
      "Iteration 407, loss = 0.46737673\n",
      "Iteration 408, loss = 0.46748578\n",
      "Iteration 409, loss = 0.46753010\n",
      "Iteration 410, loss = 0.46706166\n",
      "Iteration 411, loss = 0.46705717\n",
      "Iteration 412, loss = 0.46737303\n",
      "Iteration 413, loss = 0.46727809\n",
      "Iteration 414, loss = 0.46721462\n",
      "Iteration 415, loss = 0.46698106\n",
      "Iteration 416, loss = 0.46708735\n",
      "Iteration 417, loss = 0.46705077\n",
      "Iteration 418, loss = 0.46714677\n",
      "Iteration 419, loss = 0.46699807\n",
      "Iteration 420, loss = 0.46692296\n",
      "Iteration 421, loss = 0.46678264\n",
      "Iteration 422, loss = 0.46692625\n",
      "Iteration 423, loss = 0.46673375\n",
      "Iteration 424, loss = 0.46677134\n",
      "Iteration 425, loss = 0.46678490\n",
      "Iteration 426, loss = 0.46703780\n",
      "Iteration 427, loss = 0.46655851\n",
      "Iteration 428, loss = 0.46688729\n",
      "Iteration 429, loss = 0.46674216\n",
      "Iteration 430, loss = 0.46666980\n",
      "Iteration 431, loss = 0.46673065\n",
      "Iteration 432, loss = 0.46650490\n",
      "Iteration 433, loss = 0.46646067\n",
      "Iteration 434, loss = 0.46640969\n",
      "Iteration 435, loss = 0.46674025\n",
      "Iteration 436, loss = 0.46663280\n",
      "Iteration 437, loss = 0.46650054\n",
      "Iteration 438, loss = 0.46642963\n",
      "Iteration 439, loss = 0.46623751\n",
      "Iteration 440, loss = 0.46666434\n",
      "Iteration 441, loss = 0.46633451\n",
      "Iteration 442, loss = 0.46646505\n",
      "Iteration 443, loss = 0.46642026\n",
      "Iteration 444, loss = 0.46621150\n",
      "Iteration 445, loss = 0.46631413\n",
      "Iteration 446, loss = 0.46640221\n",
      "Iteration 447, loss = 0.46630042\n",
      "Iteration 448, loss = 0.46628482\n",
      "Iteration 449, loss = 0.46610182\n",
      "Iteration 450, loss = 0.46621940\n",
      "Iteration 451, loss = 0.46617041\n",
      "Iteration 452, loss = 0.46607173\n",
      "Iteration 453, loss = 0.46614034\n",
      "Iteration 454, loss = 0.46599102\n",
      "Iteration 455, loss = 0.46599268\n",
      "Iteration 456, loss = 0.46601040\n",
      "Iteration 457, loss = 0.46610313\n",
      "Iteration 458, loss = 0.46603696\n",
      "Iteration 459, loss = 0.46581675\n",
      "Iteration 460, loss = 0.46617329\n",
      "Iteration 461, loss = 0.46608283\n",
      "Iteration 462, loss = 0.46590255\n",
      "Iteration 463, loss = 0.46586686\n",
      "Iteration 464, loss = 0.46617884\n",
      "Iteration 465, loss = 0.46597365\n",
      "Iteration 466, loss = 0.46578553\n",
      "Iteration 467, loss = 0.46581647\n",
      "Iteration 468, loss = 0.46574766\n",
      "Iteration 469, loss = 0.46596953\n",
      "Iteration 470, loss = 0.46605498\n",
      "Iteration 471, loss = 0.46595617\n",
      "Iteration 472, loss = 0.46560800\n",
      "Iteration 473, loss = 0.46581968\n",
      "Iteration 474, loss = 0.46589349\n",
      "Iteration 475, loss = 0.46561763\n",
      "Iteration 476, loss = 0.46576213\n",
      "Iteration 477, loss = 0.46555099\n",
      "Iteration 478, loss = 0.46597114\n",
      "Iteration 479, loss = 0.46566411\n",
      "Iteration 480, loss = 0.46554230\n",
      "Iteration 481, loss = 0.46561160\n",
      "Iteration 482, loss = 0.46572903\n",
      "Iteration 483, loss = 0.46532984\n",
      "Iteration 484, loss = 0.46550983\n",
      "Iteration 485, loss = 0.46545512\n",
      "Iteration 486, loss = 0.46560667\n",
      "Iteration 487, loss = 0.46563241\n",
      "Iteration 488, loss = 0.46552073\n",
      "Iteration 489, loss = 0.46538699\n",
      "Iteration 490, loss = 0.46546661\n",
      "Iteration 491, loss = 0.46553962\n",
      "Iteration 492, loss = 0.46530642\n",
      "Iteration 493, loss = 0.46545330\n",
      "Iteration 494, loss = 0.46560688\n",
      "Iteration 495, loss = 0.46542577\n",
      "Iteration 496, loss = 0.46531836\n",
      "Iteration 497, loss = 0.46530341\n",
      "Iteration 498, loss = 0.46521041\n",
      "Iteration 499, loss = 0.46535718\n",
      "Iteration 500, loss = 0.46504154\n",
      "Iteration 501, loss = 0.46519890\n",
      "Iteration 502, loss = 0.46516278\n",
      "Iteration 503, loss = 0.46541795\n",
      "Iteration 504, loss = 0.46519553\n",
      "Iteration 505, loss = 0.46508688\n",
      "Iteration 506, loss = 0.46498322\n",
      "Iteration 507, loss = 0.46533975\n",
      "Iteration 508, loss = 0.46495351\n",
      "Iteration 509, loss = 0.46510355\n",
      "Iteration 510, loss = 0.46523066\n",
      "Iteration 511, loss = 0.46505045\n",
      "Iteration 512, loss = 0.46508967\n",
      "Iteration 513, loss = 0.46517538\n",
      "Iteration 514, loss = 0.46522086\n",
      "Iteration 515, loss = 0.46487635\n",
      "Iteration 516, loss = 0.46511436\n",
      "Iteration 517, loss = 0.46496419\n",
      "Iteration 518, loss = 0.46484048\n",
      "Iteration 519, loss = 0.46530927\n",
      "Iteration 520, loss = 0.46514201\n",
      "Iteration 521, loss = 0.46486549\n",
      "Iteration 522, loss = 0.46500369\n",
      "Iteration 523, loss = 0.46485073\n",
      "Iteration 524, loss = 0.46488935\n",
      "Iteration 525, loss = 0.46483063\n",
      "Iteration 526, loss = 0.46493438\n",
      "Iteration 527, loss = 0.46488587\n",
      "Iteration 528, loss = 0.46491310\n",
      "Iteration 529, loss = 0.46467455\n",
      "Iteration 530, loss = 0.46467833\n",
      "Iteration 531, loss = 0.46451050\n",
      "Iteration 532, loss = 0.46458559\n",
      "Iteration 533, loss = 0.46444110\n",
      "Iteration 534, loss = 0.46474432\n",
      "Iteration 535, loss = 0.46438119\n",
      "Iteration 536, loss = 0.46467374\n",
      "Iteration 537, loss = 0.46466675\n",
      "Iteration 538, loss = 0.46483870\n",
      "Iteration 539, loss = 0.46452561\n",
      "Iteration 540, loss = 0.46476689\n",
      "Iteration 541, loss = 0.46483401\n",
      "Iteration 542, loss = 0.46471273\n",
      "Iteration 543, loss = 0.46435200\n",
      "Iteration 544, loss = 0.46490940\n",
      "Iteration 545, loss = 0.46456389\n",
      "Iteration 546, loss = 0.46427425\n",
      "Iteration 547, loss = 0.46469388\n",
      "Iteration 548, loss = 0.46443765\n",
      "Iteration 549, loss = 0.46427989\n",
      "Iteration 550, loss = 0.46424691\n",
      "Iteration 551, loss = 0.46436304\n",
      "Iteration 552, loss = 0.46429552\n",
      "Iteration 553, loss = 0.46436170\n",
      "Iteration 554, loss = 0.46448147\n",
      "Iteration 555, loss = 0.46443565\n",
      "Iteration 556, loss = 0.46425884\n",
      "Iteration 557, loss = 0.46450513\n",
      "Iteration 558, loss = 0.46426600\n",
      "Iteration 559, loss = 0.46436408\n",
      "Iteration 560, loss = 0.46413110\n",
      "Iteration 561, loss = 0.46427069\n",
      "Iteration 562, loss = 0.46422604\n",
      "Iteration 563, loss = 0.46411856\n",
      "Iteration 564, loss = 0.46410619\n",
      "Iteration 565, loss = 0.46422522\n",
      "Iteration 566, loss = 0.46438792\n",
      "Iteration 567, loss = 0.46421392\n",
      "Iteration 568, loss = 0.46417907\n",
      "Iteration 569, loss = 0.46414320\n",
      "Iteration 570, loss = 0.46399458\n",
      "Iteration 571, loss = 0.46411812\n",
      "Iteration 572, loss = 0.46423577\n",
      "Iteration 573, loss = 0.46396032\n",
      "Iteration 574, loss = 0.46400970\n",
      "Iteration 575, loss = 0.46397299\n",
      "Iteration 576, loss = 0.46420759\n",
      "Iteration 577, loss = 0.46409270\n",
      "Iteration 578, loss = 0.46414499\n",
      "Iteration 579, loss = 0.46374609\n",
      "Iteration 580, loss = 0.46420464\n",
      "Iteration 581, loss = 0.46406552\n",
      "Iteration 582, loss = 0.46406915\n",
      "Iteration 583, loss = 0.46414671\n",
      "Iteration 584, loss = 0.46392146\n",
      "Iteration 585, loss = 0.46387265\n",
      "Iteration 586, loss = 0.46388252\n",
      "Iteration 587, loss = 0.46394238\n",
      "Iteration 588, loss = 0.46390828\n",
      "Iteration 589, loss = 0.46404927\n",
      "Iteration 590, loss = 0.46379531\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68523786\n",
      "Iteration 2, loss = 0.66406199\n",
      "Iteration 3, loss = 0.64996547\n",
      "Iteration 4, loss = 0.63759823\n",
      "Iteration 5, loss = 0.62499068\n",
      "Iteration 6, loss = 0.61289536\n",
      "Iteration 7, loss = 0.60132457\n",
      "Iteration 8, loss = 0.59086729\n",
      "Iteration 9, loss = 0.58194606\n",
      "Iteration 10, loss = 0.57449222\n",
      "Iteration 11, loss = 0.56874704\n",
      "Iteration 12, loss = 0.56406179\n",
      "Iteration 13, loss = 0.56048022\n",
      "Iteration 14, loss = 0.55764790\n",
      "Iteration 15, loss = 0.55493593\n",
      "Iteration 16, loss = 0.55267199\n",
      "Iteration 17, loss = 0.55051558\n",
      "Iteration 18, loss = 0.54858982\n",
      "Iteration 19, loss = 0.54664243\n",
      "Iteration 20, loss = 0.54482618\n",
      "Iteration 21, loss = 0.54306713\n",
      "Iteration 22, loss = 0.54158126\n",
      "Iteration 23, loss = 0.53994817\n",
      "Iteration 24, loss = 0.53876079\n",
      "Iteration 25, loss = 0.53713946\n",
      "Iteration 26, loss = 0.53589610\n",
      "Iteration 27, loss = 0.53455723\n",
      "Iteration 28, loss = 0.53313162\n",
      "Iteration 29, loss = 0.53176239\n",
      "Iteration 30, loss = 0.53053698\n",
      "Iteration 31, loss = 0.52933101\n",
      "Iteration 32, loss = 0.52818220\n",
      "Iteration 33, loss = 0.52709113\n",
      "Iteration 34, loss = 0.52589999\n",
      "Iteration 35, loss = 0.52493675\n",
      "Iteration 36, loss = 0.52377619\n",
      "Iteration 37, loss = 0.52277027\n",
      "Iteration 38, loss = 0.52179829\n",
      "Iteration 39, loss = 0.52067483\n",
      "Iteration 40, loss = 0.51983410\n",
      "Iteration 41, loss = 0.51871441\n",
      "Iteration 42, loss = 0.51792795\n",
      "Iteration 43, loss = 0.51714283\n",
      "Iteration 44, loss = 0.51608946\n",
      "Iteration 45, loss = 0.51501292\n",
      "Iteration 46, loss = 0.51422160\n",
      "Iteration 47, loss = 0.51369865\n",
      "Iteration 48, loss = 0.51264187\n",
      "Iteration 49, loss = 0.51206712\n",
      "Iteration 50, loss = 0.51138057\n",
      "Iteration 51, loss = 0.51071585\n",
      "Iteration 52, loss = 0.51018305\n",
      "Iteration 53, loss = 0.50947504\n",
      "Iteration 54, loss = 0.50895371\n",
      "Iteration 55, loss = 0.50825172\n",
      "Iteration 56, loss = 0.50760441\n",
      "Iteration 57, loss = 0.50721216\n",
      "Iteration 58, loss = 0.50697737\n",
      "Iteration 59, loss = 0.50634917\n",
      "Iteration 60, loss = 0.50614372\n",
      "Iteration 61, loss = 0.50553435\n",
      "Iteration 62, loss = 0.50493696\n",
      "Iteration 63, loss = 0.50446916\n",
      "Iteration 64, loss = 0.50411517\n",
      "Iteration 65, loss = 0.50375111\n",
      "Iteration 66, loss = 0.50335355\n",
      "Iteration 67, loss = 0.50323070\n",
      "Iteration 68, loss = 0.50318016\n",
      "Iteration 69, loss = 0.50240700\n",
      "Iteration 70, loss = 0.50206741\n",
      "Iteration 71, loss = 0.50204075\n",
      "Iteration 72, loss = 0.50151013\n",
      "Iteration 73, loss = 0.50124093\n",
      "Iteration 74, loss = 0.50105327\n",
      "Iteration 75, loss = 0.50069461\n",
      "Iteration 76, loss = 0.50054346\n",
      "Iteration 77, loss = 0.50024430\n",
      "Iteration 78, loss = 0.49967945\n",
      "Iteration 79, loss = 0.49953021\n",
      "Iteration 80, loss = 0.49959209\n",
      "Iteration 81, loss = 0.49922726\n",
      "Iteration 82, loss = 0.49876311\n",
      "Iteration 83, loss = 0.49915744\n",
      "Iteration 84, loss = 0.49844357\n",
      "Iteration 85, loss = 0.49811442\n",
      "Iteration 86, loss = 0.49854854\n",
      "Iteration 87, loss = 0.49781537\n",
      "Iteration 88, loss = 0.49753042\n",
      "Iteration 89, loss = 0.49749391\n",
      "Iteration 90, loss = 0.49715399\n",
      "Iteration 91, loss = 0.49689036\n",
      "Iteration 92, loss = 0.49652621\n",
      "Iteration 93, loss = 0.49666154\n",
      "Iteration 94, loss = 0.49620265\n",
      "Iteration 95, loss = 0.49596780\n",
      "Iteration 96, loss = 0.49579961\n",
      "Iteration 97, loss = 0.49552255\n",
      "Iteration 98, loss = 0.49547650\n",
      "Iteration 99, loss = 0.49531468\n",
      "Iteration 100, loss = 0.49477047\n",
      "Iteration 101, loss = 0.49462361\n",
      "Iteration 102, loss = 0.49413366\n",
      "Iteration 103, loss = 0.49461098\n",
      "Iteration 104, loss = 0.49384819\n",
      "Iteration 105, loss = 0.49374136\n",
      "Iteration 106, loss = 0.49347172\n",
      "Iteration 107, loss = 0.49332280\n",
      "Iteration 108, loss = 0.49310745\n",
      "Iteration 109, loss = 0.49294981\n",
      "Iteration 110, loss = 0.49306290\n",
      "Iteration 111, loss = 0.49283307\n",
      "Iteration 112, loss = 0.49289830\n",
      "Iteration 113, loss = 0.49262257\n",
      "Iteration 114, loss = 0.49217796\n",
      "Iteration 115, loss = 0.49229847\n",
      "Iteration 116, loss = 0.49219653\n",
      "Iteration 117, loss = 0.49191605\n",
      "Iteration 118, loss = 0.49174278\n",
      "Iteration 119, loss = 0.49182358\n",
      "Iteration 120, loss = 0.49152193\n",
      "Iteration 121, loss = 0.49132324\n",
      "Iteration 122, loss = 0.49129785\n",
      "Iteration 123, loss = 0.49145908\n",
      "Iteration 124, loss = 0.49098628\n",
      "Iteration 125, loss = 0.49110842\n",
      "Iteration 126, loss = 0.49083903\n",
      "Iteration 127, loss = 0.49106275\n",
      "Iteration 128, loss = 0.49067720\n",
      "Iteration 129, loss = 0.49054541\n",
      "Iteration 130, loss = 0.49054626\n",
      "Iteration 131, loss = 0.49022235\n",
      "Iteration 132, loss = 0.49040923\n",
      "Iteration 133, loss = 0.49032853\n",
      "Iteration 134, loss = 0.49039333\n",
      "Iteration 135, loss = 0.49015014\n",
      "Iteration 136, loss = 0.48986451\n",
      "Iteration 137, loss = 0.48989692\n",
      "Iteration 138, loss = 0.49005476\n",
      "Iteration 139, loss = 0.48981254\n",
      "Iteration 140, loss = 0.48990334\n",
      "Iteration 141, loss = 0.48959480\n",
      "Iteration 142, loss = 0.48978737\n",
      "Iteration 143, loss = 0.49032243\n",
      "Iteration 144, loss = 0.49014608\n",
      "Iteration 145, loss = 0.48928288\n",
      "Iteration 146, loss = 0.48935176\n",
      "Iteration 147, loss = 0.48960437\n",
      "Iteration 148, loss = 0.48924633\n",
      "Iteration 149, loss = 0.48929761\n",
      "Iteration 150, loss = 0.48941895\n",
      "Iteration 151, loss = 0.48902812\n",
      "Iteration 152, loss = 0.48925980\n",
      "Iteration 153, loss = 0.48881101\n",
      "Iteration 154, loss = 0.48899259\n",
      "Iteration 155, loss = 0.48871586\n",
      "Iteration 156, loss = 0.48879159\n",
      "Iteration 157, loss = 0.48886150\n",
      "Iteration 158, loss = 0.48882584\n",
      "Iteration 159, loss = 0.48874433\n",
      "Iteration 160, loss = 0.48884835\n",
      "Iteration 161, loss = 0.48850745\n",
      "Iteration 162, loss = 0.48879458\n",
      "Iteration 163, loss = 0.48860557\n",
      "Iteration 164, loss = 0.48834056\n",
      "Iteration 165, loss = 0.48876792\n",
      "Iteration 166, loss = 0.48840542\n",
      "Iteration 167, loss = 0.48834984\n",
      "Iteration 168, loss = 0.48818484\n",
      "Iteration 169, loss = 0.48829890\n",
      "Iteration 170, loss = 0.48822400\n",
      "Iteration 171, loss = 0.48809605\n",
      "Iteration 172, loss = 0.48818453\n",
      "Iteration 173, loss = 0.48795128\n",
      "Iteration 174, loss = 0.48803597\n",
      "Iteration 175, loss = 0.48822254\n",
      "Iteration 176, loss = 0.48769476\n",
      "Iteration 177, loss = 0.48772701\n",
      "Iteration 178, loss = 0.48781102\n",
      "Iteration 179, loss = 0.48780712\n",
      "Iteration 180, loss = 0.48767216\n",
      "Iteration 181, loss = 0.48757936\n",
      "Iteration 182, loss = 0.48742161\n",
      "Iteration 183, loss = 0.48749337\n",
      "Iteration 184, loss = 0.48741029\n",
      "Iteration 185, loss = 0.48731004\n",
      "Iteration 186, loss = 0.48742925\n",
      "Iteration 187, loss = 0.48708893\n",
      "Iteration 188, loss = 0.48722048\n",
      "Iteration 189, loss = 0.48746683\n",
      "Iteration 190, loss = 0.48727932\n",
      "Iteration 191, loss = 0.48705538\n",
      "Iteration 192, loss = 0.48740945\n",
      "Iteration 193, loss = 0.48672192\n",
      "Iteration 194, loss = 0.48698468\n",
      "Iteration 195, loss = 0.48675468\n",
      "Iteration 196, loss = 0.48680519\n",
      "Iteration 197, loss = 0.48680673\n",
      "Iteration 198, loss = 0.48681548\n",
      "Iteration 199, loss = 0.48648062\n",
      "Iteration 200, loss = 0.48638373\n",
      "Iteration 201, loss = 0.48665523\n",
      "Iteration 202, loss = 0.48650519\n",
      "Iteration 203, loss = 0.48622849\n",
      "Iteration 204, loss = 0.48633055\n",
      "Iteration 205, loss = 0.48630583\n",
      "Iteration 206, loss = 0.48624501\n",
      "Iteration 207, loss = 0.48623219\n",
      "Iteration 208, loss = 0.48612747\n",
      "Iteration 209, loss = 0.48616840\n",
      "Iteration 210, loss = 0.48593967\n",
      "Iteration 211, loss = 0.48611474\n",
      "Iteration 212, loss = 0.48606788\n",
      "Iteration 213, loss = 0.48597357\n",
      "Iteration 214, loss = 0.48575453\n",
      "Iteration 215, loss = 0.48581079\n",
      "Iteration 216, loss = 0.48589724\n",
      "Iteration 217, loss = 0.48578088\n",
      "Iteration 218, loss = 0.48550190\n",
      "Iteration 219, loss = 0.48572674\n",
      "Iteration 220, loss = 0.48567596\n",
      "Iteration 221, loss = 0.48552898\n",
      "Iteration 222, loss = 0.48555165\n",
      "Iteration 223, loss = 0.48531833\n",
      "Iteration 224, loss = 0.48550267\n",
      "Iteration 225, loss = 0.48531714\n",
      "Iteration 226, loss = 0.48564612\n",
      "Iteration 227, loss = 0.48538109\n",
      "Iteration 228, loss = 0.48514485\n",
      "Iteration 229, loss = 0.48499494\n",
      "Iteration 230, loss = 0.48541005\n",
      "Iteration 231, loss = 0.48540851\n",
      "Iteration 232, loss = 0.48524971\n",
      "Iteration 233, loss = 0.48504870\n",
      "Iteration 234, loss = 0.48528515\n",
      "Iteration 235, loss = 0.48585635\n",
      "Iteration 236, loss = 0.48501597\n",
      "Iteration 237, loss = 0.48516849\n",
      "Iteration 238, loss = 0.48479533\n",
      "Iteration 239, loss = 0.48468142\n",
      "Iteration 240, loss = 0.48478000\n",
      "Iteration 241, loss = 0.48477785\n",
      "Iteration 242, loss = 0.48477959\n",
      "Iteration 243, loss = 0.48458666\n",
      "Iteration 244, loss = 0.48455659\n",
      "Iteration 245, loss = 0.48430418\n",
      "Iteration 246, loss = 0.48434030\n",
      "Iteration 247, loss = 0.48437333\n",
      "Iteration 248, loss = 0.48469743\n",
      "Iteration 249, loss = 0.48408660\n",
      "Iteration 250, loss = 0.48440351\n",
      "Iteration 251, loss = 0.48426587\n",
      "Iteration 252, loss = 0.48397275\n",
      "Iteration 253, loss = 0.48418831\n",
      "Iteration 254, loss = 0.48448975\n",
      "Iteration 255, loss = 0.48452345\n",
      "Iteration 256, loss = 0.48424602\n",
      "Iteration 257, loss = 0.48397272\n",
      "Iteration 258, loss = 0.48391535\n",
      "Iteration 259, loss = 0.48403699\n",
      "Iteration 260, loss = 0.48394059\n",
      "Iteration 261, loss = 0.48399771\n",
      "Iteration 262, loss = 0.48376315\n",
      "Iteration 263, loss = 0.48383369\n",
      "Iteration 264, loss = 0.48374430\n",
      "Iteration 265, loss = 0.48359075\n",
      "Iteration 266, loss = 0.48390487\n",
      "Iteration 267, loss = 0.48354022\n",
      "Iteration 268, loss = 0.48372409\n",
      "Iteration 269, loss = 0.48350519\n",
      "Iteration 270, loss = 0.48371343\n",
      "Iteration 271, loss = 0.48346502\n",
      "Iteration 272, loss = 0.48338682\n",
      "Iteration 273, loss = 0.48314814\n",
      "Iteration 274, loss = 0.48312425\n",
      "Iteration 275, loss = 0.48310836\n",
      "Iteration 276, loss = 0.48318148\n",
      "Iteration 277, loss = 0.48342226\n",
      "Iteration 278, loss = 0.48298073\n",
      "Iteration 279, loss = 0.48301859\n",
      "Iteration 280, loss = 0.48340730\n",
      "Iteration 281, loss = 0.48285857\n",
      "Iteration 282, loss = 0.48308880\n",
      "Iteration 283, loss = 0.48284434\n",
      "Iteration 284, loss = 0.48282478\n",
      "Iteration 285, loss = 0.48292302\n",
      "Iteration 286, loss = 0.48253114\n",
      "Iteration 287, loss = 0.48270797\n",
      "Iteration 288, loss = 0.48256521\n",
      "Iteration 289, loss = 0.48234073\n",
      "Iteration 290, loss = 0.48264318\n",
      "Iteration 291, loss = 0.48242750\n",
      "Iteration 292, loss = 0.48249487\n",
      "Iteration 293, loss = 0.48234362\n",
      "Iteration 294, loss = 0.48251751\n",
      "Iteration 295, loss = 0.48210610\n",
      "Iteration 296, loss = 0.48198849\n",
      "Iteration 297, loss = 0.48189540\n",
      "Iteration 298, loss = 0.48198948\n",
      "Iteration 299, loss = 0.48213691\n",
      "Iteration 300, loss = 0.48271103\n",
      "Iteration 301, loss = 0.48147398\n",
      "Iteration 302, loss = 0.48181373\n",
      "Iteration 303, loss = 0.48184741\n",
      "Iteration 304, loss = 0.48162843\n",
      "Iteration 305, loss = 0.48148028\n",
      "Iteration 306, loss = 0.48168718\n",
      "Iteration 307, loss = 0.48145531\n",
      "Iteration 308, loss = 0.48137268\n",
      "Iteration 309, loss = 0.48138992\n",
      "Iteration 310, loss = 0.48131125\n",
      "Iteration 311, loss = 0.48110665\n",
      "Iteration 312, loss = 0.48116515\n",
      "Iteration 313, loss = 0.48132222\n",
      "Iteration 314, loss = 0.48118362\n",
      "Iteration 315, loss = 0.48096690\n",
      "Iteration 316, loss = 0.48116356\n",
      "Iteration 317, loss = 0.48118613\n",
      "Iteration 318, loss = 0.48104354\n",
      "Iteration 319, loss = 0.48086629\n",
      "Iteration 320, loss = 0.48078179\n",
      "Iteration 321, loss = 0.48093171\n",
      "Iteration 322, loss = 0.48067875\n",
      "Iteration 323, loss = 0.48082662\n",
      "Iteration 324, loss = 0.48135554\n",
      "Iteration 325, loss = 0.48084536\n",
      "Iteration 326, loss = 0.48164947\n",
      "Iteration 327, loss = 0.48075266\n",
      "Iteration 328, loss = 0.48122958\n",
      "Iteration 329, loss = 0.48035273\n",
      "Iteration 330, loss = 0.48056885\n",
      "Iteration 331, loss = 0.48042578\n",
      "Iteration 332, loss = 0.48025239\n",
      "Iteration 333, loss = 0.48035212\n",
      "Iteration 334, loss = 0.48054588\n",
      "Iteration 335, loss = 0.48013767\n",
      "Iteration 336, loss = 0.48022946\n",
      "Iteration 337, loss = 0.48008190\n",
      "Iteration 338, loss = 0.48031918\n",
      "Iteration 339, loss = 0.48025609\n",
      "Iteration 340, loss = 0.48009729\n",
      "Iteration 341, loss = 0.47991606\n",
      "Iteration 342, loss = 0.48020083\n",
      "Iteration 343, loss = 0.47977118\n",
      "Iteration 344, loss = 0.48015469\n",
      "Iteration 345, loss = 0.47978723\n",
      "Iteration 346, loss = 0.47977967\n",
      "Iteration 347, loss = 0.47998296\n",
      "Iteration 348, loss = 0.47951539\n",
      "Iteration 349, loss = 0.47951712\n",
      "Iteration 350, loss = 0.47977632\n",
      "Iteration 351, loss = 0.47960445\n",
      "Iteration 352, loss = 0.47957105\n",
      "Iteration 353, loss = 0.47923192\n",
      "Iteration 354, loss = 0.47930213\n",
      "Iteration 355, loss = 0.47918010\n",
      "Iteration 356, loss = 0.47945597\n",
      "Iteration 357, loss = 0.47915785\n",
      "Iteration 358, loss = 0.47896925\n",
      "Iteration 359, loss = 0.47908252\n",
      "Iteration 360, loss = 0.47896148\n",
      "Iteration 361, loss = 0.47917525\n",
      "Iteration 362, loss = 0.47882444\n",
      "Iteration 363, loss = 0.47887975\n",
      "Iteration 364, loss = 0.47892389\n",
      "Iteration 365, loss = 0.47891322\n",
      "Iteration 366, loss = 0.47916876\n",
      "Iteration 367, loss = 0.47877306\n",
      "Iteration 368, loss = 0.47870296\n",
      "Iteration 369, loss = 0.47892319\n",
      "Iteration 370, loss = 0.47885554\n",
      "Iteration 371, loss = 0.47854032\n",
      "Iteration 372, loss = 0.47889079\n",
      "Iteration 373, loss = 0.47860967\n",
      "Iteration 374, loss = 0.47888666\n",
      "Iteration 375, loss = 0.47832016\n",
      "Iteration 376, loss = 0.47849754\n",
      "Iteration 377, loss = 0.47821993\n",
      "Iteration 378, loss = 0.47838344\n",
      "Iteration 379, loss = 0.47854025\n",
      "Iteration 380, loss = 0.47821806\n",
      "Iteration 381, loss = 0.47812530\n",
      "Iteration 382, loss = 0.47819672\n",
      "Iteration 383, loss = 0.47871833\n",
      "Iteration 384, loss = 0.47809597\n",
      "Iteration 385, loss = 0.47807022\n",
      "Iteration 386, loss = 0.47814932\n",
      "Iteration 387, loss = 0.47773790\n",
      "Iteration 388, loss = 0.47799266\n",
      "Iteration 389, loss = 0.47805285\n",
      "Iteration 390, loss = 0.47839682\n",
      "Iteration 391, loss = 0.47762554\n",
      "Iteration 392, loss = 0.47797484\n",
      "Iteration 393, loss = 0.47769011\n",
      "Iteration 394, loss = 0.47776696\n",
      "Iteration 395, loss = 0.47784660\n",
      "Iteration 396, loss = 0.47793482\n",
      "Iteration 397, loss = 0.47753129\n",
      "Iteration 398, loss = 0.47761971\n",
      "Iteration 399, loss = 0.47800566\n",
      "Iteration 400, loss = 0.47725690\n",
      "Iteration 401, loss = 0.47730819\n",
      "Iteration 402, loss = 0.47732396\n",
      "Iteration 403, loss = 0.47711707\n",
      "Iteration 404, loss = 0.47718263\n",
      "Iteration 405, loss = 0.47745503\n",
      "Iteration 406, loss = 0.47716237\n",
      "Iteration 407, loss = 0.47716374\n",
      "Iteration 408, loss = 0.47704067\n",
      "Iteration 409, loss = 0.47708177\n",
      "Iteration 410, loss = 0.47713708\n",
      "Iteration 411, loss = 0.47707497\n",
      "Iteration 412, loss = 0.47719198\n",
      "Iteration 413, loss = 0.47671712\n",
      "Iteration 414, loss = 0.47750048\n",
      "Iteration 415, loss = 0.47691822\n",
      "Iteration 416, loss = 0.47704251\n",
      "Iteration 417, loss = 0.47693622\n",
      "Iteration 418, loss = 0.47687066\n",
      "Iteration 419, loss = 0.47682679\n",
      "Iteration 420, loss = 0.47679369\n",
      "Iteration 421, loss = 0.47692750\n",
      "Iteration 422, loss = 0.47670791\n",
      "Iteration 423, loss = 0.47667648\n",
      "Iteration 424, loss = 0.47647386\n",
      "Iteration 425, loss = 0.47685969\n",
      "Iteration 426, loss = 0.47681950\n",
      "Iteration 427, loss = 0.47666576\n",
      "Iteration 428, loss = 0.47640974\n",
      "Iteration 429, loss = 0.47641048\n",
      "Iteration 430, loss = 0.47657386\n",
      "Iteration 431, loss = 0.47625244\n",
      "Iteration 432, loss = 0.47626323\n",
      "Iteration 433, loss = 0.47669064\n",
      "Iteration 434, loss = 0.47627880\n",
      "Iteration 435, loss = 0.47674421\n",
      "Iteration 436, loss = 0.47616678\n",
      "Iteration 437, loss = 0.47605151\n",
      "Iteration 438, loss = 0.47596422\n",
      "Iteration 439, loss = 0.47625283\n",
      "Iteration 440, loss = 0.47624837\n",
      "Iteration 441, loss = 0.47594388\n",
      "Iteration 442, loss = 0.47593829\n",
      "Iteration 443, loss = 0.47564163\n",
      "Iteration 444, loss = 0.47577973\n",
      "Iteration 445, loss = 0.47607812\n",
      "Iteration 446, loss = 0.47588736\n",
      "Iteration 447, loss = 0.47565921\n",
      "Iteration 448, loss = 0.47551559\n",
      "Iteration 449, loss = 0.47541542\n",
      "Iteration 450, loss = 0.47617737\n",
      "Iteration 451, loss = 0.47633207\n",
      "Iteration 452, loss = 0.47561328\n",
      "Iteration 453, loss = 0.47517595\n",
      "Iteration 454, loss = 0.47519497\n",
      "Iteration 455, loss = 0.47520496\n",
      "Iteration 456, loss = 0.47522940\n",
      "Iteration 457, loss = 0.47530520\n",
      "Iteration 458, loss = 0.47523836\n",
      "Iteration 459, loss = 0.47529921\n",
      "Iteration 460, loss = 0.47513485\n",
      "Iteration 461, loss = 0.47522090\n",
      "Iteration 462, loss = 0.47508707\n",
      "Iteration 463, loss = 0.47492974\n",
      "Iteration 464, loss = 0.47499962\n",
      "Iteration 465, loss = 0.47486066\n",
      "Iteration 466, loss = 0.47467453\n",
      "Iteration 467, loss = 0.47506510\n",
      "Iteration 468, loss = 0.47479997\n",
      "Iteration 469, loss = 0.47451525\n",
      "Iteration 470, loss = 0.47479627\n",
      "Iteration 471, loss = 0.47470583\n",
      "Iteration 472, loss = 0.47493769\n",
      "Iteration 473, loss = 0.47468877\n",
      "Iteration 474, loss = 0.47474038\n",
      "Iteration 475, loss = 0.47447171\n",
      "Iteration 476, loss = 0.47482341\n",
      "Iteration 477, loss = 0.47463924\n",
      "Iteration 478, loss = 0.47476452\n",
      "Iteration 479, loss = 0.47446841\n",
      "Iteration 480, loss = 0.47478359\n",
      "Iteration 481, loss = 0.47442704\n",
      "Iteration 482, loss = 0.47415691\n",
      "Iteration 483, loss = 0.47467061\n",
      "Iteration 484, loss = 0.47407126\n",
      "Iteration 485, loss = 0.47426706\n",
      "Iteration 486, loss = 0.47403845\n",
      "Iteration 487, loss = 0.47431730\n",
      "Iteration 488, loss = 0.47395642\n",
      "Iteration 489, loss = 0.47403853\n",
      "Iteration 490, loss = 0.47387315\n",
      "Iteration 491, loss = 0.47403954\n",
      "Iteration 492, loss = 0.47394523\n",
      "Iteration 493, loss = 0.47392726\n",
      "Iteration 494, loss = 0.47363474\n",
      "Iteration 495, loss = 0.47426832\n",
      "Iteration 496, loss = 0.47398197\n",
      "Iteration 497, loss = 0.47392410\n",
      "Iteration 498, loss = 0.47391619\n",
      "Iteration 499, loss = 0.47368974\n",
      "Iteration 500, loss = 0.47409038\n",
      "Iteration 501, loss = 0.47383797\n",
      "Iteration 502, loss = 0.47391467\n",
      "Iteration 503, loss = 0.47349034\n",
      "Iteration 504, loss = 0.47365210\n",
      "Iteration 505, loss = 0.47348135\n",
      "Iteration 506, loss = 0.47346990\n",
      "Iteration 507, loss = 0.47339932\n",
      "Iteration 508, loss = 0.47346850\n",
      "Iteration 509, loss = 0.47336052\n",
      "Iteration 510, loss = 0.47339413\n",
      "Iteration 511, loss = 0.47356713\n",
      "Iteration 512, loss = 0.47440740\n",
      "Iteration 513, loss = 0.47378320\n",
      "Iteration 514, loss = 0.47344670\n",
      "Iteration 515, loss = 0.47364181\n",
      "Iteration 516, loss = 0.47346924\n",
      "Iteration 517, loss = 0.47363356\n",
      "Iteration 518, loss = 0.47344244\n",
      "Iteration 519, loss = 0.47306514\n",
      "Iteration 520, loss = 0.47314198\n",
      "Iteration 521, loss = 0.47356934\n",
      "Iteration 522, loss = 0.47298287\n",
      "Iteration 523, loss = 0.47300690\n",
      "Iteration 524, loss = 0.47298545\n",
      "Iteration 525, loss = 0.47290072\n",
      "Iteration 526, loss = 0.47305881\n",
      "Iteration 527, loss = 0.47295154\n",
      "Iteration 528, loss = 0.47364309\n",
      "Iteration 529, loss = 0.47299568\n",
      "Iteration 530, loss = 0.47287713\n",
      "Iteration 531, loss = 0.47271792\n",
      "Iteration 532, loss = 0.47276820\n",
      "Iteration 533, loss = 0.47281923\n",
      "Iteration 534, loss = 0.47255659\n",
      "Iteration 535, loss = 0.47278629\n",
      "Iteration 536, loss = 0.47262779\n",
      "Iteration 537, loss = 0.47253076\n",
      "Iteration 538, loss = 0.47260531\n",
      "Iteration 539, loss = 0.47257605\n",
      "Iteration 540, loss = 0.47252739\n",
      "Iteration 541, loss = 0.47256915\n",
      "Iteration 542, loss = 0.47241625\n",
      "Iteration 543, loss = 0.47254166\n",
      "Iteration 544, loss = 0.47254266\n",
      "Iteration 545, loss = 0.47236201\n",
      "Iteration 546, loss = 0.47222610\n",
      "Iteration 547, loss = 0.47233947\n",
      "Iteration 548, loss = 0.47227271\n",
      "Iteration 549, loss = 0.47202830\n",
      "Iteration 550, loss = 0.47222440\n",
      "Iteration 551, loss = 0.47244445\n",
      "Iteration 552, loss = 0.47191727\n",
      "Iteration 553, loss = 0.47194350\n",
      "Iteration 554, loss = 0.47202926\n",
      "Iteration 555, loss = 0.47203263\n",
      "Iteration 556, loss = 0.47199571\n",
      "Iteration 557, loss = 0.47178567\n",
      "Iteration 558, loss = 0.47210768\n",
      "Iteration 559, loss = 0.47169141\n",
      "Iteration 560, loss = 0.47183972\n",
      "Iteration 561, loss = 0.47185614\n",
      "Iteration 562, loss = 0.47171383\n",
      "Iteration 563, loss = 0.47194883\n",
      "Iteration 564, loss = 0.47148163\n",
      "Iteration 565, loss = 0.47162543\n",
      "Iteration 566, loss = 0.47158027\n",
      "Iteration 567, loss = 0.47142157\n",
      "Iteration 568, loss = 0.47146217\n",
      "Iteration 569, loss = 0.47141214\n",
      "Iteration 570, loss = 0.47130926\n",
      "Iteration 571, loss = 0.47173093\n",
      "Iteration 572, loss = 0.47135563\n",
      "Iteration 573, loss = 0.47138590\n",
      "Iteration 574, loss = 0.47146038\n",
      "Iteration 575, loss = 0.47115171\n",
      "Iteration 576, loss = 0.47188566\n",
      "Iteration 577, loss = 0.47158235\n",
      "Iteration 578, loss = 0.47106476\n",
      "Iteration 579, loss = 0.47111310\n",
      "Iteration 580, loss = 0.47097952\n",
      "Iteration 581, loss = 0.47101833\n",
      "Iteration 582, loss = 0.47115309\n",
      "Iteration 583, loss = 0.47094283\n",
      "Iteration 584, loss = 0.47067031\n",
      "Iteration 585, loss = 0.47091684\n",
      "Iteration 586, loss = 0.47095006\n",
      "Iteration 587, loss = 0.47183963\n",
      "Iteration 588, loss = 0.47157102\n",
      "Iteration 589, loss = 0.47135650\n",
      "Iteration 590, loss = 0.47056108\n",
      "Iteration 591, loss = 0.47076698\n",
      "Iteration 592, loss = 0.47060902\n",
      "Iteration 593, loss = 0.47065937\n",
      "Iteration 594, loss = 0.47040993\n",
      "Iteration 595, loss = 0.47045246\n",
      "Iteration 596, loss = 0.47053918\n",
      "Iteration 597, loss = 0.47051340\n",
      "Iteration 598, loss = 0.47108274\n",
      "Iteration 599, loss = 0.47076110\n",
      "Iteration 600, loss = 0.47026132\n",
      "Iteration 601, loss = 0.47043528\n",
      "Iteration 602, loss = 0.46994181\n",
      "Iteration 603, loss = 0.47018610\n",
      "Iteration 604, loss = 0.47022955\n",
      "Iteration 605, loss = 0.47061651\n",
      "Iteration 606, loss = 0.47007969\n",
      "Iteration 607, loss = 0.47030487\n",
      "Iteration 608, loss = 0.46984030\n",
      "Iteration 609, loss = 0.47002267\n",
      "Iteration 610, loss = 0.46971046\n",
      "Iteration 611, loss = 0.46984712\n",
      "Iteration 612, loss = 0.46983559\n",
      "Iteration 613, loss = 0.46975122\n",
      "Iteration 614, loss = 0.46975323\n",
      "Iteration 615, loss = 0.46969451\n",
      "Iteration 616, loss = 0.46950672\n",
      "Iteration 617, loss = 0.46962560\n",
      "Iteration 618, loss = 0.46967251\n",
      "Iteration 619, loss = 0.46955342\n",
      "Iteration 620, loss = 0.46975760\n",
      "Iteration 621, loss = 0.46961624\n",
      "Iteration 622, loss = 0.46932365\n",
      "Iteration 623, loss = 0.46933493\n",
      "Iteration 624, loss = 0.46937013\n",
      "Iteration 625, loss = 0.46953310\n",
      "Iteration 626, loss = 0.46941361\n",
      "Iteration 627, loss = 0.46920713\n",
      "Iteration 628, loss = 0.46943363\n",
      "Iteration 629, loss = 0.46948943\n",
      "Iteration 630, loss = 0.46897752\n",
      "Iteration 631, loss = 0.46909865\n",
      "Iteration 632, loss = 0.46900936\n",
      "Iteration 633, loss = 0.46891211\n",
      "Iteration 634, loss = 0.46881708\n",
      "Iteration 635, loss = 0.46925565\n",
      "Iteration 636, loss = 0.46900109\n",
      "Iteration 637, loss = 0.46897303\n",
      "Iteration 638, loss = 0.46906122\n",
      "Iteration 639, loss = 0.46877912\n",
      "Iteration 640, loss = 0.46853317\n",
      "Iteration 641, loss = 0.46863491\n",
      "Iteration 642, loss = 0.46864808\n",
      "Iteration 643, loss = 0.46870809\n",
      "Iteration 644, loss = 0.46850206\n",
      "Iteration 645, loss = 0.46857891\n",
      "Iteration 646, loss = 0.46868814\n",
      "Iteration 647, loss = 0.46853696\n",
      "Iteration 648, loss = 0.46888156\n",
      "Iteration 649, loss = 0.46857818\n",
      "Iteration 650, loss = 0.46842326\n",
      "Iteration 651, loss = 0.46855530\n",
      "Iteration 652, loss = 0.46863247\n",
      "Iteration 653, loss = 0.46830687\n",
      "Iteration 654, loss = 0.46863167\n",
      "Iteration 655, loss = 0.46817901\n",
      "Iteration 656, loss = 0.46852067\n",
      "Iteration 657, loss = 0.46814692\n",
      "Iteration 658, loss = 0.46806406\n",
      "Iteration 659, loss = 0.46817307\n",
      "Iteration 660, loss = 0.46838138\n",
      "Iteration 661, loss = 0.46799910\n",
      "Iteration 662, loss = 0.46793536\n",
      "Iteration 663, loss = 0.46824495\n",
      "Iteration 664, loss = 0.46783009\n",
      "Iteration 665, loss = 0.46784225\n",
      "Iteration 666, loss = 0.46804772\n",
      "Iteration 667, loss = 0.46788231\n",
      "Iteration 668, loss = 0.46798361\n",
      "Iteration 669, loss = 0.46795331\n",
      "Iteration 670, loss = 0.46814318\n",
      "Iteration 671, loss = 0.46809660\n",
      "Iteration 672, loss = 0.46821414\n",
      "Iteration 673, loss = 0.46781896\n",
      "Iteration 674, loss = 0.46790166\n",
      "Iteration 675, loss = 0.46807247\n",
      "Iteration 676, loss = 0.46787214\n",
      "Iteration 677, loss = 0.46743709\n",
      "Iteration 678, loss = 0.46788888\n",
      "Iteration 679, loss = 0.46828448\n",
      "Iteration 680, loss = 0.46746225\n",
      "Iteration 681, loss = 0.46775068\n",
      "Iteration 682, loss = 0.46811858\n",
      "Iteration 683, loss = 0.46743298\n",
      "Iteration 684, loss = 0.46754086\n",
      "Iteration 685, loss = 0.46739132\n",
      "Iteration 686, loss = 0.46791664\n",
      "Iteration 687, loss = 0.46767674\n",
      "Iteration 688, loss = 0.46748361\n",
      "Iteration 689, loss = 0.46766933\n",
      "Iteration 690, loss = 0.46736451\n",
      "Iteration 691, loss = 0.46747894\n",
      "Iteration 692, loss = 0.46747983\n",
      "Iteration 693, loss = 0.46749791\n",
      "Iteration 694, loss = 0.46744616\n",
      "Iteration 695, loss = 0.46736894\n",
      "Iteration 696, loss = 0.46768933\n",
      "Iteration 697, loss = 0.46755084\n",
      "Iteration 698, loss = 0.46740013\n",
      "Iteration 699, loss = 0.46751058\n",
      "Iteration 700, loss = 0.46707377\n",
      "Iteration 701, loss = 0.46721353\n",
      "Iteration 702, loss = 0.46775925\n",
      "Iteration 703, loss = 0.46716024\n",
      "Iteration 704, loss = 0.46711117\n",
      "Iteration 705, loss = 0.46732984\n",
      "Iteration 706, loss = 0.46706150\n",
      "Iteration 707, loss = 0.46682557\n",
      "Iteration 708, loss = 0.46699036\n",
      "Iteration 709, loss = 0.46714234\n",
      "Iteration 710, loss = 0.46714809\n",
      "Iteration 711, loss = 0.46700505\n",
      "Iteration 712, loss = 0.46690151\n",
      "Iteration 713, loss = 0.46676801\n",
      "Iteration 714, loss = 0.46693917\n",
      "Iteration 715, loss = 0.46680753\n",
      "Iteration 716, loss = 0.46697965\n",
      "Iteration 717, loss = 0.46682352\n",
      "Iteration 718, loss = 0.46694593\n",
      "Iteration 719, loss = 0.46669530\n",
      "Iteration 720, loss = 0.46687498\n",
      "Iteration 721, loss = 0.46684350\n",
      "Iteration 722, loss = 0.46684143\n",
      "Iteration 723, loss = 0.46699371\n",
      "Iteration 724, loss = 0.46691928\n",
      "Iteration 725, loss = 0.46677774\n",
      "Iteration 726, loss = 0.46664668\n",
      "Iteration 727, loss = 0.46665060\n",
      "Iteration 728, loss = 0.46650495\n",
      "Iteration 729, loss = 0.46664940\n",
      "Iteration 730, loss = 0.46655059\n",
      "Iteration 731, loss = 0.46668533\n",
      "Iteration 732, loss = 0.46674584\n",
      "Iteration 733, loss = 0.46649284\n",
      "Iteration 734, loss = 0.46648765\n",
      "Iteration 735, loss = 0.46622950\n",
      "Iteration 736, loss = 0.46673953\n",
      "Iteration 737, loss = 0.46618361\n",
      "Iteration 738, loss = 0.46643343\n",
      "Iteration 739, loss = 0.46684712\n",
      "Iteration 740, loss = 0.46644704\n",
      "Iteration 741, loss = 0.46640652\n",
      "Iteration 742, loss = 0.46655980\n",
      "Iteration 743, loss = 0.46625742\n",
      "Iteration 744, loss = 0.46634742\n",
      "Iteration 745, loss = 0.46628019\n",
      "Iteration 746, loss = 0.46618183\n",
      "Iteration 747, loss = 0.46610448\n",
      "Iteration 748, loss = 0.46620324\n",
      "Iteration 749, loss = 0.46607308\n",
      "Iteration 750, loss = 0.46605684\n",
      "Iteration 751, loss = 0.46581548\n",
      "Iteration 752, loss = 0.46646183\n",
      "Iteration 753, loss = 0.46620113\n",
      "Iteration 754, loss = 0.46600674\n",
      "Iteration 755, loss = 0.46639121\n",
      "Iteration 756, loss = 0.46600770\n",
      "Iteration 757, loss = 0.46606216\n",
      "Iteration 758, loss = 0.46609415\n",
      "Iteration 759, loss = 0.46601663\n",
      "Iteration 760, loss = 0.46585724\n",
      "Iteration 761, loss = 0.46595752\n",
      "Iteration 762, loss = 0.46600168\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72608822\n",
      "Iteration 2, loss = 0.70210481\n",
      "Iteration 3, loss = 0.68501051\n",
      "Iteration 4, loss = 0.67166484\n",
      "Iteration 5, loss = 0.65917992\n",
      "Iteration 6, loss = 0.64678230\n",
      "Iteration 7, loss = 0.63450053\n",
      "Iteration 8, loss = 0.62241836\n",
      "Iteration 9, loss = 0.61087492\n",
      "Iteration 10, loss = 0.59959712\n",
      "Iteration 11, loss = 0.58966421\n",
      "Iteration 12, loss = 0.58122651\n",
      "Iteration 13, loss = 0.57384455\n",
      "Iteration 14, loss = 0.56789187\n",
      "Iteration 15, loss = 0.56270955\n",
      "Iteration 16, loss = 0.55837781\n",
      "Iteration 17, loss = 0.55479156\n",
      "Iteration 18, loss = 0.55172587\n",
      "Iteration 19, loss = 0.54922627\n",
      "Iteration 20, loss = 0.54670828\n",
      "Iteration 21, loss = 0.54461931\n",
      "Iteration 22, loss = 0.54282398\n",
      "Iteration 23, loss = 0.54100335\n",
      "Iteration 24, loss = 0.53903572\n",
      "Iteration 25, loss = 0.53758021\n",
      "Iteration 26, loss = 0.53575742\n",
      "Iteration 27, loss = 0.53405917\n",
      "Iteration 28, loss = 0.53233969\n",
      "Iteration 29, loss = 0.53062759\n",
      "Iteration 30, loss = 0.52921561\n",
      "Iteration 31, loss = 0.52779804\n",
      "Iteration 32, loss = 0.52637019\n",
      "Iteration 33, loss = 0.52514967\n",
      "Iteration 34, loss = 0.52379315\n",
      "Iteration 35, loss = 0.52311459\n",
      "Iteration 36, loss = 0.52165110\n",
      "Iteration 37, loss = 0.52030953\n",
      "Iteration 38, loss = 0.51938318\n",
      "Iteration 39, loss = 0.51836067\n",
      "Iteration 40, loss = 0.51756232\n",
      "Iteration 41, loss = 0.51643616\n",
      "Iteration 42, loss = 0.51588557\n",
      "Iteration 43, loss = 0.51482718\n",
      "Iteration 44, loss = 0.51450332\n",
      "Iteration 45, loss = 0.51348007\n",
      "Iteration 46, loss = 0.51289575\n",
      "Iteration 47, loss = 0.51221669\n",
      "Iteration 48, loss = 0.51162102\n",
      "Iteration 49, loss = 0.51118876\n",
      "Iteration 50, loss = 0.51051961\n",
      "Iteration 51, loss = 0.50976638\n",
      "Iteration 52, loss = 0.50901169\n",
      "Iteration 53, loss = 0.50844790\n",
      "Iteration 54, loss = 0.50795328\n",
      "Iteration 55, loss = 0.50767854\n",
      "Iteration 56, loss = 0.50709049\n",
      "Iteration 57, loss = 0.50655838\n",
      "Iteration 58, loss = 0.50605307\n",
      "Iteration 59, loss = 0.50588474\n",
      "Iteration 60, loss = 0.50493516\n",
      "Iteration 61, loss = 0.50496189\n",
      "Iteration 62, loss = 0.50394163\n",
      "Iteration 63, loss = 0.50354350\n",
      "Iteration 64, loss = 0.50307578\n",
      "Iteration 65, loss = 0.50270358\n",
      "Iteration 66, loss = 0.50229919\n",
      "Iteration 67, loss = 0.50214386\n",
      "Iteration 68, loss = 0.50159626\n",
      "Iteration 69, loss = 0.50131702\n",
      "Iteration 70, loss = 0.50067755\n",
      "Iteration 71, loss = 0.50018858\n",
      "Iteration 72, loss = 0.50046481\n",
      "Iteration 73, loss = 0.49963690\n",
      "Iteration 74, loss = 0.49912485\n",
      "Iteration 75, loss = 0.49888464\n",
      "Iteration 76, loss = 0.49840851\n",
      "Iteration 77, loss = 0.49845267\n",
      "Iteration 78, loss = 0.49796531\n",
      "Iteration 79, loss = 0.49762992\n",
      "Iteration 80, loss = 0.49756590\n",
      "Iteration 81, loss = 0.49683339\n",
      "Iteration 82, loss = 0.49697633\n",
      "Iteration 83, loss = 0.49667297\n",
      "Iteration 84, loss = 0.49645275\n",
      "Iteration 85, loss = 0.49683538\n",
      "Iteration 86, loss = 0.49604444\n",
      "Iteration 87, loss = 0.49587005\n",
      "Iteration 88, loss = 0.49604443\n",
      "Iteration 89, loss = 0.49539418\n",
      "Iteration 90, loss = 0.49555094\n",
      "Iteration 91, loss = 0.49491039\n",
      "Iteration 92, loss = 0.49493720\n",
      "Iteration 93, loss = 0.49448735\n",
      "Iteration 94, loss = 0.49433134\n",
      "Iteration 95, loss = 0.49406611\n",
      "Iteration 96, loss = 0.49440113\n",
      "Iteration 97, loss = 0.49396761\n",
      "Iteration 98, loss = 0.49380652\n",
      "Iteration 99, loss = 0.49415651\n",
      "Iteration 100, loss = 0.49311838\n",
      "Iteration 101, loss = 0.49299979\n",
      "Iteration 102, loss = 0.49303668\n",
      "Iteration 103, loss = 0.49304065\n",
      "Iteration 104, loss = 0.49287588\n",
      "Iteration 105, loss = 0.49241637\n",
      "Iteration 106, loss = 0.49231602\n",
      "Iteration 107, loss = 0.49221614\n",
      "Iteration 108, loss = 0.49218588\n",
      "Iteration 109, loss = 0.49240935\n",
      "Iteration 110, loss = 0.49163915\n",
      "Iteration 111, loss = 0.49196131\n",
      "Iteration 112, loss = 0.49169402\n",
      "Iteration 113, loss = 0.49140137\n",
      "Iteration 114, loss = 0.49153087\n",
      "Iteration 115, loss = 0.49116413\n",
      "Iteration 116, loss = 0.49138936\n",
      "Iteration 117, loss = 0.49076891\n",
      "Iteration 118, loss = 0.49085622\n",
      "Iteration 119, loss = 0.49065453\n",
      "Iteration 120, loss = 0.49060805\n",
      "Iteration 121, loss = 0.49070472\n",
      "Iteration 122, loss = 0.49026059\n",
      "Iteration 123, loss = 0.49043162\n",
      "Iteration 124, loss = 0.49007262\n",
      "Iteration 125, loss = 0.49004370\n",
      "Iteration 126, loss = 0.48992238\n",
      "Iteration 127, loss = 0.48986170\n",
      "Iteration 128, loss = 0.48946079\n",
      "Iteration 129, loss = 0.48964056\n",
      "Iteration 130, loss = 0.48964574\n",
      "Iteration 131, loss = 0.48909416\n",
      "Iteration 132, loss = 0.48949647\n",
      "Iteration 133, loss = 0.48930501\n",
      "Iteration 134, loss = 0.48883008\n",
      "Iteration 135, loss = 0.48881464\n",
      "Iteration 136, loss = 0.48882750\n",
      "Iteration 137, loss = 0.48861319\n",
      "Iteration 138, loss = 0.48836180\n",
      "Iteration 139, loss = 0.48852687\n",
      "Iteration 140, loss = 0.48837050\n",
      "Iteration 141, loss = 0.48821240\n",
      "Iteration 142, loss = 0.48802651\n",
      "Iteration 143, loss = 0.48807369\n",
      "Iteration 144, loss = 0.48806871\n",
      "Iteration 145, loss = 0.48780643\n",
      "Iteration 146, loss = 0.48756789\n",
      "Iteration 147, loss = 0.48770268\n",
      "Iteration 148, loss = 0.48758766\n",
      "Iteration 149, loss = 0.48752104\n",
      "Iteration 150, loss = 0.48733746\n",
      "Iteration 151, loss = 0.48705562\n",
      "Iteration 152, loss = 0.48683153\n",
      "Iteration 153, loss = 0.48679199\n",
      "Iteration 154, loss = 0.48654163\n",
      "Iteration 155, loss = 0.48691577\n",
      "Iteration 156, loss = 0.48655431\n",
      "Iteration 157, loss = 0.48636798\n",
      "Iteration 158, loss = 0.48664613\n",
      "Iteration 159, loss = 0.48620855\n",
      "Iteration 160, loss = 0.48619958\n",
      "Iteration 161, loss = 0.48628395\n",
      "Iteration 162, loss = 0.48688756\n",
      "Iteration 163, loss = 0.48626125\n",
      "Iteration 164, loss = 0.48581990\n",
      "Iteration 165, loss = 0.48602927\n",
      "Iteration 166, loss = 0.48571189\n",
      "Iteration 167, loss = 0.48546836\n",
      "Iteration 168, loss = 0.48563294\n",
      "Iteration 169, loss = 0.48533341\n",
      "Iteration 170, loss = 0.48545021\n",
      "Iteration 171, loss = 0.48519241\n",
      "Iteration 172, loss = 0.48506420\n",
      "Iteration 173, loss = 0.48518795\n",
      "Iteration 174, loss = 0.48482110\n",
      "Iteration 175, loss = 0.48508693\n",
      "Iteration 176, loss = 0.48482782\n",
      "Iteration 177, loss = 0.48481159\n",
      "Iteration 178, loss = 0.48466416\n",
      "Iteration 179, loss = 0.48437243\n",
      "Iteration 180, loss = 0.48471913\n",
      "Iteration 181, loss = 0.48425408\n",
      "Iteration 182, loss = 0.48444329\n",
      "Iteration 183, loss = 0.48455900\n",
      "Iteration 184, loss = 0.48416254\n",
      "Iteration 185, loss = 0.48433653\n",
      "Iteration 186, loss = 0.48429035\n",
      "Iteration 187, loss = 0.48441876\n",
      "Iteration 188, loss = 0.48385089\n",
      "Iteration 189, loss = 0.48399865\n",
      "Iteration 190, loss = 0.48378913\n",
      "Iteration 191, loss = 0.48396580\n",
      "Iteration 192, loss = 0.48372044\n",
      "Iteration 193, loss = 0.48377580\n",
      "Iteration 194, loss = 0.48375127\n",
      "Iteration 195, loss = 0.48408287\n",
      "Iteration 196, loss = 0.48340538\n",
      "Iteration 197, loss = 0.48395444\n",
      "Iteration 198, loss = 0.48346291\n",
      "Iteration 199, loss = 0.48347149\n",
      "Iteration 200, loss = 0.48341414\n",
      "Iteration 201, loss = 0.48324662\n",
      "Iteration 202, loss = 0.48329493\n",
      "Iteration 203, loss = 0.48316896\n",
      "Iteration 204, loss = 0.48320155\n",
      "Iteration 205, loss = 0.48334184\n",
      "Iteration 206, loss = 0.48309250\n",
      "Iteration 207, loss = 0.48322104\n",
      "Iteration 208, loss = 0.48335184\n",
      "Iteration 209, loss = 0.48295515\n",
      "Iteration 210, loss = 0.48311789\n",
      "Iteration 211, loss = 0.48282391\n",
      "Iteration 212, loss = 0.48264308\n",
      "Iteration 213, loss = 0.48298027\n",
      "Iteration 214, loss = 0.48297858\n",
      "Iteration 215, loss = 0.48258865\n",
      "Iteration 216, loss = 0.48260810\n",
      "Iteration 217, loss = 0.48258449\n",
      "Iteration 218, loss = 0.48261519\n",
      "Iteration 219, loss = 0.48234114\n",
      "Iteration 220, loss = 0.48259638\n",
      "Iteration 221, loss = 0.48231311\n",
      "Iteration 222, loss = 0.48300295\n",
      "Iteration 223, loss = 0.48252342\n",
      "Iteration 224, loss = 0.48203370\n",
      "Iteration 225, loss = 0.48250806\n",
      "Iteration 226, loss = 0.48211772\n",
      "Iteration 227, loss = 0.48226266\n",
      "Iteration 228, loss = 0.48221966\n",
      "Iteration 229, loss = 0.48235197\n",
      "Iteration 230, loss = 0.48198521\n",
      "Iteration 231, loss = 0.48155970\n",
      "Iteration 232, loss = 0.48175355\n",
      "Iteration 233, loss = 0.48203028\n",
      "Iteration 234, loss = 0.48186548\n",
      "Iteration 235, loss = 0.48164131\n",
      "Iteration 236, loss = 0.48181430\n",
      "Iteration 237, loss = 0.48166168\n",
      "Iteration 238, loss = 0.48175051\n",
      "Iteration 239, loss = 0.48128161\n",
      "Iteration 240, loss = 0.48137208\n",
      "Iteration 241, loss = 0.48167184\n",
      "Iteration 242, loss = 0.48100353\n",
      "Iteration 243, loss = 0.48150220\n",
      "Iteration 244, loss = 0.48157156\n",
      "Iteration 245, loss = 0.48115745\n",
      "Iteration 246, loss = 0.48160035\n",
      "Iteration 247, loss = 0.48112779\n",
      "Iteration 248, loss = 0.48109881\n",
      "Iteration 249, loss = 0.48106692\n",
      "Iteration 250, loss = 0.48093294\n",
      "Iteration 251, loss = 0.48091961\n",
      "Iteration 252, loss = 0.48120310\n",
      "Iteration 253, loss = 0.48088939\n",
      "Iteration 254, loss = 0.48093784\n",
      "Iteration 255, loss = 0.48066874\n",
      "Iteration 256, loss = 0.48054320\n",
      "Iteration 257, loss = 0.48054935\n",
      "Iteration 258, loss = 0.48052555\n",
      "Iteration 259, loss = 0.48055281\n",
      "Iteration 260, loss = 0.48086074\n",
      "Iteration 261, loss = 0.48051439\n",
      "Iteration 262, loss = 0.48058536\n",
      "Iteration 263, loss = 0.48046444\n",
      "Iteration 264, loss = 0.48054778\n",
      "Iteration 265, loss = 0.47997283\n",
      "Iteration 266, loss = 0.48065490\n",
      "Iteration 267, loss = 0.48043701\n",
      "Iteration 268, loss = 0.48024508\n",
      "Iteration 269, loss = 0.48012182\n",
      "Iteration 270, loss = 0.48005141\n",
      "Iteration 271, loss = 0.48006327\n",
      "Iteration 272, loss = 0.47975491\n",
      "Iteration 273, loss = 0.48021999\n",
      "Iteration 274, loss = 0.47984990\n",
      "Iteration 275, loss = 0.47984555\n",
      "Iteration 276, loss = 0.47983978\n",
      "Iteration 277, loss = 0.48022997\n",
      "Iteration 278, loss = 0.47963838\n",
      "Iteration 279, loss = 0.47989635\n",
      "Iteration 280, loss = 0.47978674\n",
      "Iteration 281, loss = 0.47965732\n",
      "Iteration 282, loss = 0.47944304\n",
      "Iteration 283, loss = 0.47941177\n",
      "Iteration 284, loss = 0.47946395\n",
      "Iteration 285, loss = 0.47966192\n",
      "Iteration 286, loss = 0.47932406\n",
      "Iteration 287, loss = 0.47983841\n",
      "Iteration 288, loss = 0.47955682\n",
      "Iteration 289, loss = 0.47950792\n",
      "Iteration 290, loss = 0.47945484\n",
      "Iteration 291, loss = 0.47913190\n",
      "Iteration 292, loss = 0.47912685\n",
      "Iteration 293, loss = 0.47926945\n",
      "Iteration 294, loss = 0.47920943\n",
      "Iteration 295, loss = 0.47915562\n",
      "Iteration 296, loss = 0.47913586\n",
      "Iteration 297, loss = 0.47938573\n",
      "Iteration 298, loss = 0.47910955\n",
      "Iteration 299, loss = 0.47923707\n",
      "Iteration 300, loss = 0.47877699\n",
      "Iteration 301, loss = 0.47896671\n",
      "Iteration 302, loss = 0.47903643\n",
      "Iteration 303, loss = 0.47879723\n",
      "Iteration 304, loss = 0.47882934\n",
      "Iteration 305, loss = 0.47896860\n",
      "Iteration 306, loss = 0.47900037\n",
      "Iteration 307, loss = 0.47877687\n",
      "Iteration 308, loss = 0.47861970\n",
      "Iteration 309, loss = 0.47873311\n",
      "Iteration 310, loss = 0.47858406\n",
      "Iteration 311, loss = 0.47895484\n",
      "Iteration 312, loss = 0.47830451\n",
      "Iteration 313, loss = 0.47859777\n",
      "Iteration 314, loss = 0.47849839\n",
      "Iteration 315, loss = 0.47944917\n",
      "Iteration 316, loss = 0.47824528\n",
      "Iteration 317, loss = 0.47885347\n",
      "Iteration 318, loss = 0.47869520\n",
      "Iteration 319, loss = 0.47813678\n",
      "Iteration 320, loss = 0.47875068\n",
      "Iteration 321, loss = 0.47832783\n",
      "Iteration 322, loss = 0.47830350\n",
      "Iteration 323, loss = 0.47905355\n",
      "Iteration 324, loss = 0.47872560\n",
      "Iteration 325, loss = 0.47852172\n",
      "Iteration 326, loss = 0.47803277\n",
      "Iteration 327, loss = 0.47826163\n",
      "Iteration 328, loss = 0.47799227\n",
      "Iteration 329, loss = 0.47805633\n",
      "Iteration 330, loss = 0.47793716\n",
      "Iteration 331, loss = 0.47776088\n",
      "Iteration 332, loss = 0.47794087\n",
      "Iteration 333, loss = 0.47781688\n",
      "Iteration 334, loss = 0.47777241\n",
      "Iteration 335, loss = 0.47789455\n",
      "Iteration 336, loss = 0.47804603\n",
      "Iteration 337, loss = 0.47822999\n",
      "Iteration 338, loss = 0.47771443\n",
      "Iteration 339, loss = 0.47767629\n",
      "Iteration 340, loss = 0.47760160\n",
      "Iteration 341, loss = 0.47768192\n",
      "Iteration 342, loss = 0.47784651\n",
      "Iteration 343, loss = 0.47772792\n",
      "Iteration 344, loss = 0.47773115\n",
      "Iteration 345, loss = 0.47764975\n",
      "Iteration 346, loss = 0.47776956\n",
      "Iteration 347, loss = 0.47755994\n",
      "Iteration 348, loss = 0.47755786\n",
      "Iteration 349, loss = 0.47760367\n",
      "Iteration 350, loss = 0.47735763\n",
      "Iteration 351, loss = 0.47716210\n",
      "Iteration 352, loss = 0.47733985\n",
      "Iteration 353, loss = 0.47708116\n",
      "Iteration 354, loss = 0.47753760\n",
      "Iteration 355, loss = 0.47714612\n",
      "Iteration 356, loss = 0.47746664\n",
      "Iteration 357, loss = 0.47712931\n",
      "Iteration 358, loss = 0.47730000\n",
      "Iteration 359, loss = 0.47713645\n",
      "Iteration 360, loss = 0.47698062\n",
      "Iteration 361, loss = 0.47698311\n",
      "Iteration 362, loss = 0.47724290\n",
      "Iteration 363, loss = 0.47719482\n",
      "Iteration 364, loss = 0.47721815\n",
      "Iteration 365, loss = 0.47719472\n",
      "Iteration 366, loss = 0.47696713\n",
      "Iteration 367, loss = 0.47687647\n",
      "Iteration 368, loss = 0.47714397\n",
      "Iteration 369, loss = 0.47725456\n",
      "Iteration 370, loss = 0.47721649\n",
      "Iteration 371, loss = 0.47716363\n",
      "Iteration 372, loss = 0.47680290\n",
      "Iteration 373, loss = 0.47697370\n",
      "Iteration 374, loss = 0.47677006\n",
      "Iteration 375, loss = 0.47686331\n",
      "Iteration 376, loss = 0.47697276\n",
      "Iteration 377, loss = 0.47670574\n",
      "Iteration 378, loss = 0.47680023\n",
      "Iteration 379, loss = 0.47675532\n",
      "Iteration 380, loss = 0.47681378\n",
      "Iteration 381, loss = 0.47678897\n",
      "Iteration 382, loss = 0.47660563\n",
      "Iteration 383, loss = 0.47657007\n",
      "Iteration 384, loss = 0.47668740\n",
      "Iteration 385, loss = 0.47680503\n",
      "Iteration 386, loss = 0.47640199\n",
      "Iteration 387, loss = 0.47644108\n",
      "Iteration 388, loss = 0.47653125\n",
      "Iteration 389, loss = 0.47637575\n",
      "Iteration 390, loss = 0.47642378\n",
      "Iteration 391, loss = 0.47628412\n",
      "Iteration 392, loss = 0.47615358\n",
      "Iteration 393, loss = 0.47615887\n",
      "Iteration 394, loss = 0.47621032\n",
      "Iteration 395, loss = 0.47627747\n",
      "Iteration 396, loss = 0.47602211\n",
      "Iteration 397, loss = 0.47629688\n",
      "Iteration 398, loss = 0.47633068\n",
      "Iteration 399, loss = 0.47633285\n",
      "Iteration 400, loss = 0.47614978\n",
      "Iteration 401, loss = 0.47631999\n",
      "Iteration 402, loss = 0.47612977\n",
      "Iteration 403, loss = 0.47594497\n",
      "Iteration 404, loss = 0.47623094\n",
      "Iteration 405, loss = 0.47624988\n",
      "Iteration 406, loss = 0.47591536\n",
      "Iteration 407, loss = 0.47616645\n",
      "Iteration 408, loss = 0.47637565\n",
      "Iteration 409, loss = 0.47665603\n",
      "Iteration 410, loss = 0.47588193\n",
      "Iteration 411, loss = 0.47617850\n",
      "Iteration 412, loss = 0.47580492\n",
      "Iteration 413, loss = 0.47660718\n",
      "Iteration 414, loss = 0.47614011\n",
      "Iteration 415, loss = 0.47578509\n",
      "Iteration 416, loss = 0.47606669\n",
      "Iteration 417, loss = 0.47591600\n",
      "Iteration 418, loss = 0.47573142\n",
      "Iteration 419, loss = 0.47585147\n",
      "Iteration 420, loss = 0.47568935\n",
      "Iteration 421, loss = 0.47584216\n",
      "Iteration 422, loss = 0.47597845\n",
      "Iteration 423, loss = 0.47575828\n",
      "Iteration 424, loss = 0.47558447\n",
      "Iteration 425, loss = 0.47558547\n",
      "Iteration 426, loss = 0.47553671\n",
      "Iteration 427, loss = 0.47574445\n",
      "Iteration 428, loss = 0.47557722\n",
      "Iteration 429, loss = 0.47564876\n",
      "Iteration 430, loss = 0.47574358\n",
      "Iteration 431, loss = 0.47556074\n",
      "Iteration 432, loss = 0.47553664\n",
      "Iteration 433, loss = 0.47563164\n",
      "Iteration 434, loss = 0.47565066\n",
      "Iteration 435, loss = 0.47565422\n",
      "Iteration 436, loss = 0.47573992\n",
      "Iteration 437, loss = 0.47533913\n",
      "Iteration 438, loss = 0.47559159\n",
      "Iteration 439, loss = 0.47558289\n",
      "Iteration 440, loss = 0.47567676\n",
      "Iteration 441, loss = 0.47507452\n",
      "Iteration 442, loss = 0.47529802\n",
      "Iteration 443, loss = 0.47541593\n",
      "Iteration 444, loss = 0.47576354\n",
      "Iteration 445, loss = 0.47529506\n",
      "Iteration 446, loss = 0.47529276\n",
      "Iteration 447, loss = 0.47525793\n",
      "Iteration 448, loss = 0.47506518\n",
      "Iteration 449, loss = 0.47511003\n",
      "Iteration 450, loss = 0.47516158\n",
      "Iteration 451, loss = 0.47498185\n",
      "Iteration 452, loss = 0.47524408\n",
      "Iteration 453, loss = 0.47523152\n",
      "Iteration 454, loss = 0.47521042\n",
      "Iteration 455, loss = 0.47503382\n",
      "Iteration 456, loss = 0.47484325\n",
      "Iteration 457, loss = 0.47506492\n",
      "Iteration 458, loss = 0.47513073\n",
      "Iteration 459, loss = 0.47466984\n",
      "Iteration 460, loss = 0.47486401\n",
      "Iteration 461, loss = 0.47532514\n",
      "Iteration 462, loss = 0.47455018\n",
      "Iteration 463, loss = 0.47483923\n",
      "Iteration 464, loss = 0.47475857\n",
      "Iteration 465, loss = 0.47498641\n",
      "Iteration 466, loss = 0.47460369\n",
      "Iteration 467, loss = 0.47457062\n",
      "Iteration 468, loss = 0.47459807\n",
      "Iteration 469, loss = 0.47466034\n",
      "Iteration 470, loss = 0.47463202\n",
      "Iteration 471, loss = 0.47507868\n",
      "Iteration 472, loss = 0.47443849\n",
      "Iteration 473, loss = 0.47439871\n",
      "Iteration 474, loss = 0.47451053\n",
      "Iteration 475, loss = 0.47496703\n",
      "Iteration 476, loss = 0.47446896\n",
      "Iteration 477, loss = 0.47446927\n",
      "Iteration 478, loss = 0.47421080\n",
      "Iteration 479, loss = 0.47437120\n",
      "Iteration 480, loss = 0.47439608\n",
      "Iteration 481, loss = 0.47419306\n",
      "Iteration 482, loss = 0.47423002\n",
      "Iteration 483, loss = 0.47420906\n",
      "Iteration 484, loss = 0.47460639\n",
      "Iteration 485, loss = 0.47413427\n",
      "Iteration 486, loss = 0.47415403\n",
      "Iteration 487, loss = 0.47429030\n",
      "Iteration 488, loss = 0.47494335\n",
      "Iteration 489, loss = 0.47474397\n",
      "Iteration 490, loss = 0.47483015\n",
      "Iteration 491, loss = 0.47391412\n",
      "Iteration 492, loss = 0.47383503\n",
      "Iteration 493, loss = 0.47415071\n",
      "Iteration 494, loss = 0.47381364\n",
      "Iteration 495, loss = 0.47405578\n",
      "Iteration 496, loss = 0.47390010\n",
      "Iteration 497, loss = 0.47401108\n",
      "Iteration 498, loss = 0.47452089\n",
      "Iteration 499, loss = 0.47384374\n",
      "Iteration 500, loss = 0.47389438\n",
      "Iteration 501, loss = 0.47389880\n",
      "Iteration 502, loss = 0.47375561\n",
      "Iteration 503, loss = 0.47405991\n",
      "Iteration 504, loss = 0.47414010\n",
      "Iteration 505, loss = 0.47384488\n",
      "Iteration 506, loss = 0.47368575\n",
      "Iteration 507, loss = 0.47379996\n",
      "Iteration 508, loss = 0.47380791\n",
      "Iteration 509, loss = 0.47377627\n",
      "Iteration 510, loss = 0.47364997\n",
      "Iteration 511, loss = 0.47385409\n",
      "Iteration 512, loss = 0.47382860\n",
      "Iteration 513, loss = 0.47397495\n",
      "Iteration 514, loss = 0.47388087\n",
      "Iteration 515, loss = 0.47385580\n",
      "Iteration 516, loss = 0.47402202\n",
      "Iteration 517, loss = 0.47374665\n",
      "Iteration 518, loss = 0.47380482\n",
      "Iteration 519, loss = 0.47441372\n",
      "Iteration 520, loss = 0.47346418\n",
      "Iteration 521, loss = 0.47373917\n",
      "Iteration 522, loss = 0.47349772\n",
      "Iteration 523, loss = 0.47337112\n",
      "Iteration 524, loss = 0.47357909\n",
      "Iteration 525, loss = 0.47359588\n",
      "Iteration 526, loss = 0.47320484\n",
      "Iteration 527, loss = 0.47342140\n",
      "Iteration 528, loss = 0.47368342\n",
      "Iteration 529, loss = 0.47334511\n",
      "Iteration 530, loss = 0.47346654\n",
      "Iteration 531, loss = 0.47368923\n",
      "Iteration 532, loss = 0.47407286\n",
      "Iteration 533, loss = 0.47327269\n",
      "Iteration 534, loss = 0.47341299\n",
      "Iteration 535, loss = 0.47367211\n",
      "Iteration 536, loss = 0.47343255\n",
      "Iteration 537, loss = 0.47340870\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69558441\n",
      "Iteration 2, loss = 0.67295118\n",
      "Iteration 3, loss = 0.65740950\n",
      "Iteration 4, loss = 0.64371118\n",
      "Iteration 5, loss = 0.63110937\n",
      "Iteration 6, loss = 0.61893737\n",
      "Iteration 7, loss = 0.60752824\n",
      "Iteration 8, loss = 0.59680726\n",
      "Iteration 9, loss = 0.58748128\n",
      "Iteration 10, loss = 0.57929232\n",
      "Iteration 11, loss = 0.57352753\n",
      "Iteration 12, loss = 0.56835558\n",
      "Iteration 13, loss = 0.56463629\n",
      "Iteration 14, loss = 0.56169500\n",
      "Iteration 15, loss = 0.55956618\n",
      "Iteration 16, loss = 0.55765348\n",
      "Iteration 17, loss = 0.55575628\n",
      "Iteration 18, loss = 0.55440447\n",
      "Iteration 19, loss = 0.55286945\n",
      "Iteration 20, loss = 0.55166729\n",
      "Iteration 21, loss = 0.55025650\n",
      "Iteration 22, loss = 0.54919919\n",
      "Iteration 23, loss = 0.54804724\n",
      "Iteration 24, loss = 0.54721615\n",
      "Iteration 25, loss = 0.54639403\n",
      "Iteration 26, loss = 0.54519387\n",
      "Iteration 27, loss = 0.54416495\n",
      "Iteration 28, loss = 0.54312805\n",
      "Iteration 29, loss = 0.54210999\n",
      "Iteration 30, loss = 0.54129576\n",
      "Iteration 31, loss = 0.54011138\n",
      "Iteration 32, loss = 0.53949503\n",
      "Iteration 33, loss = 0.53864869\n",
      "Iteration 34, loss = 0.53815262\n",
      "Iteration 35, loss = 0.53698932\n",
      "Iteration 36, loss = 0.53638487\n",
      "Iteration 37, loss = 0.53587489\n",
      "Iteration 38, loss = 0.53491534\n",
      "Iteration 39, loss = 0.53389169\n",
      "Iteration 40, loss = 0.53304514\n",
      "Iteration 41, loss = 0.53239235\n",
      "Iteration 42, loss = 0.53158985\n",
      "Iteration 43, loss = 0.53059284\n",
      "Iteration 44, loss = 0.52956745\n",
      "Iteration 45, loss = 0.52895850\n",
      "Iteration 46, loss = 0.52801718\n",
      "Iteration 47, loss = 0.52693592\n",
      "Iteration 48, loss = 0.52641034\n",
      "Iteration 49, loss = 0.52497186\n",
      "Iteration 50, loss = 0.52395833\n",
      "Iteration 51, loss = 0.52295683\n",
      "Iteration 52, loss = 0.52211990\n",
      "Iteration 53, loss = 0.52110463\n",
      "Iteration 54, loss = 0.52055800\n",
      "Iteration 55, loss = 0.51969875\n",
      "Iteration 56, loss = 0.51928393\n",
      "Iteration 57, loss = 0.51753586\n",
      "Iteration 58, loss = 0.51659657\n",
      "Iteration 59, loss = 0.51565139\n",
      "Iteration 60, loss = 0.51499122\n",
      "Iteration 61, loss = 0.51425268\n",
      "Iteration 62, loss = 0.51342564\n",
      "Iteration 63, loss = 0.51301285\n",
      "Iteration 64, loss = 0.51196978\n",
      "Iteration 65, loss = 0.51108005\n",
      "Iteration 66, loss = 0.51083932\n",
      "Iteration 67, loss = 0.50947570\n",
      "Iteration 68, loss = 0.50903147\n",
      "Iteration 69, loss = 0.50830112\n",
      "Iteration 70, loss = 0.50793288\n",
      "Iteration 71, loss = 0.50766779\n",
      "Iteration 72, loss = 0.50651373\n",
      "Iteration 73, loss = 0.50608469\n",
      "Iteration 74, loss = 0.50551920\n",
      "Iteration 75, loss = 0.50523853\n",
      "Iteration 76, loss = 0.50482165\n",
      "Iteration 77, loss = 0.50428592\n",
      "Iteration 78, loss = 0.50373799\n",
      "Iteration 79, loss = 0.50353118\n",
      "Iteration 80, loss = 0.50329701\n",
      "Iteration 81, loss = 0.50271788\n",
      "Iteration 82, loss = 0.50269220\n",
      "Iteration 83, loss = 0.50175833\n",
      "Iteration 84, loss = 0.50153099\n",
      "Iteration 85, loss = 0.50126607\n",
      "Iteration 86, loss = 0.50094388\n",
      "Iteration 87, loss = 0.50135936\n",
      "Iteration 88, loss = 0.50066906\n",
      "Iteration 89, loss = 0.50109125\n",
      "Iteration 90, loss = 0.49987619\n",
      "Iteration 91, loss = 0.49969713\n",
      "Iteration 92, loss = 0.49945007\n",
      "Iteration 93, loss = 0.49944739\n",
      "Iteration 94, loss = 0.49902206\n",
      "Iteration 95, loss = 0.49858096\n",
      "Iteration 96, loss = 0.49825792\n",
      "Iteration 97, loss = 0.49834172\n",
      "Iteration 98, loss = 0.49800506\n",
      "Iteration 99, loss = 0.49739217\n",
      "Iteration 100, loss = 0.49772380\n",
      "Iteration 101, loss = 0.49729156\n",
      "Iteration 102, loss = 0.49724837\n",
      "Iteration 103, loss = 0.49734967\n",
      "Iteration 104, loss = 0.49707754\n",
      "Iteration 105, loss = 0.49657297\n",
      "Iteration 106, loss = 0.49632284\n",
      "Iteration 107, loss = 0.49619708\n",
      "Iteration 108, loss = 0.49599519\n",
      "Iteration 109, loss = 0.49607255\n",
      "Iteration 110, loss = 0.49583388\n",
      "Iteration 111, loss = 0.49571823\n",
      "Iteration 112, loss = 0.49537439\n",
      "Iteration 113, loss = 0.49555212\n",
      "Iteration 114, loss = 0.49489735\n",
      "Iteration 115, loss = 0.49511052\n",
      "Iteration 116, loss = 0.49478164\n",
      "Iteration 117, loss = 0.49481054\n",
      "Iteration 118, loss = 0.49460127\n",
      "Iteration 119, loss = 0.49436426\n",
      "Iteration 120, loss = 0.49421655\n",
      "Iteration 121, loss = 0.49397671\n",
      "Iteration 122, loss = 0.49394866\n",
      "Iteration 123, loss = 0.49388125\n",
      "Iteration 124, loss = 0.49360410\n",
      "Iteration 125, loss = 0.49364254\n",
      "Iteration 126, loss = 0.49347078\n",
      "Iteration 127, loss = 0.49322984\n",
      "Iteration 128, loss = 0.49321033\n",
      "Iteration 129, loss = 0.49347672\n",
      "Iteration 130, loss = 0.49309210\n",
      "Iteration 131, loss = 0.49311238\n",
      "Iteration 132, loss = 0.49276397\n",
      "Iteration 133, loss = 0.49260335\n",
      "Iteration 134, loss = 0.49330015\n",
      "Iteration 135, loss = 0.49304806\n",
      "Iteration 136, loss = 0.49206759\n",
      "Iteration 137, loss = 0.49260390\n",
      "Iteration 138, loss = 0.49240345\n",
      "Iteration 139, loss = 0.49248679\n",
      "Iteration 140, loss = 0.49253288\n",
      "Iteration 141, loss = 0.49147087\n",
      "Iteration 142, loss = 0.49144043\n",
      "Iteration 143, loss = 0.49156394\n",
      "Iteration 144, loss = 0.49112764\n",
      "Iteration 145, loss = 0.49141066\n",
      "Iteration 146, loss = 0.49095427\n",
      "Iteration 147, loss = 0.49117254\n",
      "Iteration 148, loss = 0.49101080\n",
      "Iteration 149, loss = 0.49113508\n",
      "Iteration 150, loss = 0.49079453\n",
      "Iteration 151, loss = 0.49083697\n",
      "Iteration 152, loss = 0.49048424\n",
      "Iteration 153, loss = 0.49050860\n",
      "Iteration 154, loss = 0.49044687\n",
      "Iteration 155, loss = 0.49052508\n",
      "Iteration 156, loss = 0.49066878\n",
      "Iteration 157, loss = 0.49032892\n",
      "Iteration 158, loss = 0.49015217\n",
      "Iteration 159, loss = 0.49005602\n",
      "Iteration 160, loss = 0.49027317\n",
      "Iteration 161, loss = 0.49039968\n",
      "Iteration 162, loss = 0.49000258\n",
      "Iteration 163, loss = 0.48990945\n",
      "Iteration 164, loss = 0.48952589\n",
      "Iteration 165, loss = 0.48955555\n",
      "Iteration 166, loss = 0.48965781\n",
      "Iteration 167, loss = 0.48924008\n",
      "Iteration 168, loss = 0.48953953\n",
      "Iteration 169, loss = 0.48990968\n",
      "Iteration 170, loss = 0.48931173\n",
      "Iteration 171, loss = 0.48892863\n",
      "Iteration 172, loss = 0.48933192\n",
      "Iteration 173, loss = 0.48904359\n",
      "Iteration 174, loss = 0.48884294\n",
      "Iteration 175, loss = 0.48882332\n",
      "Iteration 176, loss = 0.48860257\n",
      "Iteration 177, loss = 0.48882085\n",
      "Iteration 178, loss = 0.48850501\n",
      "Iteration 179, loss = 0.48866589\n",
      "Iteration 180, loss = 0.48882250\n",
      "Iteration 181, loss = 0.48940579\n",
      "Iteration 182, loss = 0.48941983\n",
      "Iteration 183, loss = 0.48834366\n",
      "Iteration 184, loss = 0.48863432\n",
      "Iteration 185, loss = 0.48798853\n",
      "Iteration 186, loss = 0.48827742\n",
      "Iteration 187, loss = 0.48795491\n",
      "Iteration 188, loss = 0.48784268\n",
      "Iteration 189, loss = 0.48802845\n",
      "Iteration 190, loss = 0.48778576\n",
      "Iteration 191, loss = 0.48762031\n",
      "Iteration 192, loss = 0.48771443\n",
      "Iteration 193, loss = 0.48787008\n",
      "Iteration 194, loss = 0.48864073\n",
      "Iteration 195, loss = 0.48791441\n",
      "Iteration 196, loss = 0.48759229\n",
      "Iteration 197, loss = 0.48735346\n",
      "Iteration 198, loss = 0.48724347\n",
      "Iteration 199, loss = 0.48722475\n",
      "Iteration 200, loss = 0.48701375\n",
      "Iteration 201, loss = 0.48706269\n",
      "Iteration 202, loss = 0.48693308\n",
      "Iteration 203, loss = 0.48684599\n",
      "Iteration 204, loss = 0.48683715\n",
      "Iteration 205, loss = 0.48704538\n",
      "Iteration 206, loss = 0.48670908\n",
      "Iteration 207, loss = 0.48662626\n",
      "Iteration 208, loss = 0.48686308\n",
      "Iteration 209, loss = 0.48643466\n",
      "Iteration 210, loss = 0.48640154\n",
      "Iteration 211, loss = 0.48663297\n",
      "Iteration 212, loss = 0.48664973\n",
      "Iteration 213, loss = 0.48627335\n",
      "Iteration 214, loss = 0.48619075\n",
      "Iteration 215, loss = 0.48595023\n",
      "Iteration 216, loss = 0.48599541\n",
      "Iteration 217, loss = 0.48628495\n",
      "Iteration 218, loss = 0.48627188\n",
      "Iteration 219, loss = 0.48623555\n",
      "Iteration 220, loss = 0.48578185\n",
      "Iteration 221, loss = 0.48621984\n",
      "Iteration 222, loss = 0.48583341\n",
      "Iteration 223, loss = 0.48542061\n",
      "Iteration 224, loss = 0.48557536\n",
      "Iteration 225, loss = 0.48548075\n",
      "Iteration 226, loss = 0.48550894\n",
      "Iteration 227, loss = 0.48576380\n",
      "Iteration 228, loss = 0.48534920\n",
      "Iteration 229, loss = 0.48536189\n",
      "Iteration 230, loss = 0.48504267\n",
      "Iteration 231, loss = 0.48544602\n",
      "Iteration 232, loss = 0.48549042\n",
      "Iteration 233, loss = 0.48470572\n",
      "Iteration 234, loss = 0.48477351\n",
      "Iteration 235, loss = 0.48497270\n",
      "Iteration 236, loss = 0.48497795\n",
      "Iteration 237, loss = 0.48503153\n",
      "Iteration 238, loss = 0.48462681\n",
      "Iteration 239, loss = 0.48467053\n",
      "Iteration 240, loss = 0.48489435\n",
      "Iteration 241, loss = 0.48427322\n",
      "Iteration 242, loss = 0.48472047\n",
      "Iteration 243, loss = 0.48463032\n",
      "Iteration 244, loss = 0.48447979\n",
      "Iteration 245, loss = 0.48485872\n",
      "Iteration 246, loss = 0.48417660\n",
      "Iteration 247, loss = 0.48422862\n",
      "Iteration 248, loss = 0.48427315\n",
      "Iteration 249, loss = 0.48433005\n",
      "Iteration 250, loss = 0.48408755\n",
      "Iteration 251, loss = 0.48393409\n",
      "Iteration 252, loss = 0.48395546\n",
      "Iteration 253, loss = 0.48415515\n",
      "Iteration 254, loss = 0.48411207\n",
      "Iteration 255, loss = 0.48383260\n",
      "Iteration 256, loss = 0.48377908\n",
      "Iteration 257, loss = 0.48365252\n",
      "Iteration 258, loss = 0.48393387\n",
      "Iteration 259, loss = 0.48379101\n",
      "Iteration 260, loss = 0.48386783\n",
      "Iteration 261, loss = 0.48380078\n",
      "Iteration 262, loss = 0.48367390\n",
      "Iteration 263, loss = 0.48351249\n",
      "Iteration 264, loss = 0.48348564\n",
      "Iteration 265, loss = 0.48369095\n",
      "Iteration 266, loss = 0.48313093\n",
      "Iteration 267, loss = 0.48357461\n",
      "Iteration 268, loss = 0.48356675\n",
      "Iteration 269, loss = 0.48347394\n",
      "Iteration 270, loss = 0.48316869\n",
      "Iteration 271, loss = 0.48312208\n",
      "Iteration 272, loss = 0.48285632\n",
      "Iteration 273, loss = 0.48309228\n",
      "Iteration 274, loss = 0.48316812\n",
      "Iteration 275, loss = 0.48295463\n",
      "Iteration 276, loss = 0.48278434\n",
      "Iteration 277, loss = 0.48267247\n",
      "Iteration 278, loss = 0.48277446\n",
      "Iteration 279, loss = 0.48290920\n",
      "Iteration 280, loss = 0.48284545\n",
      "Iteration 281, loss = 0.48255965\n",
      "Iteration 282, loss = 0.48263010\n",
      "Iteration 283, loss = 0.48269748\n",
      "Iteration 284, loss = 0.48277092\n",
      "Iteration 285, loss = 0.48222551\n",
      "Iteration 286, loss = 0.48279403\n",
      "Iteration 287, loss = 0.48219790\n",
      "Iteration 288, loss = 0.48198173\n",
      "Iteration 289, loss = 0.48224831\n",
      "Iteration 290, loss = 0.48191880\n",
      "Iteration 291, loss = 0.48292012\n",
      "Iteration 292, loss = 0.48190564\n",
      "Iteration 293, loss = 0.48196503\n",
      "Iteration 294, loss = 0.48184677\n",
      "Iteration 295, loss = 0.48186074\n",
      "Iteration 296, loss = 0.48166299\n",
      "Iteration 297, loss = 0.48151459\n",
      "Iteration 298, loss = 0.48145918\n",
      "Iteration 299, loss = 0.48180832\n",
      "Iteration 300, loss = 0.48143205\n",
      "Iteration 301, loss = 0.48139466\n",
      "Iteration 302, loss = 0.48207352\n",
      "Iteration 303, loss = 0.48193087\n",
      "Iteration 304, loss = 0.48118268\n",
      "Iteration 305, loss = 0.48112506\n",
      "Iteration 306, loss = 0.48109097\n",
      "Iteration 307, loss = 0.48082924\n",
      "Iteration 308, loss = 0.48096006\n",
      "Iteration 309, loss = 0.48094760\n",
      "Iteration 310, loss = 0.48077657\n",
      "Iteration 311, loss = 0.48063554\n",
      "Iteration 312, loss = 0.48070587\n",
      "Iteration 313, loss = 0.48052965\n",
      "Iteration 314, loss = 0.48064071\n",
      "Iteration 315, loss = 0.48063592\n",
      "Iteration 316, loss = 0.48050732\n",
      "Iteration 317, loss = 0.48039003\n",
      "Iteration 318, loss = 0.48020966\n",
      "Iteration 319, loss = 0.48019508\n",
      "Iteration 320, loss = 0.48028047\n",
      "Iteration 321, loss = 0.48026269\n",
      "Iteration 322, loss = 0.48045848\n",
      "Iteration 323, loss = 0.48017139\n",
      "Iteration 324, loss = 0.48003398\n",
      "Iteration 325, loss = 0.48010755\n",
      "Iteration 326, loss = 0.48010111\n",
      "Iteration 327, loss = 0.47986653\n",
      "Iteration 328, loss = 0.47975880\n",
      "Iteration 329, loss = 0.47980625\n",
      "Iteration 330, loss = 0.47975440\n",
      "Iteration 331, loss = 0.47957492\n",
      "Iteration 332, loss = 0.47952052\n",
      "Iteration 333, loss = 0.47957714\n",
      "Iteration 334, loss = 0.47948407\n",
      "Iteration 335, loss = 0.48054582\n",
      "Iteration 336, loss = 0.47955138\n",
      "Iteration 337, loss = 0.47999362\n",
      "Iteration 338, loss = 0.47977093\n",
      "Iteration 339, loss = 0.47928506\n",
      "Iteration 340, loss = 0.47919665\n",
      "Iteration 341, loss = 0.47899211\n",
      "Iteration 342, loss = 0.47922328\n",
      "Iteration 343, loss = 0.47902004\n",
      "Iteration 344, loss = 0.47913991\n",
      "Iteration 345, loss = 0.47913117\n",
      "Iteration 346, loss = 0.47908160\n",
      "Iteration 347, loss = 0.47891197\n",
      "Iteration 348, loss = 0.47879408\n",
      "Iteration 349, loss = 0.47887657\n",
      "Iteration 350, loss = 0.47863451\n",
      "Iteration 351, loss = 0.47901637\n",
      "Iteration 352, loss = 0.47886936\n",
      "Iteration 353, loss = 0.47848725\n",
      "Iteration 354, loss = 0.47851303\n",
      "Iteration 355, loss = 0.47871974\n",
      "Iteration 356, loss = 0.47880141\n",
      "Iteration 357, loss = 0.47851474\n",
      "Iteration 358, loss = 0.47837424\n",
      "Iteration 359, loss = 0.47837591\n",
      "Iteration 360, loss = 0.47846364\n",
      "Iteration 361, loss = 0.47829823\n",
      "Iteration 362, loss = 0.47861315\n",
      "Iteration 363, loss = 0.47808102\n",
      "Iteration 364, loss = 0.47825356\n",
      "Iteration 365, loss = 0.47838692\n",
      "Iteration 366, loss = 0.47783744\n",
      "Iteration 367, loss = 0.47845645\n",
      "Iteration 368, loss = 0.47775850\n",
      "Iteration 369, loss = 0.47795326\n",
      "Iteration 370, loss = 0.47825529\n",
      "Iteration 371, loss = 0.47802122\n",
      "Iteration 372, loss = 0.47819862\n",
      "Iteration 373, loss = 0.47795928\n",
      "Iteration 374, loss = 0.47777908\n",
      "Iteration 375, loss = 0.47826620\n",
      "Iteration 376, loss = 0.47818218\n",
      "Iteration 377, loss = 0.47760934\n",
      "Iteration 378, loss = 0.47810587\n",
      "Iteration 379, loss = 0.47772099\n",
      "Iteration 380, loss = 0.47742687\n",
      "Iteration 381, loss = 0.47781119\n",
      "Iteration 382, loss = 0.47771256\n",
      "Iteration 383, loss = 0.47770387\n",
      "Iteration 384, loss = 0.47770010\n",
      "Iteration 385, loss = 0.47756057\n",
      "Iteration 386, loss = 0.47797478\n",
      "Iteration 387, loss = 0.47783060\n",
      "Iteration 388, loss = 0.47775162\n",
      "Iteration 389, loss = 0.47744500\n",
      "Iteration 390, loss = 0.47735902\n",
      "Iteration 391, loss = 0.47773357\n",
      "Iteration 392, loss = 0.47772816\n",
      "Iteration 393, loss = 0.47722838\n",
      "Iteration 394, loss = 0.47703166\n",
      "Iteration 395, loss = 0.47756630\n",
      "Iteration 396, loss = 0.47719705\n",
      "Iteration 397, loss = 0.47711328\n",
      "Iteration 398, loss = 0.47731198\n",
      "Iteration 399, loss = 0.47693622\n",
      "Iteration 400, loss = 0.47728585\n",
      "Iteration 401, loss = 0.47819979\n",
      "Iteration 402, loss = 0.47700114\n",
      "Iteration 403, loss = 0.47711953\n",
      "Iteration 404, loss = 0.47668227\n",
      "Iteration 405, loss = 0.47680009\n",
      "Iteration 406, loss = 0.47678502\n",
      "Iteration 407, loss = 0.47662812\n",
      "Iteration 408, loss = 0.47668096\n",
      "Iteration 409, loss = 0.47636766\n",
      "Iteration 410, loss = 0.47670716\n",
      "Iteration 411, loss = 0.47674654\n",
      "Iteration 412, loss = 0.47658926\n",
      "Iteration 413, loss = 0.47664534\n",
      "Iteration 414, loss = 0.47641898\n",
      "Iteration 415, loss = 0.47610878\n",
      "Iteration 416, loss = 0.47693532\n",
      "Iteration 417, loss = 0.47676060\n",
      "Iteration 418, loss = 0.47620592\n",
      "Iteration 419, loss = 0.47623116\n",
      "Iteration 420, loss = 0.47637121\n",
      "Iteration 421, loss = 0.47660343\n",
      "Iteration 422, loss = 0.47628689\n",
      "Iteration 423, loss = 0.47616267\n",
      "Iteration 424, loss = 0.47604121\n",
      "Iteration 425, loss = 0.47622341\n",
      "Iteration 426, loss = 0.47609841\n",
      "Iteration 427, loss = 0.47618514\n",
      "Iteration 428, loss = 0.47609954\n",
      "Iteration 429, loss = 0.47582097\n",
      "Iteration 430, loss = 0.47597591\n",
      "Iteration 431, loss = 0.47598549\n",
      "Iteration 432, loss = 0.47575940\n",
      "Iteration 433, loss = 0.47568098\n",
      "Iteration 434, loss = 0.47621525\n",
      "Iteration 435, loss = 0.47573014\n",
      "Iteration 436, loss = 0.47620012\n",
      "Iteration 437, loss = 0.47566884\n",
      "Iteration 438, loss = 0.47553089\n",
      "Iteration 439, loss = 0.47570512\n",
      "Iteration 440, loss = 0.47590824\n",
      "Iteration 441, loss = 0.47580537\n",
      "Iteration 442, loss = 0.47558435\n",
      "Iteration 443, loss = 0.47542007\n",
      "Iteration 444, loss = 0.47624833\n",
      "Iteration 445, loss = 0.47573683\n",
      "Iteration 446, loss = 0.47603555\n",
      "Iteration 447, loss = 0.47543296\n",
      "Iteration 448, loss = 0.47577811\n",
      "Iteration 449, loss = 0.47524264\n",
      "Iteration 450, loss = 0.47529666\n",
      "Iteration 451, loss = 0.47552498\n",
      "Iteration 452, loss = 0.47504449\n",
      "Iteration 453, loss = 0.47517383\n",
      "Iteration 454, loss = 0.47524133\n",
      "Iteration 455, loss = 0.47513858\n",
      "Iteration 456, loss = 0.47530958\n",
      "Iteration 457, loss = 0.47576205\n",
      "Iteration 458, loss = 0.47604430\n",
      "Iteration 459, loss = 0.47546679\n",
      "Iteration 460, loss = 0.47489853\n",
      "Iteration 461, loss = 0.47516549\n",
      "Iteration 462, loss = 0.47487990\n",
      "Iteration 463, loss = 0.47494039\n",
      "Iteration 464, loss = 0.47504277\n",
      "Iteration 465, loss = 0.47513337\n",
      "Iteration 466, loss = 0.47488828\n",
      "Iteration 467, loss = 0.47502409\n",
      "Iteration 468, loss = 0.47469022\n",
      "Iteration 469, loss = 0.47464330\n",
      "Iteration 470, loss = 0.47474223\n",
      "Iteration 471, loss = 0.47491636\n",
      "Iteration 472, loss = 0.47502745\n",
      "Iteration 473, loss = 0.47466188\n",
      "Iteration 474, loss = 0.47468405\n",
      "Iteration 475, loss = 0.47462275\n",
      "Iteration 476, loss = 0.47453817\n",
      "Iteration 477, loss = 0.47487130\n",
      "Iteration 478, loss = 0.47467588\n",
      "Iteration 479, loss = 0.47433609\n",
      "Iteration 480, loss = 0.47464019\n",
      "Iteration 481, loss = 0.47449112\n",
      "Iteration 482, loss = 0.47463737\n",
      "Iteration 483, loss = 0.47424121\n",
      "Iteration 484, loss = 0.47421156\n",
      "Iteration 485, loss = 0.47423621\n",
      "Iteration 486, loss = 0.47445182\n",
      "Iteration 487, loss = 0.47427158\n",
      "Iteration 488, loss = 0.47448536\n",
      "Iteration 489, loss = 0.47440349\n",
      "Iteration 490, loss = 0.47406749\n",
      "Iteration 491, loss = 0.47445230\n",
      "Iteration 492, loss = 0.47412746\n",
      "Iteration 493, loss = 0.47434934\n",
      "Iteration 494, loss = 0.47426205\n",
      "Iteration 495, loss = 0.47403116\n",
      "Iteration 496, loss = 0.47398555\n",
      "Iteration 497, loss = 0.47419668\n",
      "Iteration 498, loss = 0.47485726\n",
      "Iteration 499, loss = 0.47437019\n",
      "Iteration 500, loss = 0.47411693\n",
      "Iteration 501, loss = 0.47408060\n",
      "Iteration 502, loss = 0.47403554\n",
      "Iteration 503, loss = 0.47391392\n",
      "Iteration 504, loss = 0.47399577\n",
      "Iteration 505, loss = 0.47441457\n",
      "Iteration 506, loss = 0.47474522\n",
      "Iteration 507, loss = 0.47451687\n",
      "Iteration 508, loss = 0.47525125\n",
      "Iteration 509, loss = 0.47406509\n",
      "Iteration 510, loss = 0.47390434\n",
      "Iteration 511, loss = 0.47371603\n",
      "Iteration 512, loss = 0.47379733\n",
      "Iteration 513, loss = 0.47424399\n",
      "Iteration 514, loss = 0.47379992\n",
      "Iteration 515, loss = 0.47425003\n",
      "Iteration 516, loss = 0.47455276\n",
      "Iteration 517, loss = 0.47448700\n",
      "Iteration 518, loss = 0.47353183\n",
      "Iteration 519, loss = 0.47390035\n",
      "Iteration 520, loss = 0.47350334\n",
      "Iteration 521, loss = 0.47353583\n",
      "Iteration 522, loss = 0.47339291\n",
      "Iteration 523, loss = 0.47375771\n",
      "Iteration 524, loss = 0.47335014\n",
      "Iteration 525, loss = 0.47361605\n",
      "Iteration 526, loss = 0.47352641\n",
      "Iteration 527, loss = 0.47375093\n",
      "Iteration 528, loss = 0.47343307\n",
      "Iteration 529, loss = 0.47364750\n",
      "Iteration 530, loss = 0.47382189\n",
      "Iteration 531, loss = 0.47335921\n",
      "Iteration 532, loss = 0.47356247\n",
      "Iteration 533, loss = 0.47341806\n",
      "Iteration 534, loss = 0.47354719\n",
      "Iteration 535, loss = 0.47388232\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73522893\n",
      "Iteration 2, loss = 0.69865816\n",
      "Iteration 3, loss = 0.67299587\n",
      "Iteration 4, loss = 0.65207065\n",
      "Iteration 5, loss = 0.63473918\n",
      "Iteration 6, loss = 0.62000094\n",
      "Iteration 7, loss = 0.60674210\n",
      "Iteration 8, loss = 0.59587310\n",
      "Iteration 9, loss = 0.58628825\n",
      "Iteration 10, loss = 0.57841942\n",
      "Iteration 11, loss = 0.57253889\n",
      "Iteration 12, loss = 0.56752797\n",
      "Iteration 13, loss = 0.56367371\n",
      "Iteration 14, loss = 0.56027680\n",
      "Iteration 15, loss = 0.55767680\n",
      "Iteration 16, loss = 0.55582422\n",
      "Iteration 17, loss = 0.55372378\n",
      "Iteration 18, loss = 0.55254388\n",
      "Iteration 19, loss = 0.55124834\n",
      "Iteration 20, loss = 0.54997916\n",
      "Iteration 21, loss = 0.54897620\n",
      "Iteration 22, loss = 0.54798863\n",
      "Iteration 23, loss = 0.54703371\n",
      "Iteration 24, loss = 0.54590366\n",
      "Iteration 25, loss = 0.54502510\n",
      "Iteration 26, loss = 0.54424537\n",
      "Iteration 27, loss = 0.54344743\n",
      "Iteration 28, loss = 0.54263447\n",
      "Iteration 29, loss = 0.54183042\n",
      "Iteration 30, loss = 0.54107961\n",
      "Iteration 31, loss = 0.54025172\n",
      "Iteration 32, loss = 0.53972363\n",
      "Iteration 33, loss = 0.53918531\n",
      "Iteration 34, loss = 0.53802650\n",
      "Iteration 35, loss = 0.53719265\n",
      "Iteration 36, loss = 0.53668634\n",
      "Iteration 37, loss = 0.53556456\n",
      "Iteration 38, loss = 0.53482957\n",
      "Iteration 39, loss = 0.53424805\n",
      "Iteration 40, loss = 0.53329982\n",
      "Iteration 41, loss = 0.53227313\n",
      "Iteration 42, loss = 0.53145680\n",
      "Iteration 43, loss = 0.53076514\n",
      "Iteration 44, loss = 0.52951944\n",
      "Iteration 45, loss = 0.52891757\n",
      "Iteration 46, loss = 0.52791047\n",
      "Iteration 47, loss = 0.52683954\n",
      "Iteration 48, loss = 0.52604687\n",
      "Iteration 49, loss = 0.52504297\n",
      "Iteration 50, loss = 0.52460114\n",
      "Iteration 51, loss = 0.52351143\n",
      "Iteration 52, loss = 0.52261271\n",
      "Iteration 53, loss = 0.52197369\n",
      "Iteration 54, loss = 0.52134050\n",
      "Iteration 55, loss = 0.52021719\n",
      "Iteration 56, loss = 0.51932433\n",
      "Iteration 57, loss = 0.51876848\n",
      "Iteration 58, loss = 0.51778094\n",
      "Iteration 59, loss = 0.51754130\n",
      "Iteration 60, loss = 0.51665553\n",
      "Iteration 61, loss = 0.51578464\n",
      "Iteration 62, loss = 0.51518991\n",
      "Iteration 63, loss = 0.51448300\n",
      "Iteration 64, loss = 0.51381101\n",
      "Iteration 65, loss = 0.51335339\n",
      "Iteration 66, loss = 0.51283635\n",
      "Iteration 67, loss = 0.51217858\n",
      "Iteration 68, loss = 0.51144403\n",
      "Iteration 69, loss = 0.51089930\n",
      "Iteration 70, loss = 0.51008158\n",
      "Iteration 71, loss = 0.50945727\n",
      "Iteration 72, loss = 0.50887982\n",
      "Iteration 73, loss = 0.50836163\n",
      "Iteration 74, loss = 0.50758177\n",
      "Iteration 75, loss = 0.50713283\n",
      "Iteration 76, loss = 0.50656348\n",
      "Iteration 77, loss = 0.50631292\n",
      "Iteration 78, loss = 0.50569263\n",
      "Iteration 79, loss = 0.50504894\n",
      "Iteration 80, loss = 0.50455067\n",
      "Iteration 81, loss = 0.50433210\n",
      "Iteration 82, loss = 0.50373974\n",
      "Iteration 83, loss = 0.50356267\n",
      "Iteration 84, loss = 0.50301186\n",
      "Iteration 85, loss = 0.50243953\n",
      "Iteration 86, loss = 0.50227974\n",
      "Iteration 87, loss = 0.50151331\n",
      "Iteration 88, loss = 0.50154130\n",
      "Iteration 89, loss = 0.50105896\n",
      "Iteration 90, loss = 0.50030598\n",
      "Iteration 91, loss = 0.50063148\n",
      "Iteration 92, loss = 0.49956293\n",
      "Iteration 93, loss = 0.49914169\n",
      "Iteration 94, loss = 0.49917735\n",
      "Iteration 95, loss = 0.49877376\n",
      "Iteration 96, loss = 0.49834164\n",
      "Iteration 97, loss = 0.49833599\n",
      "Iteration 98, loss = 0.49732941\n",
      "Iteration 99, loss = 0.49709473\n",
      "Iteration 100, loss = 0.49656373\n",
      "Iteration 101, loss = 0.49640278\n",
      "Iteration 102, loss = 0.49625333\n",
      "Iteration 103, loss = 0.49599835\n",
      "Iteration 104, loss = 0.49554743\n",
      "Iteration 105, loss = 0.49563159\n",
      "Iteration 106, loss = 0.49535043\n",
      "Iteration 107, loss = 0.49490708\n",
      "Iteration 108, loss = 0.49492634\n",
      "Iteration 109, loss = 0.49500789\n",
      "Iteration 110, loss = 0.49425835\n",
      "Iteration 111, loss = 0.49402487\n",
      "Iteration 112, loss = 0.49410451\n",
      "Iteration 113, loss = 0.49354998\n",
      "Iteration 114, loss = 0.49384675\n",
      "Iteration 115, loss = 0.49316408\n",
      "Iteration 116, loss = 0.49291262\n",
      "Iteration 117, loss = 0.49253278\n",
      "Iteration 118, loss = 0.49235813\n",
      "Iteration 119, loss = 0.49222285\n",
      "Iteration 120, loss = 0.49198823\n",
      "Iteration 121, loss = 0.49167443\n",
      "Iteration 122, loss = 0.49194637\n",
      "Iteration 123, loss = 0.49131739\n",
      "Iteration 124, loss = 0.49158666\n",
      "Iteration 125, loss = 0.49092372\n",
      "Iteration 126, loss = 0.49097399\n",
      "Iteration 127, loss = 0.49077464\n",
      "Iteration 128, loss = 0.49100266\n",
      "Iteration 129, loss = 0.49010666\n",
      "Iteration 130, loss = 0.49017868\n",
      "Iteration 131, loss = 0.48996918\n",
      "Iteration 132, loss = 0.49034024\n",
      "Iteration 133, loss = 0.48978926\n",
      "Iteration 134, loss = 0.48949732\n",
      "Iteration 135, loss = 0.48946803\n",
      "Iteration 136, loss = 0.48891375\n",
      "Iteration 137, loss = 0.48908770\n",
      "Iteration 138, loss = 0.48893141\n",
      "Iteration 139, loss = 0.48866185\n",
      "Iteration 140, loss = 0.48861858\n",
      "Iteration 141, loss = 0.48834644\n",
      "Iteration 142, loss = 0.48820397\n",
      "Iteration 143, loss = 0.48798652\n",
      "Iteration 144, loss = 0.48828701\n",
      "Iteration 145, loss = 0.48792063\n",
      "Iteration 146, loss = 0.48748302\n",
      "Iteration 147, loss = 0.48734862\n",
      "Iteration 148, loss = 0.48733290\n",
      "Iteration 149, loss = 0.48717192\n",
      "Iteration 150, loss = 0.48698863\n",
      "Iteration 151, loss = 0.48677905\n",
      "Iteration 152, loss = 0.48735732\n",
      "Iteration 153, loss = 0.48658566\n",
      "Iteration 154, loss = 0.48662915\n",
      "Iteration 155, loss = 0.48647444\n",
      "Iteration 156, loss = 0.48617700\n",
      "Iteration 157, loss = 0.48591100\n",
      "Iteration 158, loss = 0.48576936\n",
      "Iteration 159, loss = 0.48582471\n",
      "Iteration 160, loss = 0.48592096\n",
      "Iteration 161, loss = 0.48579949\n",
      "Iteration 162, loss = 0.48550363\n",
      "Iteration 163, loss = 0.48531983\n",
      "Iteration 164, loss = 0.48519029\n",
      "Iteration 165, loss = 0.48510138\n",
      "Iteration 166, loss = 0.48497455\n",
      "Iteration 167, loss = 0.48469840\n",
      "Iteration 168, loss = 0.48479876\n",
      "Iteration 169, loss = 0.48470532\n",
      "Iteration 170, loss = 0.48474451\n",
      "Iteration 171, loss = 0.48438428\n",
      "Iteration 172, loss = 0.48443358\n",
      "Iteration 173, loss = 0.48412699\n",
      "Iteration 174, loss = 0.48422004\n",
      "Iteration 175, loss = 0.48385267\n",
      "Iteration 176, loss = 0.48396013\n",
      "Iteration 177, loss = 0.48371755\n",
      "Iteration 178, loss = 0.48368723\n",
      "Iteration 179, loss = 0.48395756\n",
      "Iteration 180, loss = 0.48352874\n",
      "Iteration 181, loss = 0.48325232\n",
      "Iteration 182, loss = 0.48363348\n",
      "Iteration 183, loss = 0.48316051\n",
      "Iteration 184, loss = 0.48344006\n",
      "Iteration 185, loss = 0.48300050\n",
      "Iteration 186, loss = 0.48295243\n",
      "Iteration 187, loss = 0.48272256\n",
      "Iteration 188, loss = 0.48253466\n",
      "Iteration 189, loss = 0.48269488\n",
      "Iteration 190, loss = 0.48250571\n",
      "Iteration 191, loss = 0.48214229\n",
      "Iteration 192, loss = 0.48199210\n",
      "Iteration 193, loss = 0.48201041\n",
      "Iteration 194, loss = 0.48218910\n",
      "Iteration 195, loss = 0.48205676\n",
      "Iteration 196, loss = 0.48198238\n",
      "Iteration 197, loss = 0.48178487\n",
      "Iteration 198, loss = 0.48165639\n",
      "Iteration 199, loss = 0.48174585\n",
      "Iteration 200, loss = 0.48200206\n",
      "Iteration 201, loss = 0.48177774\n",
      "Iteration 202, loss = 0.48130315\n",
      "Iteration 203, loss = 0.48160663\n",
      "Iteration 204, loss = 0.48107441\n",
      "Iteration 205, loss = 0.48109003\n",
      "Iteration 206, loss = 0.48087574\n",
      "Iteration 207, loss = 0.48077118\n",
      "Iteration 208, loss = 0.48089458\n",
      "Iteration 209, loss = 0.48093479\n",
      "Iteration 210, loss = 0.48066471\n",
      "Iteration 211, loss = 0.48052729\n",
      "Iteration 212, loss = 0.48078612\n",
      "Iteration 213, loss = 0.48029767\n",
      "Iteration 214, loss = 0.48025359\n",
      "Iteration 215, loss = 0.48023687\n",
      "Iteration 216, loss = 0.48063165\n",
      "Iteration 217, loss = 0.48002412\n",
      "Iteration 218, loss = 0.48044698\n",
      "Iteration 219, loss = 0.48000945\n",
      "Iteration 220, loss = 0.47988545\n",
      "Iteration 221, loss = 0.48000639\n",
      "Iteration 222, loss = 0.47988641\n",
      "Iteration 223, loss = 0.47977099\n",
      "Iteration 224, loss = 0.47969949\n",
      "Iteration 225, loss = 0.47930453\n",
      "Iteration 226, loss = 0.47935208\n",
      "Iteration 227, loss = 0.47930158\n",
      "Iteration 228, loss = 0.47949384\n",
      "Iteration 229, loss = 0.47976179\n",
      "Iteration 230, loss = 0.47924271\n",
      "Iteration 231, loss = 0.47950863\n",
      "Iteration 232, loss = 0.47909344\n",
      "Iteration 233, loss = 0.47885185\n",
      "Iteration 234, loss = 0.47879965\n",
      "Iteration 235, loss = 0.47884459\n",
      "Iteration 236, loss = 0.47874032\n",
      "Iteration 237, loss = 0.47859735\n",
      "Iteration 238, loss = 0.47858526\n",
      "Iteration 239, loss = 0.47850603\n",
      "Iteration 240, loss = 0.47829230\n",
      "Iteration 241, loss = 0.47841266\n",
      "Iteration 242, loss = 0.47825578\n",
      "Iteration 243, loss = 0.47828860\n",
      "Iteration 244, loss = 0.47832339\n",
      "Iteration 245, loss = 0.47802348\n",
      "Iteration 246, loss = 0.47831633\n",
      "Iteration 247, loss = 0.47820108\n",
      "Iteration 248, loss = 0.47781899\n",
      "Iteration 249, loss = 0.47775944\n",
      "Iteration 250, loss = 0.47807462\n",
      "Iteration 251, loss = 0.47783791\n",
      "Iteration 252, loss = 0.47752742\n",
      "Iteration 253, loss = 0.47771592\n",
      "Iteration 254, loss = 0.47735586\n",
      "Iteration 255, loss = 0.47749239\n",
      "Iteration 256, loss = 0.47747740\n",
      "Iteration 257, loss = 0.47754962\n",
      "Iteration 258, loss = 0.47759605\n",
      "Iteration 259, loss = 0.47711541\n",
      "Iteration 260, loss = 0.47738384\n",
      "Iteration 261, loss = 0.47728820\n",
      "Iteration 262, loss = 0.47764394\n",
      "Iteration 263, loss = 0.47702778\n",
      "Iteration 264, loss = 0.47742264\n",
      "Iteration 265, loss = 0.47678530\n",
      "Iteration 266, loss = 0.47702854\n",
      "Iteration 267, loss = 0.47685308\n",
      "Iteration 268, loss = 0.47676806\n",
      "Iteration 269, loss = 0.47731078\n",
      "Iteration 270, loss = 0.47658442\n",
      "Iteration 271, loss = 0.47656685\n",
      "Iteration 272, loss = 0.47685349\n",
      "Iteration 273, loss = 0.47643825\n",
      "Iteration 274, loss = 0.47659357\n",
      "Iteration 275, loss = 0.47640498\n",
      "Iteration 276, loss = 0.47637514\n",
      "Iteration 277, loss = 0.47651812\n",
      "Iteration 278, loss = 0.47658524\n",
      "Iteration 279, loss = 0.47617647\n",
      "Iteration 280, loss = 0.47618295\n",
      "Iteration 281, loss = 0.47636862\n",
      "Iteration 282, loss = 0.47658542\n",
      "Iteration 283, loss = 0.47652871\n",
      "Iteration 284, loss = 0.47677692\n",
      "Iteration 285, loss = 0.47616509\n",
      "Iteration 286, loss = 0.47609416\n",
      "Iteration 287, loss = 0.47600112\n",
      "Iteration 288, loss = 0.47584118\n",
      "Iteration 289, loss = 0.47618867\n",
      "Iteration 290, loss = 0.47579281\n",
      "Iteration 291, loss = 0.47563135\n",
      "Iteration 292, loss = 0.47569279\n",
      "Iteration 293, loss = 0.47599596\n",
      "Iteration 294, loss = 0.47594510\n",
      "Iteration 295, loss = 0.47548430\n",
      "Iteration 296, loss = 0.47605048\n",
      "Iteration 297, loss = 0.47597442\n",
      "Iteration 298, loss = 0.47636250\n",
      "Iteration 299, loss = 0.47517315\n",
      "Iteration 300, loss = 0.47521382\n",
      "Iteration 301, loss = 0.47525989\n",
      "Iteration 302, loss = 0.47558705\n",
      "Iteration 303, loss = 0.47523968\n",
      "Iteration 304, loss = 0.47515745\n",
      "Iteration 305, loss = 0.47550116\n",
      "Iteration 306, loss = 0.47561617\n",
      "Iteration 307, loss = 0.47525607\n",
      "Iteration 308, loss = 0.47506392\n",
      "Iteration 309, loss = 0.47508937\n",
      "Iteration 310, loss = 0.47523533\n",
      "Iteration 311, loss = 0.47549213\n",
      "Iteration 312, loss = 0.47532297\n",
      "Iteration 313, loss = 0.47481243\n",
      "Iteration 314, loss = 0.47500593\n",
      "Iteration 315, loss = 0.47481723\n",
      "Iteration 316, loss = 0.47464682\n",
      "Iteration 317, loss = 0.47491309\n",
      "Iteration 318, loss = 0.47509706\n",
      "Iteration 319, loss = 0.47479796\n",
      "Iteration 320, loss = 0.47485137\n",
      "Iteration 321, loss = 0.47476203\n",
      "Iteration 322, loss = 0.47447637\n",
      "Iteration 323, loss = 0.47451533\n",
      "Iteration 324, loss = 0.47460207\n",
      "Iteration 325, loss = 0.47462056\n",
      "Iteration 326, loss = 0.47457756\n",
      "Iteration 327, loss = 0.47444599\n",
      "Iteration 328, loss = 0.47488577\n",
      "Iteration 329, loss = 0.47481324\n",
      "Iteration 330, loss = 0.47429778\n",
      "Iteration 331, loss = 0.47443781\n",
      "Iteration 332, loss = 0.47420495\n",
      "Iteration 333, loss = 0.47426757\n",
      "Iteration 334, loss = 0.47434549\n",
      "Iteration 335, loss = 0.47431353\n",
      "Iteration 336, loss = 0.47410990\n",
      "Iteration 337, loss = 0.47437075\n",
      "Iteration 338, loss = 0.47405783\n",
      "Iteration 339, loss = 0.47401673\n",
      "Iteration 340, loss = 0.47428725\n",
      "Iteration 341, loss = 0.47396029\n",
      "Iteration 342, loss = 0.47410981\n",
      "Iteration 343, loss = 0.47387345\n",
      "Iteration 344, loss = 0.47399075\n",
      "Iteration 345, loss = 0.47390637\n",
      "Iteration 346, loss = 0.47382239\n",
      "Iteration 347, loss = 0.47492772\n",
      "Iteration 348, loss = 0.47402437\n",
      "Iteration 349, loss = 0.47372227\n",
      "Iteration 350, loss = 0.47403973\n",
      "Iteration 351, loss = 0.47398254\n",
      "Iteration 352, loss = 0.47392217\n",
      "Iteration 353, loss = 0.47373805\n",
      "Iteration 354, loss = 0.47359336\n",
      "Iteration 355, loss = 0.47443320\n",
      "Iteration 356, loss = 0.47399600\n",
      "Iteration 357, loss = 0.47382135\n",
      "Iteration 358, loss = 0.47384046\n",
      "Iteration 359, loss = 0.47366156\n",
      "Iteration 360, loss = 0.47363597\n",
      "Iteration 361, loss = 0.47341199\n",
      "Iteration 362, loss = 0.47352426\n",
      "Iteration 363, loss = 0.47348034\n",
      "Iteration 364, loss = 0.47362443\n",
      "Iteration 365, loss = 0.47354422\n",
      "Iteration 366, loss = 0.47387160\n",
      "Iteration 367, loss = 0.47373462\n",
      "Iteration 368, loss = 0.47326412\n",
      "Iteration 369, loss = 0.47334904\n",
      "Iteration 370, loss = 0.47344580\n",
      "Iteration 371, loss = 0.47335239\n",
      "Iteration 372, loss = 0.47345951\n",
      "Iteration 373, loss = 0.47328480\n",
      "Iteration 374, loss = 0.47329663\n",
      "Iteration 375, loss = 0.47324335\n",
      "Iteration 376, loss = 0.47352475\n",
      "Iteration 377, loss = 0.47313017\n",
      "Iteration 378, loss = 0.47333631\n",
      "Iteration 379, loss = 0.47322756\n",
      "Iteration 380, loss = 0.47294735\n",
      "Iteration 381, loss = 0.47319118\n",
      "Iteration 382, loss = 0.47342279\n",
      "Iteration 383, loss = 0.47323489\n",
      "Iteration 384, loss = 0.47318127\n",
      "Iteration 385, loss = 0.47303912\n",
      "Iteration 386, loss = 0.47328512\n",
      "Iteration 387, loss = 0.47321919\n",
      "Iteration 388, loss = 0.47306232\n",
      "Iteration 389, loss = 0.47312172\n",
      "Iteration 390, loss = 0.47310522\n",
      "Iteration 391, loss = 0.47310266\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75996703\n",
      "Iteration 2, loss = 0.72645411\n",
      "Iteration 3, loss = 0.70111323\n",
      "Iteration 4, loss = 0.68249907\n",
      "Iteration 5, loss = 0.66798073\n",
      "Iteration 6, loss = 0.65725314\n",
      "Iteration 7, loss = 0.64750428\n",
      "Iteration 8, loss = 0.63899292\n",
      "Iteration 9, loss = 0.63065227\n",
      "Iteration 10, loss = 0.62310437\n",
      "Iteration 11, loss = 0.61559091\n",
      "Iteration 12, loss = 0.60844164\n",
      "Iteration 13, loss = 0.60152542\n",
      "Iteration 14, loss = 0.59505481\n",
      "Iteration 15, loss = 0.58909912\n",
      "Iteration 16, loss = 0.58348751\n",
      "Iteration 17, loss = 0.57840737\n",
      "Iteration 18, loss = 0.57416108\n",
      "Iteration 19, loss = 0.57008930\n",
      "Iteration 20, loss = 0.56702426\n",
      "Iteration 21, loss = 0.56408260\n",
      "Iteration 22, loss = 0.56161702\n",
      "Iteration 23, loss = 0.55980915\n",
      "Iteration 24, loss = 0.55748118\n",
      "Iteration 25, loss = 0.55599407\n",
      "Iteration 26, loss = 0.55457262\n",
      "Iteration 27, loss = 0.55338806\n",
      "Iteration 28, loss = 0.55223315\n",
      "Iteration 29, loss = 0.55104013\n",
      "Iteration 30, loss = 0.55015647\n",
      "Iteration 31, loss = 0.54901862\n",
      "Iteration 32, loss = 0.54817559\n",
      "Iteration 33, loss = 0.54739164\n",
      "Iteration 34, loss = 0.54637336\n",
      "Iteration 35, loss = 0.54569972\n",
      "Iteration 36, loss = 0.54490695\n",
      "Iteration 37, loss = 0.54410854\n",
      "Iteration 38, loss = 0.54334111\n",
      "Iteration 39, loss = 0.54257743\n",
      "Iteration 40, loss = 0.54194040\n",
      "Iteration 41, loss = 0.54139342\n",
      "Iteration 42, loss = 0.54052935\n",
      "Iteration 43, loss = 0.53986506\n",
      "Iteration 44, loss = 0.53923594\n",
      "Iteration 45, loss = 0.53855179\n",
      "Iteration 46, loss = 0.53792617\n",
      "Iteration 47, loss = 0.53725855\n",
      "Iteration 48, loss = 0.53656599\n",
      "Iteration 49, loss = 0.53590361\n",
      "Iteration 50, loss = 0.53493083\n",
      "Iteration 51, loss = 0.53424711\n",
      "Iteration 52, loss = 0.53336992\n",
      "Iteration 53, loss = 0.53252297\n",
      "Iteration 54, loss = 0.53186553\n",
      "Iteration 55, loss = 0.53095158\n",
      "Iteration 56, loss = 0.53005225\n",
      "Iteration 57, loss = 0.52914000\n",
      "Iteration 58, loss = 0.52817757\n",
      "Iteration 59, loss = 0.52720009\n",
      "Iteration 60, loss = 0.52626986\n",
      "Iteration 61, loss = 0.52524518\n",
      "Iteration 62, loss = 0.52444887\n",
      "Iteration 63, loss = 0.52364878\n",
      "Iteration 64, loss = 0.52256789\n",
      "Iteration 65, loss = 0.52175603\n",
      "Iteration 66, loss = 0.52092932\n",
      "Iteration 67, loss = 0.52026456\n",
      "Iteration 68, loss = 0.51964216\n",
      "Iteration 69, loss = 0.51884949\n",
      "Iteration 70, loss = 0.51801319\n",
      "Iteration 71, loss = 0.51753604\n",
      "Iteration 72, loss = 0.51689843\n",
      "Iteration 73, loss = 0.51643064\n",
      "Iteration 74, loss = 0.51578097\n",
      "Iteration 75, loss = 0.51545295\n",
      "Iteration 76, loss = 0.51456647\n",
      "Iteration 77, loss = 0.51422741\n",
      "Iteration 78, loss = 0.51373019\n",
      "Iteration 79, loss = 0.51313202\n",
      "Iteration 80, loss = 0.51269232\n",
      "Iteration 81, loss = 0.51231358\n",
      "Iteration 82, loss = 0.51183565\n",
      "Iteration 83, loss = 0.51158958\n",
      "Iteration 84, loss = 0.51097368\n",
      "Iteration 85, loss = 0.51075480\n",
      "Iteration 86, loss = 0.51042810\n",
      "Iteration 87, loss = 0.51010198\n",
      "Iteration 88, loss = 0.50949313\n",
      "Iteration 89, loss = 0.50925016\n",
      "Iteration 90, loss = 0.50861751\n",
      "Iteration 91, loss = 0.50810288\n",
      "Iteration 92, loss = 0.50801292\n",
      "Iteration 93, loss = 0.50733901\n",
      "Iteration 94, loss = 0.50681868\n",
      "Iteration 95, loss = 0.50648205\n",
      "Iteration 96, loss = 0.50588476\n",
      "Iteration 97, loss = 0.50538998\n",
      "Iteration 98, loss = 0.50517665\n",
      "Iteration 99, loss = 0.50456665\n",
      "Iteration 100, loss = 0.50427614\n",
      "Iteration 101, loss = 0.50383958\n",
      "Iteration 102, loss = 0.50364604\n",
      "Iteration 103, loss = 0.50305069\n",
      "Iteration 104, loss = 0.50282184\n",
      "Iteration 105, loss = 0.50249151\n",
      "Iteration 106, loss = 0.50218356\n",
      "Iteration 107, loss = 0.50129478\n",
      "Iteration 108, loss = 0.50101642\n",
      "Iteration 109, loss = 0.50062970\n",
      "Iteration 110, loss = 0.50038057\n",
      "Iteration 111, loss = 0.49979400\n",
      "Iteration 112, loss = 0.49958150\n",
      "Iteration 113, loss = 0.49917943\n",
      "Iteration 114, loss = 0.49884190\n",
      "Iteration 115, loss = 0.49842400\n",
      "Iteration 116, loss = 0.49798030\n",
      "Iteration 117, loss = 0.49782122\n",
      "Iteration 118, loss = 0.49711647\n",
      "Iteration 119, loss = 0.49699918\n",
      "Iteration 120, loss = 0.49654229\n",
      "Iteration 121, loss = 0.49603957\n",
      "Iteration 122, loss = 0.49571957\n",
      "Iteration 123, loss = 0.49506972\n",
      "Iteration 124, loss = 0.49490907\n",
      "Iteration 125, loss = 0.49421319\n",
      "Iteration 126, loss = 0.49398299\n",
      "Iteration 127, loss = 0.49354209\n",
      "Iteration 128, loss = 0.49335716\n",
      "Iteration 129, loss = 0.49270964\n",
      "Iteration 130, loss = 0.49260938\n",
      "Iteration 131, loss = 0.49176433\n",
      "Iteration 132, loss = 0.49167016\n",
      "Iteration 133, loss = 0.49118592\n",
      "Iteration 134, loss = 0.49066418\n",
      "Iteration 135, loss = 0.49029374\n",
      "Iteration 136, loss = 0.48984878\n",
      "Iteration 137, loss = 0.48943025\n",
      "Iteration 138, loss = 0.48910435\n",
      "Iteration 139, loss = 0.48871590\n",
      "Iteration 140, loss = 0.48860553\n",
      "Iteration 141, loss = 0.48776396\n",
      "Iteration 142, loss = 0.48749615\n",
      "Iteration 143, loss = 0.48702549\n",
      "Iteration 144, loss = 0.48655140\n",
      "Iteration 145, loss = 0.48632381\n",
      "Iteration 146, loss = 0.48612083\n",
      "Iteration 147, loss = 0.48553080\n",
      "Iteration 148, loss = 0.48559062\n",
      "Iteration 149, loss = 0.48496507\n",
      "Iteration 150, loss = 0.48478076\n",
      "Iteration 151, loss = 0.48458907\n",
      "Iteration 152, loss = 0.48426555\n",
      "Iteration 153, loss = 0.48420649\n",
      "Iteration 154, loss = 0.48388176\n",
      "Iteration 155, loss = 0.48372359\n",
      "Iteration 156, loss = 0.48325512\n",
      "Iteration 157, loss = 0.48324828\n",
      "Iteration 158, loss = 0.48289219\n",
      "Iteration 159, loss = 0.48284519\n",
      "Iteration 160, loss = 0.48258136\n",
      "Iteration 161, loss = 0.48244016\n",
      "Iteration 162, loss = 0.48215180\n",
      "Iteration 163, loss = 0.48191962\n",
      "Iteration 164, loss = 0.48188939\n",
      "Iteration 165, loss = 0.48158438\n",
      "Iteration 166, loss = 0.48154683\n",
      "Iteration 167, loss = 0.48135060\n",
      "Iteration 168, loss = 0.48115868\n",
      "Iteration 169, loss = 0.48104186\n",
      "Iteration 170, loss = 0.48087774\n",
      "Iteration 171, loss = 0.48064512\n",
      "Iteration 172, loss = 0.48048198\n",
      "Iteration 173, loss = 0.48036199\n",
      "Iteration 174, loss = 0.48036946\n",
      "Iteration 175, loss = 0.48048865\n",
      "Iteration 176, loss = 0.48059453\n",
      "Iteration 177, loss = 0.47982149\n",
      "Iteration 178, loss = 0.47987627\n",
      "Iteration 179, loss = 0.47938984\n",
      "Iteration 180, loss = 0.47980121\n",
      "Iteration 181, loss = 0.47977208\n",
      "Iteration 182, loss = 0.47949597\n",
      "Iteration 183, loss = 0.47910315\n",
      "Iteration 184, loss = 0.47908917\n",
      "Iteration 185, loss = 0.47894024\n",
      "Iteration 186, loss = 0.47881285\n",
      "Iteration 187, loss = 0.47870736\n",
      "Iteration 188, loss = 0.47867211\n",
      "Iteration 189, loss = 0.47863507\n",
      "Iteration 190, loss = 0.47845562\n",
      "Iteration 191, loss = 0.47837366\n",
      "Iteration 192, loss = 0.47821278\n",
      "Iteration 193, loss = 0.47829044\n",
      "Iteration 194, loss = 0.47796601\n",
      "Iteration 195, loss = 0.47795560\n",
      "Iteration 196, loss = 0.47800954\n",
      "Iteration 197, loss = 0.47764951\n",
      "Iteration 198, loss = 0.47772718\n",
      "Iteration 199, loss = 0.47737453\n",
      "Iteration 200, loss = 0.47743404\n",
      "Iteration 201, loss = 0.47760264\n",
      "Iteration 202, loss = 0.47748790\n",
      "Iteration 203, loss = 0.47719848\n",
      "Iteration 204, loss = 0.47700361\n",
      "Iteration 205, loss = 0.47691992\n",
      "Iteration 206, loss = 0.47723528\n",
      "Iteration 207, loss = 0.47701292\n",
      "Iteration 208, loss = 0.47672459\n",
      "Iteration 209, loss = 0.47659403\n",
      "Iteration 210, loss = 0.47677129\n",
      "Iteration 211, loss = 0.47641847\n",
      "Iteration 212, loss = 0.47638383\n",
      "Iteration 213, loss = 0.47665045\n",
      "Iteration 214, loss = 0.47615250\n",
      "Iteration 215, loss = 0.47620213\n",
      "Iteration 216, loss = 0.47601606\n",
      "Iteration 217, loss = 0.47608430\n",
      "Iteration 218, loss = 0.47587848\n",
      "Iteration 219, loss = 0.47580821\n",
      "Iteration 220, loss = 0.47554032\n",
      "Iteration 221, loss = 0.47558732\n",
      "Iteration 222, loss = 0.47551367\n",
      "Iteration 223, loss = 0.47557848\n",
      "Iteration 224, loss = 0.47550674\n",
      "Iteration 225, loss = 0.47509698\n",
      "Iteration 226, loss = 0.47570338\n",
      "Iteration 227, loss = 0.47528469\n",
      "Iteration 228, loss = 0.47565622\n",
      "Iteration 229, loss = 0.47531730\n",
      "Iteration 230, loss = 0.47522545\n",
      "Iteration 231, loss = 0.47553417\n",
      "Iteration 232, loss = 0.47463651\n",
      "Iteration 233, loss = 0.47493803\n",
      "Iteration 234, loss = 0.47477024\n",
      "Iteration 235, loss = 0.47505812\n",
      "Iteration 236, loss = 0.47466531\n",
      "Iteration 237, loss = 0.47445987\n",
      "Iteration 238, loss = 0.47444621\n",
      "Iteration 239, loss = 0.47437226\n",
      "Iteration 240, loss = 0.47473579\n",
      "Iteration 241, loss = 0.47426967\n",
      "Iteration 242, loss = 0.47424029\n",
      "Iteration 243, loss = 0.47428397\n",
      "Iteration 244, loss = 0.47394457\n",
      "Iteration 245, loss = 0.47382623\n",
      "Iteration 246, loss = 0.47409561\n",
      "Iteration 247, loss = 0.47382817\n",
      "Iteration 248, loss = 0.47407181\n",
      "Iteration 249, loss = 0.47360376\n",
      "Iteration 250, loss = 0.47388578\n",
      "Iteration 251, loss = 0.47361332\n",
      "Iteration 252, loss = 0.47350720\n",
      "Iteration 253, loss = 0.47332642\n",
      "Iteration 254, loss = 0.47347652\n",
      "Iteration 255, loss = 0.47308712\n",
      "Iteration 256, loss = 0.47333038\n",
      "Iteration 257, loss = 0.47334924\n",
      "Iteration 258, loss = 0.47300541\n",
      "Iteration 259, loss = 0.47287659\n",
      "Iteration 260, loss = 0.47278138\n",
      "Iteration 261, loss = 0.47267426\n",
      "Iteration 262, loss = 0.47289596\n",
      "Iteration 263, loss = 0.47270773\n",
      "Iteration 264, loss = 0.47260489\n",
      "Iteration 265, loss = 0.47242159\n",
      "Iteration 266, loss = 0.47256984\n",
      "Iteration 267, loss = 0.47240637\n",
      "Iteration 268, loss = 0.47230055\n",
      "Iteration 269, loss = 0.47243975\n",
      "Iteration 270, loss = 0.47271165\n",
      "Iteration 271, loss = 0.47213354\n",
      "Iteration 272, loss = 0.47214605\n",
      "Iteration 273, loss = 0.47202208\n",
      "Iteration 274, loss = 0.47184603\n",
      "Iteration 275, loss = 0.47193247\n",
      "Iteration 276, loss = 0.47164395\n",
      "Iteration 277, loss = 0.47180783\n",
      "Iteration 278, loss = 0.47214549\n",
      "Iteration 279, loss = 0.47133874\n",
      "Iteration 280, loss = 0.47152239\n",
      "Iteration 281, loss = 0.47160426\n",
      "Iteration 282, loss = 0.47151098\n",
      "Iteration 283, loss = 0.47114513\n",
      "Iteration 284, loss = 0.47136237\n",
      "Iteration 285, loss = 0.47131846\n",
      "Iteration 286, loss = 0.47101976\n",
      "Iteration 287, loss = 0.47139202\n",
      "Iteration 288, loss = 0.47092722\n",
      "Iteration 289, loss = 0.47122938\n",
      "Iteration 290, loss = 0.47119412\n",
      "Iteration 291, loss = 0.47135471\n",
      "Iteration 292, loss = 0.47076468\n",
      "Iteration 293, loss = 0.47096317\n",
      "Iteration 294, loss = 0.47084067\n",
      "Iteration 295, loss = 0.47062192\n",
      "Iteration 296, loss = 0.47070446\n",
      "Iteration 297, loss = 0.47047323\n",
      "Iteration 298, loss = 0.47078387\n",
      "Iteration 299, loss = 0.47071700\n",
      "Iteration 300, loss = 0.47056417\n",
      "Iteration 301, loss = 0.47061177\n",
      "Iteration 302, loss = 0.47095714\n",
      "Iteration 303, loss = 0.47027859\n",
      "Iteration 304, loss = 0.47039167\n",
      "Iteration 305, loss = 0.47027212\n",
      "Iteration 306, loss = 0.47013624\n",
      "Iteration 307, loss = 0.47021137\n",
      "Iteration 308, loss = 0.47039674\n",
      "Iteration 309, loss = 0.47007532\n",
      "Iteration 310, loss = 0.47028902\n",
      "Iteration 311, loss = 0.47002249\n",
      "Iteration 312, loss = 0.47022953\n",
      "Iteration 313, loss = 0.47003844\n",
      "Iteration 314, loss = 0.46991442\n",
      "Iteration 315, loss = 0.46991076\n",
      "Iteration 316, loss = 0.46977977\n",
      "Iteration 317, loss = 0.46991356\n",
      "Iteration 318, loss = 0.46971716\n",
      "Iteration 319, loss = 0.46958895\n",
      "Iteration 320, loss = 0.46970760\n",
      "Iteration 321, loss = 0.47020221\n",
      "Iteration 322, loss = 0.46959842\n",
      "Iteration 323, loss = 0.46969981\n",
      "Iteration 324, loss = 0.46953605\n",
      "Iteration 325, loss = 0.46924375\n",
      "Iteration 326, loss = 0.46927223\n",
      "Iteration 327, loss = 0.46922674\n",
      "Iteration 328, loss = 0.46924549\n",
      "Iteration 329, loss = 0.46914864\n",
      "Iteration 330, loss = 0.46921404\n",
      "Iteration 331, loss = 0.46917867\n",
      "Iteration 332, loss = 0.46941690\n",
      "Iteration 333, loss = 0.46941701\n",
      "Iteration 334, loss = 0.46896262\n",
      "Iteration 335, loss = 0.46931095\n",
      "Iteration 336, loss = 0.46913883\n",
      "Iteration 337, loss = 0.46873342\n",
      "Iteration 338, loss = 0.46934402\n",
      "Iteration 339, loss = 0.46892256\n",
      "Iteration 340, loss = 0.46873254\n",
      "Iteration 341, loss = 0.46879951\n",
      "Iteration 342, loss = 0.46888733\n",
      "Iteration 343, loss = 0.46855405\n",
      "Iteration 344, loss = 0.46866528\n",
      "Iteration 345, loss = 0.46880769\n",
      "Iteration 346, loss = 0.46898058\n",
      "Iteration 347, loss = 0.46887458\n",
      "Iteration 348, loss = 0.46858330\n",
      "Iteration 349, loss = 0.46861479\n",
      "Iteration 350, loss = 0.46871660\n",
      "Iteration 351, loss = 0.46874566\n",
      "Iteration 352, loss = 0.46841706\n",
      "Iteration 353, loss = 0.46826906\n",
      "Iteration 354, loss = 0.46825248\n",
      "Iteration 355, loss = 0.46829856\n",
      "Iteration 356, loss = 0.46828859\n",
      "Iteration 357, loss = 0.46845541\n",
      "Iteration 358, loss = 0.46884872\n",
      "Iteration 359, loss = 0.46835015\n",
      "Iteration 360, loss = 0.46846928\n",
      "Iteration 361, loss = 0.46801627\n",
      "Iteration 362, loss = 0.46829454\n",
      "Iteration 363, loss = 0.46817540\n",
      "Iteration 364, loss = 0.46814297\n",
      "Iteration 365, loss = 0.46812179\n",
      "Iteration 366, loss = 0.46812825\n",
      "Iteration 367, loss = 0.46777536\n",
      "Iteration 368, loss = 0.46792750\n",
      "Iteration 369, loss = 0.46791600\n",
      "Iteration 370, loss = 0.46806029\n",
      "Iteration 371, loss = 0.46780272\n",
      "Iteration 372, loss = 0.46810177\n",
      "Iteration 373, loss = 0.46789101\n",
      "Iteration 374, loss = 0.46786292\n",
      "Iteration 375, loss = 0.46785748\n",
      "Iteration 376, loss = 0.46775933\n",
      "Iteration 377, loss = 0.46757715\n",
      "Iteration 378, loss = 0.46767363\n",
      "Iteration 379, loss = 0.46762564\n",
      "Iteration 380, loss = 0.46757527\n",
      "Iteration 381, loss = 0.46775355\n",
      "Iteration 382, loss = 0.46752343\n",
      "Iteration 383, loss = 0.46766958\n",
      "Iteration 384, loss = 0.46747694\n",
      "Iteration 385, loss = 0.46746808\n",
      "Iteration 386, loss = 0.46754684\n",
      "Iteration 387, loss = 0.46732405\n",
      "Iteration 388, loss = 0.46737888\n",
      "Iteration 389, loss = 0.46744696\n",
      "Iteration 390, loss = 0.46724599\n",
      "Iteration 391, loss = 0.46728985\n",
      "Iteration 392, loss = 0.46709377\n",
      "Iteration 393, loss = 0.46738733\n",
      "Iteration 394, loss = 0.46751542\n",
      "Iteration 395, loss = 0.46717503\n",
      "Iteration 396, loss = 0.46755526\n",
      "Iteration 397, loss = 0.46736198\n",
      "Iteration 398, loss = 0.46707891\n",
      "Iteration 399, loss = 0.46720132\n",
      "Iteration 400, loss = 0.46750574\n",
      "Iteration 401, loss = 0.46688469\n",
      "Iteration 402, loss = 0.46706811\n",
      "Iteration 403, loss = 0.46715628\n",
      "Iteration 404, loss = 0.46711922\n",
      "Iteration 405, loss = 0.46669466\n",
      "Iteration 406, loss = 0.46687007\n",
      "Iteration 407, loss = 0.46686063\n",
      "Iteration 408, loss = 0.46688220\n",
      "Iteration 409, loss = 0.46710280\n",
      "Iteration 410, loss = 0.46671643\n",
      "Iteration 411, loss = 0.46682124\n",
      "Iteration 412, loss = 0.46679517\n",
      "Iteration 413, loss = 0.46675619\n",
      "Iteration 414, loss = 0.46660350\n",
      "Iteration 415, loss = 0.46671916\n",
      "Iteration 416, loss = 0.46679242\n",
      "Iteration 417, loss = 0.46667824\n",
      "Iteration 418, loss = 0.46643780\n",
      "Iteration 419, loss = 0.46699415\n",
      "Iteration 420, loss = 0.46673182\n",
      "Iteration 421, loss = 0.46660610\n",
      "Iteration 422, loss = 0.46660211\n",
      "Iteration 423, loss = 0.46628487\n",
      "Iteration 424, loss = 0.46677339\n",
      "Iteration 425, loss = 0.46633099\n",
      "Iteration 426, loss = 0.46635677\n",
      "Iteration 427, loss = 0.46622262\n",
      "Iteration 428, loss = 0.46643768\n",
      "Iteration 429, loss = 0.46629685\n",
      "Iteration 430, loss = 0.46672253\n",
      "Iteration 431, loss = 0.46640017\n",
      "Iteration 432, loss = 0.46672488\n",
      "Iteration 433, loss = 0.46622820\n",
      "Iteration 434, loss = 0.46633860\n",
      "Iteration 435, loss = 0.46625955\n",
      "Iteration 436, loss = 0.46614692\n",
      "Iteration 437, loss = 0.46623425\n",
      "Iteration 438, loss = 0.46614631\n",
      "Iteration 439, loss = 0.46645404\n",
      "Iteration 440, loss = 0.46580548\n",
      "Iteration 441, loss = 0.46621991\n",
      "Iteration 442, loss = 0.46590344\n",
      "Iteration 443, loss = 0.46643666\n",
      "Iteration 444, loss = 0.46597803\n",
      "Iteration 445, loss = 0.46629502\n",
      "Iteration 446, loss = 0.46577614\n",
      "Iteration 447, loss = 0.46626703\n",
      "Iteration 448, loss = 0.46599363\n",
      "Iteration 449, loss = 0.46597873\n",
      "Iteration 450, loss = 0.46582998\n",
      "Iteration 451, loss = 0.46660588\n",
      "Iteration 452, loss = 0.46599679\n",
      "Iteration 453, loss = 0.46570132\n",
      "Iteration 454, loss = 0.46584874\n",
      "Iteration 455, loss = 0.46584238\n",
      "Iteration 456, loss = 0.46582568\n",
      "Iteration 457, loss = 0.46602295\n",
      "Iteration 458, loss = 0.46578910\n",
      "Iteration 459, loss = 0.46586705\n",
      "Iteration 460, loss = 0.46567906\n",
      "Iteration 461, loss = 0.46579077\n",
      "Iteration 462, loss = 0.46577915\n",
      "Iteration 463, loss = 0.46563110\n",
      "Iteration 464, loss = 0.46551338\n",
      "Iteration 465, loss = 0.46560270\n",
      "Iteration 466, loss = 0.46545536\n",
      "Iteration 467, loss = 0.46546814\n",
      "Iteration 468, loss = 0.46555435\n",
      "Iteration 469, loss = 0.46577243\n",
      "Iteration 470, loss = 0.46541683\n",
      "Iteration 471, loss = 0.46590937\n",
      "Iteration 472, loss = 0.46573482\n",
      "Iteration 473, loss = 0.46551879\n",
      "Iteration 474, loss = 0.46555276\n",
      "Iteration 475, loss = 0.46628699\n",
      "Iteration 476, loss = 0.46552286\n",
      "Iteration 477, loss = 0.46559484\n",
      "Iteration 478, loss = 0.46554492\n",
      "Iteration 479, loss = 0.46537585\n",
      "Iteration 480, loss = 0.46538378\n",
      "Iteration 481, loss = 0.46529946\n",
      "Iteration 482, loss = 0.46543517\n",
      "Iteration 483, loss = 0.46565752\n",
      "Iteration 484, loss = 0.46508910\n",
      "Iteration 485, loss = 0.46537881\n",
      "Iteration 486, loss = 0.46536829\n",
      "Iteration 487, loss = 0.46550016\n",
      "Iteration 488, loss = 0.46503507\n",
      "Iteration 489, loss = 0.46497725\n",
      "Iteration 490, loss = 0.46513227\n",
      "Iteration 491, loss = 0.46511426\n",
      "Iteration 492, loss = 0.46513058\n",
      "Iteration 493, loss = 0.46518013\n",
      "Iteration 494, loss = 0.46499995\n",
      "Iteration 495, loss = 0.46509701\n",
      "Iteration 496, loss = 0.46505717\n",
      "Iteration 497, loss = 0.46510927\n",
      "Iteration 498, loss = 0.46504474\n",
      "Iteration 499, loss = 0.46500835\n",
      "Iteration 500, loss = 0.46501941\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71801799\n",
      "Iteration 2, loss = 0.68559569\n",
      "Iteration 3, loss = 0.66302143\n",
      "Iteration 4, loss = 0.64546695\n",
      "Iteration 5, loss = 0.62847748\n",
      "Iteration 6, loss = 0.61408669\n",
      "Iteration 7, loss = 0.60140052\n",
      "Iteration 8, loss = 0.59096597\n",
      "Iteration 9, loss = 0.58268229\n",
      "Iteration 10, loss = 0.57638318\n",
      "Iteration 11, loss = 0.57105907\n",
      "Iteration 12, loss = 0.56695465\n",
      "Iteration 13, loss = 0.56353524\n",
      "Iteration 14, loss = 0.56072129\n",
      "Iteration 15, loss = 0.55767370\n",
      "Iteration 16, loss = 0.55511324\n",
      "Iteration 17, loss = 0.55271193\n",
      "Iteration 18, loss = 0.55034196\n",
      "Iteration 19, loss = 0.54805737\n",
      "Iteration 20, loss = 0.54614963\n",
      "Iteration 21, loss = 0.54421498\n",
      "Iteration 22, loss = 0.54244220\n",
      "Iteration 23, loss = 0.54101433\n",
      "Iteration 24, loss = 0.53926396\n",
      "Iteration 25, loss = 0.53786314\n",
      "Iteration 26, loss = 0.53662477\n",
      "Iteration 27, loss = 0.53533683\n",
      "Iteration 28, loss = 0.53489082\n",
      "Iteration 29, loss = 0.53362301\n",
      "Iteration 30, loss = 0.53258967\n",
      "Iteration 31, loss = 0.53171729\n",
      "Iteration 32, loss = 0.53076094\n",
      "Iteration 33, loss = 0.52993214\n",
      "Iteration 34, loss = 0.52940141\n",
      "Iteration 35, loss = 0.52840564\n",
      "Iteration 36, loss = 0.52770031\n",
      "Iteration 37, loss = 0.52700941\n",
      "Iteration 38, loss = 0.52656774\n",
      "Iteration 39, loss = 0.52602927\n",
      "Iteration 40, loss = 0.52565205\n",
      "Iteration 41, loss = 0.52516138\n",
      "Iteration 42, loss = 0.52460010\n",
      "Iteration 43, loss = 0.52388160\n",
      "Iteration 44, loss = 0.52364072\n",
      "Iteration 45, loss = 0.52323753\n",
      "Iteration 46, loss = 0.52277160\n",
      "Iteration 47, loss = 0.52235296\n",
      "Iteration 48, loss = 0.52213164\n",
      "Iteration 49, loss = 0.52178700\n",
      "Iteration 50, loss = 0.52136498\n",
      "Iteration 51, loss = 0.52114586\n",
      "Iteration 52, loss = 0.52105332\n",
      "Iteration 53, loss = 0.52055173\n",
      "Iteration 54, loss = 0.51979347\n",
      "Iteration 55, loss = 0.51977655\n",
      "Iteration 56, loss = 0.51928549\n",
      "Iteration 57, loss = 0.51899265\n",
      "Iteration 58, loss = 0.51857113\n",
      "Iteration 59, loss = 0.51808690\n",
      "Iteration 60, loss = 0.51783816\n",
      "Iteration 61, loss = 0.51739172\n",
      "Iteration 62, loss = 0.51730833\n",
      "Iteration 63, loss = 0.51705981\n",
      "Iteration 64, loss = 0.51641086\n",
      "Iteration 65, loss = 0.51604201\n",
      "Iteration 66, loss = 0.51615267\n",
      "Iteration 67, loss = 0.51524106\n",
      "Iteration 68, loss = 0.51504749\n",
      "Iteration 69, loss = 0.51482388\n",
      "Iteration 70, loss = 0.51428114\n",
      "Iteration 71, loss = 0.51409543\n",
      "Iteration 72, loss = 0.51360632\n",
      "Iteration 73, loss = 0.51326599\n",
      "Iteration 74, loss = 0.51314806\n",
      "Iteration 75, loss = 0.51271141\n",
      "Iteration 76, loss = 0.51232458\n",
      "Iteration 77, loss = 0.51214570\n",
      "Iteration 78, loss = 0.51161498\n",
      "Iteration 79, loss = 0.51113938\n",
      "Iteration 80, loss = 0.51096061\n",
      "Iteration 81, loss = 0.51090684\n",
      "Iteration 82, loss = 0.51016031\n",
      "Iteration 83, loss = 0.50989381\n",
      "Iteration 84, loss = 0.50966153\n",
      "Iteration 85, loss = 0.50956742\n",
      "Iteration 86, loss = 0.50887411\n",
      "Iteration 87, loss = 0.50878020\n",
      "Iteration 88, loss = 0.50842778\n",
      "Iteration 89, loss = 0.50857810\n",
      "Iteration 90, loss = 0.50769661\n",
      "Iteration 91, loss = 0.50728771\n",
      "Iteration 92, loss = 0.50741501\n",
      "Iteration 93, loss = 0.50684495\n",
      "Iteration 94, loss = 0.50657436\n",
      "Iteration 95, loss = 0.50611159\n",
      "Iteration 96, loss = 0.50585077\n",
      "Iteration 97, loss = 0.50539284\n",
      "Iteration 98, loss = 0.50532245\n",
      "Iteration 99, loss = 0.50472139\n",
      "Iteration 100, loss = 0.50443380\n",
      "Iteration 101, loss = 0.50420054\n",
      "Iteration 102, loss = 0.50376433\n",
      "Iteration 103, loss = 0.50356757\n",
      "Iteration 104, loss = 0.50320261\n",
      "Iteration 105, loss = 0.50286306\n",
      "Iteration 106, loss = 0.50256169\n",
      "Iteration 107, loss = 0.50251788\n",
      "Iteration 108, loss = 0.50175218\n",
      "Iteration 109, loss = 0.50164656\n",
      "Iteration 110, loss = 0.50145900\n",
      "Iteration 111, loss = 0.50107805\n",
      "Iteration 112, loss = 0.50089037\n",
      "Iteration 113, loss = 0.50121294\n",
      "Iteration 114, loss = 0.50063888\n",
      "Iteration 115, loss = 0.50034614\n",
      "Iteration 116, loss = 0.50010374\n",
      "Iteration 117, loss = 0.49976536\n",
      "Iteration 118, loss = 0.49974192\n",
      "Iteration 119, loss = 0.49943820\n",
      "Iteration 120, loss = 0.49913504\n",
      "Iteration 121, loss = 0.49894830\n",
      "Iteration 122, loss = 0.49888501\n",
      "Iteration 123, loss = 0.49845095\n",
      "Iteration 124, loss = 0.49842417\n",
      "Iteration 125, loss = 0.49811946\n",
      "Iteration 126, loss = 0.49785455\n",
      "Iteration 127, loss = 0.49775039\n",
      "Iteration 128, loss = 0.49751254\n",
      "Iteration 129, loss = 0.49729125\n",
      "Iteration 130, loss = 0.49708975\n",
      "Iteration 131, loss = 0.49679854\n",
      "Iteration 132, loss = 0.49681388\n",
      "Iteration 133, loss = 0.49660192\n",
      "Iteration 134, loss = 0.49646697\n",
      "Iteration 135, loss = 0.49634374\n",
      "Iteration 136, loss = 0.49597161\n",
      "Iteration 137, loss = 0.49580717\n",
      "Iteration 138, loss = 0.49562498\n",
      "Iteration 139, loss = 0.49549706\n",
      "Iteration 140, loss = 0.49529234\n",
      "Iteration 141, loss = 0.49538858\n",
      "Iteration 142, loss = 0.49523307\n",
      "Iteration 143, loss = 0.49491712\n",
      "Iteration 144, loss = 0.49466992\n",
      "Iteration 145, loss = 0.49440593\n",
      "Iteration 146, loss = 0.49415529\n",
      "Iteration 147, loss = 0.49434221\n",
      "Iteration 148, loss = 0.49417965\n",
      "Iteration 149, loss = 0.49401887\n",
      "Iteration 150, loss = 0.49363276\n",
      "Iteration 151, loss = 0.49374920\n",
      "Iteration 152, loss = 0.49336274\n",
      "Iteration 153, loss = 0.49312076\n",
      "Iteration 154, loss = 0.49343280\n",
      "Iteration 155, loss = 0.49288273\n",
      "Iteration 156, loss = 0.49302810\n",
      "Iteration 157, loss = 0.49255323\n",
      "Iteration 158, loss = 0.49241360\n",
      "Iteration 159, loss = 0.49209596\n",
      "Iteration 160, loss = 0.49229136\n",
      "Iteration 161, loss = 0.49209423\n",
      "Iteration 162, loss = 0.49208086\n",
      "Iteration 163, loss = 0.49176078\n",
      "Iteration 164, loss = 0.49133975\n",
      "Iteration 165, loss = 0.49142101\n",
      "Iteration 166, loss = 0.49116736\n",
      "Iteration 167, loss = 0.49100560\n",
      "Iteration 168, loss = 0.49086418\n",
      "Iteration 169, loss = 0.49075275\n",
      "Iteration 170, loss = 0.49055557\n",
      "Iteration 171, loss = 0.49038851\n",
      "Iteration 172, loss = 0.49075414\n",
      "Iteration 173, loss = 0.49035220\n",
      "Iteration 174, loss = 0.49023481\n",
      "Iteration 175, loss = 0.49008218\n",
      "Iteration 176, loss = 0.48969251\n",
      "Iteration 177, loss = 0.48969537\n",
      "Iteration 178, loss = 0.48976283\n",
      "Iteration 179, loss = 0.48981950\n",
      "Iteration 180, loss = 0.48947729\n",
      "Iteration 181, loss = 0.48911387\n",
      "Iteration 182, loss = 0.48899950\n",
      "Iteration 183, loss = 0.48892657\n",
      "Iteration 184, loss = 0.48898937\n",
      "Iteration 185, loss = 0.48882701\n",
      "Iteration 186, loss = 0.48846094\n",
      "Iteration 187, loss = 0.48837591\n",
      "Iteration 188, loss = 0.48860047\n",
      "Iteration 189, loss = 0.48839839\n",
      "Iteration 190, loss = 0.48830638\n",
      "Iteration 191, loss = 0.48786329\n",
      "Iteration 192, loss = 0.48793874\n",
      "Iteration 193, loss = 0.48797246\n",
      "Iteration 194, loss = 0.48768143\n",
      "Iteration 195, loss = 0.48800887\n",
      "Iteration 196, loss = 0.48743239\n",
      "Iteration 197, loss = 0.48734152\n",
      "Iteration 198, loss = 0.48716534\n",
      "Iteration 199, loss = 0.48749935\n",
      "Iteration 200, loss = 0.48714823\n",
      "Iteration 201, loss = 0.48715679\n",
      "Iteration 202, loss = 0.48676917\n",
      "Iteration 203, loss = 0.48673018\n",
      "Iteration 204, loss = 0.48680258\n",
      "Iteration 205, loss = 0.48686431\n",
      "Iteration 206, loss = 0.48652486\n",
      "Iteration 207, loss = 0.48623825\n",
      "Iteration 208, loss = 0.48640593\n",
      "Iteration 209, loss = 0.48630773\n",
      "Iteration 210, loss = 0.48646231\n",
      "Iteration 211, loss = 0.48589675\n",
      "Iteration 212, loss = 0.48601049\n",
      "Iteration 213, loss = 0.48571089\n",
      "Iteration 214, loss = 0.48589703\n",
      "Iteration 215, loss = 0.48573439\n",
      "Iteration 216, loss = 0.48569270\n",
      "Iteration 217, loss = 0.48554761\n",
      "Iteration 218, loss = 0.48527379\n",
      "Iteration 219, loss = 0.48557349\n",
      "Iteration 220, loss = 0.48556460\n",
      "Iteration 221, loss = 0.48509844\n",
      "Iteration 222, loss = 0.48500679\n",
      "Iteration 223, loss = 0.48495562\n",
      "Iteration 224, loss = 0.48477117\n",
      "Iteration 225, loss = 0.48500131\n",
      "Iteration 226, loss = 0.48457899\n",
      "Iteration 227, loss = 0.48465120\n",
      "Iteration 228, loss = 0.48448383\n",
      "Iteration 229, loss = 0.48442294\n",
      "Iteration 230, loss = 0.48444879\n",
      "Iteration 231, loss = 0.48460438\n",
      "Iteration 232, loss = 0.48445228\n",
      "Iteration 233, loss = 0.48408688\n",
      "Iteration 234, loss = 0.48398481\n",
      "Iteration 235, loss = 0.48393809\n",
      "Iteration 236, loss = 0.48411280\n",
      "Iteration 237, loss = 0.48409124\n",
      "Iteration 238, loss = 0.48368824\n",
      "Iteration 239, loss = 0.48363111\n",
      "Iteration 240, loss = 0.48374344\n",
      "Iteration 241, loss = 0.48365108\n",
      "Iteration 242, loss = 0.48371903\n",
      "Iteration 243, loss = 0.48344110\n",
      "Iteration 244, loss = 0.48314837\n",
      "Iteration 245, loss = 0.48324319\n",
      "Iteration 246, loss = 0.48314577\n",
      "Iteration 247, loss = 0.48304569\n",
      "Iteration 248, loss = 0.48288550\n",
      "Iteration 249, loss = 0.48297587\n",
      "Iteration 250, loss = 0.48294718\n",
      "Iteration 251, loss = 0.48264432\n",
      "Iteration 252, loss = 0.48261622\n",
      "Iteration 253, loss = 0.48251523\n",
      "Iteration 254, loss = 0.48267506\n",
      "Iteration 255, loss = 0.48259271\n",
      "Iteration 256, loss = 0.48240054\n",
      "Iteration 257, loss = 0.48212996\n",
      "Iteration 258, loss = 0.48238244\n",
      "Iteration 259, loss = 0.48217347\n",
      "Iteration 260, loss = 0.48231613\n",
      "Iteration 261, loss = 0.48207824\n",
      "Iteration 262, loss = 0.48194471\n",
      "Iteration 263, loss = 0.48198054\n",
      "Iteration 264, loss = 0.48177636\n",
      "Iteration 265, loss = 0.48229549\n",
      "Iteration 266, loss = 0.48177344\n",
      "Iteration 267, loss = 0.48187798\n",
      "Iteration 268, loss = 0.48171984\n",
      "Iteration 269, loss = 0.48151871\n",
      "Iteration 270, loss = 0.48136172\n",
      "Iteration 271, loss = 0.48133693\n",
      "Iteration 272, loss = 0.48144459\n",
      "Iteration 273, loss = 0.48137186\n",
      "Iteration 274, loss = 0.48144448\n",
      "Iteration 275, loss = 0.48110519\n",
      "Iteration 276, loss = 0.48129351\n",
      "Iteration 277, loss = 0.48107481\n",
      "Iteration 278, loss = 0.48074478\n",
      "Iteration 279, loss = 0.48080510\n",
      "Iteration 280, loss = 0.48086994\n",
      "Iteration 281, loss = 0.48059961\n",
      "Iteration 282, loss = 0.48074733\n",
      "Iteration 283, loss = 0.48074950\n",
      "Iteration 284, loss = 0.48081389\n",
      "Iteration 285, loss = 0.48047658\n",
      "Iteration 286, loss = 0.48021601\n",
      "Iteration 287, loss = 0.48037086\n",
      "Iteration 288, loss = 0.48006681\n",
      "Iteration 289, loss = 0.48013142\n",
      "Iteration 290, loss = 0.48006997\n",
      "Iteration 291, loss = 0.47997871\n",
      "Iteration 292, loss = 0.48014795\n",
      "Iteration 293, loss = 0.48030908\n",
      "Iteration 294, loss = 0.47977898\n",
      "Iteration 295, loss = 0.47975562\n",
      "Iteration 296, loss = 0.47986317\n",
      "Iteration 297, loss = 0.47954212\n",
      "Iteration 298, loss = 0.47963666\n",
      "Iteration 299, loss = 0.48000086\n",
      "Iteration 300, loss = 0.47961607\n",
      "Iteration 301, loss = 0.47938246\n",
      "Iteration 302, loss = 0.47958342\n",
      "Iteration 303, loss = 0.47987719\n",
      "Iteration 304, loss = 0.47969829\n",
      "Iteration 305, loss = 0.47927321\n",
      "Iteration 306, loss = 0.47935378\n",
      "Iteration 307, loss = 0.47941150\n",
      "Iteration 308, loss = 0.47907965\n",
      "Iteration 309, loss = 0.47937190\n",
      "Iteration 310, loss = 0.47902374\n",
      "Iteration 311, loss = 0.47944798\n",
      "Iteration 312, loss = 0.47937894\n",
      "Iteration 313, loss = 0.47890678\n",
      "Iteration 314, loss = 0.47906919\n",
      "Iteration 315, loss = 0.47876009\n",
      "Iteration 316, loss = 0.47873672\n",
      "Iteration 317, loss = 0.47880208\n",
      "Iteration 318, loss = 0.47893524\n",
      "Iteration 319, loss = 0.47862961\n",
      "Iteration 320, loss = 0.47876602\n",
      "Iteration 321, loss = 0.47847400\n",
      "Iteration 322, loss = 0.47844109\n",
      "Iteration 323, loss = 0.47845081\n",
      "Iteration 324, loss = 0.47809347\n",
      "Iteration 325, loss = 0.47814693\n",
      "Iteration 326, loss = 0.47803831\n",
      "Iteration 327, loss = 0.47821853\n",
      "Iteration 328, loss = 0.47803280\n",
      "Iteration 329, loss = 0.47805280\n",
      "Iteration 330, loss = 0.47796527\n",
      "Iteration 331, loss = 0.47806313\n",
      "Iteration 332, loss = 0.47786239\n",
      "Iteration 333, loss = 0.47776470\n",
      "Iteration 334, loss = 0.47788280\n",
      "Iteration 335, loss = 0.47775021\n",
      "Iteration 336, loss = 0.47762813\n",
      "Iteration 337, loss = 0.47765376\n",
      "Iteration 338, loss = 0.47747484\n",
      "Iteration 339, loss = 0.47738673\n",
      "Iteration 340, loss = 0.47734616\n",
      "Iteration 341, loss = 0.47722368\n",
      "Iteration 342, loss = 0.47723489\n",
      "Iteration 343, loss = 0.47726852\n",
      "Iteration 344, loss = 0.47703604\n",
      "Iteration 345, loss = 0.47693548\n",
      "Iteration 346, loss = 0.47691765\n",
      "Iteration 347, loss = 0.47697557\n",
      "Iteration 348, loss = 0.47682423\n",
      "Iteration 349, loss = 0.47694926\n",
      "Iteration 350, loss = 0.47681207\n",
      "Iteration 351, loss = 0.47681215\n",
      "Iteration 352, loss = 0.47680857\n",
      "Iteration 353, loss = 0.47668118\n",
      "Iteration 354, loss = 0.47656968\n",
      "Iteration 355, loss = 0.47686603\n",
      "Iteration 356, loss = 0.47674544\n",
      "Iteration 357, loss = 0.47643595\n",
      "Iteration 358, loss = 0.47632328\n",
      "Iteration 359, loss = 0.47617497\n",
      "Iteration 360, loss = 0.47626920\n",
      "Iteration 361, loss = 0.47618891\n",
      "Iteration 362, loss = 0.47603181\n",
      "Iteration 363, loss = 0.47616055\n",
      "Iteration 364, loss = 0.47615854\n",
      "Iteration 365, loss = 0.47618825\n",
      "Iteration 366, loss = 0.47616719\n",
      "Iteration 367, loss = 0.47604955\n",
      "Iteration 368, loss = 0.47590453\n",
      "Iteration 369, loss = 0.47616533\n",
      "Iteration 370, loss = 0.47587541\n",
      "Iteration 371, loss = 0.47589486\n",
      "Iteration 372, loss = 0.47600277\n",
      "Iteration 373, loss = 0.47579974\n",
      "Iteration 374, loss = 0.47584968\n",
      "Iteration 375, loss = 0.47609185\n",
      "Iteration 376, loss = 0.47542249\n",
      "Iteration 377, loss = 0.47572958\n",
      "Iteration 378, loss = 0.47561978\n",
      "Iteration 379, loss = 0.47553141\n",
      "Iteration 380, loss = 0.47533852\n",
      "Iteration 381, loss = 0.47538974\n",
      "Iteration 382, loss = 0.47542897\n",
      "Iteration 383, loss = 0.47510099\n",
      "Iteration 384, loss = 0.47552077\n",
      "Iteration 385, loss = 0.47512164\n",
      "Iteration 386, loss = 0.47526279\n",
      "Iteration 387, loss = 0.47513561\n",
      "Iteration 388, loss = 0.47506107\n",
      "Iteration 389, loss = 0.47486708\n",
      "Iteration 390, loss = 0.47499880\n",
      "Iteration 391, loss = 0.47485861\n",
      "Iteration 392, loss = 0.47501912\n",
      "Iteration 393, loss = 0.47500292\n",
      "Iteration 394, loss = 0.47475930\n",
      "Iteration 395, loss = 0.47500914\n",
      "Iteration 396, loss = 0.47482572\n",
      "Iteration 397, loss = 0.47467148\n",
      "Iteration 398, loss = 0.47487280\n",
      "Iteration 399, loss = 0.47465138\n",
      "Iteration 400, loss = 0.47451301\n",
      "Iteration 401, loss = 0.47467376\n",
      "Iteration 402, loss = 0.47438149\n",
      "Iteration 403, loss = 0.47474713\n",
      "Iteration 404, loss = 0.47457851\n",
      "Iteration 405, loss = 0.47436603\n",
      "Iteration 406, loss = 0.47446087\n",
      "Iteration 407, loss = 0.47445488\n",
      "Iteration 408, loss = 0.47428985\n",
      "Iteration 409, loss = 0.47444448\n",
      "Iteration 410, loss = 0.47386169\n",
      "Iteration 411, loss = 0.47417815\n",
      "Iteration 412, loss = 0.47401103\n",
      "Iteration 413, loss = 0.47404205\n",
      "Iteration 414, loss = 0.47378884\n",
      "Iteration 415, loss = 0.47425765\n",
      "Iteration 416, loss = 0.47375870\n",
      "Iteration 417, loss = 0.47394937\n",
      "Iteration 418, loss = 0.47365034\n",
      "Iteration 419, loss = 0.47387299\n",
      "Iteration 420, loss = 0.47373159\n",
      "Iteration 421, loss = 0.47341093\n",
      "Iteration 422, loss = 0.47344379\n",
      "Iteration 423, loss = 0.47351735\n",
      "Iteration 424, loss = 0.47346039\n",
      "Iteration 425, loss = 0.47322619\n",
      "Iteration 426, loss = 0.47330237\n",
      "Iteration 427, loss = 0.47319595\n",
      "Iteration 428, loss = 0.47307204\n",
      "Iteration 429, loss = 0.47293500\n",
      "Iteration 430, loss = 0.47301452\n",
      "Iteration 431, loss = 0.47291724\n",
      "Iteration 432, loss = 0.47276988\n",
      "Iteration 433, loss = 0.47271968\n",
      "Iteration 434, loss = 0.47300089\n",
      "Iteration 435, loss = 0.47286528\n",
      "Iteration 436, loss = 0.47274837\n",
      "Iteration 437, loss = 0.47270885\n",
      "Iteration 438, loss = 0.47259730\n",
      "Iteration 439, loss = 0.47250476\n",
      "Iteration 440, loss = 0.47245275\n",
      "Iteration 441, loss = 0.47239647\n",
      "Iteration 442, loss = 0.47246152\n",
      "Iteration 443, loss = 0.47215665\n",
      "Iteration 444, loss = 0.47244416\n",
      "Iteration 445, loss = 0.47237365\n",
      "Iteration 446, loss = 0.47203500\n",
      "Iteration 447, loss = 0.47157830\n",
      "Iteration 448, loss = 0.47183595\n",
      "Iteration 449, loss = 0.47218886\n",
      "Iteration 450, loss = 0.47183098\n",
      "Iteration 451, loss = 0.47149922\n",
      "Iteration 452, loss = 0.47135101\n",
      "Iteration 453, loss = 0.47124452\n",
      "Iteration 454, loss = 0.47105158\n",
      "Iteration 455, loss = 0.47174663\n",
      "Iteration 456, loss = 0.47147877\n",
      "Iteration 457, loss = 0.47133441\n",
      "Iteration 458, loss = 0.47082666\n",
      "Iteration 459, loss = 0.47076786\n",
      "Iteration 460, loss = 0.47067773\n",
      "Iteration 461, loss = 0.47055234\n",
      "Iteration 462, loss = 0.47073384\n",
      "Iteration 463, loss = 0.47047497\n",
      "Iteration 464, loss = 0.47069338\n",
      "Iteration 465, loss = 0.47031517\n",
      "Iteration 466, loss = 0.47020042\n",
      "Iteration 467, loss = 0.47017065\n",
      "Iteration 468, loss = 0.47038288\n",
      "Iteration 469, loss = 0.47010174\n",
      "Iteration 470, loss = 0.47021201\n",
      "Iteration 471, loss = 0.46984665\n",
      "Iteration 472, loss = 0.46994799\n",
      "Iteration 473, loss = 0.46988343\n",
      "Iteration 474, loss = 0.46997833\n",
      "Iteration 475, loss = 0.47002784\n",
      "Iteration 476, loss = 0.46943922\n",
      "Iteration 477, loss = 0.46952795\n",
      "Iteration 478, loss = 0.46942809\n",
      "Iteration 479, loss = 0.46927257\n",
      "Iteration 480, loss = 0.46969695\n",
      "Iteration 481, loss = 0.46935279\n",
      "Iteration 482, loss = 0.46928886\n",
      "Iteration 483, loss = 0.46915631\n",
      "Iteration 484, loss = 0.46888348\n",
      "Iteration 485, loss = 0.46910820\n",
      "Iteration 486, loss = 0.46908164\n",
      "Iteration 487, loss = 0.46896287\n",
      "Iteration 488, loss = 0.46877314\n",
      "Iteration 489, loss = 0.46883181\n",
      "Iteration 490, loss = 0.46879015\n",
      "Iteration 491, loss = 0.46860030\n",
      "Iteration 492, loss = 0.46870056\n",
      "Iteration 493, loss = 0.46876343\n",
      "Iteration 494, loss = 0.46854180\n",
      "Iteration 495, loss = 0.46839797\n",
      "Iteration 496, loss = 0.46852410\n",
      "Iteration 497, loss = 0.46838125\n",
      "Iteration 498, loss = 0.46832676\n",
      "Iteration 499, loss = 0.46830023\n",
      "Iteration 500, loss = 0.46832841\n",
      "Iteration 501, loss = 0.46821170\n",
      "Iteration 502, loss = 0.46796511\n",
      "Iteration 503, loss = 0.46810055\n",
      "Iteration 504, loss = 0.46794821\n",
      "Iteration 505, loss = 0.46788897\n",
      "Iteration 506, loss = 0.46812672\n",
      "Iteration 507, loss = 0.46796571\n",
      "Iteration 508, loss = 0.46791073\n",
      "Iteration 509, loss = 0.46797762\n",
      "Iteration 510, loss = 0.46774374\n",
      "Iteration 511, loss = 0.46761723\n",
      "Iteration 512, loss = 0.46748271\n",
      "Iteration 513, loss = 0.46744014\n",
      "Iteration 514, loss = 0.46752448\n",
      "Iteration 515, loss = 0.46743171\n",
      "Iteration 516, loss = 0.46742174\n",
      "Iteration 517, loss = 0.46765825\n",
      "Iteration 518, loss = 0.46756901\n",
      "Iteration 519, loss = 0.46736703\n",
      "Iteration 520, loss = 0.46701337\n",
      "Iteration 521, loss = 0.46697415\n",
      "Iteration 522, loss = 0.46710534\n",
      "Iteration 523, loss = 0.46695826\n",
      "Iteration 524, loss = 0.46685426\n",
      "Iteration 525, loss = 0.46705414\n",
      "Iteration 526, loss = 0.46697523\n",
      "Iteration 527, loss = 0.46685664\n",
      "Iteration 528, loss = 0.46680707\n",
      "Iteration 529, loss = 0.46698331\n",
      "Iteration 530, loss = 0.46677563\n",
      "Iteration 531, loss = 0.46640473\n",
      "Iteration 532, loss = 0.46645645\n",
      "Iteration 533, loss = 0.46643637\n",
      "Iteration 534, loss = 0.46669065\n",
      "Iteration 535, loss = 0.46616456\n",
      "Iteration 536, loss = 0.46651601\n",
      "Iteration 537, loss = 0.46639274\n",
      "Iteration 538, loss = 0.46611459\n",
      "Iteration 539, loss = 0.46591389\n",
      "Iteration 540, loss = 0.46607549\n",
      "Iteration 541, loss = 0.46587209\n",
      "Iteration 542, loss = 0.46579695\n",
      "Iteration 543, loss = 0.46570539\n",
      "Iteration 544, loss = 0.46578300\n",
      "Iteration 545, loss = 0.46567678\n",
      "Iteration 546, loss = 0.46545930\n",
      "Iteration 547, loss = 0.46562128\n",
      "Iteration 548, loss = 0.46563714\n",
      "Iteration 549, loss = 0.46536263\n",
      "Iteration 550, loss = 0.46605483\n",
      "Iteration 551, loss = 0.46581700\n",
      "Iteration 552, loss = 0.46577470\n",
      "Iteration 553, loss = 0.46546308\n",
      "Iteration 554, loss = 0.46525896\n",
      "Iteration 555, loss = 0.46519176\n",
      "Iteration 556, loss = 0.46529706\n",
      "Iteration 557, loss = 0.46527619\n",
      "Iteration 558, loss = 0.46492346\n",
      "Iteration 559, loss = 0.46479959\n",
      "Iteration 560, loss = 0.46464506\n",
      "Iteration 561, loss = 0.46494570\n",
      "Iteration 562, loss = 0.46470086\n",
      "Iteration 563, loss = 0.46461695\n",
      "Iteration 564, loss = 0.46492000\n",
      "Iteration 565, loss = 0.46470934\n",
      "Iteration 566, loss = 0.46454465\n",
      "Iteration 567, loss = 0.46468899\n",
      "Iteration 568, loss = 0.46428567\n",
      "Iteration 569, loss = 0.46462063\n",
      "Iteration 570, loss = 0.46425678\n",
      "Iteration 571, loss = 0.46442395\n",
      "Iteration 572, loss = 0.46440662\n",
      "Iteration 573, loss = 0.46482438\n",
      "Iteration 574, loss = 0.46425343\n",
      "Iteration 575, loss = 0.46440192\n",
      "Iteration 576, loss = 0.46401869\n",
      "Iteration 577, loss = 0.46409263\n",
      "Iteration 578, loss = 0.46394380\n",
      "Iteration 579, loss = 0.46401911\n",
      "Iteration 580, loss = 0.46395443\n",
      "Iteration 581, loss = 0.46441094\n",
      "Iteration 582, loss = 0.46367607\n",
      "Iteration 583, loss = 0.46365997\n",
      "Iteration 584, loss = 0.46383928\n",
      "Iteration 585, loss = 0.46398450\n",
      "Iteration 586, loss = 0.46367362\n",
      "Iteration 587, loss = 0.46382841\n",
      "Iteration 588, loss = 0.46392205\n",
      "Iteration 589, loss = 0.46334743\n",
      "Iteration 590, loss = 0.46362034\n",
      "Iteration 591, loss = 0.46336822\n",
      "Iteration 592, loss = 0.46344674\n",
      "Iteration 593, loss = 0.46325388\n",
      "Iteration 594, loss = 0.46340798\n",
      "Iteration 595, loss = 0.46372347\n",
      "Iteration 596, loss = 0.46294702\n",
      "Iteration 597, loss = 0.46310202\n",
      "Iteration 598, loss = 0.46332507\n",
      "Iteration 599, loss = 0.46309335\n",
      "Iteration 600, loss = 0.46292563\n",
      "Iteration 601, loss = 0.46301184\n",
      "Iteration 602, loss = 0.46298006\n",
      "Iteration 603, loss = 0.46284996\n",
      "Iteration 604, loss = 0.46307105\n",
      "Iteration 605, loss = 0.46306221\n",
      "Iteration 606, loss = 0.46285108\n",
      "Iteration 607, loss = 0.46282197\n",
      "Iteration 608, loss = 0.46297226\n",
      "Iteration 609, loss = 0.46323559\n",
      "Iteration 610, loss = 0.46248386\n",
      "Iteration 611, loss = 0.46287959\n",
      "Iteration 612, loss = 0.46272426\n",
      "Iteration 613, loss = 0.46270397\n",
      "Iteration 614, loss = 0.46235812\n",
      "Iteration 615, loss = 0.46250570\n",
      "Iteration 616, loss = 0.46240033\n",
      "Iteration 617, loss = 0.46248161\n",
      "Iteration 618, loss = 0.46231460\n",
      "Iteration 619, loss = 0.46299325\n",
      "Iteration 620, loss = 0.46270810\n",
      "Iteration 621, loss = 0.46253728\n",
      "Iteration 622, loss = 0.46236070\n",
      "Iteration 623, loss = 0.46242124\n",
      "Iteration 624, loss = 0.46205075\n",
      "Iteration 625, loss = 0.46207240\n",
      "Iteration 626, loss = 0.46235252\n",
      "Iteration 627, loss = 0.46201685\n",
      "Iteration 628, loss = 0.46206956\n",
      "Iteration 629, loss = 0.46188198\n",
      "Iteration 630, loss = 0.46197040\n",
      "Iteration 631, loss = 0.46188605\n",
      "Iteration 632, loss = 0.46175004\n",
      "Iteration 633, loss = 0.46184755\n",
      "Iteration 634, loss = 0.46176867\n",
      "Iteration 635, loss = 0.46237294\n",
      "Iteration 636, loss = 0.46201446\n",
      "Iteration 637, loss = 0.46198657\n",
      "Iteration 638, loss = 0.46175178\n",
      "Iteration 639, loss = 0.46189958\n",
      "Iteration 640, loss = 0.46171601\n",
      "Iteration 641, loss = 0.46190777\n",
      "Iteration 642, loss = 0.46157494\n",
      "Iteration 643, loss = 0.46195278\n",
      "Iteration 644, loss = 0.46168572\n",
      "Iteration 645, loss = 0.46171789\n",
      "Iteration 646, loss = 0.46183242\n",
      "Iteration 647, loss = 0.46166418\n",
      "Iteration 648, loss = 0.46176340\n",
      "Iteration 649, loss = 0.46242875\n",
      "Iteration 650, loss = 0.46254364\n",
      "Iteration 651, loss = 0.46222480\n",
      "Iteration 652, loss = 0.46148193\n",
      "Iteration 653, loss = 0.46146522\n",
      "Iteration 654, loss = 0.46143214\n",
      "Iteration 655, loss = 0.46152395\n",
      "Iteration 656, loss = 0.46178644\n",
      "Iteration 657, loss = 0.46191241\n",
      "Iteration 658, loss = 0.46123588\n",
      "Iteration 659, loss = 0.46110673\n",
      "Iteration 660, loss = 0.46124904\n",
      "Iteration 661, loss = 0.46103280\n",
      "Iteration 662, loss = 0.46111594\n",
      "Iteration 663, loss = 0.46117424\n",
      "Iteration 664, loss = 0.46110482\n",
      "Iteration 665, loss = 0.46113956\n",
      "Iteration 666, loss = 0.46115223\n",
      "Iteration 667, loss = 0.46089307\n",
      "Iteration 668, loss = 0.46080439\n",
      "Iteration 669, loss = 0.46089509\n",
      "Iteration 670, loss = 0.46090686\n",
      "Iteration 671, loss = 0.46089251\n",
      "Iteration 672, loss = 0.46068077\n",
      "Iteration 673, loss = 0.46095194\n",
      "Iteration 674, loss = 0.46078715\n",
      "Iteration 675, loss = 0.46078064\n",
      "Iteration 676, loss = 0.46069672\n",
      "Iteration 677, loss = 0.46070071\n",
      "Iteration 678, loss = 0.46059125\n",
      "Iteration 679, loss = 0.46057137\n",
      "Iteration 680, loss = 0.46116059\n",
      "Iteration 681, loss = 0.46067299\n",
      "Iteration 682, loss = 0.46076433\n",
      "Iteration 683, loss = 0.46050975\n",
      "Iteration 684, loss = 0.46030076\n",
      "Iteration 685, loss = 0.46040511\n",
      "Iteration 686, loss = 0.46046799\n",
      "Iteration 687, loss = 0.46026867\n",
      "Iteration 688, loss = 0.46043714\n",
      "Iteration 689, loss = 0.46051274\n",
      "Iteration 690, loss = 0.46073203\n",
      "Iteration 691, loss = 0.46011066\n",
      "Iteration 692, loss = 0.46026687\n",
      "Iteration 693, loss = 0.46054963\n",
      "Iteration 694, loss = 0.46037308\n",
      "Iteration 695, loss = 0.46079015\n",
      "Iteration 696, loss = 0.46011663\n",
      "Iteration 697, loss = 0.46012035\n",
      "Iteration 698, loss = 0.46029663\n",
      "Iteration 699, loss = 0.46027214\n",
      "Iteration 700, loss = 0.46027776\n",
      "Iteration 701, loss = 0.46004069\n",
      "Iteration 702, loss = 0.46002945\n",
      "Iteration 703, loss = 0.45999988\n",
      "Iteration 704, loss = 0.45987636\n",
      "Iteration 705, loss = 0.46024077\n",
      "Iteration 706, loss = 0.46039120\n",
      "Iteration 707, loss = 0.45984682\n",
      "Iteration 708, loss = 0.45991307\n",
      "Iteration 709, loss = 0.46033197\n",
      "Iteration 710, loss = 0.45995851\n",
      "Iteration 711, loss = 0.45958356\n",
      "Iteration 712, loss = 0.45974931\n",
      "Iteration 713, loss = 0.45974749\n",
      "Iteration 714, loss = 0.46008523\n",
      "Iteration 715, loss = 0.46016965\n",
      "Iteration 716, loss = 0.45969758\n",
      "Iteration 717, loss = 0.45964739\n",
      "Iteration 718, loss = 0.45991660\n",
      "Iteration 719, loss = 0.45946011\n",
      "Iteration 720, loss = 0.45980869\n",
      "Iteration 721, loss = 0.45974168\n",
      "Iteration 722, loss = 0.45952968\n",
      "Iteration 723, loss = 0.45958726\n",
      "Iteration 724, loss = 0.45938684\n",
      "Iteration 725, loss = 0.45947885\n",
      "Iteration 726, loss = 0.45955464\n",
      "Iteration 727, loss = 0.45964874\n",
      "Iteration 728, loss = 0.45943273\n",
      "Iteration 729, loss = 0.45928919\n",
      "Iteration 730, loss = 0.45929201\n",
      "Iteration 731, loss = 0.45936046\n",
      "Iteration 732, loss = 0.45933047\n",
      "Iteration 733, loss = 0.45929629\n",
      "Iteration 734, loss = 0.45923286\n",
      "Iteration 735, loss = 0.45929886\n",
      "Iteration 736, loss = 0.45921559\n",
      "Iteration 737, loss = 0.45948356\n",
      "Iteration 738, loss = 0.45922598\n",
      "Iteration 739, loss = 0.45915302\n",
      "Iteration 740, loss = 0.45918300\n",
      "Iteration 741, loss = 0.45910912\n",
      "Iteration 742, loss = 0.45917028\n",
      "Iteration 743, loss = 0.45907405\n",
      "Iteration 744, loss = 0.45915907\n",
      "Iteration 745, loss = 0.45948726\n",
      "Iteration 746, loss = 0.45957809\n",
      "Iteration 747, loss = 0.45890483\n",
      "Iteration 748, loss = 0.45881814\n",
      "Iteration 749, loss = 0.45907339\n",
      "Iteration 750, loss = 0.45903418\n",
      "Iteration 751, loss = 0.45971711\n",
      "Iteration 752, loss = 0.45915778\n",
      "Iteration 753, loss = 0.45859695\n",
      "Iteration 754, loss = 0.45869421\n",
      "Iteration 755, loss = 0.45879062\n",
      "Iteration 756, loss = 0.45918558\n",
      "Iteration 757, loss = 0.45873661\n",
      "Iteration 758, loss = 0.45854894\n",
      "Iteration 759, loss = 0.45856714\n",
      "Iteration 760, loss = 0.45880154\n",
      "Iteration 761, loss = 0.45892949\n",
      "Iteration 762, loss = 0.45879208\n",
      "Iteration 763, loss = 0.45836293\n",
      "Iteration 764, loss = 0.45856024\n",
      "Iteration 765, loss = 0.45849967\n",
      "Iteration 766, loss = 0.45831939\n",
      "Iteration 767, loss = 0.45835328\n",
      "Iteration 768, loss = 0.45817968\n",
      "Iteration 769, loss = 0.45809046\n",
      "Iteration 770, loss = 0.45827166\n",
      "Iteration 771, loss = 0.45865349\n",
      "Iteration 772, loss = 0.45821025\n",
      "Iteration 773, loss = 0.45805321\n",
      "Iteration 774, loss = 0.45843084\n",
      "Iteration 775, loss = 0.45830655\n",
      "Iteration 776, loss = 0.45818653\n",
      "Iteration 777, loss = 0.45809290\n",
      "Iteration 778, loss = 0.45820781\n",
      "Iteration 779, loss = 0.45870914\n",
      "Iteration 780, loss = 0.45818807\n",
      "Iteration 781, loss = 0.45824692\n",
      "Iteration 782, loss = 0.45793107\n",
      "Iteration 783, loss = 0.45843707\n",
      "Iteration 784, loss = 0.45827982\n",
      "Iteration 785, loss = 0.45808823\n",
      "Iteration 786, loss = 0.45819018\n",
      "Iteration 787, loss = 0.45813767\n",
      "Iteration 788, loss = 0.45834147\n",
      "Iteration 789, loss = 0.45805909\n",
      "Iteration 790, loss = 0.45767036\n",
      "Iteration 791, loss = 0.45783277\n",
      "Iteration 792, loss = 0.45806556\n",
      "Iteration 793, loss = 0.45803938\n",
      "Iteration 794, loss = 0.45807334\n",
      "Iteration 795, loss = 0.45796986\n",
      "Iteration 796, loss = 0.45834623\n",
      "Iteration 797, loss = 0.45845547\n",
      "Iteration 798, loss = 0.45781666\n",
      "Iteration 799, loss = 0.45801520\n",
      "Iteration 800, loss = 0.45775556\n",
      "Iteration 801, loss = 0.45784505\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82578442\n",
      "Iteration 2, loss = 0.75170279\n",
      "Iteration 3, loss = 0.70851169\n",
      "Iteration 4, loss = 0.68510076\n",
      "Iteration 5, loss = 0.66839351\n",
      "Iteration 6, loss = 0.65496140\n",
      "Iteration 7, loss = 0.64304562\n",
      "Iteration 8, loss = 0.63292363\n",
      "Iteration 9, loss = 0.62279922\n",
      "Iteration 10, loss = 0.61397206\n",
      "Iteration 11, loss = 0.60563001\n",
      "Iteration 12, loss = 0.59817953\n",
      "Iteration 13, loss = 0.59193395\n",
      "Iteration 14, loss = 0.58595067\n",
      "Iteration 15, loss = 0.58115173\n",
      "Iteration 16, loss = 0.57693723\n",
      "Iteration 17, loss = 0.57292576\n",
      "Iteration 18, loss = 0.56991567\n",
      "Iteration 19, loss = 0.56687694\n",
      "Iteration 20, loss = 0.56436234\n",
      "Iteration 21, loss = 0.56213029\n",
      "Iteration 22, loss = 0.56029909\n",
      "Iteration 23, loss = 0.55794382\n",
      "Iteration 24, loss = 0.55628503\n",
      "Iteration 25, loss = 0.55470139\n",
      "Iteration 26, loss = 0.55268005\n",
      "Iteration 27, loss = 0.55098573\n",
      "Iteration 28, loss = 0.54944776\n",
      "Iteration 29, loss = 0.54799165\n",
      "Iteration 30, loss = 0.54672961\n",
      "Iteration 31, loss = 0.54496636\n",
      "Iteration 32, loss = 0.54360052\n",
      "Iteration 33, loss = 0.54263002\n",
      "Iteration 34, loss = 0.54087084\n",
      "Iteration 35, loss = 0.53943670\n",
      "Iteration 36, loss = 0.53832484\n",
      "Iteration 37, loss = 0.53687047\n",
      "Iteration 38, loss = 0.53554662\n",
      "Iteration 39, loss = 0.53419948\n",
      "Iteration 40, loss = 0.53313881\n",
      "Iteration 41, loss = 0.53204595\n",
      "Iteration 42, loss = 0.53096722\n",
      "Iteration 43, loss = 0.53010645\n",
      "Iteration 44, loss = 0.52862305\n",
      "Iteration 45, loss = 0.52760477\n",
      "Iteration 46, loss = 0.52651080\n",
      "Iteration 47, loss = 0.52567397\n",
      "Iteration 48, loss = 0.52475478\n",
      "Iteration 49, loss = 0.52411918\n",
      "Iteration 50, loss = 0.52311429\n",
      "Iteration 51, loss = 0.52256013\n",
      "Iteration 52, loss = 0.52173210\n",
      "Iteration 53, loss = 0.52104883\n",
      "Iteration 54, loss = 0.52061141\n",
      "Iteration 55, loss = 0.51972079\n",
      "Iteration 56, loss = 0.51902908\n",
      "Iteration 57, loss = 0.51859279\n",
      "Iteration 58, loss = 0.51794082\n",
      "Iteration 59, loss = 0.51741997\n",
      "Iteration 60, loss = 0.51690722\n",
      "Iteration 61, loss = 0.51644965\n",
      "Iteration 62, loss = 0.51597347\n",
      "Iteration 63, loss = 0.51558421\n",
      "Iteration 64, loss = 0.51493138\n",
      "Iteration 65, loss = 0.51464614\n",
      "Iteration 66, loss = 0.51427891\n",
      "Iteration 67, loss = 0.51366477\n",
      "Iteration 68, loss = 0.51339508\n",
      "Iteration 69, loss = 0.51295750\n",
      "Iteration 70, loss = 0.51250416\n",
      "Iteration 71, loss = 0.51196310\n",
      "Iteration 72, loss = 0.51199518\n",
      "Iteration 73, loss = 0.51158589\n",
      "Iteration 74, loss = 0.51209050\n",
      "Iteration 75, loss = 0.51084214\n",
      "Iteration 76, loss = 0.51054742\n",
      "Iteration 77, loss = 0.51012372\n",
      "Iteration 78, loss = 0.50975140\n",
      "Iteration 79, loss = 0.50959707\n",
      "Iteration 80, loss = 0.50940038\n",
      "Iteration 81, loss = 0.50910226\n",
      "Iteration 82, loss = 0.50876981\n",
      "Iteration 83, loss = 0.50868580\n",
      "Iteration 84, loss = 0.50876214\n",
      "Iteration 85, loss = 0.50826662\n",
      "Iteration 86, loss = 0.50798148\n",
      "Iteration 87, loss = 0.50778888\n",
      "Iteration 88, loss = 0.50762267\n",
      "Iteration 89, loss = 0.50754195\n",
      "Iteration 90, loss = 0.50749631\n",
      "Iteration 91, loss = 0.50685168\n",
      "Iteration 92, loss = 0.50670721\n",
      "Iteration 93, loss = 0.50671555\n",
      "Iteration 94, loss = 0.50673451\n",
      "Iteration 95, loss = 0.50621773\n",
      "Iteration 96, loss = 0.50592122\n",
      "Iteration 97, loss = 0.50572222\n",
      "Iteration 98, loss = 0.50550759\n",
      "Iteration 99, loss = 0.50537559\n",
      "Iteration 100, loss = 0.50527210\n",
      "Iteration 101, loss = 0.50497438\n",
      "Iteration 102, loss = 0.50512304\n",
      "Iteration 103, loss = 0.50445358\n",
      "Iteration 104, loss = 0.50433137\n",
      "Iteration 105, loss = 0.50410195\n",
      "Iteration 106, loss = 0.50406302\n",
      "Iteration 107, loss = 0.50385311\n",
      "Iteration 108, loss = 0.50409116\n",
      "Iteration 109, loss = 0.50365137\n",
      "Iteration 110, loss = 0.50328659\n",
      "Iteration 111, loss = 0.50347023\n",
      "Iteration 112, loss = 0.50345963\n",
      "Iteration 113, loss = 0.50294958\n",
      "Iteration 114, loss = 0.50286283\n",
      "Iteration 115, loss = 0.50265465\n",
      "Iteration 116, loss = 0.50253984\n",
      "Iteration 117, loss = 0.50249131\n",
      "Iteration 118, loss = 0.50204410\n",
      "Iteration 119, loss = 0.50230402\n",
      "Iteration 120, loss = 0.50222707\n",
      "Iteration 121, loss = 0.50233468\n",
      "Iteration 122, loss = 0.50165566\n",
      "Iteration 123, loss = 0.50185890\n",
      "Iteration 124, loss = 0.50151757\n",
      "Iteration 125, loss = 0.50166109\n",
      "Iteration 126, loss = 0.50113979\n",
      "Iteration 127, loss = 0.50097384\n",
      "Iteration 128, loss = 0.50102057\n",
      "Iteration 129, loss = 0.50101844\n",
      "Iteration 130, loss = 0.50075364\n",
      "Iteration 131, loss = 0.50063684\n",
      "Iteration 132, loss = 0.50037896\n",
      "Iteration 133, loss = 0.50034936\n",
      "Iteration 134, loss = 0.50040815\n",
      "Iteration 135, loss = 0.50063352\n",
      "Iteration 136, loss = 0.49974776\n",
      "Iteration 137, loss = 0.50011322\n",
      "Iteration 138, loss = 0.49980031\n",
      "Iteration 139, loss = 0.49954750\n",
      "Iteration 140, loss = 0.49955436\n",
      "Iteration 141, loss = 0.49956578\n",
      "Iteration 142, loss = 0.49904614\n",
      "Iteration 143, loss = 0.49910058\n",
      "Iteration 144, loss = 0.49880188\n",
      "Iteration 145, loss = 0.49889131\n",
      "Iteration 146, loss = 0.49875120\n",
      "Iteration 147, loss = 0.49876052\n",
      "Iteration 148, loss = 0.49844945\n",
      "Iteration 149, loss = 0.49837857\n",
      "Iteration 150, loss = 0.49836473\n",
      "Iteration 151, loss = 0.49841864\n",
      "Iteration 152, loss = 0.49818659\n",
      "Iteration 153, loss = 0.49783541\n",
      "Iteration 154, loss = 0.49779539\n",
      "Iteration 155, loss = 0.49771407\n",
      "Iteration 156, loss = 0.49750257\n",
      "Iteration 157, loss = 0.49755405\n",
      "Iteration 158, loss = 0.49725891\n",
      "Iteration 159, loss = 0.49783720\n",
      "Iteration 160, loss = 0.49727918\n",
      "Iteration 161, loss = 0.49714014\n",
      "Iteration 162, loss = 0.49676146\n",
      "Iteration 163, loss = 0.49679774\n",
      "Iteration 164, loss = 0.49673854\n",
      "Iteration 165, loss = 0.49685307\n",
      "Iteration 166, loss = 0.49655918\n",
      "Iteration 167, loss = 0.49635378\n",
      "Iteration 168, loss = 0.49658493\n",
      "Iteration 169, loss = 0.49630222\n",
      "Iteration 170, loss = 0.49611890\n",
      "Iteration 171, loss = 0.49608115\n",
      "Iteration 172, loss = 0.49590820\n",
      "Iteration 173, loss = 0.49613506\n",
      "Iteration 174, loss = 0.49596463\n",
      "Iteration 175, loss = 0.49550765\n",
      "Iteration 176, loss = 0.49540512\n",
      "Iteration 177, loss = 0.49530388\n",
      "Iteration 178, loss = 0.49533799\n",
      "Iteration 179, loss = 0.49503199\n",
      "Iteration 180, loss = 0.49504637\n",
      "Iteration 181, loss = 0.49484204\n",
      "Iteration 182, loss = 0.49493212\n",
      "Iteration 183, loss = 0.49506251\n",
      "Iteration 184, loss = 0.49491620\n",
      "Iteration 185, loss = 0.49456755\n",
      "Iteration 186, loss = 0.49443042\n",
      "Iteration 187, loss = 0.49424073\n",
      "Iteration 188, loss = 0.49443031\n",
      "Iteration 189, loss = 0.49423919\n",
      "Iteration 190, loss = 0.49413647\n",
      "Iteration 191, loss = 0.49413507\n",
      "Iteration 192, loss = 0.49378161\n",
      "Iteration 193, loss = 0.49392355\n",
      "Iteration 194, loss = 0.49359998\n",
      "Iteration 195, loss = 0.49371693\n",
      "Iteration 196, loss = 0.49359983\n",
      "Iteration 197, loss = 0.49350984\n",
      "Iteration 198, loss = 0.49356478\n",
      "Iteration 199, loss = 0.49350660\n",
      "Iteration 200, loss = 0.49360966\n",
      "Iteration 201, loss = 0.49321958\n",
      "Iteration 202, loss = 0.49332645\n",
      "Iteration 203, loss = 0.49318158\n",
      "Iteration 204, loss = 0.49298366\n",
      "Iteration 205, loss = 0.49284429\n",
      "Iteration 206, loss = 0.49282630\n",
      "Iteration 207, loss = 0.49272208\n",
      "Iteration 208, loss = 0.49269081\n",
      "Iteration 209, loss = 0.49274082\n",
      "Iteration 210, loss = 0.49237031\n",
      "Iteration 211, loss = 0.49282065\n",
      "Iteration 212, loss = 0.49258684\n",
      "Iteration 213, loss = 0.49231136\n",
      "Iteration 214, loss = 0.49230740\n",
      "Iteration 215, loss = 0.49213792\n",
      "Iteration 216, loss = 0.49201027\n",
      "Iteration 217, loss = 0.49214400\n",
      "Iteration 218, loss = 0.49198598\n",
      "Iteration 219, loss = 0.49200395\n",
      "Iteration 220, loss = 0.49190077\n",
      "Iteration 221, loss = 0.49194566\n",
      "Iteration 222, loss = 0.49255154\n",
      "Iteration 223, loss = 0.49217705\n",
      "Iteration 224, loss = 0.49183610\n",
      "Iteration 225, loss = 0.49183377\n",
      "Iteration 226, loss = 0.49163118\n",
      "Iteration 227, loss = 0.49152523\n",
      "Iteration 228, loss = 0.49139862\n",
      "Iteration 229, loss = 0.49156913\n",
      "Iteration 230, loss = 0.49154869\n",
      "Iteration 231, loss = 0.49134766\n",
      "Iteration 232, loss = 0.49142564\n",
      "Iteration 233, loss = 0.49123987\n",
      "Iteration 234, loss = 0.49113348\n",
      "Iteration 235, loss = 0.49091361\n",
      "Iteration 236, loss = 0.49091228\n",
      "Iteration 237, loss = 0.49086760\n",
      "Iteration 238, loss = 0.49078002\n",
      "Iteration 239, loss = 0.49107748\n",
      "Iteration 240, loss = 0.49068103\n",
      "Iteration 241, loss = 0.49069274\n",
      "Iteration 242, loss = 0.49063049\n",
      "Iteration 243, loss = 0.49049688\n",
      "Iteration 244, loss = 0.49072744\n",
      "Iteration 245, loss = 0.49062811\n",
      "Iteration 246, loss = 0.49041816\n",
      "Iteration 247, loss = 0.49048008\n",
      "Iteration 248, loss = 0.49047781\n",
      "Iteration 249, loss = 0.49035976\n",
      "Iteration 250, loss = 0.49016294\n",
      "Iteration 251, loss = 0.49018922\n",
      "Iteration 252, loss = 0.49044307\n",
      "Iteration 253, loss = 0.49030028\n",
      "Iteration 254, loss = 0.49032328\n",
      "Iteration 255, loss = 0.49023597\n",
      "Iteration 256, loss = 0.49018231\n",
      "Iteration 257, loss = 0.49034158\n",
      "Iteration 258, loss = 0.48976944\n",
      "Iteration 259, loss = 0.49004230\n",
      "Iteration 260, loss = 0.48976290\n",
      "Iteration 261, loss = 0.48975736\n",
      "Iteration 262, loss = 0.48949859\n",
      "Iteration 263, loss = 0.48993961\n",
      "Iteration 264, loss = 0.48969556\n",
      "Iteration 265, loss = 0.48968817\n",
      "Iteration 266, loss = 0.48943257\n",
      "Iteration 267, loss = 0.48938740\n",
      "Iteration 268, loss = 0.48949813\n",
      "Iteration 269, loss = 0.48952417\n",
      "Iteration 270, loss = 0.48939678\n",
      "Iteration 271, loss = 0.48910770\n",
      "Iteration 272, loss = 0.48906277\n",
      "Iteration 273, loss = 0.48910522\n",
      "Iteration 274, loss = 0.48927303\n",
      "Iteration 275, loss = 0.48888109\n",
      "Iteration 276, loss = 0.48915390\n",
      "Iteration 277, loss = 0.48921567\n",
      "Iteration 278, loss = 0.48906757\n",
      "Iteration 279, loss = 0.48914020\n",
      "Iteration 280, loss = 0.48874782\n",
      "Iteration 281, loss = 0.48888812\n",
      "Iteration 282, loss = 0.48868060\n",
      "Iteration 283, loss = 0.48898335\n",
      "Iteration 284, loss = 0.48891780\n",
      "Iteration 285, loss = 0.48856177\n",
      "Iteration 286, loss = 0.48855257\n",
      "Iteration 287, loss = 0.48825791\n",
      "Iteration 288, loss = 0.48830572\n",
      "Iteration 289, loss = 0.48815252\n",
      "Iteration 290, loss = 0.48850831\n",
      "Iteration 291, loss = 0.48833472\n",
      "Iteration 292, loss = 0.48818034\n",
      "Iteration 293, loss = 0.48809951\n",
      "Iteration 294, loss = 0.48800966\n",
      "Iteration 295, loss = 0.48809728\n",
      "Iteration 296, loss = 0.48807428\n",
      "Iteration 297, loss = 0.48782485\n",
      "Iteration 298, loss = 0.48796891\n",
      "Iteration 299, loss = 0.48810216\n",
      "Iteration 300, loss = 0.48762810\n",
      "Iteration 301, loss = 0.48782149\n",
      "Iteration 302, loss = 0.48768482\n",
      "Iteration 303, loss = 0.48767406\n",
      "Iteration 304, loss = 0.48744901\n",
      "Iteration 305, loss = 0.48786456\n",
      "Iteration 306, loss = 0.48738884\n",
      "Iteration 307, loss = 0.48747526\n",
      "Iteration 308, loss = 0.48734656\n",
      "Iteration 309, loss = 0.48763638\n",
      "Iteration 310, loss = 0.48747130\n",
      "Iteration 311, loss = 0.48751229\n",
      "Iteration 312, loss = 0.48729348\n",
      "Iteration 313, loss = 0.48730421\n",
      "Iteration 314, loss = 0.48738895\n",
      "Iteration 315, loss = 0.48745312\n",
      "Iteration 316, loss = 0.48759898\n",
      "Iteration 317, loss = 0.48703792\n",
      "Iteration 318, loss = 0.48687100\n",
      "Iteration 319, loss = 0.48761612\n",
      "Iteration 320, loss = 0.48725211\n",
      "Iteration 321, loss = 0.48712920\n",
      "Iteration 322, loss = 0.48728515\n",
      "Iteration 323, loss = 0.48688536\n",
      "Iteration 324, loss = 0.48675593\n",
      "Iteration 325, loss = 0.48675667\n",
      "Iteration 326, loss = 0.48695631\n",
      "Iteration 327, loss = 0.48667648\n",
      "Iteration 328, loss = 0.48681744\n",
      "Iteration 329, loss = 0.48672618\n",
      "Iteration 330, loss = 0.48698053\n",
      "Iteration 331, loss = 0.48662716\n",
      "Iteration 332, loss = 0.48656708\n",
      "Iteration 333, loss = 0.48651504\n",
      "Iteration 334, loss = 0.48635289\n",
      "Iteration 335, loss = 0.48633401\n",
      "Iteration 336, loss = 0.48641272\n",
      "Iteration 337, loss = 0.48643751\n",
      "Iteration 338, loss = 0.48658632\n",
      "Iteration 339, loss = 0.48626002\n",
      "Iteration 340, loss = 0.48628619\n",
      "Iteration 341, loss = 0.48627540\n",
      "Iteration 342, loss = 0.48628805\n",
      "Iteration 343, loss = 0.48640492\n",
      "Iteration 344, loss = 0.48615052\n",
      "Iteration 345, loss = 0.48610160\n",
      "Iteration 346, loss = 0.48590046\n",
      "Iteration 347, loss = 0.48586902\n",
      "Iteration 348, loss = 0.48590965\n",
      "Iteration 349, loss = 0.48578649\n",
      "Iteration 350, loss = 0.48577878\n",
      "Iteration 351, loss = 0.48598428\n",
      "Iteration 352, loss = 0.48572662\n",
      "Iteration 353, loss = 0.48612836\n",
      "Iteration 354, loss = 0.48572631\n",
      "Iteration 355, loss = 0.48549320\n",
      "Iteration 356, loss = 0.48566241\n",
      "Iteration 357, loss = 0.48560031\n",
      "Iteration 358, loss = 0.48556445\n",
      "Iteration 359, loss = 0.48557561\n",
      "Iteration 360, loss = 0.48587426\n",
      "Iteration 361, loss = 0.48545514\n",
      "Iteration 362, loss = 0.48539354\n",
      "Iteration 363, loss = 0.48535108\n",
      "Iteration 364, loss = 0.48542206\n",
      "Iteration 365, loss = 0.48552506\n",
      "Iteration 366, loss = 0.48567876\n",
      "Iteration 367, loss = 0.48531958\n",
      "Iteration 368, loss = 0.48533908\n",
      "Iteration 369, loss = 0.48538018\n",
      "Iteration 370, loss = 0.48501333\n",
      "Iteration 371, loss = 0.48507950\n",
      "Iteration 372, loss = 0.48556662\n",
      "Iteration 373, loss = 0.48553093\n",
      "Iteration 374, loss = 0.48498207\n",
      "Iteration 375, loss = 0.48510938\n",
      "Iteration 376, loss = 0.48502326\n",
      "Iteration 377, loss = 0.48486990\n",
      "Iteration 378, loss = 0.48492491\n",
      "Iteration 379, loss = 0.48483737\n",
      "Iteration 380, loss = 0.48562975\n",
      "Iteration 381, loss = 0.48551598\n",
      "Iteration 382, loss = 0.48470348\n",
      "Iteration 383, loss = 0.48458619\n",
      "Iteration 384, loss = 0.48453042\n",
      "Iteration 385, loss = 0.48492383\n",
      "Iteration 386, loss = 0.48464784\n",
      "Iteration 387, loss = 0.48466431\n",
      "Iteration 388, loss = 0.48470654\n",
      "Iteration 389, loss = 0.48466684\n",
      "Iteration 390, loss = 0.48482981\n",
      "Iteration 391, loss = 0.48429155\n",
      "Iteration 392, loss = 0.48461113\n",
      "Iteration 393, loss = 0.48467623\n",
      "Iteration 394, loss = 0.48461279\n",
      "Iteration 395, loss = 0.48443513\n",
      "Iteration 396, loss = 0.48427246\n",
      "Iteration 397, loss = 0.48456523\n",
      "Iteration 398, loss = 0.48443420\n",
      "Iteration 399, loss = 0.48440666\n",
      "Iteration 400, loss = 0.48436843\n",
      "Iteration 401, loss = 0.48424883\n",
      "Iteration 402, loss = 0.48404402\n",
      "Iteration 403, loss = 0.48454076\n",
      "Iteration 404, loss = 0.48435834\n",
      "Iteration 405, loss = 0.48385175\n",
      "Iteration 406, loss = 0.48392754\n",
      "Iteration 407, loss = 0.48407190\n",
      "Iteration 408, loss = 0.48384280\n",
      "Iteration 409, loss = 0.48380155\n",
      "Iteration 410, loss = 0.48383988\n",
      "Iteration 411, loss = 0.48376615\n",
      "Iteration 412, loss = 0.48410791\n",
      "Iteration 413, loss = 0.48395357\n",
      "Iteration 414, loss = 0.48379468\n",
      "Iteration 415, loss = 0.48359709\n",
      "Iteration 416, loss = 0.48359658\n",
      "Iteration 417, loss = 0.48379851\n",
      "Iteration 418, loss = 0.48386684\n",
      "Iteration 419, loss = 0.48419628\n",
      "Iteration 420, loss = 0.48383793\n",
      "Iteration 421, loss = 0.48373167\n",
      "Iteration 422, loss = 0.48321227\n",
      "Iteration 423, loss = 0.48350210\n",
      "Iteration 424, loss = 0.48331195\n",
      "Iteration 425, loss = 0.48362756\n",
      "Iteration 426, loss = 0.48343877\n",
      "Iteration 427, loss = 0.48350925\n",
      "Iteration 428, loss = 0.48340759\n",
      "Iteration 429, loss = 0.48302310\n",
      "Iteration 430, loss = 0.48351561\n",
      "Iteration 431, loss = 0.48313200\n",
      "Iteration 432, loss = 0.48333178\n",
      "Iteration 433, loss = 0.48346329\n",
      "Iteration 434, loss = 0.48318886\n",
      "Iteration 435, loss = 0.48319847\n",
      "Iteration 436, loss = 0.48296001\n",
      "Iteration 437, loss = 0.48294390\n",
      "Iteration 438, loss = 0.48298536\n",
      "Iteration 439, loss = 0.48280712\n",
      "Iteration 440, loss = 0.48312049\n",
      "Iteration 441, loss = 0.48348004\n",
      "Iteration 442, loss = 0.48357391\n",
      "Iteration 443, loss = 0.48302476\n",
      "Iteration 444, loss = 0.48261262\n",
      "Iteration 445, loss = 0.48297773\n",
      "Iteration 446, loss = 0.48249317\n",
      "Iteration 447, loss = 0.48266060\n",
      "Iteration 448, loss = 0.48287295\n",
      "Iteration 449, loss = 0.48271261\n",
      "Iteration 450, loss = 0.48286832\n",
      "Iteration 451, loss = 0.48246340\n",
      "Iteration 452, loss = 0.48253134\n",
      "Iteration 453, loss = 0.48283102\n",
      "Iteration 454, loss = 0.48280982\n",
      "Iteration 455, loss = 0.48252448\n",
      "Iteration 456, loss = 0.48240128\n",
      "Iteration 457, loss = 0.48277576\n",
      "Iteration 458, loss = 0.48248099\n",
      "Iteration 459, loss = 0.48241291\n",
      "Iteration 460, loss = 0.48260546\n",
      "Iteration 461, loss = 0.48270763\n",
      "Iteration 462, loss = 0.48249305\n",
      "Iteration 463, loss = 0.48258151\n",
      "Iteration 464, loss = 0.48258966\n",
      "Iteration 465, loss = 0.48256934\n",
      "Iteration 466, loss = 0.48255557\n",
      "Iteration 467, loss = 0.48247396\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72735353\n",
      "Iteration 2, loss = 0.68744850\n",
      "Iteration 3, loss = 0.66592372\n",
      "Iteration 4, loss = 0.65182601\n",
      "Iteration 5, loss = 0.64048474\n",
      "Iteration 6, loss = 0.62935329\n",
      "Iteration 7, loss = 0.61959613\n",
      "Iteration 8, loss = 0.61005062\n",
      "Iteration 9, loss = 0.60121091\n",
      "Iteration 10, loss = 0.59351415\n",
      "Iteration 11, loss = 0.58685578\n",
      "Iteration 12, loss = 0.58094593\n",
      "Iteration 13, loss = 0.57624200\n",
      "Iteration 14, loss = 0.57244628\n",
      "Iteration 15, loss = 0.56938091\n",
      "Iteration 16, loss = 0.56699862\n",
      "Iteration 17, loss = 0.56440747\n",
      "Iteration 18, loss = 0.56222553\n",
      "Iteration 19, loss = 0.56028393\n",
      "Iteration 20, loss = 0.55848970\n",
      "Iteration 21, loss = 0.55718054\n",
      "Iteration 22, loss = 0.55539361\n",
      "Iteration 23, loss = 0.55409593\n",
      "Iteration 24, loss = 0.55265836\n",
      "Iteration 25, loss = 0.55141259\n",
      "Iteration 26, loss = 0.54993472\n",
      "Iteration 27, loss = 0.54861362\n",
      "Iteration 28, loss = 0.54748468\n",
      "Iteration 29, loss = 0.54608355\n",
      "Iteration 30, loss = 0.54500239\n",
      "Iteration 31, loss = 0.54391968\n",
      "Iteration 32, loss = 0.54249772\n",
      "Iteration 33, loss = 0.54138282\n",
      "Iteration 34, loss = 0.54011433\n",
      "Iteration 35, loss = 0.53893909\n",
      "Iteration 36, loss = 0.53782390\n",
      "Iteration 37, loss = 0.53675298\n",
      "Iteration 38, loss = 0.53555077\n",
      "Iteration 39, loss = 0.53446517\n",
      "Iteration 40, loss = 0.53351611\n",
      "Iteration 41, loss = 0.53237464\n",
      "Iteration 42, loss = 0.53127156\n",
      "Iteration 43, loss = 0.52994234\n",
      "Iteration 44, loss = 0.52896622\n",
      "Iteration 45, loss = 0.52769819\n",
      "Iteration 46, loss = 0.52673441\n",
      "Iteration 47, loss = 0.52580083\n",
      "Iteration 48, loss = 0.52478184\n",
      "Iteration 49, loss = 0.52369610\n",
      "Iteration 50, loss = 0.52233708\n",
      "Iteration 51, loss = 0.52125499\n",
      "Iteration 52, loss = 0.52041786\n",
      "Iteration 53, loss = 0.51919394\n",
      "Iteration 54, loss = 0.51824765\n",
      "Iteration 55, loss = 0.51746545\n",
      "Iteration 56, loss = 0.51709990\n",
      "Iteration 57, loss = 0.51582034\n",
      "Iteration 58, loss = 0.51455188\n",
      "Iteration 59, loss = 0.51350233\n",
      "Iteration 60, loss = 0.51285281\n",
      "Iteration 61, loss = 0.51203933\n",
      "Iteration 62, loss = 0.51103724\n",
      "Iteration 63, loss = 0.51028360\n",
      "Iteration 64, loss = 0.50964187\n",
      "Iteration 65, loss = 0.50911978\n",
      "Iteration 66, loss = 0.50825979\n",
      "Iteration 67, loss = 0.50778620\n",
      "Iteration 68, loss = 0.50705441\n",
      "Iteration 69, loss = 0.50675191\n",
      "Iteration 70, loss = 0.50604985\n",
      "Iteration 71, loss = 0.50576859\n",
      "Iteration 72, loss = 0.50496733\n",
      "Iteration 73, loss = 0.50456178\n",
      "Iteration 74, loss = 0.50410031\n",
      "Iteration 75, loss = 0.50355270\n",
      "Iteration 76, loss = 0.50280573\n",
      "Iteration 77, loss = 0.50257193\n",
      "Iteration 78, loss = 0.50240535\n",
      "Iteration 79, loss = 0.50155134\n",
      "Iteration 80, loss = 0.50133600\n",
      "Iteration 81, loss = 0.50134809\n",
      "Iteration 82, loss = 0.50062724\n",
      "Iteration 83, loss = 0.50026465\n",
      "Iteration 84, loss = 0.50001813\n",
      "Iteration 85, loss = 0.49972321\n",
      "Iteration 86, loss = 0.49960586\n",
      "Iteration 87, loss = 0.49900629\n",
      "Iteration 88, loss = 0.49888812\n",
      "Iteration 89, loss = 0.49891744\n",
      "Iteration 90, loss = 0.49847298\n",
      "Iteration 91, loss = 0.49834676\n",
      "Iteration 92, loss = 0.49817958\n",
      "Iteration 93, loss = 0.49843929\n",
      "Iteration 94, loss = 0.49772002\n",
      "Iteration 95, loss = 0.49732783\n",
      "Iteration 96, loss = 0.49778866\n",
      "Iteration 97, loss = 0.49710501\n",
      "Iteration 98, loss = 0.49671605\n",
      "Iteration 99, loss = 0.49681425\n",
      "Iteration 100, loss = 0.49657857\n",
      "Iteration 101, loss = 0.49629463\n",
      "Iteration 102, loss = 0.49698879\n",
      "Iteration 103, loss = 0.49567290\n",
      "Iteration 104, loss = 0.49557513\n",
      "Iteration 105, loss = 0.49593287\n",
      "Iteration 106, loss = 0.49540395\n",
      "Iteration 107, loss = 0.49537518\n",
      "Iteration 108, loss = 0.49486764\n",
      "Iteration 109, loss = 0.49473555\n",
      "Iteration 110, loss = 0.49494986\n",
      "Iteration 111, loss = 0.49464911\n",
      "Iteration 112, loss = 0.49488962\n",
      "Iteration 113, loss = 0.49425793\n",
      "Iteration 114, loss = 0.49416696\n",
      "Iteration 115, loss = 0.49404825\n",
      "Iteration 116, loss = 0.49400110\n",
      "Iteration 117, loss = 0.49363418\n",
      "Iteration 118, loss = 0.49395222\n",
      "Iteration 119, loss = 0.49364910\n",
      "Iteration 120, loss = 0.49336853\n",
      "Iteration 121, loss = 0.49350607\n",
      "Iteration 122, loss = 0.49352093\n",
      "Iteration 123, loss = 0.49326119\n",
      "Iteration 124, loss = 0.49303418\n",
      "Iteration 125, loss = 0.49285451\n",
      "Iteration 126, loss = 0.49311379\n",
      "Iteration 127, loss = 0.49260133\n",
      "Iteration 128, loss = 0.49276401\n",
      "Iteration 129, loss = 0.49251297\n",
      "Iteration 130, loss = 0.49231235\n",
      "Iteration 131, loss = 0.49205188\n",
      "Iteration 132, loss = 0.49198437\n",
      "Iteration 133, loss = 0.49180873\n",
      "Iteration 134, loss = 0.49190457\n",
      "Iteration 135, loss = 0.49164449\n",
      "Iteration 136, loss = 0.49166857\n",
      "Iteration 137, loss = 0.49133276\n",
      "Iteration 138, loss = 0.49108993\n",
      "Iteration 139, loss = 0.49146730\n",
      "Iteration 140, loss = 0.49156895\n",
      "Iteration 141, loss = 0.49103121\n",
      "Iteration 142, loss = 0.49079464\n",
      "Iteration 143, loss = 0.49074068\n",
      "Iteration 144, loss = 0.49064494\n",
      "Iteration 145, loss = 0.49062294\n",
      "Iteration 146, loss = 0.49057625\n",
      "Iteration 147, loss = 0.48997442\n",
      "Iteration 148, loss = 0.49036509\n",
      "Iteration 149, loss = 0.49003961\n",
      "Iteration 150, loss = 0.49019461\n",
      "Iteration 151, loss = 0.48984487\n",
      "Iteration 152, loss = 0.49012647\n",
      "Iteration 153, loss = 0.48998299\n",
      "Iteration 154, loss = 0.49029254\n",
      "Iteration 155, loss = 0.48985385\n",
      "Iteration 156, loss = 0.48970973\n",
      "Iteration 157, loss = 0.48961646\n",
      "Iteration 158, loss = 0.48944952\n",
      "Iteration 159, loss = 0.48922471\n",
      "Iteration 160, loss = 0.48898494\n",
      "Iteration 161, loss = 0.48925263\n",
      "Iteration 162, loss = 0.48874048\n",
      "Iteration 163, loss = 0.48872308\n",
      "Iteration 164, loss = 0.48893858\n",
      "Iteration 165, loss = 0.48905888\n",
      "Iteration 166, loss = 0.48857077\n",
      "Iteration 167, loss = 0.48876423\n",
      "Iteration 168, loss = 0.48817176\n",
      "Iteration 169, loss = 0.48821726\n",
      "Iteration 170, loss = 0.48796818\n",
      "Iteration 171, loss = 0.48807771\n",
      "Iteration 172, loss = 0.48803445\n",
      "Iteration 173, loss = 0.48803950\n",
      "Iteration 174, loss = 0.48775004\n",
      "Iteration 175, loss = 0.48791641\n",
      "Iteration 176, loss = 0.48764496\n",
      "Iteration 177, loss = 0.48735114\n",
      "Iteration 178, loss = 0.48760830\n",
      "Iteration 179, loss = 0.48751213\n",
      "Iteration 180, loss = 0.48823234\n",
      "Iteration 181, loss = 0.48708977\n",
      "Iteration 182, loss = 0.48703540\n",
      "Iteration 183, loss = 0.48686020\n",
      "Iteration 184, loss = 0.48689643\n",
      "Iteration 185, loss = 0.48689086\n",
      "Iteration 186, loss = 0.48698419\n",
      "Iteration 187, loss = 0.48679119\n",
      "Iteration 188, loss = 0.48660924\n",
      "Iteration 189, loss = 0.48660083\n",
      "Iteration 190, loss = 0.48652399\n",
      "Iteration 191, loss = 0.48623423\n",
      "Iteration 192, loss = 0.48602053\n",
      "Iteration 193, loss = 0.48658677\n",
      "Iteration 194, loss = 0.48640647\n",
      "Iteration 195, loss = 0.48580911\n",
      "Iteration 196, loss = 0.48565781\n",
      "Iteration 197, loss = 0.48540975\n",
      "Iteration 198, loss = 0.48552472\n",
      "Iteration 199, loss = 0.48536823\n",
      "Iteration 200, loss = 0.48510571\n",
      "Iteration 201, loss = 0.48512162\n",
      "Iteration 202, loss = 0.48493292\n",
      "Iteration 203, loss = 0.48497421\n",
      "Iteration 204, loss = 0.48502026\n",
      "Iteration 205, loss = 0.48483095\n",
      "Iteration 206, loss = 0.48468925\n",
      "Iteration 207, loss = 0.48508683\n",
      "Iteration 208, loss = 0.48460218\n",
      "Iteration 209, loss = 0.48456016\n",
      "Iteration 210, loss = 0.48431999\n",
      "Iteration 211, loss = 0.48432152\n",
      "Iteration 212, loss = 0.48408119\n",
      "Iteration 213, loss = 0.48408001\n",
      "Iteration 214, loss = 0.48410147\n",
      "Iteration 215, loss = 0.48373726\n",
      "Iteration 216, loss = 0.48383655\n",
      "Iteration 217, loss = 0.48417351\n",
      "Iteration 218, loss = 0.48403148\n",
      "Iteration 219, loss = 0.48360183\n",
      "Iteration 220, loss = 0.48336161\n",
      "Iteration 221, loss = 0.48326394\n",
      "Iteration 222, loss = 0.48338569\n",
      "Iteration 223, loss = 0.48337704\n",
      "Iteration 224, loss = 0.48306258\n",
      "Iteration 225, loss = 0.48294203\n",
      "Iteration 226, loss = 0.48326401\n",
      "Iteration 227, loss = 0.48313171\n",
      "Iteration 228, loss = 0.48283012\n",
      "Iteration 229, loss = 0.48306792\n",
      "Iteration 230, loss = 0.48270394\n",
      "Iteration 231, loss = 0.48245479\n",
      "Iteration 232, loss = 0.48270002\n",
      "Iteration 233, loss = 0.48279880\n",
      "Iteration 234, loss = 0.48235991\n",
      "Iteration 235, loss = 0.48214452\n",
      "Iteration 236, loss = 0.48297559\n",
      "Iteration 237, loss = 0.48198456\n",
      "Iteration 238, loss = 0.48204813\n",
      "Iteration 239, loss = 0.48197126\n",
      "Iteration 240, loss = 0.48190865\n",
      "Iteration 241, loss = 0.48203616\n",
      "Iteration 242, loss = 0.48220808\n",
      "Iteration 243, loss = 0.48252646\n",
      "Iteration 244, loss = 0.48159103\n",
      "Iteration 245, loss = 0.48165938\n",
      "Iteration 246, loss = 0.48173436\n",
      "Iteration 247, loss = 0.48170439\n",
      "Iteration 248, loss = 0.48150369\n",
      "Iteration 249, loss = 0.48126129\n",
      "Iteration 250, loss = 0.48133478\n",
      "Iteration 251, loss = 0.48118014\n",
      "Iteration 252, loss = 0.48125596\n",
      "Iteration 253, loss = 0.48103473\n",
      "Iteration 254, loss = 0.48120158\n",
      "Iteration 255, loss = 0.48175290\n",
      "Iteration 256, loss = 0.48163775\n",
      "Iteration 257, loss = 0.48096310\n",
      "Iteration 258, loss = 0.48132676\n",
      "Iteration 259, loss = 0.48063626\n",
      "Iteration 260, loss = 0.48094158\n",
      "Iteration 261, loss = 0.48070285\n",
      "Iteration 262, loss = 0.48074984\n",
      "Iteration 263, loss = 0.48072611\n",
      "Iteration 264, loss = 0.48055895\n",
      "Iteration 265, loss = 0.48062747\n",
      "Iteration 266, loss = 0.48066519\n",
      "Iteration 267, loss = 0.48019528\n",
      "Iteration 268, loss = 0.48046223\n",
      "Iteration 269, loss = 0.48009209\n",
      "Iteration 270, loss = 0.48049501\n",
      "Iteration 271, loss = 0.48037461\n",
      "Iteration 272, loss = 0.48020478\n",
      "Iteration 273, loss = 0.48009576\n",
      "Iteration 274, loss = 0.48023796\n",
      "Iteration 275, loss = 0.48011865\n",
      "Iteration 276, loss = 0.48003596\n",
      "Iteration 277, loss = 0.47999111\n",
      "Iteration 278, loss = 0.47996510\n",
      "Iteration 279, loss = 0.48002615\n",
      "Iteration 280, loss = 0.47987985\n",
      "Iteration 281, loss = 0.47957421\n",
      "Iteration 282, loss = 0.47965164\n",
      "Iteration 283, loss = 0.47975788\n",
      "Iteration 284, loss = 0.47970461\n",
      "Iteration 285, loss = 0.47974968\n",
      "Iteration 286, loss = 0.47967820\n",
      "Iteration 287, loss = 0.48001483\n",
      "Iteration 288, loss = 0.47949459\n",
      "Iteration 289, loss = 0.47932634\n",
      "Iteration 290, loss = 0.47943813\n",
      "Iteration 291, loss = 0.47915669\n",
      "Iteration 292, loss = 0.47916269\n",
      "Iteration 293, loss = 0.47955788\n",
      "Iteration 294, loss = 0.47925020\n",
      "Iteration 295, loss = 0.47929750\n",
      "Iteration 296, loss = 0.47863880\n",
      "Iteration 297, loss = 0.47915683\n",
      "Iteration 298, loss = 0.47901155\n",
      "Iteration 299, loss = 0.47907258\n",
      "Iteration 300, loss = 0.47852514\n",
      "Iteration 301, loss = 0.47874367\n",
      "Iteration 302, loss = 0.47861070\n",
      "Iteration 303, loss = 0.47843801\n",
      "Iteration 304, loss = 0.47846256\n",
      "Iteration 305, loss = 0.47835436\n",
      "Iteration 306, loss = 0.47851638\n",
      "Iteration 307, loss = 0.47823757\n",
      "Iteration 308, loss = 0.47865813\n",
      "Iteration 309, loss = 0.47814810\n",
      "Iteration 310, loss = 0.47882065\n",
      "Iteration 311, loss = 0.47891023\n",
      "Iteration 312, loss = 0.47793700\n",
      "Iteration 313, loss = 0.47819891\n",
      "Iteration 314, loss = 0.47766950\n",
      "Iteration 315, loss = 0.47811635\n",
      "Iteration 316, loss = 0.47774877\n",
      "Iteration 317, loss = 0.47808001\n",
      "Iteration 318, loss = 0.47877255\n",
      "Iteration 319, loss = 0.47804816\n",
      "Iteration 320, loss = 0.47794010\n",
      "Iteration 321, loss = 0.47797697\n",
      "Iteration 322, loss = 0.47782741\n",
      "Iteration 323, loss = 0.47767935\n",
      "Iteration 324, loss = 0.47790140\n",
      "Iteration 325, loss = 0.47771395\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67980410\n",
      "Iteration 2, loss = 0.66649483\n",
      "Iteration 3, loss = 0.65346431\n",
      "Iteration 4, loss = 0.63985644\n",
      "Iteration 5, loss = 0.62676883\n",
      "Iteration 6, loss = 0.61437166\n",
      "Iteration 7, loss = 0.60287205\n",
      "Iteration 8, loss = 0.59310884\n",
      "Iteration 9, loss = 0.58484761\n",
      "Iteration 10, loss = 0.57785393\n",
      "Iteration 11, loss = 0.57255997\n",
      "Iteration 12, loss = 0.56781217\n",
      "Iteration 13, loss = 0.56418929\n",
      "Iteration 14, loss = 0.56107383\n",
      "Iteration 15, loss = 0.55833995\n",
      "Iteration 16, loss = 0.55588630\n",
      "Iteration 17, loss = 0.55364243\n",
      "Iteration 18, loss = 0.55143342\n",
      "Iteration 19, loss = 0.54945479\n",
      "Iteration 20, loss = 0.54731210\n",
      "Iteration 21, loss = 0.54546042\n",
      "Iteration 22, loss = 0.54327817\n",
      "Iteration 23, loss = 0.54143244\n",
      "Iteration 24, loss = 0.53921306\n",
      "Iteration 25, loss = 0.53707587\n",
      "Iteration 26, loss = 0.53506566\n",
      "Iteration 27, loss = 0.53297955\n",
      "Iteration 28, loss = 0.53078118\n",
      "Iteration 29, loss = 0.52902999\n",
      "Iteration 30, loss = 0.52723429\n",
      "Iteration 31, loss = 0.52500819\n",
      "Iteration 32, loss = 0.52353871\n",
      "Iteration 33, loss = 0.52172250\n",
      "Iteration 34, loss = 0.52009041\n",
      "Iteration 35, loss = 0.51857201\n",
      "Iteration 36, loss = 0.51714382\n",
      "Iteration 37, loss = 0.51598885\n",
      "Iteration 38, loss = 0.51430462\n",
      "Iteration 39, loss = 0.51304248\n",
      "Iteration 40, loss = 0.51187644\n",
      "Iteration 41, loss = 0.51072718\n",
      "Iteration 42, loss = 0.50963741\n",
      "Iteration 43, loss = 0.50881309\n",
      "Iteration 44, loss = 0.50793558\n",
      "Iteration 45, loss = 0.50716393\n",
      "Iteration 46, loss = 0.50636300\n",
      "Iteration 47, loss = 0.50560689\n",
      "Iteration 48, loss = 0.50493126\n",
      "Iteration 49, loss = 0.50423716\n",
      "Iteration 50, loss = 0.50346796\n",
      "Iteration 51, loss = 0.50285965\n",
      "Iteration 52, loss = 0.50236811\n",
      "Iteration 53, loss = 0.50177805\n",
      "Iteration 54, loss = 0.50176334\n",
      "Iteration 55, loss = 0.50067082\n",
      "Iteration 56, loss = 0.50042357\n",
      "Iteration 57, loss = 0.49986370\n",
      "Iteration 58, loss = 0.49956407\n",
      "Iteration 59, loss = 0.49928431\n",
      "Iteration 60, loss = 0.49870984\n",
      "Iteration 61, loss = 0.49858875\n",
      "Iteration 62, loss = 0.49831195\n",
      "Iteration 63, loss = 0.49793182\n",
      "Iteration 64, loss = 0.49782566\n",
      "Iteration 65, loss = 0.49735931\n",
      "Iteration 66, loss = 0.49750852\n",
      "Iteration 67, loss = 0.49669031\n",
      "Iteration 68, loss = 0.49661193\n",
      "Iteration 69, loss = 0.49655183\n",
      "Iteration 70, loss = 0.49647336\n",
      "Iteration 71, loss = 0.49588467\n",
      "Iteration 72, loss = 0.49566711\n",
      "Iteration 73, loss = 0.49545405\n",
      "Iteration 74, loss = 0.49560095\n",
      "Iteration 75, loss = 0.49503663\n",
      "Iteration 76, loss = 0.49525925\n",
      "Iteration 77, loss = 0.49484036\n",
      "Iteration 78, loss = 0.49453923\n",
      "Iteration 79, loss = 0.49425640\n",
      "Iteration 80, loss = 0.49417414\n",
      "Iteration 81, loss = 0.49413350\n",
      "Iteration 82, loss = 0.49374939\n",
      "Iteration 83, loss = 0.49364047\n",
      "Iteration 84, loss = 0.49364424\n",
      "Iteration 85, loss = 0.49340191\n",
      "Iteration 86, loss = 0.49346437\n",
      "Iteration 87, loss = 0.49320404\n",
      "Iteration 88, loss = 0.49291606\n",
      "Iteration 89, loss = 0.49313749\n",
      "Iteration 90, loss = 0.49297803\n",
      "Iteration 91, loss = 0.49269449\n",
      "Iteration 92, loss = 0.49259404\n",
      "Iteration 93, loss = 0.49255489\n",
      "Iteration 94, loss = 0.49254831\n",
      "Iteration 95, loss = 0.49234925\n",
      "Iteration 96, loss = 0.49236427\n",
      "Iteration 97, loss = 0.49206287\n",
      "Iteration 98, loss = 0.49195021\n",
      "Iteration 99, loss = 0.49189931\n",
      "Iteration 100, loss = 0.49177928\n",
      "Iteration 101, loss = 0.49165407\n",
      "Iteration 102, loss = 0.49180880\n",
      "Iteration 103, loss = 0.49161651\n",
      "Iteration 104, loss = 0.49142827\n",
      "Iteration 105, loss = 0.49143078\n",
      "Iteration 106, loss = 0.49130842\n",
      "Iteration 107, loss = 0.49142214\n",
      "Iteration 108, loss = 0.49126549\n",
      "Iteration 109, loss = 0.49123301\n",
      "Iteration 110, loss = 0.49127976\n",
      "Iteration 111, loss = 0.49126967\n",
      "Iteration 112, loss = 0.49076706\n",
      "Iteration 113, loss = 0.49102597\n",
      "Iteration 114, loss = 0.49082722\n",
      "Iteration 115, loss = 0.49068830\n",
      "Iteration 116, loss = 0.49060406\n",
      "Iteration 117, loss = 0.49048705\n",
      "Iteration 118, loss = 0.49075117\n",
      "Iteration 119, loss = 0.49048641\n",
      "Iteration 120, loss = 0.48980317\n",
      "Iteration 121, loss = 0.49000812\n",
      "Iteration 122, loss = 0.49016140\n",
      "Iteration 123, loss = 0.48970948\n",
      "Iteration 124, loss = 0.48987237\n",
      "Iteration 125, loss = 0.48952033\n",
      "Iteration 126, loss = 0.48964394\n",
      "Iteration 127, loss = 0.48952145\n",
      "Iteration 128, loss = 0.48920282\n",
      "Iteration 129, loss = 0.48944078\n",
      "Iteration 130, loss = 0.48931384\n",
      "Iteration 131, loss = 0.48919378\n",
      "Iteration 132, loss = 0.48899440\n",
      "Iteration 133, loss = 0.48895667\n",
      "Iteration 134, loss = 0.48874427\n",
      "Iteration 135, loss = 0.48864904\n",
      "Iteration 136, loss = 0.48868138\n",
      "Iteration 137, loss = 0.48850428\n",
      "Iteration 138, loss = 0.48856077\n",
      "Iteration 139, loss = 0.48838722\n",
      "Iteration 140, loss = 0.48824378\n",
      "Iteration 141, loss = 0.48843755\n",
      "Iteration 142, loss = 0.48851777\n",
      "Iteration 143, loss = 0.48819210\n",
      "Iteration 144, loss = 0.48816999\n",
      "Iteration 145, loss = 0.48803986\n",
      "Iteration 146, loss = 0.48801063\n",
      "Iteration 147, loss = 0.48788183\n",
      "Iteration 148, loss = 0.48776411\n",
      "Iteration 149, loss = 0.48776544\n",
      "Iteration 150, loss = 0.48761709\n",
      "Iteration 151, loss = 0.48756485\n",
      "Iteration 152, loss = 0.48741726\n",
      "Iteration 153, loss = 0.48761459\n",
      "Iteration 154, loss = 0.48740709\n",
      "Iteration 155, loss = 0.48760959\n",
      "Iteration 156, loss = 0.48720075\n",
      "Iteration 157, loss = 0.48711536\n",
      "Iteration 158, loss = 0.48709194\n",
      "Iteration 159, loss = 0.48701591\n",
      "Iteration 160, loss = 0.48699899\n",
      "Iteration 161, loss = 0.48707146\n",
      "Iteration 162, loss = 0.48683813\n",
      "Iteration 163, loss = 0.48673520\n",
      "Iteration 164, loss = 0.48668353\n",
      "Iteration 165, loss = 0.48689485\n",
      "Iteration 166, loss = 0.48703120\n",
      "Iteration 167, loss = 0.48653408\n",
      "Iteration 168, loss = 0.48653052\n",
      "Iteration 169, loss = 0.48648515\n",
      "Iteration 170, loss = 0.48637054\n",
      "Iteration 171, loss = 0.48636180\n",
      "Iteration 172, loss = 0.48617154\n",
      "Iteration 173, loss = 0.48627733\n",
      "Iteration 174, loss = 0.48606151\n",
      "Iteration 175, loss = 0.48611892\n",
      "Iteration 176, loss = 0.48603742\n",
      "Iteration 177, loss = 0.48601684\n",
      "Iteration 178, loss = 0.48608039\n",
      "Iteration 179, loss = 0.48625595\n",
      "Iteration 180, loss = 0.48583805\n",
      "Iteration 181, loss = 0.48599722\n",
      "Iteration 182, loss = 0.48580777\n",
      "Iteration 183, loss = 0.48583692\n",
      "Iteration 184, loss = 0.48581397\n",
      "Iteration 185, loss = 0.48588565\n",
      "Iteration 186, loss = 0.48562728\n",
      "Iteration 187, loss = 0.48539113\n",
      "Iteration 188, loss = 0.48533713\n",
      "Iteration 189, loss = 0.48544082\n",
      "Iteration 190, loss = 0.48546246\n",
      "Iteration 191, loss = 0.48544871\n",
      "Iteration 192, loss = 0.48561587\n",
      "Iteration 193, loss = 0.48511740\n",
      "Iteration 194, loss = 0.48536671\n",
      "Iteration 195, loss = 0.48508085\n",
      "Iteration 196, loss = 0.48560091\n",
      "Iteration 197, loss = 0.48516364\n",
      "Iteration 198, loss = 0.48499556\n",
      "Iteration 199, loss = 0.48518224\n",
      "Iteration 200, loss = 0.48482974\n",
      "Iteration 201, loss = 0.48495728\n",
      "Iteration 202, loss = 0.48509329\n",
      "Iteration 203, loss = 0.48461415\n",
      "Iteration 204, loss = 0.48467406\n",
      "Iteration 205, loss = 0.48479725\n",
      "Iteration 206, loss = 0.48512539\n",
      "Iteration 207, loss = 0.48463128\n",
      "Iteration 208, loss = 0.48493044\n",
      "Iteration 209, loss = 0.48443118\n",
      "Iteration 210, loss = 0.48427540\n",
      "Iteration 211, loss = 0.48507665\n",
      "Iteration 212, loss = 0.48436520\n",
      "Iteration 213, loss = 0.48441183\n",
      "Iteration 214, loss = 0.48441476\n",
      "Iteration 215, loss = 0.48436000\n",
      "Iteration 216, loss = 0.48402835\n",
      "Iteration 217, loss = 0.48415917\n",
      "Iteration 218, loss = 0.48411946\n",
      "Iteration 219, loss = 0.48411906\n",
      "Iteration 220, loss = 0.48391816\n",
      "Iteration 221, loss = 0.48404450\n",
      "Iteration 222, loss = 0.48416734\n",
      "Iteration 223, loss = 0.48390311\n",
      "Iteration 224, loss = 0.48372299\n",
      "Iteration 225, loss = 0.48380740\n",
      "Iteration 226, loss = 0.48370218\n",
      "Iteration 227, loss = 0.48377724\n",
      "Iteration 228, loss = 0.48373114\n",
      "Iteration 229, loss = 0.48358486\n",
      "Iteration 230, loss = 0.48358506\n",
      "Iteration 231, loss = 0.48362881\n",
      "Iteration 232, loss = 0.48336229\n",
      "Iteration 233, loss = 0.48328041\n",
      "Iteration 234, loss = 0.48327655\n",
      "Iteration 235, loss = 0.48331140\n",
      "Iteration 236, loss = 0.48330936\n",
      "Iteration 237, loss = 0.48312983\n",
      "Iteration 238, loss = 0.48312483\n",
      "Iteration 239, loss = 0.48330740\n",
      "Iteration 240, loss = 0.48298455\n",
      "Iteration 241, loss = 0.48309491\n",
      "Iteration 242, loss = 0.48303950\n",
      "Iteration 243, loss = 0.48290064\n",
      "Iteration 244, loss = 0.48317366\n",
      "Iteration 245, loss = 0.48304788\n",
      "Iteration 246, loss = 0.48285729\n",
      "Iteration 247, loss = 0.48283585\n",
      "Iteration 248, loss = 0.48256881\n",
      "Iteration 249, loss = 0.48247866\n",
      "Iteration 250, loss = 0.48244877\n",
      "Iteration 251, loss = 0.48240731\n",
      "Iteration 252, loss = 0.48259804\n",
      "Iteration 253, loss = 0.48256595\n",
      "Iteration 254, loss = 0.48229119\n",
      "Iteration 255, loss = 0.48221972\n",
      "Iteration 256, loss = 0.48248326\n",
      "Iteration 257, loss = 0.48209504\n",
      "Iteration 258, loss = 0.48227174\n",
      "Iteration 259, loss = 0.48188890\n",
      "Iteration 260, loss = 0.48225834\n",
      "Iteration 261, loss = 0.48188967\n",
      "Iteration 262, loss = 0.48179305\n",
      "Iteration 263, loss = 0.48164939\n",
      "Iteration 264, loss = 0.48180172\n",
      "Iteration 265, loss = 0.48158417\n",
      "Iteration 266, loss = 0.48151882\n",
      "Iteration 267, loss = 0.48140907\n",
      "Iteration 268, loss = 0.48150573\n",
      "Iteration 269, loss = 0.48161005\n",
      "Iteration 270, loss = 0.48159058\n",
      "Iteration 271, loss = 0.48143206\n",
      "Iteration 272, loss = 0.48107500\n",
      "Iteration 273, loss = 0.48120338\n",
      "Iteration 274, loss = 0.48104549\n",
      "Iteration 275, loss = 0.48100058\n",
      "Iteration 276, loss = 0.48121007\n",
      "Iteration 277, loss = 0.48085789\n",
      "Iteration 278, loss = 0.48093838\n",
      "Iteration 279, loss = 0.48094329\n",
      "Iteration 280, loss = 0.48072165\n",
      "Iteration 281, loss = 0.48076023\n",
      "Iteration 282, loss = 0.48060971\n",
      "Iteration 283, loss = 0.48085367\n",
      "Iteration 284, loss = 0.48083231\n",
      "Iteration 285, loss = 0.48079109\n",
      "Iteration 286, loss = 0.48069615\n",
      "Iteration 287, loss = 0.48024673\n",
      "Iteration 288, loss = 0.48052050\n",
      "Iteration 289, loss = 0.48028877\n",
      "Iteration 290, loss = 0.48031702\n",
      "Iteration 291, loss = 0.48032188\n",
      "Iteration 292, loss = 0.48019893\n",
      "Iteration 293, loss = 0.48091054\n",
      "Iteration 294, loss = 0.48041782\n",
      "Iteration 295, loss = 0.48039971\n",
      "Iteration 296, loss = 0.48036308\n",
      "Iteration 297, loss = 0.48002426\n",
      "Iteration 298, loss = 0.48010665\n",
      "Iteration 299, loss = 0.47971317\n",
      "Iteration 300, loss = 0.47990046\n",
      "Iteration 301, loss = 0.47987716\n",
      "Iteration 302, loss = 0.47973631\n",
      "Iteration 303, loss = 0.47972497\n",
      "Iteration 304, loss = 0.47984318\n",
      "Iteration 305, loss = 0.47977360\n",
      "Iteration 306, loss = 0.47949533\n",
      "Iteration 307, loss = 0.47954802\n",
      "Iteration 308, loss = 0.47953382\n",
      "Iteration 309, loss = 0.47951542\n",
      "Iteration 310, loss = 0.47938079\n",
      "Iteration 311, loss = 0.47951940\n",
      "Iteration 312, loss = 0.47938955\n",
      "Iteration 313, loss = 0.47966553\n",
      "Iteration 314, loss = 0.47933128\n",
      "Iteration 315, loss = 0.47928714\n",
      "Iteration 316, loss = 0.47913464\n",
      "Iteration 317, loss = 0.47911531\n",
      "Iteration 318, loss = 0.47910483\n",
      "Iteration 319, loss = 0.47915155\n",
      "Iteration 320, loss = 0.47909903\n",
      "Iteration 321, loss = 0.47932304\n",
      "Iteration 322, loss = 0.47925259\n",
      "Iteration 323, loss = 0.47908036\n",
      "Iteration 324, loss = 0.47886831\n",
      "Iteration 325, loss = 0.47938255\n",
      "Iteration 326, loss = 0.47881878\n",
      "Iteration 327, loss = 0.47886215\n",
      "Iteration 328, loss = 0.47875125\n",
      "Iteration 329, loss = 0.47873047\n",
      "Iteration 330, loss = 0.47889351\n",
      "Iteration 331, loss = 0.47868788\n",
      "Iteration 332, loss = 0.47870069\n",
      "Iteration 333, loss = 0.47858113\n",
      "Iteration 334, loss = 0.47845979\n",
      "Iteration 335, loss = 0.47855493\n",
      "Iteration 336, loss = 0.47843504\n",
      "Iteration 337, loss = 0.47855948\n",
      "Iteration 338, loss = 0.47848665\n",
      "Iteration 339, loss = 0.47837868\n",
      "Iteration 340, loss = 0.47857706\n",
      "Iteration 341, loss = 0.47845657\n",
      "Iteration 342, loss = 0.47820278\n",
      "Iteration 343, loss = 0.47819219\n",
      "Iteration 344, loss = 0.47833058\n",
      "Iteration 345, loss = 0.47803977\n",
      "Iteration 346, loss = 0.47831990\n",
      "Iteration 347, loss = 0.47805504\n",
      "Iteration 348, loss = 0.47793062\n",
      "Iteration 349, loss = 0.47801427\n",
      "Iteration 350, loss = 0.47780761\n",
      "Iteration 351, loss = 0.47785780\n",
      "Iteration 352, loss = 0.47786254\n",
      "Iteration 353, loss = 0.47798011\n",
      "Iteration 354, loss = 0.47759521\n",
      "Iteration 355, loss = 0.47768446\n",
      "Iteration 356, loss = 0.47802303\n",
      "Iteration 357, loss = 0.47791664\n",
      "Iteration 358, loss = 0.47752389\n",
      "Iteration 359, loss = 0.47787347\n",
      "Iteration 360, loss = 0.47740907\n",
      "Iteration 361, loss = 0.47759899\n",
      "Iteration 362, loss = 0.47772539\n",
      "Iteration 363, loss = 0.47732299\n",
      "Iteration 364, loss = 0.47759886\n",
      "Iteration 365, loss = 0.47726851\n",
      "Iteration 366, loss = 0.47719997\n",
      "Iteration 367, loss = 0.47720124\n",
      "Iteration 368, loss = 0.47720958\n",
      "Iteration 369, loss = 0.47741946\n",
      "Iteration 370, loss = 0.47721931\n",
      "Iteration 371, loss = 0.47676540\n",
      "Iteration 372, loss = 0.47705513\n",
      "Iteration 373, loss = 0.47714050\n",
      "Iteration 374, loss = 0.47684216\n",
      "Iteration 375, loss = 0.47691152\n",
      "Iteration 376, loss = 0.47681447\n",
      "Iteration 377, loss = 0.47667258\n",
      "Iteration 378, loss = 0.47686536\n",
      "Iteration 379, loss = 0.47662651\n",
      "Iteration 380, loss = 0.47671272\n",
      "Iteration 381, loss = 0.47654451\n",
      "Iteration 382, loss = 0.47638822\n",
      "Iteration 383, loss = 0.47662985\n",
      "Iteration 384, loss = 0.47637224\n",
      "Iteration 385, loss = 0.47653319\n",
      "Iteration 386, loss = 0.47620214\n",
      "Iteration 387, loss = 0.47624788\n",
      "Iteration 388, loss = 0.47626230\n",
      "Iteration 389, loss = 0.47612517\n",
      "Iteration 390, loss = 0.47606551\n",
      "Iteration 391, loss = 0.47592252\n",
      "Iteration 392, loss = 0.47607194\n",
      "Iteration 393, loss = 0.47578272\n",
      "Iteration 394, loss = 0.47581244\n",
      "Iteration 395, loss = 0.47572554\n",
      "Iteration 396, loss = 0.47589703\n",
      "Iteration 397, loss = 0.47552781\n",
      "Iteration 398, loss = 0.47552486\n",
      "Iteration 399, loss = 0.47571717\n",
      "Iteration 400, loss = 0.47540954\n",
      "Iteration 401, loss = 0.47556573\n",
      "Iteration 402, loss = 0.47533009\n",
      "Iteration 403, loss = 0.47530466\n",
      "Iteration 404, loss = 0.47548049\n",
      "Iteration 405, loss = 0.47534535\n",
      "Iteration 406, loss = 0.47495550\n",
      "Iteration 407, loss = 0.47491574\n",
      "Iteration 408, loss = 0.47516956\n",
      "Iteration 409, loss = 0.47485264\n",
      "Iteration 410, loss = 0.47491243\n",
      "Iteration 411, loss = 0.47485806\n",
      "Iteration 412, loss = 0.47488933\n",
      "Iteration 413, loss = 0.47467750\n",
      "Iteration 414, loss = 0.47497720\n",
      "Iteration 415, loss = 0.47480684\n",
      "Iteration 416, loss = 0.47484977\n",
      "Iteration 417, loss = 0.47453165\n",
      "Iteration 418, loss = 0.47446364\n",
      "Iteration 419, loss = 0.47461642\n",
      "Iteration 420, loss = 0.47440596\n",
      "Iteration 421, loss = 0.47418001\n",
      "Iteration 422, loss = 0.47415507\n",
      "Iteration 423, loss = 0.47426701\n",
      "Iteration 424, loss = 0.47415700\n",
      "Iteration 425, loss = 0.47419244\n",
      "Iteration 426, loss = 0.47386420\n",
      "Iteration 427, loss = 0.47409983\n",
      "Iteration 428, loss = 0.47402902\n",
      "Iteration 429, loss = 0.47380710\n",
      "Iteration 430, loss = 0.47390156\n",
      "Iteration 431, loss = 0.47393383\n",
      "Iteration 432, loss = 0.47394835\n",
      "Iteration 433, loss = 0.47381804\n",
      "Iteration 434, loss = 0.47349684\n",
      "Iteration 435, loss = 0.47357081\n",
      "Iteration 436, loss = 0.47357996\n",
      "Iteration 437, loss = 0.47350935\n",
      "Iteration 438, loss = 0.47341733\n",
      "Iteration 439, loss = 0.47340057\n",
      "Iteration 440, loss = 0.47388902\n",
      "Iteration 441, loss = 0.47340614\n",
      "Iteration 442, loss = 0.47322154\n",
      "Iteration 443, loss = 0.47333004\n",
      "Iteration 444, loss = 0.47325233\n",
      "Iteration 445, loss = 0.47313087\n",
      "Iteration 446, loss = 0.47299892\n",
      "Iteration 447, loss = 0.47303896\n",
      "Iteration 448, loss = 0.47288926\n",
      "Iteration 449, loss = 0.47288801\n",
      "Iteration 450, loss = 0.47278141\n",
      "Iteration 451, loss = 0.47270888\n",
      "Iteration 452, loss = 0.47320778\n",
      "Iteration 453, loss = 0.47269532\n",
      "Iteration 454, loss = 0.47256029\n",
      "Iteration 455, loss = 0.47254899\n",
      "Iteration 456, loss = 0.47275193\n",
      "Iteration 457, loss = 0.47241197\n",
      "Iteration 458, loss = 0.47243336\n",
      "Iteration 459, loss = 0.47222177\n",
      "Iteration 460, loss = 0.47233744\n",
      "Iteration 461, loss = 0.47226075\n",
      "Iteration 462, loss = 0.47232386\n",
      "Iteration 463, loss = 0.47241988\n",
      "Iteration 464, loss = 0.47207724\n",
      "Iteration 465, loss = 0.47213009\n",
      "Iteration 466, loss = 0.47202702\n",
      "Iteration 467, loss = 0.47224329\n",
      "Iteration 468, loss = 0.47200628\n",
      "Iteration 469, loss = 0.47198158\n",
      "Iteration 470, loss = 0.47199916\n",
      "Iteration 471, loss = 0.47201615\n",
      "Iteration 472, loss = 0.47179174\n",
      "Iteration 473, loss = 0.47184809\n",
      "Iteration 474, loss = 0.47175250\n",
      "Iteration 475, loss = 0.47169515\n",
      "Iteration 476, loss = 0.47173369\n",
      "Iteration 477, loss = 0.47181718\n",
      "Iteration 478, loss = 0.47169178\n",
      "Iteration 479, loss = 0.47161802\n",
      "Iteration 480, loss = 0.47154508\n",
      "Iteration 481, loss = 0.47158439\n",
      "Iteration 482, loss = 0.47158448\n",
      "Iteration 483, loss = 0.47141126\n",
      "Iteration 484, loss = 0.47137065\n",
      "Iteration 485, loss = 0.47176977\n",
      "Iteration 486, loss = 0.47157929\n",
      "Iteration 487, loss = 0.47125176\n",
      "Iteration 488, loss = 0.47141196\n",
      "Iteration 489, loss = 0.47116712\n",
      "Iteration 490, loss = 0.47132698\n",
      "Iteration 491, loss = 0.47136795\n",
      "Iteration 492, loss = 0.47103927\n",
      "Iteration 493, loss = 0.47122959\n",
      "Iteration 494, loss = 0.47120443\n",
      "Iteration 495, loss = 0.47120349\n",
      "Iteration 496, loss = 0.47107045\n",
      "Iteration 497, loss = 0.47088307\n",
      "Iteration 498, loss = 0.47091047\n",
      "Iteration 499, loss = 0.47101148\n",
      "Iteration 500, loss = 0.47097183\n",
      "Iteration 501, loss = 0.47109543\n",
      "Iteration 502, loss = 0.47094549\n",
      "Iteration 503, loss = 0.47127650\n",
      "Iteration 504, loss = 0.47092280\n",
      "Iteration 505, loss = 0.47087116\n",
      "Iteration 506, loss = 0.47101239\n",
      "Iteration 507, loss = 0.47058693\n",
      "Iteration 508, loss = 0.47083628\n",
      "Iteration 509, loss = 0.47054921\n",
      "Iteration 510, loss = 0.47090343\n",
      "Iteration 511, loss = 0.47109487\n",
      "Iteration 512, loss = 0.47056249\n",
      "Iteration 513, loss = 0.47048396\n",
      "Iteration 514, loss = 0.47074886\n",
      "Iteration 515, loss = 0.47052165\n",
      "Iteration 516, loss = 0.47041242\n",
      "Iteration 517, loss = 0.47054845\n",
      "Iteration 518, loss = 0.47030195\n",
      "Iteration 519, loss = 0.47049162\n",
      "Iteration 520, loss = 0.47023851\n",
      "Iteration 521, loss = 0.47030300\n",
      "Iteration 522, loss = 0.47075874\n",
      "Iteration 523, loss = 0.47020154\n",
      "Iteration 524, loss = 0.47025795\n",
      "Iteration 525, loss = 0.47066210\n",
      "Iteration 526, loss = 0.46997213\n",
      "Iteration 527, loss = 0.47035953\n",
      "Iteration 528, loss = 0.47004567\n",
      "Iteration 529, loss = 0.47037866\n",
      "Iteration 530, loss = 0.47008832\n",
      "Iteration 531, loss = 0.47001575\n",
      "Iteration 532, loss = 0.47021175\n",
      "Iteration 533, loss = 0.46969715\n",
      "Iteration 534, loss = 0.47013547\n",
      "Iteration 535, loss = 0.46957673\n",
      "Iteration 536, loss = 0.46978568\n",
      "Iteration 537, loss = 0.47007288\n",
      "Iteration 538, loss = 0.46995650\n",
      "Iteration 539, loss = 0.46969075\n",
      "Iteration 540, loss = 0.46980665\n",
      "Iteration 541, loss = 0.46984966\n",
      "Iteration 542, loss = 0.46947041\n",
      "Iteration 543, loss = 0.46949948\n",
      "Iteration 544, loss = 0.46976495\n",
      "Iteration 545, loss = 0.46954711\n",
      "Iteration 546, loss = 0.46956537\n",
      "Iteration 547, loss = 0.46931952\n",
      "Iteration 548, loss = 0.46943244\n",
      "Iteration 549, loss = 0.46930155\n",
      "Iteration 550, loss = 0.46970297\n",
      "Iteration 551, loss = 0.46954776\n",
      "Iteration 552, loss = 0.46947172\n",
      "Iteration 553, loss = 0.46944430\n",
      "Iteration 554, loss = 0.46900111\n",
      "Iteration 555, loss = 0.46913852\n",
      "Iteration 556, loss = 0.46908506\n",
      "Iteration 557, loss = 0.46897897\n",
      "Iteration 558, loss = 0.46905833\n",
      "Iteration 559, loss = 0.46912179\n",
      "Iteration 560, loss = 0.46929637\n",
      "Iteration 561, loss = 0.46889118\n",
      "Iteration 562, loss = 0.46890859\n",
      "Iteration 563, loss = 0.46898972\n",
      "Iteration 564, loss = 0.46916028\n",
      "Iteration 565, loss = 0.46875969\n",
      "Iteration 566, loss = 0.46896112\n",
      "Iteration 567, loss = 0.46867630\n",
      "Iteration 568, loss = 0.46870118\n",
      "Iteration 569, loss = 0.46956947\n",
      "Iteration 570, loss = 0.46879342\n",
      "Iteration 571, loss = 0.46861729\n",
      "Iteration 572, loss = 0.46873568\n",
      "Iteration 573, loss = 0.46854033\n",
      "Iteration 574, loss = 0.46873776\n",
      "Iteration 575, loss = 0.46866645\n",
      "Iteration 576, loss = 0.46857401\n",
      "Iteration 577, loss = 0.46848307\n",
      "Iteration 578, loss = 0.46876168\n",
      "Iteration 579, loss = 0.46867325\n",
      "Iteration 580, loss = 0.46841877\n",
      "Iteration 581, loss = 0.46834038\n",
      "Iteration 582, loss = 0.46871996\n",
      "Iteration 583, loss = 0.46818919\n",
      "Iteration 584, loss = 0.46836496\n",
      "Iteration 585, loss = 0.46833553\n",
      "Iteration 586, loss = 0.46837477\n",
      "Iteration 587, loss = 0.46829343\n",
      "Iteration 588, loss = 0.46853890\n",
      "Iteration 589, loss = 0.46870930\n",
      "Iteration 590, loss = 0.46823325\n",
      "Iteration 591, loss = 0.46817806\n",
      "Iteration 592, loss = 0.46814250\n",
      "Iteration 593, loss = 0.46810722\n",
      "Iteration 594, loss = 0.46833666\n",
      "Iteration 595, loss = 0.46781426\n",
      "Iteration 596, loss = 0.46785871\n",
      "Iteration 597, loss = 0.46817298\n",
      "Iteration 598, loss = 0.46810080\n",
      "Iteration 599, loss = 0.46803638\n",
      "Iteration 600, loss = 0.46803218\n",
      "Iteration 601, loss = 0.46813639\n",
      "Iteration 602, loss = 0.46766858\n",
      "Iteration 603, loss = 0.46817252\n",
      "Iteration 604, loss = 0.46778267\n",
      "Iteration 605, loss = 0.46822565\n",
      "Iteration 606, loss = 0.46774786\n",
      "Iteration 607, loss = 0.46804606\n",
      "Iteration 608, loss = 0.46788300\n",
      "Iteration 609, loss = 0.46794586\n",
      "Iteration 610, loss = 0.46778613\n",
      "Iteration 611, loss = 0.46755908\n",
      "Iteration 612, loss = 0.46757550\n",
      "Iteration 613, loss = 0.46765404\n",
      "Iteration 614, loss = 0.46780064\n",
      "Iteration 615, loss = 0.46770881\n",
      "Iteration 616, loss = 0.46744694\n",
      "Iteration 617, loss = 0.46778707\n",
      "Iteration 618, loss = 0.46815789\n",
      "Iteration 619, loss = 0.46800241\n",
      "Iteration 620, loss = 0.46751275\n",
      "Iteration 621, loss = 0.46772646\n",
      "Iteration 622, loss = 0.46758152\n",
      "Iteration 623, loss = 0.46756179\n",
      "Iteration 624, loss = 0.46741364\n",
      "Iteration 625, loss = 0.46782477\n",
      "Iteration 626, loss = 0.46786068\n",
      "Iteration 627, loss = 0.46762753\n",
      "Iteration 628, loss = 0.46767196\n",
      "Iteration 629, loss = 0.46778164\n",
      "Iteration 630, loss = 0.46746533\n",
      "Iteration 631, loss = 0.46758586\n",
      "Iteration 632, loss = 0.46782260\n",
      "Iteration 633, loss = 0.46777311\n",
      "Iteration 634, loss = 0.46745991\n",
      "Iteration 635, loss = 0.46737302\n",
      "Iteration 636, loss = 0.46755884\n",
      "Iteration 637, loss = 0.46753677\n",
      "Iteration 638, loss = 0.46731906\n",
      "Iteration 639, loss = 0.46759341\n",
      "Iteration 640, loss = 0.46828905\n",
      "Iteration 641, loss = 0.46761396\n",
      "Iteration 642, loss = 0.46747842\n",
      "Iteration 643, loss = 0.46709628\n",
      "Iteration 644, loss = 0.46737903\n",
      "Iteration 645, loss = 0.46757925\n",
      "Iteration 646, loss = 0.46747110\n",
      "Iteration 647, loss = 0.46748634\n",
      "Iteration 648, loss = 0.46736151\n",
      "Iteration 649, loss = 0.46708625\n",
      "Iteration 650, loss = 0.46738834\n",
      "Iteration 651, loss = 0.46744870\n",
      "Iteration 652, loss = 0.46820275\n",
      "Iteration 653, loss = 0.46726719\n",
      "Iteration 654, loss = 0.46713939\n",
      "Iteration 655, loss = 0.46712070\n",
      "Iteration 656, loss = 0.46712374\n",
      "Iteration 657, loss = 0.46714689\n",
      "Iteration 658, loss = 0.46711786\n",
      "Iteration 659, loss = 0.46729953\n",
      "Iteration 660, loss = 0.46709046\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "scores_ann = cross_val_score(model_ann, X_customer_balanced, Y_customer_balanced, cv=kf_ann, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.74234694 0.76020408 0.73979592 0.75       0.69897959 0.71938776\n",
      " 0.74489796 0.77295918 0.7314578  0.7544757 ]\n",
      "Score médio: 0.7414504932407746\n",
      "Desvio padrão: 0.01998265962323871\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_ann}\")\n",
    "print(f\"Score médio: {np.mean(scores_ann)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_ann)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - 66%(Normal) 72%(Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "model_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted\n",
    "model_tree_boosted = DecisionTreeClassifier(criterion='gini', splitter='random', min_samples_leaf=10, min_samples_split=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_tree = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree = cross_val_score(model_tree, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.6505102  0.65306122 0.66071429 0.67091837 0.62755102 0.63010204\n",
      " 0.68367347 0.69642857 0.69309463 0.68030691]\n",
      "Score médio: 0.6646360718200324\n",
      "Desvio padrão: 0.023205420177937815\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree_boosted = cross_val_score(model_tree_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70918367 0.70153061 0.73979592 0.73469388 0.72193878 0.69897959\n",
      " 0.70918367 0.7627551  0.73401535 0.73657289]\n",
      "Score médio: 0.7248649459783915\n",
      "Desvio padrão: 0.019211770606043753\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "parametros = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.7253707613313524\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculando desvio padrão\n",
    "# std_dev = np.std(scores_tree_boosted)\n",
    "\n",
    "# # Plotando o desvio padrão\n",
    "# plt.bar('Desvio Padrão', std_dev)\n",
    "# plt.ylabel('Valor')\n",
    "# plt.title('Desvio Padrão dos Scores')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p = 2)\n",
    "kf_knn = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn = cross_val_score(model_knn, X_customer_balanced, Y_customer_balanced, cv=kf_knn, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70153061 0.70408163 0.72704082 0.69132653 0.68877551 0.66071429\n",
      " 0.68367347 0.75510204 0.70588235 0.71611253]\n",
      "Score médio: 0.703423978286967\n",
      "Desvio padrão: 0.02444292091659416\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_knn}\")\n",
    "print(f\"Score médio: {np.mean(scores_knn)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(random_state=42, max_iter=150)\n",
    "kf_logistic = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic = cross_val_score(model_logistic, X_customer_balanced, Y_customer_balanced, cv=kf_logistic, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.67602041 0.70663265 0.71938776 0.71683673 0.66326531 0.67091837\n",
      " 0.72959184 0.75255102 0.71355499 0.74680307]\n",
      "Score médio: 0.7095562137898638\n",
      "Desvio padrão: 0.029277793572685756\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_logistic}\")\n",
    "print(f\"Score médio: {np.mean(scores_logistic)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_logistic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = GaussianNB()\n",
    "kf_naive = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = cross_val_score(model_naive, X_customer_balanced, Y_customer_balanced, cv=kf_naive, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.68877551 0.7244898  0.70918367 0.73214286 0.68877551 0.68112245\n",
      " 0.72193878 0.77295918 0.72634271 0.75959079]\n",
      "Score médio: 0.7205321258938358\n",
      "Desvio padrão: 0.028564427059807447\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_naive}\")\n",
    "print(f\"Score médio: {np.mean(scores_naive)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_naive)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(n_estimators=80, criterion='entropy', random_state=42)\n",
    "kf_forest = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_forest = cross_val_score(model_forest, X_customer_balanced, Y_customer_balanced, cv=kf_forest, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.75       0.75765306 0.7627551  0.75       0.7372449  0.72193878\n",
      " 0.73469388 0.78316327 0.76470588 0.76982097]\n",
      "Score médio: 0.7531975833811785\n",
      "Desvio padrão: 0.01735613718267566\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_forest}\")\n",
    "print(f\"Score médio: {np.mean(scores_forest)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_forest)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
