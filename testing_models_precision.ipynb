{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('customer.pkl', 'rb') as f:\n",
    "    X_customer_balanced, Y_customer_balanced = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *SVM - 74,17%(Normal) 74,96%(Boosted)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3918, 10), (3918,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_customer_balanced.shape, Y_customer_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', random_state=42, C=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boosted = SVC(kernel='rbf', random_state=42, C=10.0, tol=0.001, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_customer_balanced, Y_customer_balanced, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_boosted = cross_val_score(model_boosted, X_customer_balanced, Y_customer_balanced, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.7372449  0.75       0.75255102 0.73469388 0.72704082 0.69897959\n",
      " 0.73469388 0.78316327 0.72890026 0.76982097]\n",
      "Score médio: 0.7417088574560259\n",
      "Desvio padrão: 0.022401792602462955\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores}\")\n",
    "print(f\"Score médio: {np.mean(scores)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.73979592 0.75       0.76785714 0.75255102 0.71938776 0.70918367\n",
      " 0.75       0.78316327 0.75191816 0.77237852]\n",
      "Score médio: 0.749623545070202\n",
      "Desvio padrão: 0.02153010197017537\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularização\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],  # Kernels mais comuns\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.001],  # Para 'rbf' kernel\n",
    "    'tol': [0.001, 0.0001]  # Tolerância de otimização\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Artificial Neural Network - 74,5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ann_boosted = MLPClassifier(max_iter=1500, verbose=True, solver='adam', activation='relu', hidden_layer_sizes=(50, 50), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = MLPClassifier(max_iter=1500, verbose=True, tol=0.000000, solver='adam', activation='relu', hidden_layer_sizes=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_ann = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73863469\n",
      "Iteration 2, loss = 0.71626398\n",
      "Iteration 3, loss = 0.69898458\n",
      "Iteration 4, loss = 0.68418740\n",
      "Iteration 5, loss = 0.67120495\n",
      "Iteration 6, loss = 0.65907086\n",
      "Iteration 7, loss = 0.64748423\n",
      "Iteration 8, loss = 0.63641259\n",
      "Iteration 9, loss = 0.62541587\n",
      "Iteration 10, loss = 0.61502725\n",
      "Iteration 11, loss = 0.60497337\n",
      "Iteration 12, loss = 0.59606421\n",
      "Iteration 13, loss = 0.58805333\n",
      "Iteration 14, loss = 0.58129999\n",
      "Iteration 15, loss = 0.57499346\n",
      "Iteration 16, loss = 0.56989449\n",
      "Iteration 17, loss = 0.56546591\n",
      "Iteration 18, loss = 0.56181457\n",
      "Iteration 19, loss = 0.55838251\n",
      "Iteration 20, loss = 0.55552057\n",
      "Iteration 21, loss = 0.55296423\n",
      "Iteration 22, loss = 0.55061383\n",
      "Iteration 23, loss = 0.54860011\n",
      "Iteration 24, loss = 0.54653116\n",
      "Iteration 25, loss = 0.54468255\n",
      "Iteration 26, loss = 0.54284336\n",
      "Iteration 27, loss = 0.54066644\n",
      "Iteration 28, loss = 0.53881459\n",
      "Iteration 29, loss = 0.53694524\n",
      "Iteration 30, loss = 0.53503057\n",
      "Iteration 31, loss = 0.53364282\n",
      "Iteration 32, loss = 0.53155326\n",
      "Iteration 33, loss = 0.52999288\n",
      "Iteration 34, loss = 0.52851929\n",
      "Iteration 35, loss = 0.52694586\n",
      "Iteration 36, loss = 0.52521763\n",
      "Iteration 37, loss = 0.52337585\n",
      "Iteration 38, loss = 0.52188382\n",
      "Iteration 39, loss = 0.52049747\n",
      "Iteration 40, loss = 0.51904764\n",
      "Iteration 41, loss = 0.51764110\n",
      "Iteration 42, loss = 0.51643234\n",
      "Iteration 43, loss = 0.51527381\n",
      "Iteration 44, loss = 0.51366234\n",
      "Iteration 45, loss = 0.51269123\n",
      "Iteration 46, loss = 0.51146826\n",
      "Iteration 47, loss = 0.51037211\n",
      "Iteration 48, loss = 0.50922122\n",
      "Iteration 49, loss = 0.50844878\n",
      "Iteration 50, loss = 0.50702749\n",
      "Iteration 51, loss = 0.50615990\n",
      "Iteration 52, loss = 0.50543189\n",
      "Iteration 53, loss = 0.50452415\n",
      "Iteration 54, loss = 0.50361903\n",
      "Iteration 55, loss = 0.50270790\n",
      "Iteration 56, loss = 0.50206388\n",
      "Iteration 57, loss = 0.50139266\n",
      "Iteration 58, loss = 0.50067607\n",
      "Iteration 59, loss = 0.49989222\n",
      "Iteration 60, loss = 0.49952779\n",
      "Iteration 61, loss = 0.49870192\n",
      "Iteration 62, loss = 0.49814354\n",
      "Iteration 63, loss = 0.49756373\n",
      "Iteration 64, loss = 0.49720474\n",
      "Iteration 65, loss = 0.49672588\n",
      "Iteration 66, loss = 0.49635845\n",
      "Iteration 67, loss = 0.49574618\n",
      "Iteration 68, loss = 0.49542515\n",
      "Iteration 69, loss = 0.49525859\n",
      "Iteration 70, loss = 0.49474089\n",
      "Iteration 71, loss = 0.49461353\n",
      "Iteration 72, loss = 0.49409402\n",
      "Iteration 73, loss = 0.49390095\n",
      "Iteration 74, loss = 0.49373793\n",
      "Iteration 75, loss = 0.49341704\n",
      "Iteration 76, loss = 0.49312912\n",
      "Iteration 77, loss = 0.49275968\n",
      "Iteration 78, loss = 0.49243910\n",
      "Iteration 79, loss = 0.49249381\n",
      "Iteration 80, loss = 0.49240446\n",
      "Iteration 81, loss = 0.49160378\n",
      "Iteration 82, loss = 0.49138986\n",
      "Iteration 83, loss = 0.49113357\n",
      "Iteration 84, loss = 0.49117227\n",
      "Iteration 85, loss = 0.49042753\n",
      "Iteration 86, loss = 0.49014164\n",
      "Iteration 87, loss = 0.48996237\n",
      "Iteration 88, loss = 0.48964238\n",
      "Iteration 89, loss = 0.48936706\n",
      "Iteration 90, loss = 0.48912152\n",
      "Iteration 91, loss = 0.48868973\n",
      "Iteration 92, loss = 0.48850758\n",
      "Iteration 93, loss = 0.48818133\n",
      "Iteration 94, loss = 0.48810569\n",
      "Iteration 95, loss = 0.48793488\n",
      "Iteration 96, loss = 0.48814071\n",
      "Iteration 97, loss = 0.48739850\n",
      "Iteration 98, loss = 0.48728940\n",
      "Iteration 99, loss = 0.48711745\n",
      "Iteration 100, loss = 0.48683734\n",
      "Iteration 101, loss = 0.48671713\n",
      "Iteration 102, loss = 0.48669436\n",
      "Iteration 103, loss = 0.48663910\n",
      "Iteration 104, loss = 0.48624456\n",
      "Iteration 105, loss = 0.48625769\n",
      "Iteration 106, loss = 0.48604570\n",
      "Iteration 107, loss = 0.48568334\n",
      "Iteration 108, loss = 0.48550108\n",
      "Iteration 109, loss = 0.48554002\n",
      "Iteration 110, loss = 0.48521032\n",
      "Iteration 111, loss = 0.48521785\n",
      "Iteration 112, loss = 0.48512033\n",
      "Iteration 113, loss = 0.48487364\n",
      "Iteration 114, loss = 0.48449711\n",
      "Iteration 115, loss = 0.48463472\n",
      "Iteration 116, loss = 0.48429611\n",
      "Iteration 117, loss = 0.48413666\n",
      "Iteration 118, loss = 0.48398197\n",
      "Iteration 119, loss = 0.48403685\n",
      "Iteration 120, loss = 0.48379440\n",
      "Iteration 121, loss = 0.48353353\n",
      "Iteration 122, loss = 0.48337101\n",
      "Iteration 123, loss = 0.48317377\n",
      "Iteration 124, loss = 0.48311415\n",
      "Iteration 125, loss = 0.48309722\n",
      "Iteration 126, loss = 0.48288183\n",
      "Iteration 127, loss = 0.48289912\n",
      "Iteration 128, loss = 0.48266001\n",
      "Iteration 129, loss = 0.48245271\n",
      "Iteration 130, loss = 0.48294230\n",
      "Iteration 131, loss = 0.48237107\n",
      "Iteration 132, loss = 0.48234013\n",
      "Iteration 133, loss = 0.48228262\n",
      "Iteration 134, loss = 0.48194219\n",
      "Iteration 135, loss = 0.48195990\n",
      "Iteration 136, loss = 0.48190300\n",
      "Iteration 137, loss = 0.48193794\n",
      "Iteration 138, loss = 0.48190245\n",
      "Iteration 139, loss = 0.48182137\n",
      "Iteration 140, loss = 0.48157019\n",
      "Iteration 141, loss = 0.48138590\n",
      "Iteration 142, loss = 0.48147219\n",
      "Iteration 143, loss = 0.48119609\n",
      "Iteration 144, loss = 0.48136671\n",
      "Iteration 145, loss = 0.48102905\n",
      "Iteration 146, loss = 0.48101587\n",
      "Iteration 147, loss = 0.48087684\n",
      "Iteration 148, loss = 0.48083588\n",
      "Iteration 149, loss = 0.48075893\n",
      "Iteration 150, loss = 0.48069906\n",
      "Iteration 151, loss = 0.48063038\n",
      "Iteration 152, loss = 0.48073904\n",
      "Iteration 153, loss = 0.48054527\n",
      "Iteration 154, loss = 0.48047809\n",
      "Iteration 155, loss = 0.48045115\n",
      "Iteration 156, loss = 0.48026058\n",
      "Iteration 157, loss = 0.48019285\n",
      "Iteration 158, loss = 0.48024350\n",
      "Iteration 159, loss = 0.48007045\n",
      "Iteration 160, loss = 0.48021924\n",
      "Iteration 161, loss = 0.48067309\n",
      "Iteration 162, loss = 0.47997064\n",
      "Iteration 163, loss = 0.47990670\n",
      "Iteration 164, loss = 0.47986719\n",
      "Iteration 165, loss = 0.47994689\n",
      "Iteration 166, loss = 0.47978347\n",
      "Iteration 167, loss = 0.47956426\n",
      "Iteration 168, loss = 0.47947936\n",
      "Iteration 169, loss = 0.47948359\n",
      "Iteration 170, loss = 0.47947867\n",
      "Iteration 171, loss = 0.47939277\n",
      "Iteration 172, loss = 0.47935752\n",
      "Iteration 173, loss = 0.47919601\n",
      "Iteration 174, loss = 0.47922856\n",
      "Iteration 175, loss = 0.47914400\n",
      "Iteration 176, loss = 0.47885090\n",
      "Iteration 177, loss = 0.47883585\n",
      "Iteration 178, loss = 0.47877936\n",
      "Iteration 179, loss = 0.47894075\n",
      "Iteration 180, loss = 0.47880265\n",
      "Iteration 181, loss = 0.47864383\n",
      "Iteration 182, loss = 0.47855758\n",
      "Iteration 183, loss = 0.47837792\n",
      "Iteration 184, loss = 0.47871756\n",
      "Iteration 185, loss = 0.47820480\n",
      "Iteration 186, loss = 0.47834395\n",
      "Iteration 187, loss = 0.47827998\n",
      "Iteration 188, loss = 0.47821888\n",
      "Iteration 189, loss = 0.47828743\n",
      "Iteration 190, loss = 0.47830079\n",
      "Iteration 191, loss = 0.47802169\n",
      "Iteration 192, loss = 0.47799544\n",
      "Iteration 193, loss = 0.47785251\n",
      "Iteration 194, loss = 0.47792773\n",
      "Iteration 195, loss = 0.47769366\n",
      "Iteration 196, loss = 0.47778946\n",
      "Iteration 197, loss = 0.47754305\n",
      "Iteration 198, loss = 0.47752456\n",
      "Iteration 199, loss = 0.47782269\n",
      "Iteration 200, loss = 0.47742947\n",
      "Iteration 201, loss = 0.47766574\n",
      "Iteration 202, loss = 0.47739474\n",
      "Iteration 203, loss = 0.47755690\n",
      "Iteration 204, loss = 0.47739117\n",
      "Iteration 205, loss = 0.47720427\n",
      "Iteration 206, loss = 0.47724705\n",
      "Iteration 207, loss = 0.47712525\n",
      "Iteration 208, loss = 0.47723543\n",
      "Iteration 209, loss = 0.47712984\n",
      "Iteration 210, loss = 0.47707929\n",
      "Iteration 211, loss = 0.47717925\n",
      "Iteration 212, loss = 0.47696063\n",
      "Iteration 213, loss = 0.47690290\n",
      "Iteration 214, loss = 0.47687244\n",
      "Iteration 215, loss = 0.47680348\n",
      "Iteration 216, loss = 0.47693878\n",
      "Iteration 217, loss = 0.47667952\n",
      "Iteration 218, loss = 0.47677968\n",
      "Iteration 219, loss = 0.47656547\n",
      "Iteration 220, loss = 0.47650068\n",
      "Iteration 221, loss = 0.47650115\n",
      "Iteration 222, loss = 0.47645577\n",
      "Iteration 223, loss = 0.47656720\n",
      "Iteration 224, loss = 0.47629841\n",
      "Iteration 225, loss = 0.47635057\n",
      "Iteration 226, loss = 0.47638969\n",
      "Iteration 227, loss = 0.47634399\n",
      "Iteration 228, loss = 0.47621840\n",
      "Iteration 229, loss = 0.47603742\n",
      "Iteration 230, loss = 0.47600070\n",
      "Iteration 231, loss = 0.47597869\n",
      "Iteration 232, loss = 0.47616544\n",
      "Iteration 233, loss = 0.47594994\n",
      "Iteration 234, loss = 0.47593097\n",
      "Iteration 235, loss = 0.47595587\n",
      "Iteration 236, loss = 0.47576382\n",
      "Iteration 237, loss = 0.47584849\n",
      "Iteration 238, loss = 0.47566889\n",
      "Iteration 239, loss = 0.47586645\n",
      "Iteration 240, loss = 0.47575692\n",
      "Iteration 241, loss = 0.47564825\n",
      "Iteration 242, loss = 0.47558529\n",
      "Iteration 243, loss = 0.47543938\n",
      "Iteration 244, loss = 0.47537028\n",
      "Iteration 245, loss = 0.47535297\n",
      "Iteration 246, loss = 0.47533961\n",
      "Iteration 247, loss = 0.47516999\n",
      "Iteration 248, loss = 0.47531928\n",
      "Iteration 249, loss = 0.47520044\n",
      "Iteration 250, loss = 0.47509911\n",
      "Iteration 251, loss = 0.47506711\n",
      "Iteration 252, loss = 0.47481679\n",
      "Iteration 253, loss = 0.47497535\n",
      "Iteration 254, loss = 0.47506969\n",
      "Iteration 255, loss = 0.47463725\n",
      "Iteration 256, loss = 0.47468498\n",
      "Iteration 257, loss = 0.47463380\n",
      "Iteration 258, loss = 0.47485267\n",
      "Iteration 259, loss = 0.47475380\n",
      "Iteration 260, loss = 0.47454361\n",
      "Iteration 261, loss = 0.47486072\n",
      "Iteration 262, loss = 0.47428039\n",
      "Iteration 263, loss = 0.47436253\n",
      "Iteration 264, loss = 0.47427134\n",
      "Iteration 265, loss = 0.47419725\n",
      "Iteration 266, loss = 0.47412100\n",
      "Iteration 267, loss = 0.47411877\n",
      "Iteration 268, loss = 0.47408280\n",
      "Iteration 269, loss = 0.47394871\n",
      "Iteration 270, loss = 0.47392079\n",
      "Iteration 271, loss = 0.47383655\n",
      "Iteration 272, loss = 0.47424685\n",
      "Iteration 273, loss = 0.47365887\n",
      "Iteration 274, loss = 0.47390071\n",
      "Iteration 275, loss = 0.47366991\n",
      "Iteration 276, loss = 0.47374543\n",
      "Iteration 277, loss = 0.47341548\n",
      "Iteration 278, loss = 0.47348534\n",
      "Iteration 279, loss = 0.47356667\n",
      "Iteration 280, loss = 0.47351020\n",
      "Iteration 281, loss = 0.47337548\n",
      "Iteration 282, loss = 0.47325726\n",
      "Iteration 283, loss = 0.47332324\n",
      "Iteration 284, loss = 0.47307052\n",
      "Iteration 285, loss = 0.47335911\n",
      "Iteration 286, loss = 0.47310001\n",
      "Iteration 287, loss = 0.47295241\n",
      "Iteration 288, loss = 0.47310082\n",
      "Iteration 289, loss = 0.47308724\n",
      "Iteration 290, loss = 0.47276217\n",
      "Iteration 291, loss = 0.47310849\n",
      "Iteration 292, loss = 0.47277414\n",
      "Iteration 293, loss = 0.47294506\n",
      "Iteration 294, loss = 0.47255476\n",
      "Iteration 295, loss = 0.47272232\n",
      "Iteration 296, loss = 0.47256626\n",
      "Iteration 297, loss = 0.47253084\n",
      "Iteration 298, loss = 0.47245612\n",
      "Iteration 299, loss = 0.47259260\n",
      "Iteration 300, loss = 0.47250300\n",
      "Iteration 301, loss = 0.47217965\n",
      "Iteration 302, loss = 0.47224023\n",
      "Iteration 303, loss = 0.47222951\n",
      "Iteration 304, loss = 0.47229483\n",
      "Iteration 305, loss = 0.47200197\n",
      "Iteration 306, loss = 0.47190252\n",
      "Iteration 307, loss = 0.47210107\n",
      "Iteration 308, loss = 0.47215715\n",
      "Iteration 309, loss = 0.47201045\n",
      "Iteration 310, loss = 0.47190425\n",
      "Iteration 311, loss = 0.47202413\n",
      "Iteration 312, loss = 0.47169737\n",
      "Iteration 313, loss = 0.47173223\n",
      "Iteration 314, loss = 0.47143310\n",
      "Iteration 315, loss = 0.47147875\n",
      "Iteration 316, loss = 0.47145312\n",
      "Iteration 317, loss = 0.47140219\n",
      "Iteration 318, loss = 0.47149037\n",
      "Iteration 319, loss = 0.47125318\n",
      "Iteration 320, loss = 0.47144616\n",
      "Iteration 321, loss = 0.47111421\n",
      "Iteration 322, loss = 0.47107162\n",
      "Iteration 323, loss = 0.47096372\n",
      "Iteration 324, loss = 0.47099143\n",
      "Iteration 325, loss = 0.47082773\n",
      "Iteration 326, loss = 0.47100688\n",
      "Iteration 327, loss = 0.47089514\n",
      "Iteration 328, loss = 0.47076877\n",
      "Iteration 329, loss = 0.47072764\n",
      "Iteration 330, loss = 0.47070985\n",
      "Iteration 331, loss = 0.47061648\n",
      "Iteration 332, loss = 0.47034148\n",
      "Iteration 333, loss = 0.47067065\n",
      "Iteration 334, loss = 0.47036483\n",
      "Iteration 335, loss = 0.47026395\n",
      "Iteration 336, loss = 0.47030219\n",
      "Iteration 337, loss = 0.47024201\n",
      "Iteration 338, loss = 0.47007834\n",
      "Iteration 339, loss = 0.47002714\n",
      "Iteration 340, loss = 0.46992275\n",
      "Iteration 341, loss = 0.47008373\n",
      "Iteration 342, loss = 0.47002819\n",
      "Iteration 343, loss = 0.46994284\n",
      "Iteration 344, loss = 0.46963944\n",
      "Iteration 345, loss = 0.46966448\n",
      "Iteration 346, loss = 0.46966804\n",
      "Iteration 347, loss = 0.46947004\n",
      "Iteration 348, loss = 0.46960373\n",
      "Iteration 349, loss = 0.46982567\n",
      "Iteration 350, loss = 0.46944667\n",
      "Iteration 351, loss = 0.46938761\n",
      "Iteration 352, loss = 0.46932928\n",
      "Iteration 353, loss = 0.46924567\n",
      "Iteration 354, loss = 0.46934707\n",
      "Iteration 355, loss = 0.46941521\n",
      "Iteration 356, loss = 0.46923193\n",
      "Iteration 357, loss = 0.46910307\n",
      "Iteration 358, loss = 0.46910534\n",
      "Iteration 359, loss = 0.46896536\n",
      "Iteration 360, loss = 0.46890089\n",
      "Iteration 361, loss = 0.46898081\n",
      "Iteration 362, loss = 0.46890206\n",
      "Iteration 363, loss = 0.46864295\n",
      "Iteration 364, loss = 0.46886788\n",
      "Iteration 365, loss = 0.46867835\n",
      "Iteration 366, loss = 0.46870766\n",
      "Iteration 367, loss = 0.46856194\n",
      "Iteration 368, loss = 0.46876667\n",
      "Iteration 369, loss = 0.46854259\n",
      "Iteration 370, loss = 0.46844166\n",
      "Iteration 371, loss = 0.46853630\n",
      "Iteration 372, loss = 0.46878479\n",
      "Iteration 373, loss = 0.46854410\n",
      "Iteration 374, loss = 0.46827536\n",
      "Iteration 375, loss = 0.46847956\n",
      "Iteration 376, loss = 0.46836867\n",
      "Iteration 377, loss = 0.46835069\n",
      "Iteration 378, loss = 0.46827478\n",
      "Iteration 379, loss = 0.46800995\n",
      "Iteration 380, loss = 0.46820862\n",
      "Iteration 381, loss = 0.46823883\n",
      "Iteration 382, loss = 0.46790074\n",
      "Iteration 383, loss = 0.46792799\n",
      "Iteration 384, loss = 0.46814145\n",
      "Iteration 385, loss = 0.46813405\n",
      "Iteration 386, loss = 0.46822967\n",
      "Iteration 387, loss = 0.46832900\n",
      "Iteration 388, loss = 0.46781075\n",
      "Iteration 389, loss = 0.46785847\n",
      "Iteration 390, loss = 0.46772628\n",
      "Iteration 391, loss = 0.46778739\n",
      "Iteration 392, loss = 0.46778516\n",
      "Iteration 393, loss = 0.46769739\n",
      "Iteration 394, loss = 0.46778336\n",
      "Iteration 395, loss = 0.46765117\n",
      "Iteration 396, loss = 0.46788277\n",
      "Iteration 397, loss = 0.46766008\n",
      "Iteration 398, loss = 0.46751193\n",
      "Iteration 399, loss = 0.46747353\n",
      "Iteration 400, loss = 0.46791628\n",
      "Iteration 401, loss = 0.46755938\n",
      "Iteration 402, loss = 0.46768061\n",
      "Iteration 403, loss = 0.46765554\n",
      "Iteration 404, loss = 0.46736056\n",
      "Iteration 405, loss = 0.46744584\n",
      "Iteration 406, loss = 0.46763048\n",
      "Iteration 407, loss = 0.46737673\n",
      "Iteration 408, loss = 0.46748578\n",
      "Iteration 409, loss = 0.46753010\n",
      "Iteration 410, loss = 0.46706166\n",
      "Iteration 411, loss = 0.46705717\n",
      "Iteration 412, loss = 0.46737303\n",
      "Iteration 413, loss = 0.46727809\n",
      "Iteration 414, loss = 0.46721462\n",
      "Iteration 415, loss = 0.46698106\n",
      "Iteration 416, loss = 0.46708735\n",
      "Iteration 417, loss = 0.46705077\n",
      "Iteration 418, loss = 0.46714677\n",
      "Iteration 419, loss = 0.46699807\n",
      "Iteration 420, loss = 0.46692296\n",
      "Iteration 421, loss = 0.46678264\n",
      "Iteration 422, loss = 0.46692625\n",
      "Iteration 423, loss = 0.46673375\n",
      "Iteration 424, loss = 0.46677134\n",
      "Iteration 425, loss = 0.46678490\n",
      "Iteration 426, loss = 0.46703780\n",
      "Iteration 427, loss = 0.46655851\n",
      "Iteration 428, loss = 0.46688729\n",
      "Iteration 429, loss = 0.46674216\n",
      "Iteration 430, loss = 0.46666980\n",
      "Iteration 431, loss = 0.46673065\n",
      "Iteration 432, loss = 0.46650490\n",
      "Iteration 433, loss = 0.46646067\n",
      "Iteration 434, loss = 0.46640969\n",
      "Iteration 435, loss = 0.46674025\n",
      "Iteration 436, loss = 0.46663280\n",
      "Iteration 437, loss = 0.46650054\n",
      "Iteration 438, loss = 0.46642963\n",
      "Iteration 439, loss = 0.46623751\n",
      "Iteration 440, loss = 0.46666434\n",
      "Iteration 441, loss = 0.46633451\n",
      "Iteration 442, loss = 0.46646505\n",
      "Iteration 443, loss = 0.46642026\n",
      "Iteration 444, loss = 0.46621150\n",
      "Iteration 445, loss = 0.46631413\n",
      "Iteration 446, loss = 0.46640221\n",
      "Iteration 447, loss = 0.46630042\n",
      "Iteration 448, loss = 0.46628482\n",
      "Iteration 449, loss = 0.46610182\n",
      "Iteration 450, loss = 0.46621940\n",
      "Iteration 451, loss = 0.46617041\n",
      "Iteration 452, loss = 0.46607173\n",
      "Iteration 453, loss = 0.46614034\n",
      "Iteration 454, loss = 0.46599102\n",
      "Iteration 455, loss = 0.46599268\n",
      "Iteration 456, loss = 0.46601040\n",
      "Iteration 457, loss = 0.46610313\n",
      "Iteration 458, loss = 0.46603696\n",
      "Iteration 459, loss = 0.46581675\n",
      "Iteration 460, loss = 0.46617329\n",
      "Iteration 461, loss = 0.46608283\n",
      "Iteration 462, loss = 0.46590255\n",
      "Iteration 463, loss = 0.46586686\n",
      "Iteration 464, loss = 0.46617884\n",
      "Iteration 465, loss = 0.46597365\n",
      "Iteration 466, loss = 0.46578553\n",
      "Iteration 467, loss = 0.46581647\n",
      "Iteration 468, loss = 0.46574766\n",
      "Iteration 469, loss = 0.46596953\n",
      "Iteration 470, loss = 0.46605498\n",
      "Iteration 471, loss = 0.46595617\n",
      "Iteration 472, loss = 0.46560800\n",
      "Iteration 473, loss = 0.46581968\n",
      "Iteration 474, loss = 0.46589349\n",
      "Iteration 475, loss = 0.46561763\n",
      "Iteration 476, loss = 0.46576213\n",
      "Iteration 477, loss = 0.46555099\n",
      "Iteration 478, loss = 0.46597114\n",
      "Iteration 479, loss = 0.46566411\n",
      "Iteration 480, loss = 0.46554230\n",
      "Iteration 481, loss = 0.46561160\n",
      "Iteration 482, loss = 0.46572903\n",
      "Iteration 483, loss = 0.46532984\n",
      "Iteration 484, loss = 0.46550983\n",
      "Iteration 485, loss = 0.46545512\n",
      "Iteration 486, loss = 0.46560667\n",
      "Iteration 487, loss = 0.46563241\n",
      "Iteration 488, loss = 0.46552073\n",
      "Iteration 489, loss = 0.46538699\n",
      "Iteration 490, loss = 0.46546661\n",
      "Iteration 491, loss = 0.46553962\n",
      "Iteration 492, loss = 0.46530642\n",
      "Iteration 493, loss = 0.46545330\n",
      "Iteration 494, loss = 0.46560688\n",
      "Iteration 495, loss = 0.46542577\n",
      "Iteration 496, loss = 0.46531836\n",
      "Iteration 497, loss = 0.46530341\n",
      "Iteration 498, loss = 0.46521041\n",
      "Iteration 499, loss = 0.46535718\n",
      "Iteration 500, loss = 0.46504154\n",
      "Iteration 501, loss = 0.46519890\n",
      "Iteration 502, loss = 0.46516278\n",
      "Iteration 503, loss = 0.46541795\n",
      "Iteration 504, loss = 0.46519553\n",
      "Iteration 505, loss = 0.46508688\n",
      "Iteration 506, loss = 0.46498322\n",
      "Iteration 507, loss = 0.46533975\n",
      "Iteration 508, loss = 0.46495351\n",
      "Iteration 509, loss = 0.46510355\n",
      "Iteration 510, loss = 0.46523066\n",
      "Iteration 511, loss = 0.46505045\n",
      "Iteration 512, loss = 0.46508967\n",
      "Iteration 513, loss = 0.46517538\n",
      "Iteration 514, loss = 0.46522086\n",
      "Iteration 515, loss = 0.46487635\n",
      "Iteration 516, loss = 0.46511436\n",
      "Iteration 517, loss = 0.46496419\n",
      "Iteration 518, loss = 0.46484048\n",
      "Iteration 519, loss = 0.46530927\n",
      "Iteration 520, loss = 0.46514201\n",
      "Iteration 521, loss = 0.46486549\n",
      "Iteration 522, loss = 0.46500369\n",
      "Iteration 523, loss = 0.46485073\n",
      "Iteration 524, loss = 0.46488935\n",
      "Iteration 525, loss = 0.46483063\n",
      "Iteration 526, loss = 0.46493438\n",
      "Iteration 527, loss = 0.46488587\n",
      "Iteration 528, loss = 0.46491310\n",
      "Iteration 529, loss = 0.46467455\n",
      "Iteration 530, loss = 0.46467833\n",
      "Iteration 531, loss = 0.46451050\n",
      "Iteration 532, loss = 0.46458559\n",
      "Iteration 533, loss = 0.46444110\n",
      "Iteration 534, loss = 0.46474432\n",
      "Iteration 535, loss = 0.46438119\n",
      "Iteration 536, loss = 0.46467374\n",
      "Iteration 537, loss = 0.46466675\n",
      "Iteration 538, loss = 0.46483870\n",
      "Iteration 539, loss = 0.46452561\n",
      "Iteration 540, loss = 0.46476689\n",
      "Iteration 541, loss = 0.46483401\n",
      "Iteration 542, loss = 0.46471273\n",
      "Iteration 543, loss = 0.46435200\n",
      "Iteration 544, loss = 0.46490940\n",
      "Iteration 545, loss = 0.46456389\n",
      "Iteration 546, loss = 0.46427425\n",
      "Iteration 547, loss = 0.46469388\n",
      "Iteration 548, loss = 0.46443765\n",
      "Iteration 549, loss = 0.46427989\n",
      "Iteration 550, loss = 0.46424691\n",
      "Iteration 551, loss = 0.46436304\n",
      "Iteration 552, loss = 0.46429552\n",
      "Iteration 553, loss = 0.46436170\n",
      "Iteration 554, loss = 0.46448147\n",
      "Iteration 555, loss = 0.46443565\n",
      "Iteration 556, loss = 0.46425884\n",
      "Iteration 557, loss = 0.46450513\n",
      "Iteration 558, loss = 0.46426600\n",
      "Iteration 559, loss = 0.46436408\n",
      "Iteration 560, loss = 0.46413110\n",
      "Iteration 561, loss = 0.46427069\n",
      "Iteration 562, loss = 0.46422604\n",
      "Iteration 563, loss = 0.46411856\n",
      "Iteration 564, loss = 0.46410619\n",
      "Iteration 565, loss = 0.46422522\n",
      "Iteration 566, loss = 0.46438792\n",
      "Iteration 567, loss = 0.46421392\n",
      "Iteration 568, loss = 0.46417907\n",
      "Iteration 569, loss = 0.46414320\n",
      "Iteration 570, loss = 0.46399458\n",
      "Iteration 571, loss = 0.46411812\n",
      "Iteration 572, loss = 0.46423577\n",
      "Iteration 573, loss = 0.46396032\n",
      "Iteration 574, loss = 0.46400970\n",
      "Iteration 575, loss = 0.46397299\n",
      "Iteration 576, loss = 0.46420759\n",
      "Iteration 577, loss = 0.46409270\n",
      "Iteration 578, loss = 0.46414499\n",
      "Iteration 579, loss = 0.46374609\n",
      "Iteration 580, loss = 0.46420464\n",
      "Iteration 581, loss = 0.46406552\n",
      "Iteration 582, loss = 0.46406915\n",
      "Iteration 583, loss = 0.46414671\n",
      "Iteration 584, loss = 0.46392146\n",
      "Iteration 585, loss = 0.46387265\n",
      "Iteration 586, loss = 0.46388252\n",
      "Iteration 587, loss = 0.46394238\n",
      "Iteration 588, loss = 0.46390828\n",
      "Iteration 589, loss = 0.46404927\n",
      "Iteration 590, loss = 0.46379531\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68523786\n",
      "Iteration 2, loss = 0.66406199\n",
      "Iteration 3, loss = 0.64996547\n",
      "Iteration 4, loss = 0.63759823\n",
      "Iteration 5, loss = 0.62499068\n",
      "Iteration 6, loss = 0.61289536\n",
      "Iteration 7, loss = 0.60132457\n",
      "Iteration 8, loss = 0.59086729\n",
      "Iteration 9, loss = 0.58194606\n",
      "Iteration 10, loss = 0.57449222\n",
      "Iteration 11, loss = 0.56874704\n",
      "Iteration 12, loss = 0.56406179\n",
      "Iteration 13, loss = 0.56048022\n",
      "Iteration 14, loss = 0.55764790\n",
      "Iteration 15, loss = 0.55493593\n",
      "Iteration 16, loss = 0.55267199\n",
      "Iteration 17, loss = 0.55051558\n",
      "Iteration 18, loss = 0.54858982\n",
      "Iteration 19, loss = 0.54664243\n",
      "Iteration 20, loss = 0.54482618\n",
      "Iteration 21, loss = 0.54306713\n",
      "Iteration 22, loss = 0.54158126\n",
      "Iteration 23, loss = 0.53994817\n",
      "Iteration 24, loss = 0.53876079\n",
      "Iteration 25, loss = 0.53713946\n",
      "Iteration 26, loss = 0.53589610\n",
      "Iteration 27, loss = 0.53455723\n",
      "Iteration 28, loss = 0.53313162\n",
      "Iteration 29, loss = 0.53176239\n",
      "Iteration 30, loss = 0.53053698\n",
      "Iteration 31, loss = 0.52933101\n",
      "Iteration 32, loss = 0.52818220\n",
      "Iteration 33, loss = 0.52709113\n",
      "Iteration 34, loss = 0.52589999\n",
      "Iteration 35, loss = 0.52493675\n",
      "Iteration 36, loss = 0.52377619\n",
      "Iteration 37, loss = 0.52277027\n",
      "Iteration 38, loss = 0.52179829\n",
      "Iteration 39, loss = 0.52067483\n",
      "Iteration 40, loss = 0.51983410\n",
      "Iteration 41, loss = 0.51871441\n",
      "Iteration 42, loss = 0.51792795\n",
      "Iteration 43, loss = 0.51714283\n",
      "Iteration 44, loss = 0.51608946\n",
      "Iteration 45, loss = 0.51501292\n",
      "Iteration 46, loss = 0.51422160\n",
      "Iteration 47, loss = 0.51369865\n",
      "Iteration 48, loss = 0.51264187\n",
      "Iteration 49, loss = 0.51206712\n",
      "Iteration 50, loss = 0.51138057\n",
      "Iteration 51, loss = 0.51071585\n",
      "Iteration 52, loss = 0.51018305\n",
      "Iteration 53, loss = 0.50947504\n",
      "Iteration 54, loss = 0.50895371\n",
      "Iteration 55, loss = 0.50825172\n",
      "Iteration 56, loss = 0.50760441\n",
      "Iteration 57, loss = 0.50721216\n",
      "Iteration 58, loss = 0.50697737\n",
      "Iteration 59, loss = 0.50634917\n",
      "Iteration 60, loss = 0.50614372\n",
      "Iteration 61, loss = 0.50553435\n",
      "Iteration 62, loss = 0.50493696\n",
      "Iteration 63, loss = 0.50446916\n",
      "Iteration 64, loss = 0.50411517\n",
      "Iteration 65, loss = 0.50375111\n",
      "Iteration 66, loss = 0.50335355\n",
      "Iteration 67, loss = 0.50323070\n",
      "Iteration 68, loss = 0.50318016\n",
      "Iteration 69, loss = 0.50240700\n",
      "Iteration 70, loss = 0.50206741\n",
      "Iteration 71, loss = 0.50204075\n",
      "Iteration 72, loss = 0.50151013\n",
      "Iteration 73, loss = 0.50124093\n",
      "Iteration 74, loss = 0.50105327\n",
      "Iteration 75, loss = 0.50069461\n",
      "Iteration 76, loss = 0.50054346\n",
      "Iteration 77, loss = 0.50024430\n",
      "Iteration 78, loss = 0.49967945\n",
      "Iteration 79, loss = 0.49953021\n",
      "Iteration 80, loss = 0.49959209\n",
      "Iteration 81, loss = 0.49922726\n",
      "Iteration 82, loss = 0.49876311\n",
      "Iteration 83, loss = 0.49915744\n",
      "Iteration 84, loss = 0.49844357\n",
      "Iteration 85, loss = 0.49811442\n",
      "Iteration 86, loss = 0.49854854\n",
      "Iteration 87, loss = 0.49781537\n",
      "Iteration 88, loss = 0.49753042\n",
      "Iteration 89, loss = 0.49749391\n",
      "Iteration 90, loss = 0.49715399\n",
      "Iteration 91, loss = 0.49689036\n",
      "Iteration 92, loss = 0.49652621\n",
      "Iteration 93, loss = 0.49666154\n",
      "Iteration 94, loss = 0.49620265\n",
      "Iteration 95, loss = 0.49596780\n",
      "Iteration 96, loss = 0.49579961\n",
      "Iteration 97, loss = 0.49552255\n",
      "Iteration 98, loss = 0.49547650\n",
      "Iteration 99, loss = 0.49531468\n",
      "Iteration 100, loss = 0.49477047\n",
      "Iteration 101, loss = 0.49462361\n",
      "Iteration 102, loss = 0.49413366\n",
      "Iteration 103, loss = 0.49461098\n",
      "Iteration 104, loss = 0.49384819\n",
      "Iteration 105, loss = 0.49374136\n",
      "Iteration 106, loss = 0.49347172\n",
      "Iteration 107, loss = 0.49332280\n",
      "Iteration 108, loss = 0.49310745\n",
      "Iteration 109, loss = 0.49294981\n",
      "Iteration 110, loss = 0.49306290\n",
      "Iteration 111, loss = 0.49283307\n",
      "Iteration 112, loss = 0.49289830\n",
      "Iteration 113, loss = 0.49262257\n",
      "Iteration 114, loss = 0.49217796\n",
      "Iteration 115, loss = 0.49229847\n",
      "Iteration 116, loss = 0.49219653\n",
      "Iteration 117, loss = 0.49191605\n",
      "Iteration 118, loss = 0.49174278\n",
      "Iteration 119, loss = 0.49182358\n",
      "Iteration 120, loss = 0.49152193\n",
      "Iteration 121, loss = 0.49132324\n",
      "Iteration 122, loss = 0.49129785\n",
      "Iteration 123, loss = 0.49145908\n",
      "Iteration 124, loss = 0.49098628\n",
      "Iteration 125, loss = 0.49110842\n",
      "Iteration 126, loss = 0.49083903\n",
      "Iteration 127, loss = 0.49106275\n",
      "Iteration 128, loss = 0.49067720\n",
      "Iteration 129, loss = 0.49054541\n",
      "Iteration 130, loss = 0.49054626\n",
      "Iteration 131, loss = 0.49022235\n",
      "Iteration 132, loss = 0.49040923\n",
      "Iteration 133, loss = 0.49032853\n",
      "Iteration 134, loss = 0.49039333\n",
      "Iteration 135, loss = 0.49015014\n",
      "Iteration 136, loss = 0.48986451\n",
      "Iteration 137, loss = 0.48989692\n",
      "Iteration 138, loss = 0.49005476\n",
      "Iteration 139, loss = 0.48981254\n",
      "Iteration 140, loss = 0.48990334\n",
      "Iteration 141, loss = 0.48959480\n",
      "Iteration 142, loss = 0.48978737\n",
      "Iteration 143, loss = 0.49032243\n",
      "Iteration 144, loss = 0.49014608\n",
      "Iteration 145, loss = 0.48928288\n",
      "Iteration 146, loss = 0.48935176\n",
      "Iteration 147, loss = 0.48960437\n",
      "Iteration 148, loss = 0.48924633\n",
      "Iteration 149, loss = 0.48929761\n",
      "Iteration 150, loss = 0.48941895\n",
      "Iteration 151, loss = 0.48902812\n",
      "Iteration 152, loss = 0.48925980\n",
      "Iteration 153, loss = 0.48881101\n",
      "Iteration 154, loss = 0.48899259\n",
      "Iteration 155, loss = 0.48871586\n",
      "Iteration 156, loss = 0.48879159\n",
      "Iteration 157, loss = 0.48886150\n",
      "Iteration 158, loss = 0.48882584\n",
      "Iteration 159, loss = 0.48874433\n",
      "Iteration 160, loss = 0.48884835\n",
      "Iteration 161, loss = 0.48850745\n",
      "Iteration 162, loss = 0.48879458\n",
      "Iteration 163, loss = 0.48860557\n",
      "Iteration 164, loss = 0.48834056\n",
      "Iteration 165, loss = 0.48876792\n",
      "Iteration 166, loss = 0.48840542\n",
      "Iteration 167, loss = 0.48834984\n",
      "Iteration 168, loss = 0.48818484\n",
      "Iteration 169, loss = 0.48829890\n",
      "Iteration 170, loss = 0.48822400\n",
      "Iteration 171, loss = 0.48809605\n",
      "Iteration 172, loss = 0.48818453\n",
      "Iteration 173, loss = 0.48795128\n",
      "Iteration 174, loss = 0.48803597\n",
      "Iteration 175, loss = 0.48822254\n",
      "Iteration 176, loss = 0.48769476\n",
      "Iteration 177, loss = 0.48772701\n",
      "Iteration 178, loss = 0.48781102\n",
      "Iteration 179, loss = 0.48780712\n",
      "Iteration 180, loss = 0.48767216\n",
      "Iteration 181, loss = 0.48757936\n",
      "Iteration 182, loss = 0.48742161\n",
      "Iteration 183, loss = 0.48749337\n",
      "Iteration 184, loss = 0.48741029\n",
      "Iteration 185, loss = 0.48731004\n",
      "Iteration 186, loss = 0.48742925\n",
      "Iteration 187, loss = 0.48708893\n",
      "Iteration 188, loss = 0.48722048\n",
      "Iteration 189, loss = 0.48746683\n",
      "Iteration 190, loss = 0.48727932\n",
      "Iteration 191, loss = 0.48705538\n",
      "Iteration 192, loss = 0.48740945\n",
      "Iteration 193, loss = 0.48672192\n",
      "Iteration 194, loss = 0.48698468\n",
      "Iteration 195, loss = 0.48675468\n",
      "Iteration 196, loss = 0.48680519\n",
      "Iteration 197, loss = 0.48680673\n",
      "Iteration 198, loss = 0.48681548\n",
      "Iteration 199, loss = 0.48648062\n",
      "Iteration 200, loss = 0.48638373\n",
      "Iteration 201, loss = 0.48665523\n",
      "Iteration 202, loss = 0.48650519\n",
      "Iteration 203, loss = 0.48622849\n",
      "Iteration 204, loss = 0.48633055\n",
      "Iteration 205, loss = 0.48630583\n",
      "Iteration 206, loss = 0.48624501\n",
      "Iteration 207, loss = 0.48623219\n",
      "Iteration 208, loss = 0.48612747\n",
      "Iteration 209, loss = 0.48616840\n",
      "Iteration 210, loss = 0.48593967\n",
      "Iteration 211, loss = 0.48611474\n",
      "Iteration 212, loss = 0.48606788\n",
      "Iteration 213, loss = 0.48597357\n",
      "Iteration 214, loss = 0.48575453\n",
      "Iteration 215, loss = 0.48581079\n",
      "Iteration 216, loss = 0.48589724\n",
      "Iteration 217, loss = 0.48578088\n",
      "Iteration 218, loss = 0.48550190\n",
      "Iteration 219, loss = 0.48572674\n",
      "Iteration 220, loss = 0.48567596\n",
      "Iteration 221, loss = 0.48552898\n",
      "Iteration 222, loss = 0.48555165\n",
      "Iteration 223, loss = 0.48531833\n",
      "Iteration 224, loss = 0.48550267\n",
      "Iteration 225, loss = 0.48531714\n",
      "Iteration 226, loss = 0.48564612\n",
      "Iteration 227, loss = 0.48538109\n",
      "Iteration 228, loss = 0.48514485\n",
      "Iteration 229, loss = 0.48499494\n",
      "Iteration 230, loss = 0.48541005\n",
      "Iteration 231, loss = 0.48540851\n",
      "Iteration 232, loss = 0.48524971\n",
      "Iteration 233, loss = 0.48504870\n",
      "Iteration 234, loss = 0.48528515\n",
      "Iteration 235, loss = 0.48585635\n",
      "Iteration 236, loss = 0.48501597\n",
      "Iteration 237, loss = 0.48516849\n",
      "Iteration 238, loss = 0.48479533\n",
      "Iteration 239, loss = 0.48468142\n",
      "Iteration 240, loss = 0.48478000\n",
      "Iteration 241, loss = 0.48477785\n",
      "Iteration 242, loss = 0.48477959\n",
      "Iteration 243, loss = 0.48458666\n",
      "Iteration 244, loss = 0.48455659\n",
      "Iteration 245, loss = 0.48430418\n",
      "Iteration 246, loss = 0.48434030\n",
      "Iteration 247, loss = 0.48437333\n",
      "Iteration 248, loss = 0.48469743\n",
      "Iteration 249, loss = 0.48408660\n",
      "Iteration 250, loss = 0.48440351\n",
      "Iteration 251, loss = 0.48426587\n",
      "Iteration 252, loss = 0.48397275\n",
      "Iteration 253, loss = 0.48418831\n",
      "Iteration 254, loss = 0.48448975\n",
      "Iteration 255, loss = 0.48452345\n",
      "Iteration 256, loss = 0.48424602\n",
      "Iteration 257, loss = 0.48397272\n",
      "Iteration 258, loss = 0.48391535\n",
      "Iteration 259, loss = 0.48403699\n",
      "Iteration 260, loss = 0.48394059\n",
      "Iteration 261, loss = 0.48399771\n",
      "Iteration 262, loss = 0.48376315\n",
      "Iteration 263, loss = 0.48383369\n",
      "Iteration 264, loss = 0.48374430\n",
      "Iteration 265, loss = 0.48359075\n",
      "Iteration 266, loss = 0.48390487\n",
      "Iteration 267, loss = 0.48354022\n",
      "Iteration 268, loss = 0.48372409\n",
      "Iteration 269, loss = 0.48350519\n",
      "Iteration 270, loss = 0.48371343\n",
      "Iteration 271, loss = 0.48346502\n",
      "Iteration 272, loss = 0.48338682\n",
      "Iteration 273, loss = 0.48314814\n",
      "Iteration 274, loss = 0.48312425\n",
      "Iteration 275, loss = 0.48310836\n",
      "Iteration 276, loss = 0.48318148\n",
      "Iteration 277, loss = 0.48342226\n",
      "Iteration 278, loss = 0.48298073\n",
      "Iteration 279, loss = 0.48301859\n",
      "Iteration 280, loss = 0.48340730\n",
      "Iteration 281, loss = 0.48285857\n",
      "Iteration 282, loss = 0.48308880\n",
      "Iteration 283, loss = 0.48284434\n",
      "Iteration 284, loss = 0.48282478\n",
      "Iteration 285, loss = 0.48292302\n",
      "Iteration 286, loss = 0.48253114\n",
      "Iteration 287, loss = 0.48270797\n",
      "Iteration 288, loss = 0.48256521\n",
      "Iteration 289, loss = 0.48234073\n",
      "Iteration 290, loss = 0.48264318\n",
      "Iteration 291, loss = 0.48242750\n",
      "Iteration 292, loss = 0.48249487\n",
      "Iteration 293, loss = 0.48234362\n",
      "Iteration 294, loss = 0.48251751\n",
      "Iteration 295, loss = 0.48210610\n",
      "Iteration 296, loss = 0.48198849\n",
      "Iteration 297, loss = 0.48189540\n",
      "Iteration 298, loss = 0.48198948\n",
      "Iteration 299, loss = 0.48213691\n",
      "Iteration 300, loss = 0.48271103\n",
      "Iteration 301, loss = 0.48147398\n",
      "Iteration 302, loss = 0.48181373\n",
      "Iteration 303, loss = 0.48184741\n",
      "Iteration 304, loss = 0.48162843\n",
      "Iteration 305, loss = 0.48148028\n",
      "Iteration 306, loss = 0.48168718\n",
      "Iteration 307, loss = 0.48145531\n",
      "Iteration 308, loss = 0.48137268\n",
      "Iteration 309, loss = 0.48138992\n",
      "Iteration 310, loss = 0.48131125\n",
      "Iteration 311, loss = 0.48110665\n",
      "Iteration 312, loss = 0.48116515\n",
      "Iteration 313, loss = 0.48132222\n",
      "Iteration 314, loss = 0.48118362\n",
      "Iteration 315, loss = 0.48096690\n",
      "Iteration 316, loss = 0.48116356\n",
      "Iteration 317, loss = 0.48118613\n",
      "Iteration 318, loss = 0.48104354\n",
      "Iteration 319, loss = 0.48086629\n",
      "Iteration 320, loss = 0.48078179\n",
      "Iteration 321, loss = 0.48093171\n",
      "Iteration 322, loss = 0.48067875\n",
      "Iteration 323, loss = 0.48082662\n",
      "Iteration 324, loss = 0.48135554\n",
      "Iteration 325, loss = 0.48084536\n",
      "Iteration 326, loss = 0.48164947\n",
      "Iteration 327, loss = 0.48075266\n",
      "Iteration 328, loss = 0.48122958\n",
      "Iteration 329, loss = 0.48035273\n",
      "Iteration 330, loss = 0.48056885\n",
      "Iteration 331, loss = 0.48042578\n",
      "Iteration 332, loss = 0.48025239\n",
      "Iteration 333, loss = 0.48035212\n",
      "Iteration 334, loss = 0.48054588\n",
      "Iteration 335, loss = 0.48013767\n",
      "Iteration 336, loss = 0.48022946\n",
      "Iteration 337, loss = 0.48008190\n",
      "Iteration 338, loss = 0.48031918\n",
      "Iteration 339, loss = 0.48025609\n",
      "Iteration 340, loss = 0.48009729\n",
      "Iteration 341, loss = 0.47991606\n",
      "Iteration 342, loss = 0.48020083\n",
      "Iteration 343, loss = 0.47977118\n",
      "Iteration 344, loss = 0.48015469\n",
      "Iteration 345, loss = 0.47978723\n",
      "Iteration 346, loss = 0.47977967\n",
      "Iteration 347, loss = 0.47998296\n",
      "Iteration 348, loss = 0.47951539\n",
      "Iteration 349, loss = 0.47951712\n",
      "Iteration 350, loss = 0.47977632\n",
      "Iteration 351, loss = 0.47960445\n",
      "Iteration 352, loss = 0.47957105\n",
      "Iteration 353, loss = 0.47923192\n",
      "Iteration 354, loss = 0.47930213\n",
      "Iteration 355, loss = 0.47918010\n",
      "Iteration 356, loss = 0.47945597\n",
      "Iteration 357, loss = 0.47915785\n",
      "Iteration 358, loss = 0.47896925\n",
      "Iteration 359, loss = 0.47908252\n",
      "Iteration 360, loss = 0.47896148\n",
      "Iteration 361, loss = 0.47917525\n",
      "Iteration 362, loss = 0.47882444\n",
      "Iteration 363, loss = 0.47887975\n",
      "Iteration 364, loss = 0.47892389\n",
      "Iteration 365, loss = 0.47891322\n",
      "Iteration 366, loss = 0.47916876\n",
      "Iteration 367, loss = 0.47877306\n",
      "Iteration 368, loss = 0.47870296\n",
      "Iteration 369, loss = 0.47892319\n",
      "Iteration 370, loss = 0.47885554\n",
      "Iteration 371, loss = 0.47854032\n",
      "Iteration 372, loss = 0.47889079\n",
      "Iteration 373, loss = 0.47860967\n",
      "Iteration 374, loss = 0.47888666\n",
      "Iteration 375, loss = 0.47832016\n",
      "Iteration 376, loss = 0.47849754\n",
      "Iteration 377, loss = 0.47821993\n",
      "Iteration 378, loss = 0.47838344\n",
      "Iteration 379, loss = 0.47854025\n",
      "Iteration 380, loss = 0.47821806\n",
      "Iteration 381, loss = 0.47812530\n",
      "Iteration 382, loss = 0.47819672\n",
      "Iteration 383, loss = 0.47871833\n",
      "Iteration 384, loss = 0.47809597\n",
      "Iteration 385, loss = 0.47807022\n",
      "Iteration 386, loss = 0.47814932\n",
      "Iteration 387, loss = 0.47773790\n",
      "Iteration 388, loss = 0.47799266\n",
      "Iteration 389, loss = 0.47805285\n",
      "Iteration 390, loss = 0.47839682\n",
      "Iteration 391, loss = 0.47762554\n",
      "Iteration 392, loss = 0.47797484\n",
      "Iteration 393, loss = 0.47769011\n",
      "Iteration 394, loss = 0.47776696\n",
      "Iteration 395, loss = 0.47784660\n",
      "Iteration 396, loss = 0.47793482\n",
      "Iteration 397, loss = 0.47753129\n",
      "Iteration 398, loss = 0.47761971\n",
      "Iteration 399, loss = 0.47800566\n",
      "Iteration 400, loss = 0.47725690\n",
      "Iteration 401, loss = 0.47730819\n",
      "Iteration 402, loss = 0.47732396\n",
      "Iteration 403, loss = 0.47711707\n",
      "Iteration 404, loss = 0.47718263\n",
      "Iteration 405, loss = 0.47745503\n",
      "Iteration 406, loss = 0.47716237\n",
      "Iteration 407, loss = 0.47716374\n",
      "Iteration 408, loss = 0.47704067\n",
      "Iteration 409, loss = 0.47708177\n",
      "Iteration 410, loss = 0.47713708\n",
      "Iteration 411, loss = 0.47707497\n",
      "Iteration 412, loss = 0.47719198\n",
      "Iteration 413, loss = 0.47671712\n",
      "Iteration 414, loss = 0.47750048\n",
      "Iteration 415, loss = 0.47691822\n",
      "Iteration 416, loss = 0.47704251\n",
      "Iteration 417, loss = 0.47693622\n",
      "Iteration 418, loss = 0.47687066\n",
      "Iteration 419, loss = 0.47682679\n",
      "Iteration 420, loss = 0.47679369\n",
      "Iteration 421, loss = 0.47692750\n",
      "Iteration 422, loss = 0.47670791\n",
      "Iteration 423, loss = 0.47667648\n",
      "Iteration 424, loss = 0.47647386\n",
      "Iteration 425, loss = 0.47685969\n",
      "Iteration 426, loss = 0.47681950\n",
      "Iteration 427, loss = 0.47666576\n",
      "Iteration 428, loss = 0.47640974\n",
      "Iteration 429, loss = 0.47641048\n",
      "Iteration 430, loss = 0.47657386\n",
      "Iteration 431, loss = 0.47625244\n",
      "Iteration 432, loss = 0.47626323\n",
      "Iteration 433, loss = 0.47669064\n",
      "Iteration 434, loss = 0.47627880\n",
      "Iteration 435, loss = 0.47674421\n",
      "Iteration 436, loss = 0.47616678\n",
      "Iteration 437, loss = 0.47605151\n",
      "Iteration 438, loss = 0.47596422\n",
      "Iteration 439, loss = 0.47625283\n",
      "Iteration 440, loss = 0.47624837\n",
      "Iteration 441, loss = 0.47594388\n",
      "Iteration 442, loss = 0.47593829\n",
      "Iteration 443, loss = 0.47564163\n",
      "Iteration 444, loss = 0.47577973\n",
      "Iteration 445, loss = 0.47607812\n",
      "Iteration 446, loss = 0.47588736\n",
      "Iteration 447, loss = 0.47565921\n",
      "Iteration 448, loss = 0.47551559\n",
      "Iteration 449, loss = 0.47541542\n",
      "Iteration 450, loss = 0.47617737\n",
      "Iteration 451, loss = 0.47633207\n",
      "Iteration 452, loss = 0.47561328\n",
      "Iteration 453, loss = 0.47517595\n",
      "Iteration 454, loss = 0.47519497\n",
      "Iteration 455, loss = 0.47520496\n",
      "Iteration 456, loss = 0.47522940\n",
      "Iteration 457, loss = 0.47530520\n",
      "Iteration 458, loss = 0.47523836\n",
      "Iteration 459, loss = 0.47529921\n",
      "Iteration 460, loss = 0.47513485\n",
      "Iteration 461, loss = 0.47522090\n",
      "Iteration 462, loss = 0.47508707\n",
      "Iteration 463, loss = 0.47492974\n",
      "Iteration 464, loss = 0.47499962\n",
      "Iteration 465, loss = 0.47486066\n",
      "Iteration 466, loss = 0.47467453\n",
      "Iteration 467, loss = 0.47506510\n",
      "Iteration 468, loss = 0.47479997\n",
      "Iteration 469, loss = 0.47451525\n",
      "Iteration 470, loss = 0.47479627\n",
      "Iteration 471, loss = 0.47470583\n",
      "Iteration 472, loss = 0.47493769\n",
      "Iteration 473, loss = 0.47468877\n",
      "Iteration 474, loss = 0.47474038\n",
      "Iteration 475, loss = 0.47447171\n",
      "Iteration 476, loss = 0.47482341\n",
      "Iteration 477, loss = 0.47463924\n",
      "Iteration 478, loss = 0.47476452\n",
      "Iteration 479, loss = 0.47446841\n",
      "Iteration 480, loss = 0.47478359\n",
      "Iteration 481, loss = 0.47442704\n",
      "Iteration 482, loss = 0.47415691\n",
      "Iteration 483, loss = 0.47467061\n",
      "Iteration 484, loss = 0.47407126\n",
      "Iteration 485, loss = 0.47426706\n",
      "Iteration 486, loss = 0.47403845\n",
      "Iteration 487, loss = 0.47431730\n",
      "Iteration 488, loss = 0.47395642\n",
      "Iteration 489, loss = 0.47403853\n",
      "Iteration 490, loss = 0.47387315\n",
      "Iteration 491, loss = 0.47403954\n",
      "Iteration 492, loss = 0.47394523\n",
      "Iteration 493, loss = 0.47392726\n",
      "Iteration 494, loss = 0.47363474\n",
      "Iteration 495, loss = 0.47426832\n",
      "Iteration 496, loss = 0.47398197\n",
      "Iteration 497, loss = 0.47392410\n",
      "Iteration 498, loss = 0.47391619\n",
      "Iteration 499, loss = 0.47368974\n",
      "Iteration 500, loss = 0.47409038\n",
      "Iteration 501, loss = 0.47383797\n",
      "Iteration 502, loss = 0.47391467\n",
      "Iteration 503, loss = 0.47349034\n",
      "Iteration 504, loss = 0.47365210\n",
      "Iteration 505, loss = 0.47348135\n",
      "Iteration 506, loss = 0.47346990\n",
      "Iteration 507, loss = 0.47339932\n",
      "Iteration 508, loss = 0.47346850\n",
      "Iteration 509, loss = 0.47336052\n",
      "Iteration 510, loss = 0.47339413\n",
      "Iteration 511, loss = 0.47356713\n",
      "Iteration 512, loss = 0.47440740\n",
      "Iteration 513, loss = 0.47378320\n",
      "Iteration 514, loss = 0.47344670\n",
      "Iteration 515, loss = 0.47364181\n",
      "Iteration 516, loss = 0.47346924\n",
      "Iteration 517, loss = 0.47363356\n",
      "Iteration 518, loss = 0.47344244\n",
      "Iteration 519, loss = 0.47306514\n",
      "Iteration 520, loss = 0.47314198\n",
      "Iteration 521, loss = 0.47356934\n",
      "Iteration 522, loss = 0.47298287\n",
      "Iteration 523, loss = 0.47300690\n",
      "Iteration 524, loss = 0.47298545\n",
      "Iteration 525, loss = 0.47290072\n",
      "Iteration 526, loss = 0.47305881\n",
      "Iteration 527, loss = 0.47295154\n",
      "Iteration 528, loss = 0.47364309\n",
      "Iteration 529, loss = 0.47299568\n",
      "Iteration 530, loss = 0.47287713\n",
      "Iteration 531, loss = 0.47271792\n",
      "Iteration 532, loss = 0.47276820\n",
      "Iteration 533, loss = 0.47281923\n",
      "Iteration 534, loss = 0.47255659\n",
      "Iteration 535, loss = 0.47278629\n",
      "Iteration 536, loss = 0.47262779\n",
      "Iteration 537, loss = 0.47253076\n",
      "Iteration 538, loss = 0.47260531\n",
      "Iteration 539, loss = 0.47257605\n",
      "Iteration 540, loss = 0.47252739\n",
      "Iteration 541, loss = 0.47256915\n",
      "Iteration 542, loss = 0.47241625\n",
      "Iteration 543, loss = 0.47254166\n",
      "Iteration 544, loss = 0.47254266\n",
      "Iteration 545, loss = 0.47236201\n",
      "Iteration 546, loss = 0.47222610\n",
      "Iteration 547, loss = 0.47233947\n",
      "Iteration 548, loss = 0.47227271\n",
      "Iteration 549, loss = 0.47202830\n",
      "Iteration 550, loss = 0.47222440\n",
      "Iteration 551, loss = 0.47244445\n",
      "Iteration 552, loss = 0.47191727\n",
      "Iteration 553, loss = 0.47194350\n",
      "Iteration 554, loss = 0.47202926\n",
      "Iteration 555, loss = 0.47203263\n",
      "Iteration 556, loss = 0.47199571\n",
      "Iteration 557, loss = 0.47178567\n",
      "Iteration 558, loss = 0.47210768\n",
      "Iteration 559, loss = 0.47169141\n",
      "Iteration 560, loss = 0.47183972\n",
      "Iteration 561, loss = 0.47185614\n",
      "Iteration 562, loss = 0.47171383\n",
      "Iteration 563, loss = 0.47194883\n",
      "Iteration 564, loss = 0.47148163\n",
      "Iteration 565, loss = 0.47162543\n",
      "Iteration 566, loss = 0.47158027\n",
      "Iteration 567, loss = 0.47142157\n",
      "Iteration 568, loss = 0.47146217\n",
      "Iteration 569, loss = 0.47141214\n",
      "Iteration 570, loss = 0.47130926\n",
      "Iteration 571, loss = 0.47173093\n",
      "Iteration 572, loss = 0.47135563\n",
      "Iteration 573, loss = 0.47138590\n",
      "Iteration 574, loss = 0.47146038\n",
      "Iteration 575, loss = 0.47115171\n",
      "Iteration 576, loss = 0.47188566\n",
      "Iteration 577, loss = 0.47158235\n",
      "Iteration 578, loss = 0.47106476\n",
      "Iteration 579, loss = 0.47111310\n",
      "Iteration 580, loss = 0.47097952\n",
      "Iteration 581, loss = 0.47101833\n",
      "Iteration 582, loss = 0.47115309\n",
      "Iteration 583, loss = 0.47094283\n",
      "Iteration 584, loss = 0.47067031\n",
      "Iteration 585, loss = 0.47091684\n",
      "Iteration 586, loss = 0.47095006\n",
      "Iteration 587, loss = 0.47183963\n",
      "Iteration 588, loss = 0.47157102\n",
      "Iteration 589, loss = 0.47135650\n",
      "Iteration 590, loss = 0.47056108\n",
      "Iteration 591, loss = 0.47076698\n",
      "Iteration 592, loss = 0.47060902\n",
      "Iteration 593, loss = 0.47065937\n",
      "Iteration 594, loss = 0.47040993\n",
      "Iteration 595, loss = 0.47045246\n",
      "Iteration 596, loss = 0.47053918\n",
      "Iteration 597, loss = 0.47051340\n",
      "Iteration 598, loss = 0.47108274\n",
      "Iteration 599, loss = 0.47076110\n",
      "Iteration 600, loss = 0.47026132\n",
      "Iteration 601, loss = 0.47043528\n",
      "Iteration 602, loss = 0.46994181\n",
      "Iteration 603, loss = 0.47018610\n",
      "Iteration 604, loss = 0.47022955\n",
      "Iteration 605, loss = 0.47061651\n",
      "Iteration 606, loss = 0.47007969\n",
      "Iteration 607, loss = 0.47030487\n",
      "Iteration 608, loss = 0.46984030\n",
      "Iteration 609, loss = 0.47002267\n",
      "Iteration 610, loss = 0.46971046\n",
      "Iteration 611, loss = 0.46984712\n",
      "Iteration 612, loss = 0.46983559\n",
      "Iteration 613, loss = 0.46975122\n",
      "Iteration 614, loss = 0.46975323\n",
      "Iteration 615, loss = 0.46969451\n",
      "Iteration 616, loss = 0.46950672\n",
      "Iteration 617, loss = 0.46962560\n",
      "Iteration 618, loss = 0.46967251\n",
      "Iteration 619, loss = 0.46955342\n",
      "Iteration 620, loss = 0.46975760\n",
      "Iteration 621, loss = 0.46961624\n",
      "Iteration 622, loss = 0.46932365\n",
      "Iteration 623, loss = 0.46933493\n",
      "Iteration 624, loss = 0.46937013\n",
      "Iteration 625, loss = 0.46953310\n",
      "Iteration 626, loss = 0.46941361\n",
      "Iteration 627, loss = 0.46920713\n",
      "Iteration 628, loss = 0.46943363\n",
      "Iteration 629, loss = 0.46948943\n",
      "Iteration 630, loss = 0.46897752\n",
      "Iteration 631, loss = 0.46909865\n",
      "Iteration 632, loss = 0.46900936\n",
      "Iteration 633, loss = 0.46891211\n",
      "Iteration 634, loss = 0.46881708\n",
      "Iteration 635, loss = 0.46925565\n",
      "Iteration 636, loss = 0.46900109\n",
      "Iteration 637, loss = 0.46897303\n",
      "Iteration 638, loss = 0.46906122\n",
      "Iteration 639, loss = 0.46877912\n",
      "Iteration 640, loss = 0.46853317\n",
      "Iteration 641, loss = 0.46863491\n",
      "Iteration 642, loss = 0.46864808\n",
      "Iteration 643, loss = 0.46870809\n",
      "Iteration 644, loss = 0.46850206\n",
      "Iteration 645, loss = 0.46857891\n",
      "Iteration 646, loss = 0.46868814\n",
      "Iteration 647, loss = 0.46853696\n",
      "Iteration 648, loss = 0.46888156\n",
      "Iteration 649, loss = 0.46857818\n",
      "Iteration 650, loss = 0.46842326\n",
      "Iteration 651, loss = 0.46855530\n",
      "Iteration 652, loss = 0.46863247\n",
      "Iteration 653, loss = 0.46830687\n",
      "Iteration 654, loss = 0.46863167\n",
      "Iteration 655, loss = 0.46817901\n",
      "Iteration 656, loss = 0.46852067\n",
      "Iteration 657, loss = 0.46814692\n",
      "Iteration 658, loss = 0.46806406\n",
      "Iteration 659, loss = 0.46817307\n",
      "Iteration 660, loss = 0.46838138\n",
      "Iteration 661, loss = 0.46799910\n",
      "Iteration 662, loss = 0.46793536\n",
      "Iteration 663, loss = 0.46824495\n",
      "Iteration 664, loss = 0.46783009\n",
      "Iteration 665, loss = 0.46784225\n",
      "Iteration 666, loss = 0.46804772\n",
      "Iteration 667, loss = 0.46788231\n",
      "Iteration 668, loss = 0.46798361\n",
      "Iteration 669, loss = 0.46795331\n",
      "Iteration 670, loss = 0.46814318\n",
      "Iteration 671, loss = 0.46809660\n",
      "Iteration 672, loss = 0.46821414\n",
      "Iteration 673, loss = 0.46781896\n",
      "Iteration 674, loss = 0.46790166\n",
      "Iteration 675, loss = 0.46807247\n",
      "Iteration 676, loss = 0.46787214\n",
      "Iteration 677, loss = 0.46743709\n",
      "Iteration 678, loss = 0.46788888\n",
      "Iteration 679, loss = 0.46828448\n",
      "Iteration 680, loss = 0.46746225\n",
      "Iteration 681, loss = 0.46775068\n",
      "Iteration 682, loss = 0.46811858\n",
      "Iteration 683, loss = 0.46743298\n",
      "Iteration 684, loss = 0.46754086\n",
      "Iteration 685, loss = 0.46739132\n",
      "Iteration 686, loss = 0.46791664\n",
      "Iteration 687, loss = 0.46767674\n",
      "Iteration 688, loss = 0.46748361\n",
      "Iteration 689, loss = 0.46766933\n",
      "Iteration 690, loss = 0.46736451\n",
      "Iteration 691, loss = 0.46747894\n",
      "Iteration 692, loss = 0.46747983\n",
      "Iteration 693, loss = 0.46749791\n",
      "Iteration 694, loss = 0.46744616\n",
      "Iteration 695, loss = 0.46736894\n",
      "Iteration 696, loss = 0.46768933\n",
      "Iteration 697, loss = 0.46755084\n",
      "Iteration 698, loss = 0.46740013\n",
      "Iteration 699, loss = 0.46751058\n",
      "Iteration 700, loss = 0.46707377\n",
      "Iteration 701, loss = 0.46721353\n",
      "Iteration 702, loss = 0.46775925\n",
      "Iteration 703, loss = 0.46716024\n",
      "Iteration 704, loss = 0.46711117\n",
      "Iteration 705, loss = 0.46732984\n",
      "Iteration 706, loss = 0.46706150\n",
      "Iteration 707, loss = 0.46682557\n",
      "Iteration 708, loss = 0.46699036\n",
      "Iteration 709, loss = 0.46714234\n",
      "Iteration 710, loss = 0.46714809\n",
      "Iteration 711, loss = 0.46700505\n",
      "Iteration 712, loss = 0.46690151\n",
      "Iteration 713, loss = 0.46676801\n",
      "Iteration 714, loss = 0.46693917\n",
      "Iteration 715, loss = 0.46680753\n",
      "Iteration 716, loss = 0.46697965\n",
      "Iteration 717, loss = 0.46682352\n",
      "Iteration 718, loss = 0.46694593\n",
      "Iteration 719, loss = 0.46669530\n",
      "Iteration 720, loss = 0.46687498\n",
      "Iteration 721, loss = 0.46684350\n",
      "Iteration 722, loss = 0.46684143\n",
      "Iteration 723, loss = 0.46699371\n",
      "Iteration 724, loss = 0.46691928\n",
      "Iteration 725, loss = 0.46677774\n",
      "Iteration 726, loss = 0.46664668\n",
      "Iteration 727, loss = 0.46665060\n",
      "Iteration 728, loss = 0.46650495\n",
      "Iteration 729, loss = 0.46664940\n",
      "Iteration 730, loss = 0.46655059\n",
      "Iteration 731, loss = 0.46668533\n",
      "Iteration 732, loss = 0.46674584\n",
      "Iteration 733, loss = 0.46649284\n",
      "Iteration 734, loss = 0.46648765\n",
      "Iteration 735, loss = 0.46622950\n",
      "Iteration 736, loss = 0.46673953\n",
      "Iteration 737, loss = 0.46618361\n",
      "Iteration 738, loss = 0.46643343\n",
      "Iteration 739, loss = 0.46684712\n",
      "Iteration 740, loss = 0.46644704\n",
      "Iteration 741, loss = 0.46640652\n",
      "Iteration 742, loss = 0.46655980\n",
      "Iteration 743, loss = 0.46625742\n",
      "Iteration 744, loss = 0.46634742\n",
      "Iteration 745, loss = 0.46628019\n",
      "Iteration 746, loss = 0.46618183\n",
      "Iteration 747, loss = 0.46610448\n",
      "Iteration 748, loss = 0.46620324\n",
      "Iteration 749, loss = 0.46607308\n",
      "Iteration 750, loss = 0.46605684\n",
      "Iteration 751, loss = 0.46581548\n",
      "Iteration 752, loss = 0.46646183\n",
      "Iteration 753, loss = 0.46620113\n",
      "Iteration 754, loss = 0.46600674\n",
      "Iteration 755, loss = 0.46639121\n",
      "Iteration 756, loss = 0.46600770\n",
      "Iteration 757, loss = 0.46606216\n",
      "Iteration 758, loss = 0.46609415\n",
      "Iteration 759, loss = 0.46601663\n",
      "Iteration 760, loss = 0.46585724\n",
      "Iteration 761, loss = 0.46595752\n",
      "Iteration 762, loss = 0.46600168\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72608822\n",
      "Iteration 2, loss = 0.70210481\n",
      "Iteration 3, loss = 0.68501051\n",
      "Iteration 4, loss = 0.67166484\n",
      "Iteration 5, loss = 0.65917992\n",
      "Iteration 6, loss = 0.64678230\n",
      "Iteration 7, loss = 0.63450053\n",
      "Iteration 8, loss = 0.62241836\n",
      "Iteration 9, loss = 0.61087492\n",
      "Iteration 10, loss = 0.59959712\n",
      "Iteration 11, loss = 0.58966421\n",
      "Iteration 12, loss = 0.58122651\n",
      "Iteration 13, loss = 0.57384455\n",
      "Iteration 14, loss = 0.56789187\n",
      "Iteration 15, loss = 0.56270955\n",
      "Iteration 16, loss = 0.55837781\n",
      "Iteration 17, loss = 0.55479156\n",
      "Iteration 18, loss = 0.55172587\n",
      "Iteration 19, loss = 0.54922627\n",
      "Iteration 20, loss = 0.54670828\n",
      "Iteration 21, loss = 0.54461931\n",
      "Iteration 22, loss = 0.54282398\n",
      "Iteration 23, loss = 0.54100335\n",
      "Iteration 24, loss = 0.53903572\n",
      "Iteration 25, loss = 0.53758021\n",
      "Iteration 26, loss = 0.53575742\n",
      "Iteration 27, loss = 0.53405917\n",
      "Iteration 28, loss = 0.53233969\n",
      "Iteration 29, loss = 0.53062759\n",
      "Iteration 30, loss = 0.52921561\n",
      "Iteration 31, loss = 0.52779804\n",
      "Iteration 32, loss = 0.52637019\n",
      "Iteration 33, loss = 0.52514967\n",
      "Iteration 34, loss = 0.52379315\n",
      "Iteration 35, loss = 0.52311459\n",
      "Iteration 36, loss = 0.52165110\n",
      "Iteration 37, loss = 0.52030953\n",
      "Iteration 38, loss = 0.51938318\n",
      "Iteration 39, loss = 0.51836067\n",
      "Iteration 40, loss = 0.51756232\n",
      "Iteration 41, loss = 0.51643616\n",
      "Iteration 42, loss = 0.51588557\n",
      "Iteration 43, loss = 0.51482718\n",
      "Iteration 44, loss = 0.51450332\n",
      "Iteration 45, loss = 0.51348007\n",
      "Iteration 46, loss = 0.51289575\n",
      "Iteration 47, loss = 0.51221669\n",
      "Iteration 48, loss = 0.51162102\n",
      "Iteration 49, loss = 0.51118876\n",
      "Iteration 50, loss = 0.51051961\n",
      "Iteration 51, loss = 0.50976638\n",
      "Iteration 52, loss = 0.50901169\n",
      "Iteration 53, loss = 0.50844790\n",
      "Iteration 54, loss = 0.50795328\n",
      "Iteration 55, loss = 0.50767854\n",
      "Iteration 56, loss = 0.50709049\n",
      "Iteration 57, loss = 0.50655838\n",
      "Iteration 58, loss = 0.50605307\n",
      "Iteration 59, loss = 0.50588474\n",
      "Iteration 60, loss = 0.50493516\n",
      "Iteration 61, loss = 0.50496189\n",
      "Iteration 62, loss = 0.50394163\n",
      "Iteration 63, loss = 0.50354350\n",
      "Iteration 64, loss = 0.50307578\n",
      "Iteration 65, loss = 0.50270358\n",
      "Iteration 66, loss = 0.50229919\n",
      "Iteration 67, loss = 0.50214386\n",
      "Iteration 68, loss = 0.50159626\n",
      "Iteration 69, loss = 0.50131702\n",
      "Iteration 70, loss = 0.50067755\n",
      "Iteration 71, loss = 0.50018858\n",
      "Iteration 72, loss = 0.50046481\n",
      "Iteration 73, loss = 0.49963690\n",
      "Iteration 74, loss = 0.49912485\n",
      "Iteration 75, loss = 0.49888464\n",
      "Iteration 76, loss = 0.49840851\n",
      "Iteration 77, loss = 0.49845267\n",
      "Iteration 78, loss = 0.49796531\n",
      "Iteration 79, loss = 0.49762992\n",
      "Iteration 80, loss = 0.49756590\n",
      "Iteration 81, loss = 0.49683339\n",
      "Iteration 82, loss = 0.49697633\n",
      "Iteration 83, loss = 0.49667297\n",
      "Iteration 84, loss = 0.49645275\n",
      "Iteration 85, loss = 0.49683538\n",
      "Iteration 86, loss = 0.49604444\n",
      "Iteration 87, loss = 0.49587005\n",
      "Iteration 88, loss = 0.49604443\n",
      "Iteration 89, loss = 0.49539418\n",
      "Iteration 90, loss = 0.49555094\n",
      "Iteration 91, loss = 0.49491039\n",
      "Iteration 92, loss = 0.49493720\n",
      "Iteration 93, loss = 0.49448735\n",
      "Iteration 94, loss = 0.49433134\n",
      "Iteration 95, loss = 0.49406611\n",
      "Iteration 96, loss = 0.49440113\n",
      "Iteration 97, loss = 0.49396761\n",
      "Iteration 98, loss = 0.49380652\n",
      "Iteration 99, loss = 0.49415651\n",
      "Iteration 100, loss = 0.49311838\n",
      "Iteration 101, loss = 0.49299979\n",
      "Iteration 102, loss = 0.49303668\n",
      "Iteration 103, loss = 0.49304065\n",
      "Iteration 104, loss = 0.49287588\n",
      "Iteration 105, loss = 0.49241637\n",
      "Iteration 106, loss = 0.49231602\n",
      "Iteration 107, loss = 0.49221614\n",
      "Iteration 108, loss = 0.49218588\n",
      "Iteration 109, loss = 0.49240935\n",
      "Iteration 110, loss = 0.49163915\n",
      "Iteration 111, loss = 0.49196131\n",
      "Iteration 112, loss = 0.49169402\n",
      "Iteration 113, loss = 0.49140137\n",
      "Iteration 114, loss = 0.49153087\n",
      "Iteration 115, loss = 0.49116413\n",
      "Iteration 116, loss = 0.49138936\n",
      "Iteration 117, loss = 0.49076891\n",
      "Iteration 118, loss = 0.49085622\n",
      "Iteration 119, loss = 0.49065453\n",
      "Iteration 120, loss = 0.49060805\n",
      "Iteration 121, loss = 0.49070472\n",
      "Iteration 122, loss = 0.49026059\n",
      "Iteration 123, loss = 0.49043162\n",
      "Iteration 124, loss = 0.49007262\n",
      "Iteration 125, loss = 0.49004370\n",
      "Iteration 126, loss = 0.48992238\n",
      "Iteration 127, loss = 0.48986170\n",
      "Iteration 128, loss = 0.48946079\n",
      "Iteration 129, loss = 0.48964056\n",
      "Iteration 130, loss = 0.48964574\n",
      "Iteration 131, loss = 0.48909416\n",
      "Iteration 132, loss = 0.48949647\n",
      "Iteration 133, loss = 0.48930501\n",
      "Iteration 134, loss = 0.48883008\n",
      "Iteration 135, loss = 0.48881464\n",
      "Iteration 136, loss = 0.48882750\n",
      "Iteration 137, loss = 0.48861319\n",
      "Iteration 138, loss = 0.48836180\n",
      "Iteration 139, loss = 0.48852687\n",
      "Iteration 140, loss = 0.48837050\n",
      "Iteration 141, loss = 0.48821240\n",
      "Iteration 142, loss = 0.48802651\n",
      "Iteration 143, loss = 0.48807369\n",
      "Iteration 144, loss = 0.48806871\n",
      "Iteration 145, loss = 0.48780643\n",
      "Iteration 146, loss = 0.48756789\n",
      "Iteration 147, loss = 0.48770268\n",
      "Iteration 148, loss = 0.48758766\n",
      "Iteration 149, loss = 0.48752104\n",
      "Iteration 150, loss = 0.48733746\n",
      "Iteration 151, loss = 0.48705562\n",
      "Iteration 152, loss = 0.48683153\n",
      "Iteration 153, loss = 0.48679199\n",
      "Iteration 154, loss = 0.48654163\n",
      "Iteration 155, loss = 0.48691577\n",
      "Iteration 156, loss = 0.48655431\n",
      "Iteration 157, loss = 0.48636798\n",
      "Iteration 158, loss = 0.48664613\n",
      "Iteration 159, loss = 0.48620855\n",
      "Iteration 160, loss = 0.48619958\n",
      "Iteration 161, loss = 0.48628395\n",
      "Iteration 162, loss = 0.48688756\n",
      "Iteration 163, loss = 0.48626125\n",
      "Iteration 164, loss = 0.48581990\n",
      "Iteration 165, loss = 0.48602927\n",
      "Iteration 166, loss = 0.48571189\n",
      "Iteration 167, loss = 0.48546836\n",
      "Iteration 168, loss = 0.48563294\n",
      "Iteration 169, loss = 0.48533341\n",
      "Iteration 170, loss = 0.48545021\n",
      "Iteration 171, loss = 0.48519241\n",
      "Iteration 172, loss = 0.48506420\n",
      "Iteration 173, loss = 0.48518795\n",
      "Iteration 174, loss = 0.48482110\n",
      "Iteration 175, loss = 0.48508693\n",
      "Iteration 176, loss = 0.48482782\n",
      "Iteration 177, loss = 0.48481159\n",
      "Iteration 178, loss = 0.48466416\n",
      "Iteration 179, loss = 0.48437243\n",
      "Iteration 180, loss = 0.48471913\n",
      "Iteration 181, loss = 0.48425408\n",
      "Iteration 182, loss = 0.48444329\n",
      "Iteration 183, loss = 0.48455900\n",
      "Iteration 184, loss = 0.48416254\n",
      "Iteration 185, loss = 0.48433653\n",
      "Iteration 186, loss = 0.48429035\n",
      "Iteration 187, loss = 0.48441876\n",
      "Iteration 188, loss = 0.48385089\n",
      "Iteration 189, loss = 0.48399865\n",
      "Iteration 190, loss = 0.48378913\n",
      "Iteration 191, loss = 0.48396580\n",
      "Iteration 192, loss = 0.48372044\n",
      "Iteration 193, loss = 0.48377580\n",
      "Iteration 194, loss = 0.48375127\n",
      "Iteration 195, loss = 0.48408287\n",
      "Iteration 196, loss = 0.48340538\n",
      "Iteration 197, loss = 0.48395444\n",
      "Iteration 198, loss = 0.48346291\n",
      "Iteration 199, loss = 0.48347149\n",
      "Iteration 200, loss = 0.48341414\n",
      "Iteration 201, loss = 0.48324662\n",
      "Iteration 202, loss = 0.48329493\n",
      "Iteration 203, loss = 0.48316896\n",
      "Iteration 204, loss = 0.48320155\n",
      "Iteration 205, loss = 0.48334184\n",
      "Iteration 206, loss = 0.48309250\n",
      "Iteration 207, loss = 0.48322104\n",
      "Iteration 208, loss = 0.48335184\n",
      "Iteration 209, loss = 0.48295515\n",
      "Iteration 210, loss = 0.48311789\n",
      "Iteration 211, loss = 0.48282391\n",
      "Iteration 212, loss = 0.48264308\n",
      "Iteration 213, loss = 0.48298027\n",
      "Iteration 214, loss = 0.48297858\n",
      "Iteration 215, loss = 0.48258865\n",
      "Iteration 216, loss = 0.48260810\n",
      "Iteration 217, loss = 0.48258449\n",
      "Iteration 218, loss = 0.48261519\n",
      "Iteration 219, loss = 0.48234114\n",
      "Iteration 220, loss = 0.48259638\n",
      "Iteration 221, loss = 0.48231311\n",
      "Iteration 222, loss = 0.48300295\n",
      "Iteration 223, loss = 0.48252342\n",
      "Iteration 224, loss = 0.48203370\n",
      "Iteration 225, loss = 0.48250806\n",
      "Iteration 226, loss = 0.48211772\n",
      "Iteration 227, loss = 0.48226266\n",
      "Iteration 228, loss = 0.48221966\n",
      "Iteration 229, loss = 0.48235197\n",
      "Iteration 230, loss = 0.48198521\n",
      "Iteration 231, loss = 0.48155970\n",
      "Iteration 232, loss = 0.48175355\n",
      "Iteration 233, loss = 0.48203028\n",
      "Iteration 234, loss = 0.48186548\n",
      "Iteration 235, loss = 0.48164131\n",
      "Iteration 236, loss = 0.48181430\n",
      "Iteration 237, loss = 0.48166168\n",
      "Iteration 238, loss = 0.48175051\n",
      "Iteration 239, loss = 0.48128161\n",
      "Iteration 240, loss = 0.48137208\n",
      "Iteration 241, loss = 0.48167184\n",
      "Iteration 242, loss = 0.48100353\n",
      "Iteration 243, loss = 0.48150220\n",
      "Iteration 244, loss = 0.48157156\n",
      "Iteration 245, loss = 0.48115745\n",
      "Iteration 246, loss = 0.48160035\n",
      "Iteration 247, loss = 0.48112779\n",
      "Iteration 248, loss = 0.48109881\n",
      "Iteration 249, loss = 0.48106692\n",
      "Iteration 250, loss = 0.48093294\n",
      "Iteration 251, loss = 0.48091961\n",
      "Iteration 252, loss = 0.48120310\n",
      "Iteration 253, loss = 0.48088939\n",
      "Iteration 254, loss = 0.48093784\n",
      "Iteration 255, loss = 0.48066874\n",
      "Iteration 256, loss = 0.48054320\n",
      "Iteration 257, loss = 0.48054935\n",
      "Iteration 258, loss = 0.48052555\n",
      "Iteration 259, loss = 0.48055281\n",
      "Iteration 260, loss = 0.48086074\n",
      "Iteration 261, loss = 0.48051439\n",
      "Iteration 262, loss = 0.48058536\n",
      "Iteration 263, loss = 0.48046444\n",
      "Iteration 264, loss = 0.48054778\n",
      "Iteration 265, loss = 0.47997283\n",
      "Iteration 266, loss = 0.48065490\n",
      "Iteration 267, loss = 0.48043701\n",
      "Iteration 268, loss = 0.48024508\n",
      "Iteration 269, loss = 0.48012182\n",
      "Iteration 270, loss = 0.48005141\n",
      "Iteration 271, loss = 0.48006327\n",
      "Iteration 272, loss = 0.47975491\n",
      "Iteration 273, loss = 0.48021999\n",
      "Iteration 274, loss = 0.47984990\n",
      "Iteration 275, loss = 0.47984555\n",
      "Iteration 276, loss = 0.47983978\n",
      "Iteration 277, loss = 0.48022997\n",
      "Iteration 278, loss = 0.47963838\n",
      "Iteration 279, loss = 0.47989635\n",
      "Iteration 280, loss = 0.47978674\n",
      "Iteration 281, loss = 0.47965732\n",
      "Iteration 282, loss = 0.47944304\n",
      "Iteration 283, loss = 0.47941177\n",
      "Iteration 284, loss = 0.47946395\n",
      "Iteration 285, loss = 0.47966192\n",
      "Iteration 286, loss = 0.47932406\n",
      "Iteration 287, loss = 0.47983841\n",
      "Iteration 288, loss = 0.47955682\n",
      "Iteration 289, loss = 0.47950792\n",
      "Iteration 290, loss = 0.47945484\n",
      "Iteration 291, loss = 0.47913190\n",
      "Iteration 292, loss = 0.47912685\n",
      "Iteration 293, loss = 0.47926945\n",
      "Iteration 294, loss = 0.47920943\n",
      "Iteration 295, loss = 0.47915562\n",
      "Iteration 296, loss = 0.47913586\n",
      "Iteration 297, loss = 0.47938573\n",
      "Iteration 298, loss = 0.47910955\n",
      "Iteration 299, loss = 0.47923707\n",
      "Iteration 300, loss = 0.47877699\n",
      "Iteration 301, loss = 0.47896671\n",
      "Iteration 302, loss = 0.47903643\n",
      "Iteration 303, loss = 0.47879723\n",
      "Iteration 304, loss = 0.47882934\n",
      "Iteration 305, loss = 0.47896860\n",
      "Iteration 306, loss = 0.47900037\n",
      "Iteration 307, loss = 0.47877687\n",
      "Iteration 308, loss = 0.47861970\n",
      "Iteration 309, loss = 0.47873311\n",
      "Iteration 310, loss = 0.47858406\n",
      "Iteration 311, loss = 0.47895484\n",
      "Iteration 312, loss = 0.47830451\n",
      "Iteration 313, loss = 0.47859777\n",
      "Iteration 314, loss = 0.47849839\n",
      "Iteration 315, loss = 0.47944917\n",
      "Iteration 316, loss = 0.47824528\n",
      "Iteration 317, loss = 0.47885347\n",
      "Iteration 318, loss = 0.47869520\n",
      "Iteration 319, loss = 0.47813678\n",
      "Iteration 320, loss = 0.47875068\n",
      "Iteration 321, loss = 0.47832783\n",
      "Iteration 322, loss = 0.47830350\n",
      "Iteration 323, loss = 0.47905355\n",
      "Iteration 324, loss = 0.47872560\n",
      "Iteration 325, loss = 0.47852172\n",
      "Iteration 326, loss = 0.47803277\n",
      "Iteration 327, loss = 0.47826163\n",
      "Iteration 328, loss = 0.47799227\n",
      "Iteration 329, loss = 0.47805633\n",
      "Iteration 330, loss = 0.47793716\n",
      "Iteration 331, loss = 0.47776088\n",
      "Iteration 332, loss = 0.47794087\n",
      "Iteration 333, loss = 0.47781688\n",
      "Iteration 334, loss = 0.47777241\n",
      "Iteration 335, loss = 0.47789455\n",
      "Iteration 336, loss = 0.47804603\n",
      "Iteration 337, loss = 0.47822999\n",
      "Iteration 338, loss = 0.47771443\n",
      "Iteration 339, loss = 0.47767629\n",
      "Iteration 340, loss = 0.47760160\n",
      "Iteration 341, loss = 0.47768192\n",
      "Iteration 342, loss = 0.47784651\n",
      "Iteration 343, loss = 0.47772792\n",
      "Iteration 344, loss = 0.47773115\n",
      "Iteration 345, loss = 0.47764975\n",
      "Iteration 346, loss = 0.47776956\n",
      "Iteration 347, loss = 0.47755994\n",
      "Iteration 348, loss = 0.47755786\n",
      "Iteration 349, loss = 0.47760367\n",
      "Iteration 350, loss = 0.47735763\n",
      "Iteration 351, loss = 0.47716210\n",
      "Iteration 352, loss = 0.47733985\n",
      "Iteration 353, loss = 0.47708116\n",
      "Iteration 354, loss = 0.47753760\n",
      "Iteration 355, loss = 0.47714612\n",
      "Iteration 356, loss = 0.47746664\n",
      "Iteration 357, loss = 0.47712931\n",
      "Iteration 358, loss = 0.47730000\n",
      "Iteration 359, loss = 0.47713645\n",
      "Iteration 360, loss = 0.47698062\n",
      "Iteration 361, loss = 0.47698311\n",
      "Iteration 362, loss = 0.47724290\n",
      "Iteration 363, loss = 0.47719482\n",
      "Iteration 364, loss = 0.47721815\n",
      "Iteration 365, loss = 0.47719472\n",
      "Iteration 366, loss = 0.47696713\n",
      "Iteration 367, loss = 0.47687647\n",
      "Iteration 368, loss = 0.47714397\n",
      "Iteration 369, loss = 0.47725456\n",
      "Iteration 370, loss = 0.47721649\n",
      "Iteration 371, loss = 0.47716363\n",
      "Iteration 372, loss = 0.47680290\n",
      "Iteration 373, loss = 0.47697370\n",
      "Iteration 374, loss = 0.47677006\n",
      "Iteration 375, loss = 0.47686331\n",
      "Iteration 376, loss = 0.47697276\n",
      "Iteration 377, loss = 0.47670574\n",
      "Iteration 378, loss = 0.47680023\n",
      "Iteration 379, loss = 0.47675532\n",
      "Iteration 380, loss = 0.47681378\n",
      "Iteration 381, loss = 0.47678897\n",
      "Iteration 382, loss = 0.47660563\n",
      "Iteration 383, loss = 0.47657007\n",
      "Iteration 384, loss = 0.47668740\n",
      "Iteration 385, loss = 0.47680503\n",
      "Iteration 386, loss = 0.47640199\n",
      "Iteration 387, loss = 0.47644108\n",
      "Iteration 388, loss = 0.47653125\n",
      "Iteration 389, loss = 0.47637575\n",
      "Iteration 390, loss = 0.47642378\n",
      "Iteration 391, loss = 0.47628412\n",
      "Iteration 392, loss = 0.47615358\n",
      "Iteration 393, loss = 0.47615887\n",
      "Iteration 394, loss = 0.47621032\n",
      "Iteration 395, loss = 0.47627747\n",
      "Iteration 396, loss = 0.47602211\n",
      "Iteration 397, loss = 0.47629688\n",
      "Iteration 398, loss = 0.47633068\n",
      "Iteration 399, loss = 0.47633285\n",
      "Iteration 400, loss = 0.47614978\n",
      "Iteration 401, loss = 0.47631999\n",
      "Iteration 402, loss = 0.47612977\n",
      "Iteration 403, loss = 0.47594497\n",
      "Iteration 404, loss = 0.47623094\n",
      "Iteration 405, loss = 0.47624988\n",
      "Iteration 406, loss = 0.47591536\n",
      "Iteration 407, loss = 0.47616645\n",
      "Iteration 408, loss = 0.47637565\n",
      "Iteration 409, loss = 0.47665603\n",
      "Iteration 410, loss = 0.47588193\n",
      "Iteration 411, loss = 0.47617850\n",
      "Iteration 412, loss = 0.47580492\n",
      "Iteration 413, loss = 0.47660718\n",
      "Iteration 414, loss = 0.47614011\n",
      "Iteration 415, loss = 0.47578509\n",
      "Iteration 416, loss = 0.47606669\n",
      "Iteration 417, loss = 0.47591600\n",
      "Iteration 418, loss = 0.47573142\n",
      "Iteration 419, loss = 0.47585147\n",
      "Iteration 420, loss = 0.47568935\n",
      "Iteration 421, loss = 0.47584216\n",
      "Iteration 422, loss = 0.47597845\n",
      "Iteration 423, loss = 0.47575828\n",
      "Iteration 424, loss = 0.47558447\n",
      "Iteration 425, loss = 0.47558547\n",
      "Iteration 426, loss = 0.47553671\n",
      "Iteration 427, loss = 0.47574445\n",
      "Iteration 428, loss = 0.47557722\n",
      "Iteration 429, loss = 0.47564876\n",
      "Iteration 430, loss = 0.47574358\n",
      "Iteration 431, loss = 0.47556074\n",
      "Iteration 432, loss = 0.47553664\n",
      "Iteration 433, loss = 0.47563164\n",
      "Iteration 434, loss = 0.47565066\n",
      "Iteration 435, loss = 0.47565422\n",
      "Iteration 436, loss = 0.47573992\n",
      "Iteration 437, loss = 0.47533913\n",
      "Iteration 438, loss = 0.47559159\n",
      "Iteration 439, loss = 0.47558289\n",
      "Iteration 440, loss = 0.47567676\n",
      "Iteration 441, loss = 0.47507452\n",
      "Iteration 442, loss = 0.47529802\n",
      "Iteration 443, loss = 0.47541593\n",
      "Iteration 444, loss = 0.47576354\n",
      "Iteration 445, loss = 0.47529506\n",
      "Iteration 446, loss = 0.47529276\n",
      "Iteration 447, loss = 0.47525793\n",
      "Iteration 448, loss = 0.47506518\n",
      "Iteration 449, loss = 0.47511003\n",
      "Iteration 450, loss = 0.47516158\n",
      "Iteration 451, loss = 0.47498185\n",
      "Iteration 452, loss = 0.47524408\n",
      "Iteration 453, loss = 0.47523152\n",
      "Iteration 454, loss = 0.47521042\n",
      "Iteration 455, loss = 0.47503382\n",
      "Iteration 456, loss = 0.47484325\n",
      "Iteration 457, loss = 0.47506492\n",
      "Iteration 458, loss = 0.47513073\n",
      "Iteration 459, loss = 0.47466984\n",
      "Iteration 460, loss = 0.47486401\n",
      "Iteration 461, loss = 0.47532514\n",
      "Iteration 462, loss = 0.47455018\n",
      "Iteration 463, loss = 0.47483923\n",
      "Iteration 464, loss = 0.47475857\n",
      "Iteration 465, loss = 0.47498641\n",
      "Iteration 466, loss = 0.47460369\n",
      "Iteration 467, loss = 0.47457062\n",
      "Iteration 468, loss = 0.47459807\n",
      "Iteration 469, loss = 0.47466034\n",
      "Iteration 470, loss = 0.47463202\n",
      "Iteration 471, loss = 0.47507868\n",
      "Iteration 472, loss = 0.47443849\n",
      "Iteration 473, loss = 0.47439871\n",
      "Iteration 474, loss = 0.47451053\n",
      "Iteration 475, loss = 0.47496703\n",
      "Iteration 476, loss = 0.47446896\n",
      "Iteration 477, loss = 0.47446927\n",
      "Iteration 478, loss = 0.47421080\n",
      "Iteration 479, loss = 0.47437120\n",
      "Iteration 480, loss = 0.47439608\n",
      "Iteration 481, loss = 0.47419306\n",
      "Iteration 482, loss = 0.47423002\n",
      "Iteration 483, loss = 0.47420906\n",
      "Iteration 484, loss = 0.47460639\n",
      "Iteration 485, loss = 0.47413427\n",
      "Iteration 486, loss = 0.47415403\n",
      "Iteration 487, loss = 0.47429030\n",
      "Iteration 488, loss = 0.47494335\n",
      "Iteration 489, loss = 0.47474397\n",
      "Iteration 490, loss = 0.47483015\n",
      "Iteration 491, loss = 0.47391412\n",
      "Iteration 492, loss = 0.47383503\n",
      "Iteration 493, loss = 0.47415071\n",
      "Iteration 494, loss = 0.47381364\n",
      "Iteration 495, loss = 0.47405578\n",
      "Iteration 496, loss = 0.47390010\n",
      "Iteration 497, loss = 0.47401108\n",
      "Iteration 498, loss = 0.47452089\n",
      "Iteration 499, loss = 0.47384374\n",
      "Iteration 500, loss = 0.47389438\n",
      "Iteration 501, loss = 0.47389880\n",
      "Iteration 502, loss = 0.47375561\n",
      "Iteration 503, loss = 0.47405991\n",
      "Iteration 504, loss = 0.47414010\n",
      "Iteration 505, loss = 0.47384488\n",
      "Iteration 506, loss = 0.47368575\n",
      "Iteration 507, loss = 0.47379996\n",
      "Iteration 508, loss = 0.47380791\n",
      "Iteration 509, loss = 0.47377627\n",
      "Iteration 510, loss = 0.47364997\n",
      "Iteration 511, loss = 0.47385409\n",
      "Iteration 512, loss = 0.47382860\n",
      "Iteration 513, loss = 0.47397495\n",
      "Iteration 514, loss = 0.47388087\n",
      "Iteration 515, loss = 0.47385580\n",
      "Iteration 516, loss = 0.47402202\n",
      "Iteration 517, loss = 0.47374665\n",
      "Iteration 518, loss = 0.47380482\n",
      "Iteration 519, loss = 0.47441372\n",
      "Iteration 520, loss = 0.47346418\n",
      "Iteration 521, loss = 0.47373917\n",
      "Iteration 522, loss = 0.47349772\n",
      "Iteration 523, loss = 0.47337112\n",
      "Iteration 524, loss = 0.47357909\n",
      "Iteration 525, loss = 0.47359588\n",
      "Iteration 526, loss = 0.47320484\n",
      "Iteration 527, loss = 0.47342140\n",
      "Iteration 528, loss = 0.47368342\n",
      "Iteration 529, loss = 0.47334511\n",
      "Iteration 530, loss = 0.47346654\n",
      "Iteration 531, loss = 0.47368923\n",
      "Iteration 532, loss = 0.47407286\n",
      "Iteration 533, loss = 0.47327269\n",
      "Iteration 534, loss = 0.47341299\n",
      "Iteration 535, loss = 0.47367211\n",
      "Iteration 536, loss = 0.47343255\n",
      "Iteration 537, loss = 0.47340870\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69558441\n",
      "Iteration 2, loss = 0.67295118\n",
      "Iteration 3, loss = 0.65740950\n",
      "Iteration 4, loss = 0.64371118\n",
      "Iteration 5, loss = 0.63110937\n",
      "Iteration 6, loss = 0.61893737\n",
      "Iteration 7, loss = 0.60752824\n",
      "Iteration 8, loss = 0.59680726\n",
      "Iteration 9, loss = 0.58748128\n",
      "Iteration 10, loss = 0.57929232\n",
      "Iteration 11, loss = 0.57352753\n",
      "Iteration 12, loss = 0.56835558\n",
      "Iteration 13, loss = 0.56463629\n",
      "Iteration 14, loss = 0.56169500\n",
      "Iteration 15, loss = 0.55956618\n",
      "Iteration 16, loss = 0.55765348\n",
      "Iteration 17, loss = 0.55575628\n",
      "Iteration 18, loss = 0.55440447\n",
      "Iteration 19, loss = 0.55286945\n",
      "Iteration 20, loss = 0.55166729\n",
      "Iteration 21, loss = 0.55025650\n",
      "Iteration 22, loss = 0.54919919\n",
      "Iteration 23, loss = 0.54804724\n",
      "Iteration 24, loss = 0.54721615\n",
      "Iteration 25, loss = 0.54639403\n",
      "Iteration 26, loss = 0.54519387\n",
      "Iteration 27, loss = 0.54416495\n",
      "Iteration 28, loss = 0.54312805\n",
      "Iteration 29, loss = 0.54210999\n",
      "Iteration 30, loss = 0.54129576\n",
      "Iteration 31, loss = 0.54011138\n",
      "Iteration 32, loss = 0.53949503\n",
      "Iteration 33, loss = 0.53864869\n",
      "Iteration 34, loss = 0.53815262\n",
      "Iteration 35, loss = 0.53698932\n",
      "Iteration 36, loss = 0.53638487\n",
      "Iteration 37, loss = 0.53587489\n",
      "Iteration 38, loss = 0.53491534\n",
      "Iteration 39, loss = 0.53389169\n",
      "Iteration 40, loss = 0.53304514\n",
      "Iteration 41, loss = 0.53239235\n",
      "Iteration 42, loss = 0.53158985\n",
      "Iteration 43, loss = 0.53059284\n",
      "Iteration 44, loss = 0.52956745\n",
      "Iteration 45, loss = 0.52895850\n",
      "Iteration 46, loss = 0.52801718\n",
      "Iteration 47, loss = 0.52693592\n",
      "Iteration 48, loss = 0.52641034\n",
      "Iteration 49, loss = 0.52497186\n",
      "Iteration 50, loss = 0.52395833\n",
      "Iteration 51, loss = 0.52295683\n",
      "Iteration 52, loss = 0.52211990\n",
      "Iteration 53, loss = 0.52110463\n",
      "Iteration 54, loss = 0.52055800\n",
      "Iteration 55, loss = 0.51969875\n",
      "Iteration 56, loss = 0.51928393\n",
      "Iteration 57, loss = 0.51753586\n",
      "Iteration 58, loss = 0.51659657\n",
      "Iteration 59, loss = 0.51565139\n",
      "Iteration 60, loss = 0.51499122\n",
      "Iteration 61, loss = 0.51425268\n",
      "Iteration 62, loss = 0.51342564\n",
      "Iteration 63, loss = 0.51301285\n",
      "Iteration 64, loss = 0.51196978\n",
      "Iteration 65, loss = 0.51108005\n",
      "Iteration 66, loss = 0.51083932\n",
      "Iteration 67, loss = 0.50947570\n",
      "Iteration 68, loss = 0.50903147\n",
      "Iteration 69, loss = 0.50830112\n",
      "Iteration 70, loss = 0.50793288\n",
      "Iteration 71, loss = 0.50766779\n",
      "Iteration 72, loss = 0.50651373\n",
      "Iteration 73, loss = 0.50608469\n",
      "Iteration 74, loss = 0.50551920\n",
      "Iteration 75, loss = 0.50523853\n",
      "Iteration 76, loss = 0.50482165\n",
      "Iteration 77, loss = 0.50428592\n",
      "Iteration 78, loss = 0.50373799\n",
      "Iteration 79, loss = 0.50353118\n",
      "Iteration 80, loss = 0.50329701\n",
      "Iteration 81, loss = 0.50271788\n",
      "Iteration 82, loss = 0.50269220\n",
      "Iteration 83, loss = 0.50175833\n",
      "Iteration 84, loss = 0.50153099\n",
      "Iteration 85, loss = 0.50126607\n",
      "Iteration 86, loss = 0.50094388\n",
      "Iteration 87, loss = 0.50135936\n",
      "Iteration 88, loss = 0.50066906\n",
      "Iteration 89, loss = 0.50109125\n",
      "Iteration 90, loss = 0.49987619\n",
      "Iteration 91, loss = 0.49969713\n",
      "Iteration 92, loss = 0.49945007\n",
      "Iteration 93, loss = 0.49944739\n",
      "Iteration 94, loss = 0.49902206\n",
      "Iteration 95, loss = 0.49858096\n",
      "Iteration 96, loss = 0.49825792\n",
      "Iteration 97, loss = 0.49834172\n",
      "Iteration 98, loss = 0.49800506\n",
      "Iteration 99, loss = 0.49739217\n",
      "Iteration 100, loss = 0.49772380\n",
      "Iteration 101, loss = 0.49729156\n",
      "Iteration 102, loss = 0.49724837\n",
      "Iteration 103, loss = 0.49734967\n",
      "Iteration 104, loss = 0.49707754\n",
      "Iteration 105, loss = 0.49657297\n",
      "Iteration 106, loss = 0.49632284\n",
      "Iteration 107, loss = 0.49619708\n",
      "Iteration 108, loss = 0.49599519\n",
      "Iteration 109, loss = 0.49607255\n",
      "Iteration 110, loss = 0.49583388\n",
      "Iteration 111, loss = 0.49571823\n",
      "Iteration 112, loss = 0.49537439\n",
      "Iteration 113, loss = 0.49555212\n",
      "Iteration 114, loss = 0.49489735\n",
      "Iteration 115, loss = 0.49511052\n",
      "Iteration 116, loss = 0.49478164\n",
      "Iteration 117, loss = 0.49481054\n",
      "Iteration 118, loss = 0.49460127\n",
      "Iteration 119, loss = 0.49436426\n",
      "Iteration 120, loss = 0.49421655\n",
      "Iteration 121, loss = 0.49397671\n",
      "Iteration 122, loss = 0.49394866\n",
      "Iteration 123, loss = 0.49388125\n",
      "Iteration 124, loss = 0.49360410\n",
      "Iteration 125, loss = 0.49364254\n",
      "Iteration 126, loss = 0.49347078\n",
      "Iteration 127, loss = 0.49322984\n",
      "Iteration 128, loss = 0.49321033\n",
      "Iteration 129, loss = 0.49347672\n",
      "Iteration 130, loss = 0.49309210\n",
      "Iteration 131, loss = 0.49311238\n",
      "Iteration 132, loss = 0.49276397\n",
      "Iteration 133, loss = 0.49260335\n",
      "Iteration 134, loss = 0.49330015\n",
      "Iteration 135, loss = 0.49304806\n",
      "Iteration 136, loss = 0.49206759\n",
      "Iteration 137, loss = 0.49260390\n",
      "Iteration 138, loss = 0.49240345\n",
      "Iteration 139, loss = 0.49248679\n",
      "Iteration 140, loss = 0.49253288\n",
      "Iteration 141, loss = 0.49147087\n",
      "Iteration 142, loss = 0.49144043\n",
      "Iteration 143, loss = 0.49156394\n",
      "Iteration 144, loss = 0.49112764\n",
      "Iteration 145, loss = 0.49141066\n",
      "Iteration 146, loss = 0.49095427\n",
      "Iteration 147, loss = 0.49117254\n",
      "Iteration 148, loss = 0.49101080\n",
      "Iteration 149, loss = 0.49113508\n",
      "Iteration 150, loss = 0.49079453\n",
      "Iteration 151, loss = 0.49083697\n",
      "Iteration 152, loss = 0.49048424\n",
      "Iteration 153, loss = 0.49050860\n",
      "Iteration 154, loss = 0.49044687\n",
      "Iteration 155, loss = 0.49052508\n",
      "Iteration 156, loss = 0.49066878\n",
      "Iteration 157, loss = 0.49032892\n",
      "Iteration 158, loss = 0.49015217\n",
      "Iteration 159, loss = 0.49005602\n",
      "Iteration 160, loss = 0.49027317\n",
      "Iteration 161, loss = 0.49039968\n",
      "Iteration 162, loss = 0.49000258\n",
      "Iteration 163, loss = 0.48990945\n",
      "Iteration 164, loss = 0.48952589\n",
      "Iteration 165, loss = 0.48955555\n",
      "Iteration 166, loss = 0.48965781\n",
      "Iteration 167, loss = 0.48924008\n",
      "Iteration 168, loss = 0.48953953\n",
      "Iteration 169, loss = 0.48990968\n",
      "Iteration 170, loss = 0.48931173\n",
      "Iteration 171, loss = 0.48892863\n",
      "Iteration 172, loss = 0.48933192\n",
      "Iteration 173, loss = 0.48904359\n",
      "Iteration 174, loss = 0.48884294\n",
      "Iteration 175, loss = 0.48882332\n",
      "Iteration 176, loss = 0.48860257\n",
      "Iteration 177, loss = 0.48882085\n",
      "Iteration 178, loss = 0.48850501\n",
      "Iteration 179, loss = 0.48866589\n",
      "Iteration 180, loss = 0.48882250\n",
      "Iteration 181, loss = 0.48940579\n",
      "Iteration 182, loss = 0.48941983\n",
      "Iteration 183, loss = 0.48834366\n",
      "Iteration 184, loss = 0.48863432\n",
      "Iteration 185, loss = 0.48798853\n",
      "Iteration 186, loss = 0.48827742\n",
      "Iteration 187, loss = 0.48795491\n",
      "Iteration 188, loss = 0.48784268\n",
      "Iteration 189, loss = 0.48802845\n",
      "Iteration 190, loss = 0.48778576\n",
      "Iteration 191, loss = 0.48762031\n",
      "Iteration 192, loss = 0.48771443\n",
      "Iteration 193, loss = 0.48787008\n",
      "Iteration 194, loss = 0.48864073\n",
      "Iteration 195, loss = 0.48791441\n",
      "Iteration 196, loss = 0.48759229\n",
      "Iteration 197, loss = 0.48735346\n",
      "Iteration 198, loss = 0.48724347\n",
      "Iteration 199, loss = 0.48722475\n",
      "Iteration 200, loss = 0.48701375\n",
      "Iteration 201, loss = 0.48706269\n",
      "Iteration 202, loss = 0.48693308\n",
      "Iteration 203, loss = 0.48684599\n",
      "Iteration 204, loss = 0.48683715\n",
      "Iteration 205, loss = 0.48704538\n",
      "Iteration 206, loss = 0.48670908\n",
      "Iteration 207, loss = 0.48662626\n",
      "Iteration 208, loss = 0.48686308\n",
      "Iteration 209, loss = 0.48643466\n",
      "Iteration 210, loss = 0.48640154\n",
      "Iteration 211, loss = 0.48663297\n",
      "Iteration 212, loss = 0.48664973\n",
      "Iteration 213, loss = 0.48627335\n",
      "Iteration 214, loss = 0.48619075\n",
      "Iteration 215, loss = 0.48595023\n",
      "Iteration 216, loss = 0.48599541\n",
      "Iteration 217, loss = 0.48628495\n",
      "Iteration 218, loss = 0.48627188\n",
      "Iteration 219, loss = 0.48623555\n",
      "Iteration 220, loss = 0.48578185\n",
      "Iteration 221, loss = 0.48621984\n",
      "Iteration 222, loss = 0.48583341\n",
      "Iteration 223, loss = 0.48542061\n",
      "Iteration 224, loss = 0.48557536\n",
      "Iteration 225, loss = 0.48548075\n",
      "Iteration 226, loss = 0.48550894\n",
      "Iteration 227, loss = 0.48576380\n",
      "Iteration 228, loss = 0.48534920\n",
      "Iteration 229, loss = 0.48536189\n",
      "Iteration 230, loss = 0.48504267\n",
      "Iteration 231, loss = 0.48544602\n",
      "Iteration 232, loss = 0.48549042\n",
      "Iteration 233, loss = 0.48470572\n",
      "Iteration 234, loss = 0.48477351\n",
      "Iteration 235, loss = 0.48497270\n",
      "Iteration 236, loss = 0.48497795\n",
      "Iteration 237, loss = 0.48503153\n",
      "Iteration 238, loss = 0.48462681\n",
      "Iteration 239, loss = 0.48467053\n",
      "Iteration 240, loss = 0.48489435\n",
      "Iteration 241, loss = 0.48427322\n",
      "Iteration 242, loss = 0.48472047\n",
      "Iteration 243, loss = 0.48463032\n",
      "Iteration 244, loss = 0.48447979\n",
      "Iteration 245, loss = 0.48485872\n",
      "Iteration 246, loss = 0.48417660\n",
      "Iteration 247, loss = 0.48422862\n",
      "Iteration 248, loss = 0.48427315\n",
      "Iteration 249, loss = 0.48433005\n",
      "Iteration 250, loss = 0.48408755\n",
      "Iteration 251, loss = 0.48393409\n",
      "Iteration 252, loss = 0.48395546\n",
      "Iteration 253, loss = 0.48415515\n",
      "Iteration 254, loss = 0.48411207\n",
      "Iteration 255, loss = 0.48383260\n",
      "Iteration 256, loss = 0.48377908\n",
      "Iteration 257, loss = 0.48365252\n",
      "Iteration 258, loss = 0.48393387\n",
      "Iteration 259, loss = 0.48379101\n",
      "Iteration 260, loss = 0.48386783\n",
      "Iteration 261, loss = 0.48380078\n",
      "Iteration 262, loss = 0.48367390\n",
      "Iteration 263, loss = 0.48351249\n",
      "Iteration 264, loss = 0.48348564\n",
      "Iteration 265, loss = 0.48369095\n",
      "Iteration 266, loss = 0.48313093\n",
      "Iteration 267, loss = 0.48357461\n",
      "Iteration 268, loss = 0.48356675\n",
      "Iteration 269, loss = 0.48347394\n",
      "Iteration 270, loss = 0.48316869\n",
      "Iteration 271, loss = 0.48312208\n",
      "Iteration 272, loss = 0.48285632\n",
      "Iteration 273, loss = 0.48309228\n",
      "Iteration 274, loss = 0.48316812\n",
      "Iteration 275, loss = 0.48295463\n",
      "Iteration 276, loss = 0.48278434\n",
      "Iteration 277, loss = 0.48267247\n",
      "Iteration 278, loss = 0.48277446\n",
      "Iteration 279, loss = 0.48290920\n",
      "Iteration 280, loss = 0.48284545\n",
      "Iteration 281, loss = 0.48255965\n",
      "Iteration 282, loss = 0.48263010\n",
      "Iteration 283, loss = 0.48269748\n",
      "Iteration 284, loss = 0.48277092\n",
      "Iteration 285, loss = 0.48222551\n",
      "Iteration 286, loss = 0.48279403\n",
      "Iteration 287, loss = 0.48219790\n",
      "Iteration 288, loss = 0.48198173\n",
      "Iteration 289, loss = 0.48224831\n",
      "Iteration 290, loss = 0.48191880\n",
      "Iteration 291, loss = 0.48292012\n",
      "Iteration 292, loss = 0.48190564\n",
      "Iteration 293, loss = 0.48196503\n",
      "Iteration 294, loss = 0.48184677\n",
      "Iteration 295, loss = 0.48186074\n",
      "Iteration 296, loss = 0.48166299\n",
      "Iteration 297, loss = 0.48151459\n",
      "Iteration 298, loss = 0.48145918\n",
      "Iteration 299, loss = 0.48180832\n",
      "Iteration 300, loss = 0.48143205\n",
      "Iteration 301, loss = 0.48139466\n",
      "Iteration 302, loss = 0.48207352\n",
      "Iteration 303, loss = 0.48193087\n",
      "Iteration 304, loss = 0.48118268\n",
      "Iteration 305, loss = 0.48112506\n",
      "Iteration 306, loss = 0.48109097\n",
      "Iteration 307, loss = 0.48082924\n",
      "Iteration 308, loss = 0.48096006\n",
      "Iteration 309, loss = 0.48094760\n",
      "Iteration 310, loss = 0.48077657\n",
      "Iteration 311, loss = 0.48063554\n",
      "Iteration 312, loss = 0.48070587\n",
      "Iteration 313, loss = 0.48052965\n",
      "Iteration 314, loss = 0.48064071\n",
      "Iteration 315, loss = 0.48063592\n",
      "Iteration 316, loss = 0.48050732\n",
      "Iteration 317, loss = 0.48039003\n",
      "Iteration 318, loss = 0.48020966\n",
      "Iteration 319, loss = 0.48019508\n",
      "Iteration 320, loss = 0.48028047\n",
      "Iteration 321, loss = 0.48026269\n",
      "Iteration 322, loss = 0.48045848\n",
      "Iteration 323, loss = 0.48017139\n",
      "Iteration 324, loss = 0.48003398\n",
      "Iteration 325, loss = 0.48010755\n",
      "Iteration 326, loss = 0.48010111\n",
      "Iteration 327, loss = 0.47986653\n",
      "Iteration 328, loss = 0.47975880\n",
      "Iteration 329, loss = 0.47980625\n",
      "Iteration 330, loss = 0.47975440\n",
      "Iteration 331, loss = 0.47957492\n",
      "Iteration 332, loss = 0.47952052\n",
      "Iteration 333, loss = 0.47957714\n",
      "Iteration 334, loss = 0.47948407\n",
      "Iteration 335, loss = 0.48054582\n",
      "Iteration 336, loss = 0.47955138\n",
      "Iteration 337, loss = 0.47999362\n",
      "Iteration 338, loss = 0.47977093\n",
      "Iteration 339, loss = 0.47928506\n",
      "Iteration 340, loss = 0.47919665\n",
      "Iteration 341, loss = 0.47899211\n",
      "Iteration 342, loss = 0.47922328\n",
      "Iteration 343, loss = 0.47902004\n",
      "Iteration 344, loss = 0.47913991\n",
      "Iteration 345, loss = 0.47913117\n",
      "Iteration 346, loss = 0.47908160\n",
      "Iteration 347, loss = 0.47891197\n",
      "Iteration 348, loss = 0.47879408\n",
      "Iteration 349, loss = 0.47887657\n",
      "Iteration 350, loss = 0.47863451\n",
      "Iteration 351, loss = 0.47901637\n",
      "Iteration 352, loss = 0.47886936\n",
      "Iteration 353, loss = 0.47848725\n",
      "Iteration 354, loss = 0.47851303\n",
      "Iteration 355, loss = 0.47871974\n",
      "Iteration 356, loss = 0.47880141\n",
      "Iteration 357, loss = 0.47851474\n",
      "Iteration 358, loss = 0.47837424\n",
      "Iteration 359, loss = 0.47837591\n",
      "Iteration 360, loss = 0.47846364\n",
      "Iteration 361, loss = 0.47829823\n",
      "Iteration 362, loss = 0.47861315\n",
      "Iteration 363, loss = 0.47808102\n",
      "Iteration 364, loss = 0.47825356\n",
      "Iteration 365, loss = 0.47838692\n",
      "Iteration 366, loss = 0.47783744\n",
      "Iteration 367, loss = 0.47845645\n",
      "Iteration 368, loss = 0.47775850\n",
      "Iteration 369, loss = 0.47795326\n",
      "Iteration 370, loss = 0.47825529\n",
      "Iteration 371, loss = 0.47802122\n",
      "Iteration 372, loss = 0.47819862\n",
      "Iteration 373, loss = 0.47795928\n",
      "Iteration 374, loss = 0.47777908\n",
      "Iteration 375, loss = 0.47826620\n",
      "Iteration 376, loss = 0.47818218\n",
      "Iteration 377, loss = 0.47760934\n",
      "Iteration 378, loss = 0.47810587\n",
      "Iteration 379, loss = 0.47772099\n",
      "Iteration 380, loss = 0.47742687\n",
      "Iteration 381, loss = 0.47781119\n",
      "Iteration 382, loss = 0.47771256\n",
      "Iteration 383, loss = 0.47770387\n",
      "Iteration 384, loss = 0.47770010\n",
      "Iteration 385, loss = 0.47756057\n",
      "Iteration 386, loss = 0.47797478\n",
      "Iteration 387, loss = 0.47783060\n",
      "Iteration 388, loss = 0.47775162\n",
      "Iteration 389, loss = 0.47744500\n",
      "Iteration 390, loss = 0.47735902\n",
      "Iteration 391, loss = 0.47773357\n",
      "Iteration 392, loss = 0.47772816\n",
      "Iteration 393, loss = 0.47722838\n",
      "Iteration 394, loss = 0.47703166\n",
      "Iteration 395, loss = 0.47756630\n",
      "Iteration 396, loss = 0.47719705\n",
      "Iteration 397, loss = 0.47711328\n",
      "Iteration 398, loss = 0.47731198\n",
      "Iteration 399, loss = 0.47693622\n",
      "Iteration 400, loss = 0.47728585\n",
      "Iteration 401, loss = 0.47819979\n",
      "Iteration 402, loss = 0.47700114\n",
      "Iteration 403, loss = 0.47711953\n",
      "Iteration 404, loss = 0.47668227\n",
      "Iteration 405, loss = 0.47680009\n",
      "Iteration 406, loss = 0.47678502\n",
      "Iteration 407, loss = 0.47662812\n",
      "Iteration 408, loss = 0.47668096\n",
      "Iteration 409, loss = 0.47636766\n",
      "Iteration 410, loss = 0.47670716\n",
      "Iteration 411, loss = 0.47674654\n",
      "Iteration 412, loss = 0.47658926\n",
      "Iteration 413, loss = 0.47664534\n",
      "Iteration 414, loss = 0.47641898\n",
      "Iteration 415, loss = 0.47610878\n",
      "Iteration 416, loss = 0.47693532\n",
      "Iteration 417, loss = 0.47676060\n",
      "Iteration 418, loss = 0.47620592\n",
      "Iteration 419, loss = 0.47623116\n",
      "Iteration 420, loss = 0.47637121\n",
      "Iteration 421, loss = 0.47660343\n",
      "Iteration 422, loss = 0.47628689\n",
      "Iteration 423, loss = 0.47616267\n",
      "Iteration 424, loss = 0.47604121\n",
      "Iteration 425, loss = 0.47622341\n",
      "Iteration 426, loss = 0.47609841\n",
      "Iteration 427, loss = 0.47618514\n",
      "Iteration 428, loss = 0.47609954\n",
      "Iteration 429, loss = 0.47582097\n",
      "Iteration 430, loss = 0.47597591\n",
      "Iteration 431, loss = 0.47598549\n",
      "Iteration 432, loss = 0.47575940\n",
      "Iteration 433, loss = 0.47568098\n",
      "Iteration 434, loss = 0.47621525\n",
      "Iteration 435, loss = 0.47573014\n",
      "Iteration 436, loss = 0.47620012\n",
      "Iteration 437, loss = 0.47566884\n",
      "Iteration 438, loss = 0.47553089\n",
      "Iteration 439, loss = 0.47570512\n",
      "Iteration 440, loss = 0.47590824\n",
      "Iteration 441, loss = 0.47580537\n",
      "Iteration 442, loss = 0.47558435\n",
      "Iteration 443, loss = 0.47542007\n",
      "Iteration 444, loss = 0.47624833\n",
      "Iteration 445, loss = 0.47573683\n",
      "Iteration 446, loss = 0.47603555\n",
      "Iteration 447, loss = 0.47543296\n",
      "Iteration 448, loss = 0.47577811\n",
      "Iteration 449, loss = 0.47524264\n",
      "Iteration 450, loss = 0.47529666\n",
      "Iteration 451, loss = 0.47552498\n",
      "Iteration 452, loss = 0.47504449\n",
      "Iteration 453, loss = 0.47517383\n",
      "Iteration 454, loss = 0.47524133\n",
      "Iteration 455, loss = 0.47513858\n",
      "Iteration 456, loss = 0.47530958\n",
      "Iteration 457, loss = 0.47576205\n",
      "Iteration 458, loss = 0.47604430\n",
      "Iteration 459, loss = 0.47546679\n",
      "Iteration 460, loss = 0.47489853\n",
      "Iteration 461, loss = 0.47516549\n",
      "Iteration 462, loss = 0.47487990\n",
      "Iteration 463, loss = 0.47494039\n",
      "Iteration 464, loss = 0.47504277\n",
      "Iteration 465, loss = 0.47513337\n",
      "Iteration 466, loss = 0.47488828\n",
      "Iteration 467, loss = 0.47502409\n",
      "Iteration 468, loss = 0.47469022\n",
      "Iteration 469, loss = 0.47464330\n",
      "Iteration 470, loss = 0.47474223\n",
      "Iteration 471, loss = 0.47491636\n",
      "Iteration 472, loss = 0.47502745\n",
      "Iteration 473, loss = 0.47466188\n",
      "Iteration 474, loss = 0.47468405\n",
      "Iteration 475, loss = 0.47462275\n",
      "Iteration 476, loss = 0.47453817\n",
      "Iteration 477, loss = 0.47487130\n",
      "Iteration 478, loss = 0.47467588\n",
      "Iteration 479, loss = 0.47433609\n",
      "Iteration 480, loss = 0.47464019\n",
      "Iteration 481, loss = 0.47449112\n",
      "Iteration 482, loss = 0.47463737\n",
      "Iteration 483, loss = 0.47424121\n",
      "Iteration 484, loss = 0.47421156\n",
      "Iteration 485, loss = 0.47423621\n",
      "Iteration 486, loss = 0.47445182\n",
      "Iteration 487, loss = 0.47427158\n",
      "Iteration 488, loss = 0.47448536\n",
      "Iteration 489, loss = 0.47440349\n",
      "Iteration 490, loss = 0.47406749\n",
      "Iteration 491, loss = 0.47445230\n",
      "Iteration 492, loss = 0.47412746\n",
      "Iteration 493, loss = 0.47434934\n",
      "Iteration 494, loss = 0.47426205\n",
      "Iteration 495, loss = 0.47403116\n",
      "Iteration 496, loss = 0.47398555\n",
      "Iteration 497, loss = 0.47419668\n",
      "Iteration 498, loss = 0.47485726\n",
      "Iteration 499, loss = 0.47437019\n",
      "Iteration 500, loss = 0.47411693\n",
      "Iteration 501, loss = 0.47408060\n",
      "Iteration 502, loss = 0.47403554\n",
      "Iteration 503, loss = 0.47391392\n",
      "Iteration 504, loss = 0.47399577\n",
      "Iteration 505, loss = 0.47441457\n",
      "Iteration 506, loss = 0.47474522\n",
      "Iteration 507, loss = 0.47451687\n",
      "Iteration 508, loss = 0.47525125\n",
      "Iteration 509, loss = 0.47406509\n",
      "Iteration 510, loss = 0.47390434\n",
      "Iteration 511, loss = 0.47371603\n",
      "Iteration 512, loss = 0.47379733\n",
      "Iteration 513, loss = 0.47424399\n",
      "Iteration 514, loss = 0.47379992\n",
      "Iteration 515, loss = 0.47425003\n",
      "Iteration 516, loss = 0.47455276\n",
      "Iteration 517, loss = 0.47448700\n",
      "Iteration 518, loss = 0.47353183\n",
      "Iteration 519, loss = 0.47390035\n",
      "Iteration 520, loss = 0.47350334\n",
      "Iteration 521, loss = 0.47353583\n",
      "Iteration 522, loss = 0.47339291\n",
      "Iteration 523, loss = 0.47375771\n",
      "Iteration 524, loss = 0.47335014\n",
      "Iteration 525, loss = 0.47361605\n",
      "Iteration 526, loss = 0.47352641\n",
      "Iteration 527, loss = 0.47375093\n",
      "Iteration 528, loss = 0.47343307\n",
      "Iteration 529, loss = 0.47364750\n",
      "Iteration 530, loss = 0.47382189\n",
      "Iteration 531, loss = 0.47335921\n",
      "Iteration 532, loss = 0.47356247\n",
      "Iteration 533, loss = 0.47341806\n",
      "Iteration 534, loss = 0.47354719\n",
      "Iteration 535, loss = 0.47388232\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73522893\n",
      "Iteration 2, loss = 0.69865816\n",
      "Iteration 3, loss = 0.67299587\n",
      "Iteration 4, loss = 0.65207065\n",
      "Iteration 5, loss = 0.63473918\n",
      "Iteration 6, loss = 0.62000094\n",
      "Iteration 7, loss = 0.60674210\n",
      "Iteration 8, loss = 0.59587310\n",
      "Iteration 9, loss = 0.58628825\n",
      "Iteration 10, loss = 0.57841942\n",
      "Iteration 11, loss = 0.57253889\n",
      "Iteration 12, loss = 0.56752797\n",
      "Iteration 13, loss = 0.56367371\n",
      "Iteration 14, loss = 0.56027680\n",
      "Iteration 15, loss = 0.55767680\n",
      "Iteration 16, loss = 0.55582422\n",
      "Iteration 17, loss = 0.55372378\n",
      "Iteration 18, loss = 0.55254388\n",
      "Iteration 19, loss = 0.55124834\n",
      "Iteration 20, loss = 0.54997916\n",
      "Iteration 21, loss = 0.54897620\n",
      "Iteration 22, loss = 0.54798863\n",
      "Iteration 23, loss = 0.54703371\n",
      "Iteration 24, loss = 0.54590366\n",
      "Iteration 25, loss = 0.54502510\n",
      "Iteration 26, loss = 0.54424537\n",
      "Iteration 27, loss = 0.54344743\n",
      "Iteration 28, loss = 0.54263447\n",
      "Iteration 29, loss = 0.54183042\n",
      "Iteration 30, loss = 0.54107961\n",
      "Iteration 31, loss = 0.54025172\n",
      "Iteration 32, loss = 0.53972363\n",
      "Iteration 33, loss = 0.53918531\n",
      "Iteration 34, loss = 0.53802650\n",
      "Iteration 35, loss = 0.53719265\n",
      "Iteration 36, loss = 0.53668634\n",
      "Iteration 37, loss = 0.53556456\n",
      "Iteration 38, loss = 0.53482957\n",
      "Iteration 39, loss = 0.53424805\n",
      "Iteration 40, loss = 0.53329982\n",
      "Iteration 41, loss = 0.53227313\n",
      "Iteration 42, loss = 0.53145680\n",
      "Iteration 43, loss = 0.53076514\n",
      "Iteration 44, loss = 0.52951944\n",
      "Iteration 45, loss = 0.52891757\n",
      "Iteration 46, loss = 0.52791047\n",
      "Iteration 47, loss = 0.52683954\n",
      "Iteration 48, loss = 0.52604687\n",
      "Iteration 49, loss = 0.52504297\n",
      "Iteration 50, loss = 0.52460114\n",
      "Iteration 51, loss = 0.52351143\n",
      "Iteration 52, loss = 0.52261271\n",
      "Iteration 53, loss = 0.52197369\n",
      "Iteration 54, loss = 0.52134050\n",
      "Iteration 55, loss = 0.52021719\n",
      "Iteration 56, loss = 0.51932433\n",
      "Iteration 57, loss = 0.51876848\n",
      "Iteration 58, loss = 0.51778094\n",
      "Iteration 59, loss = 0.51754130\n",
      "Iteration 60, loss = 0.51665553\n",
      "Iteration 61, loss = 0.51578464\n",
      "Iteration 62, loss = 0.51518991\n",
      "Iteration 63, loss = 0.51448300\n",
      "Iteration 64, loss = 0.51381101\n",
      "Iteration 65, loss = 0.51335339\n",
      "Iteration 66, loss = 0.51283635\n",
      "Iteration 67, loss = 0.51217858\n",
      "Iteration 68, loss = 0.51144403\n",
      "Iteration 69, loss = 0.51089930\n",
      "Iteration 70, loss = 0.51008158\n",
      "Iteration 71, loss = 0.50945727\n",
      "Iteration 72, loss = 0.50887982\n",
      "Iteration 73, loss = 0.50836163\n",
      "Iteration 74, loss = 0.50758177\n",
      "Iteration 75, loss = 0.50713283\n",
      "Iteration 76, loss = 0.50656348\n",
      "Iteration 77, loss = 0.50631292\n",
      "Iteration 78, loss = 0.50569263\n",
      "Iteration 79, loss = 0.50504894\n",
      "Iteration 80, loss = 0.50455067\n",
      "Iteration 81, loss = 0.50433210\n",
      "Iteration 82, loss = 0.50373974\n",
      "Iteration 83, loss = 0.50356267\n",
      "Iteration 84, loss = 0.50301186\n",
      "Iteration 85, loss = 0.50243953\n",
      "Iteration 86, loss = 0.50227974\n",
      "Iteration 87, loss = 0.50151331\n",
      "Iteration 88, loss = 0.50154130\n",
      "Iteration 89, loss = 0.50105896\n",
      "Iteration 90, loss = 0.50030598\n",
      "Iteration 91, loss = 0.50063148\n",
      "Iteration 92, loss = 0.49956293\n",
      "Iteration 93, loss = 0.49914169\n",
      "Iteration 94, loss = 0.49917735\n",
      "Iteration 95, loss = 0.49877376\n",
      "Iteration 96, loss = 0.49834164\n",
      "Iteration 97, loss = 0.49833599\n",
      "Iteration 98, loss = 0.49732941\n",
      "Iteration 99, loss = 0.49709473\n",
      "Iteration 100, loss = 0.49656373\n",
      "Iteration 101, loss = 0.49640278\n",
      "Iteration 102, loss = 0.49625333\n",
      "Iteration 103, loss = 0.49599835\n",
      "Iteration 104, loss = 0.49554743\n",
      "Iteration 105, loss = 0.49563159\n",
      "Iteration 106, loss = 0.49535043\n",
      "Iteration 107, loss = 0.49490708\n",
      "Iteration 108, loss = 0.49492634\n",
      "Iteration 109, loss = 0.49500789\n",
      "Iteration 110, loss = 0.49425835\n",
      "Iteration 111, loss = 0.49402487\n",
      "Iteration 112, loss = 0.49410451\n",
      "Iteration 113, loss = 0.49354998\n",
      "Iteration 114, loss = 0.49384675\n",
      "Iteration 115, loss = 0.49316408\n",
      "Iteration 116, loss = 0.49291262\n",
      "Iteration 117, loss = 0.49253278\n",
      "Iteration 118, loss = 0.49235813\n",
      "Iteration 119, loss = 0.49222285\n",
      "Iteration 120, loss = 0.49198823\n",
      "Iteration 121, loss = 0.49167443\n",
      "Iteration 122, loss = 0.49194637\n",
      "Iteration 123, loss = 0.49131739\n",
      "Iteration 124, loss = 0.49158666\n",
      "Iteration 125, loss = 0.49092372\n",
      "Iteration 126, loss = 0.49097399\n",
      "Iteration 127, loss = 0.49077464\n",
      "Iteration 128, loss = 0.49100266\n",
      "Iteration 129, loss = 0.49010666\n",
      "Iteration 130, loss = 0.49017868\n",
      "Iteration 131, loss = 0.48996918\n",
      "Iteration 132, loss = 0.49034024\n",
      "Iteration 133, loss = 0.48978926\n",
      "Iteration 134, loss = 0.48949732\n",
      "Iteration 135, loss = 0.48946803\n",
      "Iteration 136, loss = 0.48891375\n",
      "Iteration 137, loss = 0.48908770\n",
      "Iteration 138, loss = 0.48893141\n",
      "Iteration 139, loss = 0.48866185\n",
      "Iteration 140, loss = 0.48861858\n",
      "Iteration 141, loss = 0.48834644\n",
      "Iteration 142, loss = 0.48820397\n",
      "Iteration 143, loss = 0.48798652\n",
      "Iteration 144, loss = 0.48828701\n",
      "Iteration 145, loss = 0.48792063\n",
      "Iteration 146, loss = 0.48748302\n",
      "Iteration 147, loss = 0.48734862\n",
      "Iteration 148, loss = 0.48733290\n",
      "Iteration 149, loss = 0.48717192\n",
      "Iteration 150, loss = 0.48698863\n",
      "Iteration 151, loss = 0.48677905\n",
      "Iteration 152, loss = 0.48735732\n",
      "Iteration 153, loss = 0.48658566\n",
      "Iteration 154, loss = 0.48662915\n",
      "Iteration 155, loss = 0.48647444\n",
      "Iteration 156, loss = 0.48617700\n",
      "Iteration 157, loss = 0.48591100\n",
      "Iteration 158, loss = 0.48576936\n",
      "Iteration 159, loss = 0.48582471\n",
      "Iteration 160, loss = 0.48592096\n",
      "Iteration 161, loss = 0.48579949\n",
      "Iteration 162, loss = 0.48550363\n",
      "Iteration 163, loss = 0.48531983\n",
      "Iteration 164, loss = 0.48519029\n",
      "Iteration 165, loss = 0.48510138\n",
      "Iteration 166, loss = 0.48497455\n",
      "Iteration 167, loss = 0.48469840\n",
      "Iteration 168, loss = 0.48479876\n",
      "Iteration 169, loss = 0.48470532\n",
      "Iteration 170, loss = 0.48474451\n",
      "Iteration 171, loss = 0.48438428\n",
      "Iteration 172, loss = 0.48443358\n",
      "Iteration 173, loss = 0.48412699\n",
      "Iteration 174, loss = 0.48422004\n",
      "Iteration 175, loss = 0.48385267\n",
      "Iteration 176, loss = 0.48396013\n",
      "Iteration 177, loss = 0.48371755\n",
      "Iteration 178, loss = 0.48368723\n",
      "Iteration 179, loss = 0.48395756\n",
      "Iteration 180, loss = 0.48352874\n",
      "Iteration 181, loss = 0.48325232\n",
      "Iteration 182, loss = 0.48363348\n",
      "Iteration 183, loss = 0.48316051\n",
      "Iteration 184, loss = 0.48344006\n",
      "Iteration 185, loss = 0.48300050\n",
      "Iteration 186, loss = 0.48295243\n",
      "Iteration 187, loss = 0.48272256\n",
      "Iteration 188, loss = 0.48253466\n",
      "Iteration 189, loss = 0.48269488\n",
      "Iteration 190, loss = 0.48250571\n",
      "Iteration 191, loss = 0.48214229\n",
      "Iteration 192, loss = 0.48199210\n",
      "Iteration 193, loss = 0.48201041\n",
      "Iteration 194, loss = 0.48218910\n",
      "Iteration 195, loss = 0.48205676\n",
      "Iteration 196, loss = 0.48198238\n",
      "Iteration 197, loss = 0.48178487\n",
      "Iteration 198, loss = 0.48165639\n",
      "Iteration 199, loss = 0.48174585\n",
      "Iteration 200, loss = 0.48200206\n",
      "Iteration 201, loss = 0.48177774\n",
      "Iteration 202, loss = 0.48130315\n",
      "Iteration 203, loss = 0.48160663\n",
      "Iteration 204, loss = 0.48107441\n",
      "Iteration 205, loss = 0.48109003\n",
      "Iteration 206, loss = 0.48087574\n",
      "Iteration 207, loss = 0.48077118\n",
      "Iteration 208, loss = 0.48089458\n",
      "Iteration 209, loss = 0.48093479\n",
      "Iteration 210, loss = 0.48066471\n",
      "Iteration 211, loss = 0.48052729\n",
      "Iteration 212, loss = 0.48078612\n",
      "Iteration 213, loss = 0.48029767\n",
      "Iteration 214, loss = 0.48025359\n",
      "Iteration 215, loss = 0.48023687\n",
      "Iteration 216, loss = 0.48063165\n",
      "Iteration 217, loss = 0.48002412\n",
      "Iteration 218, loss = 0.48044698\n",
      "Iteration 219, loss = 0.48000945\n",
      "Iteration 220, loss = 0.47988545\n",
      "Iteration 221, loss = 0.48000639\n",
      "Iteration 222, loss = 0.47988641\n",
      "Iteration 223, loss = 0.47977099\n",
      "Iteration 224, loss = 0.47969949\n",
      "Iteration 225, loss = 0.47930453\n",
      "Iteration 226, loss = 0.47935208\n",
      "Iteration 227, loss = 0.47930158\n",
      "Iteration 228, loss = 0.47949384\n",
      "Iteration 229, loss = 0.47976179\n",
      "Iteration 230, loss = 0.47924271\n",
      "Iteration 231, loss = 0.47950863\n",
      "Iteration 232, loss = 0.47909344\n",
      "Iteration 233, loss = 0.47885185\n",
      "Iteration 234, loss = 0.47879965\n",
      "Iteration 235, loss = 0.47884459\n",
      "Iteration 236, loss = 0.47874032\n",
      "Iteration 237, loss = 0.47859735\n",
      "Iteration 238, loss = 0.47858526\n",
      "Iteration 239, loss = 0.47850603\n",
      "Iteration 240, loss = 0.47829230\n",
      "Iteration 241, loss = 0.47841266\n",
      "Iteration 242, loss = 0.47825578\n",
      "Iteration 243, loss = 0.47828860\n",
      "Iteration 244, loss = 0.47832339\n",
      "Iteration 245, loss = 0.47802348\n",
      "Iteration 246, loss = 0.47831633\n",
      "Iteration 247, loss = 0.47820108\n",
      "Iteration 248, loss = 0.47781899\n",
      "Iteration 249, loss = 0.47775944\n",
      "Iteration 250, loss = 0.47807462\n",
      "Iteration 251, loss = 0.47783791\n",
      "Iteration 252, loss = 0.47752742\n",
      "Iteration 253, loss = 0.47771592\n",
      "Iteration 254, loss = 0.47735586\n",
      "Iteration 255, loss = 0.47749239\n",
      "Iteration 256, loss = 0.47747740\n",
      "Iteration 257, loss = 0.47754962\n",
      "Iteration 258, loss = 0.47759605\n",
      "Iteration 259, loss = 0.47711541\n",
      "Iteration 260, loss = 0.47738384\n",
      "Iteration 261, loss = 0.47728820\n",
      "Iteration 262, loss = 0.47764394\n",
      "Iteration 263, loss = 0.47702778\n",
      "Iteration 264, loss = 0.47742264\n",
      "Iteration 265, loss = 0.47678530\n",
      "Iteration 266, loss = 0.47702854\n",
      "Iteration 267, loss = 0.47685308\n",
      "Iteration 268, loss = 0.47676806\n",
      "Iteration 269, loss = 0.47731078\n",
      "Iteration 270, loss = 0.47658442\n",
      "Iteration 271, loss = 0.47656685\n",
      "Iteration 272, loss = 0.47685349\n",
      "Iteration 273, loss = 0.47643825\n",
      "Iteration 274, loss = 0.47659357\n",
      "Iteration 275, loss = 0.47640498\n",
      "Iteration 276, loss = 0.47637514\n",
      "Iteration 277, loss = 0.47651812\n",
      "Iteration 278, loss = 0.47658524\n",
      "Iteration 279, loss = 0.47617647\n",
      "Iteration 280, loss = 0.47618295\n",
      "Iteration 281, loss = 0.47636862\n",
      "Iteration 282, loss = 0.47658542\n",
      "Iteration 283, loss = 0.47652871\n",
      "Iteration 284, loss = 0.47677692\n",
      "Iteration 285, loss = 0.47616509\n",
      "Iteration 286, loss = 0.47609416\n",
      "Iteration 287, loss = 0.47600112\n",
      "Iteration 288, loss = 0.47584118\n",
      "Iteration 289, loss = 0.47618867\n",
      "Iteration 290, loss = 0.47579281\n",
      "Iteration 291, loss = 0.47563135\n",
      "Iteration 292, loss = 0.47569279\n",
      "Iteration 293, loss = 0.47599596\n",
      "Iteration 294, loss = 0.47594510\n",
      "Iteration 295, loss = 0.47548430\n",
      "Iteration 296, loss = 0.47605048\n",
      "Iteration 297, loss = 0.47597442\n",
      "Iteration 298, loss = 0.47636250\n",
      "Iteration 299, loss = 0.47517315\n",
      "Iteration 300, loss = 0.47521382\n",
      "Iteration 301, loss = 0.47525989\n",
      "Iteration 302, loss = 0.47558705\n",
      "Iteration 303, loss = 0.47523968\n",
      "Iteration 304, loss = 0.47515745\n",
      "Iteration 305, loss = 0.47550116\n",
      "Iteration 306, loss = 0.47561617\n",
      "Iteration 307, loss = 0.47525607\n",
      "Iteration 308, loss = 0.47506392\n",
      "Iteration 309, loss = 0.47508937\n",
      "Iteration 310, loss = 0.47523533\n",
      "Iteration 311, loss = 0.47549213\n",
      "Iteration 312, loss = 0.47532297\n",
      "Iteration 313, loss = 0.47481243\n",
      "Iteration 314, loss = 0.47500593\n",
      "Iteration 315, loss = 0.47481723\n",
      "Iteration 316, loss = 0.47464682\n",
      "Iteration 317, loss = 0.47491309\n",
      "Iteration 318, loss = 0.47509706\n",
      "Iteration 319, loss = 0.47479796\n",
      "Iteration 320, loss = 0.47485137\n",
      "Iteration 321, loss = 0.47476203\n",
      "Iteration 322, loss = 0.47447637\n",
      "Iteration 323, loss = 0.47451533\n",
      "Iteration 324, loss = 0.47460207\n",
      "Iteration 325, loss = 0.47462056\n",
      "Iteration 326, loss = 0.47457756\n",
      "Iteration 327, loss = 0.47444599\n",
      "Iteration 328, loss = 0.47488577\n",
      "Iteration 329, loss = 0.47481324\n",
      "Iteration 330, loss = 0.47429778\n",
      "Iteration 331, loss = 0.47443781\n",
      "Iteration 332, loss = 0.47420495\n",
      "Iteration 333, loss = 0.47426757\n",
      "Iteration 334, loss = 0.47434549\n",
      "Iteration 335, loss = 0.47431353\n",
      "Iteration 336, loss = 0.47410990\n",
      "Iteration 337, loss = 0.47437075\n",
      "Iteration 338, loss = 0.47405783\n",
      "Iteration 339, loss = 0.47401673\n",
      "Iteration 340, loss = 0.47428725\n",
      "Iteration 341, loss = 0.47396029\n",
      "Iteration 342, loss = 0.47410981\n",
      "Iteration 343, loss = 0.47387345\n",
      "Iteration 344, loss = 0.47399075\n",
      "Iteration 345, loss = 0.47390637\n",
      "Iteration 346, loss = 0.47382239\n",
      "Iteration 347, loss = 0.47492772\n",
      "Iteration 348, loss = 0.47402437\n",
      "Iteration 349, loss = 0.47372227\n",
      "Iteration 350, loss = 0.47403973\n",
      "Iteration 351, loss = 0.47398254\n",
      "Iteration 352, loss = 0.47392217\n",
      "Iteration 353, loss = 0.47373805\n",
      "Iteration 354, loss = 0.47359336\n",
      "Iteration 355, loss = 0.47443320\n",
      "Iteration 356, loss = 0.47399600\n",
      "Iteration 357, loss = 0.47382135\n",
      "Iteration 358, loss = 0.47384046\n",
      "Iteration 359, loss = 0.47366156\n",
      "Iteration 360, loss = 0.47363597\n",
      "Iteration 361, loss = 0.47341199\n",
      "Iteration 362, loss = 0.47352426\n",
      "Iteration 363, loss = 0.47348034\n",
      "Iteration 364, loss = 0.47362443\n",
      "Iteration 365, loss = 0.47354422\n",
      "Iteration 366, loss = 0.47387160\n",
      "Iteration 367, loss = 0.47373462\n",
      "Iteration 368, loss = 0.47326412\n",
      "Iteration 369, loss = 0.47334904\n",
      "Iteration 370, loss = 0.47344580\n",
      "Iteration 371, loss = 0.47335239\n",
      "Iteration 372, loss = 0.47345951\n",
      "Iteration 373, loss = 0.47328480\n",
      "Iteration 374, loss = 0.47329663\n",
      "Iteration 375, loss = 0.47324335\n",
      "Iteration 376, loss = 0.47352475\n",
      "Iteration 377, loss = 0.47313017\n",
      "Iteration 378, loss = 0.47333631\n",
      "Iteration 379, loss = 0.47322756\n",
      "Iteration 380, loss = 0.47294735\n",
      "Iteration 381, loss = 0.47319118\n",
      "Iteration 382, loss = 0.47342279\n",
      "Iteration 383, loss = 0.47323489\n",
      "Iteration 384, loss = 0.47318127\n",
      "Iteration 385, loss = 0.47303912\n",
      "Iteration 386, loss = 0.47328512\n",
      "Iteration 387, loss = 0.47321919\n",
      "Iteration 388, loss = 0.47306232\n",
      "Iteration 389, loss = 0.47312172\n",
      "Iteration 390, loss = 0.47310522\n",
      "Iteration 391, loss = 0.47310266\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75996703\n",
      "Iteration 2, loss = 0.72645411\n",
      "Iteration 3, loss = 0.70111323\n",
      "Iteration 4, loss = 0.68249907\n",
      "Iteration 5, loss = 0.66798073\n",
      "Iteration 6, loss = 0.65725314\n",
      "Iteration 7, loss = 0.64750428\n",
      "Iteration 8, loss = 0.63899292\n",
      "Iteration 9, loss = 0.63065227\n",
      "Iteration 10, loss = 0.62310437\n",
      "Iteration 11, loss = 0.61559091\n",
      "Iteration 12, loss = 0.60844164\n",
      "Iteration 13, loss = 0.60152542\n",
      "Iteration 14, loss = 0.59505481\n",
      "Iteration 15, loss = 0.58909912\n",
      "Iteration 16, loss = 0.58348751\n",
      "Iteration 17, loss = 0.57840737\n",
      "Iteration 18, loss = 0.57416108\n",
      "Iteration 19, loss = 0.57008930\n",
      "Iteration 20, loss = 0.56702426\n",
      "Iteration 21, loss = 0.56408260\n",
      "Iteration 22, loss = 0.56161702\n",
      "Iteration 23, loss = 0.55980915\n",
      "Iteration 24, loss = 0.55748118\n",
      "Iteration 25, loss = 0.55599407\n",
      "Iteration 26, loss = 0.55457262\n",
      "Iteration 27, loss = 0.55338806\n",
      "Iteration 28, loss = 0.55223315\n",
      "Iteration 29, loss = 0.55104013\n",
      "Iteration 30, loss = 0.55015647\n",
      "Iteration 31, loss = 0.54901862\n",
      "Iteration 32, loss = 0.54817559\n",
      "Iteration 33, loss = 0.54739164\n",
      "Iteration 34, loss = 0.54637336\n",
      "Iteration 35, loss = 0.54569972\n",
      "Iteration 36, loss = 0.54490695\n",
      "Iteration 37, loss = 0.54410854\n",
      "Iteration 38, loss = 0.54334111\n",
      "Iteration 39, loss = 0.54257743\n",
      "Iteration 40, loss = 0.54194040\n",
      "Iteration 41, loss = 0.54139342\n",
      "Iteration 42, loss = 0.54052935\n",
      "Iteration 43, loss = 0.53986506\n",
      "Iteration 44, loss = 0.53923594\n",
      "Iteration 45, loss = 0.53855179\n",
      "Iteration 46, loss = 0.53792617\n",
      "Iteration 47, loss = 0.53725855\n",
      "Iteration 48, loss = 0.53656599\n",
      "Iteration 49, loss = 0.53590361\n",
      "Iteration 50, loss = 0.53493083\n",
      "Iteration 51, loss = 0.53424711\n",
      "Iteration 52, loss = 0.53336992\n",
      "Iteration 53, loss = 0.53252297\n",
      "Iteration 54, loss = 0.53186553\n",
      "Iteration 55, loss = 0.53095158\n",
      "Iteration 56, loss = 0.53005225\n",
      "Iteration 57, loss = 0.52914000\n",
      "Iteration 58, loss = 0.52817757\n",
      "Iteration 59, loss = 0.52720009\n",
      "Iteration 60, loss = 0.52626986\n",
      "Iteration 61, loss = 0.52524518\n",
      "Iteration 62, loss = 0.52444887\n",
      "Iteration 63, loss = 0.52364878\n",
      "Iteration 64, loss = 0.52256789\n",
      "Iteration 65, loss = 0.52175603\n",
      "Iteration 66, loss = 0.52092932\n",
      "Iteration 67, loss = 0.52026456\n",
      "Iteration 68, loss = 0.51964216\n",
      "Iteration 69, loss = 0.51884949\n",
      "Iteration 70, loss = 0.51801319\n",
      "Iteration 71, loss = 0.51753604\n",
      "Iteration 72, loss = 0.51689843\n",
      "Iteration 73, loss = 0.51643064\n",
      "Iteration 74, loss = 0.51578097\n",
      "Iteration 75, loss = 0.51545295\n",
      "Iteration 76, loss = 0.51456647\n",
      "Iteration 77, loss = 0.51422741\n",
      "Iteration 78, loss = 0.51373019\n",
      "Iteration 79, loss = 0.51313202\n",
      "Iteration 80, loss = 0.51269232\n",
      "Iteration 81, loss = 0.51231358\n",
      "Iteration 82, loss = 0.51183565\n",
      "Iteration 83, loss = 0.51158958\n",
      "Iteration 84, loss = 0.51097368\n",
      "Iteration 85, loss = 0.51075480\n",
      "Iteration 86, loss = 0.51042810\n",
      "Iteration 87, loss = 0.51010198\n",
      "Iteration 88, loss = 0.50949313\n",
      "Iteration 89, loss = 0.50925016\n",
      "Iteration 90, loss = 0.50861751\n",
      "Iteration 91, loss = 0.50810288\n",
      "Iteration 92, loss = 0.50801292\n",
      "Iteration 93, loss = 0.50733901\n",
      "Iteration 94, loss = 0.50681868\n",
      "Iteration 95, loss = 0.50648205\n",
      "Iteration 96, loss = 0.50588476\n",
      "Iteration 97, loss = 0.50538998\n",
      "Iteration 98, loss = 0.50517665\n",
      "Iteration 99, loss = 0.50456665\n",
      "Iteration 100, loss = 0.50427614\n",
      "Iteration 101, loss = 0.50383958\n",
      "Iteration 102, loss = 0.50364604\n",
      "Iteration 103, loss = 0.50305069\n",
      "Iteration 104, loss = 0.50282184\n",
      "Iteration 105, loss = 0.50249151\n",
      "Iteration 106, loss = 0.50218356\n",
      "Iteration 107, loss = 0.50129478\n",
      "Iteration 108, loss = 0.50101642\n",
      "Iteration 109, loss = 0.50062970\n",
      "Iteration 110, loss = 0.50038057\n",
      "Iteration 111, loss = 0.49979400\n",
      "Iteration 112, loss = 0.49958150\n",
      "Iteration 113, loss = 0.49917943\n",
      "Iteration 114, loss = 0.49884190\n",
      "Iteration 115, loss = 0.49842400\n",
      "Iteration 116, loss = 0.49798030\n",
      "Iteration 117, loss = 0.49782122\n",
      "Iteration 118, loss = 0.49711647\n",
      "Iteration 119, loss = 0.49699918\n",
      "Iteration 120, loss = 0.49654229\n",
      "Iteration 121, loss = 0.49603957\n",
      "Iteration 122, loss = 0.49571957\n",
      "Iteration 123, loss = 0.49506972\n",
      "Iteration 124, loss = 0.49490907\n",
      "Iteration 125, loss = 0.49421319\n",
      "Iteration 126, loss = 0.49398299\n",
      "Iteration 127, loss = 0.49354209\n",
      "Iteration 128, loss = 0.49335716\n",
      "Iteration 129, loss = 0.49270964\n",
      "Iteration 130, loss = 0.49260938\n",
      "Iteration 131, loss = 0.49176433\n",
      "Iteration 132, loss = 0.49167016\n",
      "Iteration 133, loss = 0.49118592\n",
      "Iteration 134, loss = 0.49066418\n",
      "Iteration 135, loss = 0.49029374\n",
      "Iteration 136, loss = 0.48984878\n",
      "Iteration 137, loss = 0.48943025\n",
      "Iteration 138, loss = 0.48910435\n",
      "Iteration 139, loss = 0.48871590\n",
      "Iteration 140, loss = 0.48860553\n",
      "Iteration 141, loss = 0.48776396\n",
      "Iteration 142, loss = 0.48749615\n",
      "Iteration 143, loss = 0.48702549\n",
      "Iteration 144, loss = 0.48655140\n",
      "Iteration 145, loss = 0.48632381\n",
      "Iteration 146, loss = 0.48612083\n",
      "Iteration 147, loss = 0.48553080\n",
      "Iteration 148, loss = 0.48559062\n",
      "Iteration 149, loss = 0.48496507\n",
      "Iteration 150, loss = 0.48478076\n",
      "Iteration 151, loss = 0.48458907\n",
      "Iteration 152, loss = 0.48426555\n",
      "Iteration 153, loss = 0.48420649\n",
      "Iteration 154, loss = 0.48388176\n",
      "Iteration 155, loss = 0.48372359\n",
      "Iteration 156, loss = 0.48325512\n",
      "Iteration 157, loss = 0.48324828\n",
      "Iteration 158, loss = 0.48289219\n",
      "Iteration 159, loss = 0.48284519\n",
      "Iteration 160, loss = 0.48258136\n",
      "Iteration 161, loss = 0.48244016\n",
      "Iteration 162, loss = 0.48215180\n",
      "Iteration 163, loss = 0.48191962\n",
      "Iteration 164, loss = 0.48188939\n",
      "Iteration 165, loss = 0.48158438\n",
      "Iteration 166, loss = 0.48154683\n",
      "Iteration 167, loss = 0.48135060\n",
      "Iteration 168, loss = 0.48115868\n",
      "Iteration 169, loss = 0.48104186\n",
      "Iteration 170, loss = 0.48087774\n",
      "Iteration 171, loss = 0.48064512\n",
      "Iteration 172, loss = 0.48048198\n",
      "Iteration 173, loss = 0.48036199\n",
      "Iteration 174, loss = 0.48036946\n",
      "Iteration 175, loss = 0.48048865\n",
      "Iteration 176, loss = 0.48059453\n",
      "Iteration 177, loss = 0.47982149\n",
      "Iteration 178, loss = 0.47987627\n",
      "Iteration 179, loss = 0.47938984\n",
      "Iteration 180, loss = 0.47980121\n",
      "Iteration 181, loss = 0.47977208\n",
      "Iteration 182, loss = 0.47949597\n",
      "Iteration 183, loss = 0.47910315\n",
      "Iteration 184, loss = 0.47908917\n",
      "Iteration 185, loss = 0.47894024\n",
      "Iteration 186, loss = 0.47881285\n",
      "Iteration 187, loss = 0.47870736\n",
      "Iteration 188, loss = 0.47867211\n",
      "Iteration 189, loss = 0.47863507\n",
      "Iteration 190, loss = 0.47845562\n",
      "Iteration 191, loss = 0.47837366\n",
      "Iteration 192, loss = 0.47821278\n",
      "Iteration 193, loss = 0.47829044\n",
      "Iteration 194, loss = 0.47796601\n",
      "Iteration 195, loss = 0.47795560\n",
      "Iteration 196, loss = 0.47800954\n",
      "Iteration 197, loss = 0.47764951\n",
      "Iteration 198, loss = 0.47772718\n",
      "Iteration 199, loss = 0.47737453\n",
      "Iteration 200, loss = 0.47743404\n",
      "Iteration 201, loss = 0.47760264\n",
      "Iteration 202, loss = 0.47748790\n",
      "Iteration 203, loss = 0.47719848\n",
      "Iteration 204, loss = 0.47700361\n",
      "Iteration 205, loss = 0.47691992\n",
      "Iteration 206, loss = 0.47723528\n",
      "Iteration 207, loss = 0.47701292\n",
      "Iteration 208, loss = 0.47672459\n",
      "Iteration 209, loss = 0.47659403\n",
      "Iteration 210, loss = 0.47677129\n",
      "Iteration 211, loss = 0.47641847\n",
      "Iteration 212, loss = 0.47638383\n",
      "Iteration 213, loss = 0.47665045\n",
      "Iteration 214, loss = 0.47615250\n",
      "Iteration 215, loss = 0.47620213\n",
      "Iteration 216, loss = 0.47601606\n",
      "Iteration 217, loss = 0.47608430\n",
      "Iteration 218, loss = 0.47587848\n",
      "Iteration 219, loss = 0.47580821\n",
      "Iteration 220, loss = 0.47554032\n",
      "Iteration 221, loss = 0.47558732\n",
      "Iteration 222, loss = 0.47551367\n",
      "Iteration 223, loss = 0.47557848\n",
      "Iteration 224, loss = 0.47550674\n",
      "Iteration 225, loss = 0.47509698\n",
      "Iteration 226, loss = 0.47570338\n",
      "Iteration 227, loss = 0.47528469\n",
      "Iteration 228, loss = 0.47565622\n",
      "Iteration 229, loss = 0.47531730\n",
      "Iteration 230, loss = 0.47522545\n",
      "Iteration 231, loss = 0.47553417\n",
      "Iteration 232, loss = 0.47463651\n",
      "Iteration 233, loss = 0.47493803\n",
      "Iteration 234, loss = 0.47477024\n",
      "Iteration 235, loss = 0.47505812\n",
      "Iteration 236, loss = 0.47466531\n",
      "Iteration 237, loss = 0.47445987\n",
      "Iteration 238, loss = 0.47444621\n",
      "Iteration 239, loss = 0.47437226\n",
      "Iteration 240, loss = 0.47473579\n",
      "Iteration 241, loss = 0.47426967\n",
      "Iteration 242, loss = 0.47424029\n",
      "Iteration 243, loss = 0.47428397\n",
      "Iteration 244, loss = 0.47394457\n",
      "Iteration 245, loss = 0.47382623\n",
      "Iteration 246, loss = 0.47409561\n",
      "Iteration 247, loss = 0.47382817\n",
      "Iteration 248, loss = 0.47407181\n",
      "Iteration 249, loss = 0.47360376\n",
      "Iteration 250, loss = 0.47388578\n",
      "Iteration 251, loss = 0.47361332\n",
      "Iteration 252, loss = 0.47350720\n",
      "Iteration 253, loss = 0.47332642\n",
      "Iteration 254, loss = 0.47347652\n",
      "Iteration 255, loss = 0.47308712\n",
      "Iteration 256, loss = 0.47333038\n",
      "Iteration 257, loss = 0.47334924\n",
      "Iteration 258, loss = 0.47300541\n",
      "Iteration 259, loss = 0.47287659\n",
      "Iteration 260, loss = 0.47278138\n",
      "Iteration 261, loss = 0.47267426\n",
      "Iteration 262, loss = 0.47289596\n",
      "Iteration 263, loss = 0.47270773\n",
      "Iteration 264, loss = 0.47260489\n",
      "Iteration 265, loss = 0.47242159\n",
      "Iteration 266, loss = 0.47256984\n",
      "Iteration 267, loss = 0.47240637\n",
      "Iteration 268, loss = 0.47230055\n",
      "Iteration 269, loss = 0.47243975\n",
      "Iteration 270, loss = 0.47271165\n",
      "Iteration 271, loss = 0.47213354\n",
      "Iteration 272, loss = 0.47214605\n",
      "Iteration 273, loss = 0.47202208\n",
      "Iteration 274, loss = 0.47184603\n",
      "Iteration 275, loss = 0.47193247\n",
      "Iteration 276, loss = 0.47164395\n",
      "Iteration 277, loss = 0.47180783\n",
      "Iteration 278, loss = 0.47214549\n",
      "Iteration 279, loss = 0.47133874\n",
      "Iteration 280, loss = 0.47152239\n",
      "Iteration 281, loss = 0.47160426\n",
      "Iteration 282, loss = 0.47151098\n",
      "Iteration 283, loss = 0.47114513\n",
      "Iteration 284, loss = 0.47136237\n",
      "Iteration 285, loss = 0.47131846\n",
      "Iteration 286, loss = 0.47101976\n",
      "Iteration 287, loss = 0.47139202\n",
      "Iteration 288, loss = 0.47092722\n",
      "Iteration 289, loss = 0.47122938\n",
      "Iteration 290, loss = 0.47119412\n",
      "Iteration 291, loss = 0.47135471\n",
      "Iteration 292, loss = 0.47076468\n",
      "Iteration 293, loss = 0.47096317\n",
      "Iteration 294, loss = 0.47084067\n",
      "Iteration 295, loss = 0.47062192\n",
      "Iteration 296, loss = 0.47070446\n",
      "Iteration 297, loss = 0.47047323\n",
      "Iteration 298, loss = 0.47078387\n",
      "Iteration 299, loss = 0.47071700\n",
      "Iteration 300, loss = 0.47056417\n",
      "Iteration 301, loss = 0.47061177\n",
      "Iteration 302, loss = 0.47095714\n",
      "Iteration 303, loss = 0.47027859\n",
      "Iteration 304, loss = 0.47039167\n",
      "Iteration 305, loss = 0.47027212\n",
      "Iteration 306, loss = 0.47013624\n",
      "Iteration 307, loss = 0.47021137\n",
      "Iteration 308, loss = 0.47039674\n",
      "Iteration 309, loss = 0.47007532\n",
      "Iteration 310, loss = 0.47028902\n",
      "Iteration 311, loss = 0.47002249\n",
      "Iteration 312, loss = 0.47022953\n",
      "Iteration 313, loss = 0.47003844\n",
      "Iteration 314, loss = 0.46991442\n",
      "Iteration 315, loss = 0.46991076\n",
      "Iteration 316, loss = 0.46977977\n",
      "Iteration 317, loss = 0.46991356\n",
      "Iteration 318, loss = 0.46971716\n",
      "Iteration 319, loss = 0.46958895\n",
      "Iteration 320, loss = 0.46970760\n",
      "Iteration 321, loss = 0.47020221\n",
      "Iteration 322, loss = 0.46959842\n",
      "Iteration 323, loss = 0.46969981\n",
      "Iteration 324, loss = 0.46953605\n",
      "Iteration 325, loss = 0.46924375\n",
      "Iteration 326, loss = 0.46927223\n",
      "Iteration 327, loss = 0.46922674\n",
      "Iteration 328, loss = 0.46924549\n",
      "Iteration 329, loss = 0.46914864\n",
      "Iteration 330, loss = 0.46921404\n",
      "Iteration 331, loss = 0.46917867\n",
      "Iteration 332, loss = 0.46941690\n",
      "Iteration 333, loss = 0.46941701\n",
      "Iteration 334, loss = 0.46896262\n",
      "Iteration 335, loss = 0.46931095\n",
      "Iteration 336, loss = 0.46913883\n",
      "Iteration 337, loss = 0.46873342\n",
      "Iteration 338, loss = 0.46934402\n",
      "Iteration 339, loss = 0.46892256\n",
      "Iteration 340, loss = 0.46873254\n",
      "Iteration 341, loss = 0.46879951\n",
      "Iteration 342, loss = 0.46888733\n",
      "Iteration 343, loss = 0.46855405\n",
      "Iteration 344, loss = 0.46866528\n",
      "Iteration 345, loss = 0.46880769\n",
      "Iteration 346, loss = 0.46898058\n",
      "Iteration 347, loss = 0.46887458\n",
      "Iteration 348, loss = 0.46858330\n",
      "Iteration 349, loss = 0.46861479\n",
      "Iteration 350, loss = 0.46871660\n",
      "Iteration 351, loss = 0.46874566\n",
      "Iteration 352, loss = 0.46841706\n",
      "Iteration 353, loss = 0.46826906\n",
      "Iteration 354, loss = 0.46825248\n",
      "Iteration 355, loss = 0.46829856\n",
      "Iteration 356, loss = 0.46828859\n",
      "Iteration 357, loss = 0.46845541\n",
      "Iteration 358, loss = 0.46884872\n",
      "Iteration 359, loss = 0.46835015\n",
      "Iteration 360, loss = 0.46846928\n",
      "Iteration 361, loss = 0.46801627\n",
      "Iteration 362, loss = 0.46829454\n",
      "Iteration 363, loss = 0.46817540\n",
      "Iteration 364, loss = 0.46814297\n",
      "Iteration 365, loss = 0.46812179\n",
      "Iteration 366, loss = 0.46812825\n",
      "Iteration 367, loss = 0.46777536\n",
      "Iteration 368, loss = 0.46792750\n",
      "Iteration 369, loss = 0.46791600\n",
      "Iteration 370, loss = 0.46806029\n",
      "Iteration 371, loss = 0.46780272\n",
      "Iteration 372, loss = 0.46810177\n",
      "Iteration 373, loss = 0.46789101\n",
      "Iteration 374, loss = 0.46786292\n",
      "Iteration 375, loss = 0.46785748\n",
      "Iteration 376, loss = 0.46775933\n",
      "Iteration 377, loss = 0.46757715\n",
      "Iteration 378, loss = 0.46767363\n",
      "Iteration 379, loss = 0.46762564\n",
      "Iteration 380, loss = 0.46757527\n",
      "Iteration 381, loss = 0.46775355\n",
      "Iteration 382, loss = 0.46752343\n",
      "Iteration 383, loss = 0.46766958\n",
      "Iteration 384, loss = 0.46747694\n",
      "Iteration 385, loss = 0.46746808\n",
      "Iteration 386, loss = 0.46754684\n",
      "Iteration 387, loss = 0.46732405\n",
      "Iteration 388, loss = 0.46737888\n",
      "Iteration 389, loss = 0.46744696\n",
      "Iteration 390, loss = 0.46724599\n",
      "Iteration 391, loss = 0.46728985\n",
      "Iteration 392, loss = 0.46709377\n",
      "Iteration 393, loss = 0.46738733\n",
      "Iteration 394, loss = 0.46751542\n",
      "Iteration 395, loss = 0.46717503\n",
      "Iteration 396, loss = 0.46755526\n",
      "Iteration 397, loss = 0.46736198\n",
      "Iteration 398, loss = 0.46707891\n",
      "Iteration 399, loss = 0.46720132\n",
      "Iteration 400, loss = 0.46750574\n",
      "Iteration 401, loss = 0.46688469\n",
      "Iteration 402, loss = 0.46706811\n",
      "Iteration 403, loss = 0.46715628\n",
      "Iteration 404, loss = 0.46711922\n",
      "Iteration 405, loss = 0.46669466\n",
      "Iteration 406, loss = 0.46687007\n",
      "Iteration 407, loss = 0.46686063\n",
      "Iteration 408, loss = 0.46688220\n",
      "Iteration 409, loss = 0.46710280\n",
      "Iteration 410, loss = 0.46671643\n",
      "Iteration 411, loss = 0.46682124\n",
      "Iteration 412, loss = 0.46679517\n",
      "Iteration 413, loss = 0.46675619\n",
      "Iteration 414, loss = 0.46660350\n",
      "Iteration 415, loss = 0.46671916\n",
      "Iteration 416, loss = 0.46679242\n",
      "Iteration 417, loss = 0.46667824\n",
      "Iteration 418, loss = 0.46643780\n",
      "Iteration 419, loss = 0.46699415\n",
      "Iteration 420, loss = 0.46673182\n",
      "Iteration 421, loss = 0.46660610\n",
      "Iteration 422, loss = 0.46660211\n",
      "Iteration 423, loss = 0.46628487\n",
      "Iteration 424, loss = 0.46677339\n",
      "Iteration 425, loss = 0.46633099\n",
      "Iteration 426, loss = 0.46635677\n",
      "Iteration 427, loss = 0.46622262\n",
      "Iteration 428, loss = 0.46643768\n",
      "Iteration 429, loss = 0.46629685\n",
      "Iteration 430, loss = 0.46672253\n",
      "Iteration 431, loss = 0.46640017\n",
      "Iteration 432, loss = 0.46672488\n",
      "Iteration 433, loss = 0.46622820\n",
      "Iteration 434, loss = 0.46633860\n",
      "Iteration 435, loss = 0.46625955\n",
      "Iteration 436, loss = 0.46614692\n",
      "Iteration 437, loss = 0.46623425\n",
      "Iteration 438, loss = 0.46614631\n",
      "Iteration 439, loss = 0.46645404\n",
      "Iteration 440, loss = 0.46580548\n",
      "Iteration 441, loss = 0.46621991\n",
      "Iteration 442, loss = 0.46590344\n",
      "Iteration 443, loss = 0.46643666\n",
      "Iteration 444, loss = 0.46597803\n",
      "Iteration 445, loss = 0.46629502\n",
      "Iteration 446, loss = 0.46577614\n",
      "Iteration 447, loss = 0.46626703\n",
      "Iteration 448, loss = 0.46599363\n",
      "Iteration 449, loss = 0.46597873\n",
      "Iteration 450, loss = 0.46582998\n",
      "Iteration 451, loss = 0.46660588\n",
      "Iteration 452, loss = 0.46599679\n",
      "Iteration 453, loss = 0.46570132\n",
      "Iteration 454, loss = 0.46584874\n",
      "Iteration 455, loss = 0.46584238\n",
      "Iteration 456, loss = 0.46582568\n",
      "Iteration 457, loss = 0.46602295\n",
      "Iteration 458, loss = 0.46578910\n",
      "Iteration 459, loss = 0.46586705\n",
      "Iteration 460, loss = 0.46567906\n",
      "Iteration 461, loss = 0.46579077\n",
      "Iteration 462, loss = 0.46577915\n",
      "Iteration 463, loss = 0.46563110\n",
      "Iteration 464, loss = 0.46551338\n",
      "Iteration 465, loss = 0.46560270\n",
      "Iteration 466, loss = 0.46545536\n",
      "Iteration 467, loss = 0.46546814\n",
      "Iteration 468, loss = 0.46555435\n",
      "Iteration 469, loss = 0.46577243\n",
      "Iteration 470, loss = 0.46541683\n",
      "Iteration 471, loss = 0.46590937\n",
      "Iteration 472, loss = 0.46573482\n",
      "Iteration 473, loss = 0.46551879\n",
      "Iteration 474, loss = 0.46555276\n",
      "Iteration 475, loss = 0.46628699\n",
      "Iteration 476, loss = 0.46552286\n",
      "Iteration 477, loss = 0.46559484\n",
      "Iteration 478, loss = 0.46554492\n",
      "Iteration 479, loss = 0.46537585\n",
      "Iteration 480, loss = 0.46538378\n",
      "Iteration 481, loss = 0.46529946\n",
      "Iteration 482, loss = 0.46543517\n",
      "Iteration 483, loss = 0.46565752\n",
      "Iteration 484, loss = 0.46508910\n",
      "Iteration 485, loss = 0.46537881\n",
      "Iteration 486, loss = 0.46536829\n",
      "Iteration 487, loss = 0.46550016\n",
      "Iteration 488, loss = 0.46503507\n",
      "Iteration 489, loss = 0.46497725\n",
      "Iteration 490, loss = 0.46513227\n",
      "Iteration 491, loss = 0.46511426\n",
      "Iteration 492, loss = 0.46513058\n",
      "Iteration 493, loss = 0.46518013\n",
      "Iteration 494, loss = 0.46499995\n",
      "Iteration 495, loss = 0.46509701\n",
      "Iteration 496, loss = 0.46505717\n",
      "Iteration 497, loss = 0.46510927\n",
      "Iteration 498, loss = 0.46504474\n",
      "Iteration 499, loss = 0.46500835\n",
      "Iteration 500, loss = 0.46501941\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71801799\n",
      "Iteration 2, loss = 0.68559569\n",
      "Iteration 3, loss = 0.66302143\n",
      "Iteration 4, loss = 0.64546695\n",
      "Iteration 5, loss = 0.62847748\n",
      "Iteration 6, loss = 0.61408669\n",
      "Iteration 7, loss = 0.60140052\n",
      "Iteration 8, loss = 0.59096597\n",
      "Iteration 9, loss = 0.58268229\n",
      "Iteration 10, loss = 0.57638318\n",
      "Iteration 11, loss = 0.57105907\n",
      "Iteration 12, loss = 0.56695465\n",
      "Iteration 13, loss = 0.56353524\n",
      "Iteration 14, loss = 0.56072129\n",
      "Iteration 15, loss = 0.55767370\n",
      "Iteration 16, loss = 0.55511324\n",
      "Iteration 17, loss = 0.55271193\n",
      "Iteration 18, loss = 0.55034196\n",
      "Iteration 19, loss = 0.54805737\n",
      "Iteration 20, loss = 0.54614963\n",
      "Iteration 21, loss = 0.54421498\n",
      "Iteration 22, loss = 0.54244220\n",
      "Iteration 23, loss = 0.54101433\n",
      "Iteration 24, loss = 0.53926396\n",
      "Iteration 25, loss = 0.53786314\n",
      "Iteration 26, loss = 0.53662477\n",
      "Iteration 27, loss = 0.53533683\n",
      "Iteration 28, loss = 0.53489082\n",
      "Iteration 29, loss = 0.53362301\n",
      "Iteration 30, loss = 0.53258967\n",
      "Iteration 31, loss = 0.53171729\n",
      "Iteration 32, loss = 0.53076094\n",
      "Iteration 33, loss = 0.52993214\n",
      "Iteration 34, loss = 0.52940141\n",
      "Iteration 35, loss = 0.52840564\n",
      "Iteration 36, loss = 0.52770031\n",
      "Iteration 37, loss = 0.52700941\n",
      "Iteration 38, loss = 0.52656774\n",
      "Iteration 39, loss = 0.52602927\n",
      "Iteration 40, loss = 0.52565205\n",
      "Iteration 41, loss = 0.52516138\n",
      "Iteration 42, loss = 0.52460010\n",
      "Iteration 43, loss = 0.52388160\n",
      "Iteration 44, loss = 0.52364072\n",
      "Iteration 45, loss = 0.52323753\n",
      "Iteration 46, loss = 0.52277160\n",
      "Iteration 47, loss = 0.52235296\n",
      "Iteration 48, loss = 0.52213164\n",
      "Iteration 49, loss = 0.52178700\n",
      "Iteration 50, loss = 0.52136498\n",
      "Iteration 51, loss = 0.52114586\n",
      "Iteration 52, loss = 0.52105332\n",
      "Iteration 53, loss = 0.52055173\n",
      "Iteration 54, loss = 0.51979347\n",
      "Iteration 55, loss = 0.51977655\n",
      "Iteration 56, loss = 0.51928549\n",
      "Iteration 57, loss = 0.51899265\n",
      "Iteration 58, loss = 0.51857113\n",
      "Iteration 59, loss = 0.51808690\n",
      "Iteration 60, loss = 0.51783816\n",
      "Iteration 61, loss = 0.51739172\n",
      "Iteration 62, loss = 0.51730833\n",
      "Iteration 63, loss = 0.51705981\n",
      "Iteration 64, loss = 0.51641086\n",
      "Iteration 65, loss = 0.51604201\n",
      "Iteration 66, loss = 0.51615267\n",
      "Iteration 67, loss = 0.51524106\n",
      "Iteration 68, loss = 0.51504749\n",
      "Iteration 69, loss = 0.51482388\n",
      "Iteration 70, loss = 0.51428114\n",
      "Iteration 71, loss = 0.51409543\n",
      "Iteration 72, loss = 0.51360632\n",
      "Iteration 73, loss = 0.51326599\n",
      "Iteration 74, loss = 0.51314806\n",
      "Iteration 75, loss = 0.51271141\n",
      "Iteration 76, loss = 0.51232458\n",
      "Iteration 77, loss = 0.51214570\n",
      "Iteration 78, loss = 0.51161498\n",
      "Iteration 79, loss = 0.51113938\n",
      "Iteration 80, loss = 0.51096061\n",
      "Iteration 81, loss = 0.51090684\n",
      "Iteration 82, loss = 0.51016031\n",
      "Iteration 83, loss = 0.50989381\n",
      "Iteration 84, loss = 0.50966153\n",
      "Iteration 85, loss = 0.50956742\n",
      "Iteration 86, loss = 0.50887411\n",
      "Iteration 87, loss = 0.50878020\n",
      "Iteration 88, loss = 0.50842778\n",
      "Iteration 89, loss = 0.50857810\n",
      "Iteration 90, loss = 0.50769661\n",
      "Iteration 91, loss = 0.50728771\n",
      "Iteration 92, loss = 0.50741501\n",
      "Iteration 93, loss = 0.50684495\n",
      "Iteration 94, loss = 0.50657436\n",
      "Iteration 95, loss = 0.50611159\n",
      "Iteration 96, loss = 0.50585077\n",
      "Iteration 97, loss = 0.50539284\n",
      "Iteration 98, loss = 0.50532245\n",
      "Iteration 99, loss = 0.50472139\n",
      "Iteration 100, loss = 0.50443380\n",
      "Iteration 101, loss = 0.50420054\n",
      "Iteration 102, loss = 0.50376433\n",
      "Iteration 103, loss = 0.50356757\n",
      "Iteration 104, loss = 0.50320261\n",
      "Iteration 105, loss = 0.50286306\n",
      "Iteration 106, loss = 0.50256169\n",
      "Iteration 107, loss = 0.50251788\n",
      "Iteration 108, loss = 0.50175218\n",
      "Iteration 109, loss = 0.50164656\n",
      "Iteration 110, loss = 0.50145900\n",
      "Iteration 111, loss = 0.50107805\n",
      "Iteration 112, loss = 0.50089037\n",
      "Iteration 113, loss = 0.50121294\n",
      "Iteration 114, loss = 0.50063888\n",
      "Iteration 115, loss = 0.50034614\n",
      "Iteration 116, loss = 0.50010374\n",
      "Iteration 117, loss = 0.49976536\n",
      "Iteration 118, loss = 0.49974192\n",
      "Iteration 119, loss = 0.49943820\n",
      "Iteration 120, loss = 0.49913504\n",
      "Iteration 121, loss = 0.49894830\n",
      "Iteration 122, loss = 0.49888501\n",
      "Iteration 123, loss = 0.49845095\n",
      "Iteration 124, loss = 0.49842417\n",
      "Iteration 125, loss = 0.49811946\n",
      "Iteration 126, loss = 0.49785455\n",
      "Iteration 127, loss = 0.49775039\n",
      "Iteration 128, loss = 0.49751254\n",
      "Iteration 129, loss = 0.49729125\n",
      "Iteration 130, loss = 0.49708975\n",
      "Iteration 131, loss = 0.49679854\n",
      "Iteration 132, loss = 0.49681388\n",
      "Iteration 133, loss = 0.49660192\n",
      "Iteration 134, loss = 0.49646697\n",
      "Iteration 135, loss = 0.49634374\n",
      "Iteration 136, loss = 0.49597161\n",
      "Iteration 137, loss = 0.49580717\n",
      "Iteration 138, loss = 0.49562498\n",
      "Iteration 139, loss = 0.49549706\n",
      "Iteration 140, loss = 0.49529234\n",
      "Iteration 141, loss = 0.49538858\n",
      "Iteration 142, loss = 0.49523307\n",
      "Iteration 143, loss = 0.49491712\n",
      "Iteration 144, loss = 0.49466992\n",
      "Iteration 145, loss = 0.49440593\n",
      "Iteration 146, loss = 0.49415529\n",
      "Iteration 147, loss = 0.49434221\n",
      "Iteration 148, loss = 0.49417965\n",
      "Iteration 149, loss = 0.49401887\n",
      "Iteration 150, loss = 0.49363276\n",
      "Iteration 151, loss = 0.49374920\n",
      "Iteration 152, loss = 0.49336274\n",
      "Iteration 153, loss = 0.49312076\n",
      "Iteration 154, loss = 0.49343280\n",
      "Iteration 155, loss = 0.49288273\n",
      "Iteration 156, loss = 0.49302810\n",
      "Iteration 157, loss = 0.49255323\n",
      "Iteration 158, loss = 0.49241360\n",
      "Iteration 159, loss = 0.49209596\n",
      "Iteration 160, loss = 0.49229136\n",
      "Iteration 161, loss = 0.49209423\n",
      "Iteration 162, loss = 0.49208086\n",
      "Iteration 163, loss = 0.49176078\n",
      "Iteration 164, loss = 0.49133975\n",
      "Iteration 165, loss = 0.49142101\n",
      "Iteration 166, loss = 0.49116736\n",
      "Iteration 167, loss = 0.49100560\n",
      "Iteration 168, loss = 0.49086418\n",
      "Iteration 169, loss = 0.49075275\n",
      "Iteration 170, loss = 0.49055557\n",
      "Iteration 171, loss = 0.49038851\n",
      "Iteration 172, loss = 0.49075414\n",
      "Iteration 173, loss = 0.49035220\n",
      "Iteration 174, loss = 0.49023481\n",
      "Iteration 175, loss = 0.49008218\n",
      "Iteration 176, loss = 0.48969251\n",
      "Iteration 177, loss = 0.48969537\n",
      "Iteration 178, loss = 0.48976283\n",
      "Iteration 179, loss = 0.48981950\n",
      "Iteration 180, loss = 0.48947729\n",
      "Iteration 181, loss = 0.48911387\n",
      "Iteration 182, loss = 0.48899950\n",
      "Iteration 183, loss = 0.48892657\n",
      "Iteration 184, loss = 0.48898937\n",
      "Iteration 185, loss = 0.48882701\n",
      "Iteration 186, loss = 0.48846094\n",
      "Iteration 187, loss = 0.48837591\n",
      "Iteration 188, loss = 0.48860047\n",
      "Iteration 189, loss = 0.48839839\n",
      "Iteration 190, loss = 0.48830638\n",
      "Iteration 191, loss = 0.48786329\n",
      "Iteration 192, loss = 0.48793874\n",
      "Iteration 193, loss = 0.48797246\n",
      "Iteration 194, loss = 0.48768143\n",
      "Iteration 195, loss = 0.48800887\n",
      "Iteration 196, loss = 0.48743239\n",
      "Iteration 197, loss = 0.48734152\n",
      "Iteration 198, loss = 0.48716534\n",
      "Iteration 199, loss = 0.48749935\n",
      "Iteration 200, loss = 0.48714823\n",
      "Iteration 201, loss = 0.48715679\n",
      "Iteration 202, loss = 0.48676917\n",
      "Iteration 203, loss = 0.48673018\n",
      "Iteration 204, loss = 0.48680258\n",
      "Iteration 205, loss = 0.48686431\n",
      "Iteration 206, loss = 0.48652486\n",
      "Iteration 207, loss = 0.48623825\n",
      "Iteration 208, loss = 0.48640593\n",
      "Iteration 209, loss = 0.48630773\n",
      "Iteration 210, loss = 0.48646231\n",
      "Iteration 211, loss = 0.48589675\n",
      "Iteration 212, loss = 0.48601049\n",
      "Iteration 213, loss = 0.48571089\n",
      "Iteration 214, loss = 0.48589703\n",
      "Iteration 215, loss = 0.48573439\n",
      "Iteration 216, loss = 0.48569270\n",
      "Iteration 217, loss = 0.48554761\n",
      "Iteration 218, loss = 0.48527379\n",
      "Iteration 219, loss = 0.48557349\n",
      "Iteration 220, loss = 0.48556460\n",
      "Iteration 221, loss = 0.48509844\n",
      "Iteration 222, loss = 0.48500679\n",
      "Iteration 223, loss = 0.48495562\n",
      "Iteration 224, loss = 0.48477117\n",
      "Iteration 225, loss = 0.48500131\n",
      "Iteration 226, loss = 0.48457899\n",
      "Iteration 227, loss = 0.48465120\n",
      "Iteration 228, loss = 0.48448383\n",
      "Iteration 229, loss = 0.48442294\n",
      "Iteration 230, loss = 0.48444879\n",
      "Iteration 231, loss = 0.48460438\n",
      "Iteration 232, loss = 0.48445228\n",
      "Iteration 233, loss = 0.48408688\n",
      "Iteration 234, loss = 0.48398481\n",
      "Iteration 235, loss = 0.48393809\n",
      "Iteration 236, loss = 0.48411280\n",
      "Iteration 237, loss = 0.48409124\n",
      "Iteration 238, loss = 0.48368824\n",
      "Iteration 239, loss = 0.48363111\n",
      "Iteration 240, loss = 0.48374344\n",
      "Iteration 241, loss = 0.48365108\n",
      "Iteration 242, loss = 0.48371903\n",
      "Iteration 243, loss = 0.48344110\n",
      "Iteration 244, loss = 0.48314837\n",
      "Iteration 245, loss = 0.48324319\n",
      "Iteration 246, loss = 0.48314577\n",
      "Iteration 247, loss = 0.48304569\n",
      "Iteration 248, loss = 0.48288550\n",
      "Iteration 249, loss = 0.48297587\n",
      "Iteration 250, loss = 0.48294718\n",
      "Iteration 251, loss = 0.48264432\n",
      "Iteration 252, loss = 0.48261622\n",
      "Iteration 253, loss = 0.48251523\n",
      "Iteration 254, loss = 0.48267506\n",
      "Iteration 255, loss = 0.48259271\n",
      "Iteration 256, loss = 0.48240054\n",
      "Iteration 257, loss = 0.48212996\n",
      "Iteration 258, loss = 0.48238244\n",
      "Iteration 259, loss = 0.48217347\n",
      "Iteration 260, loss = 0.48231613\n",
      "Iteration 261, loss = 0.48207824\n",
      "Iteration 262, loss = 0.48194471\n",
      "Iteration 263, loss = 0.48198054\n",
      "Iteration 264, loss = 0.48177636\n",
      "Iteration 265, loss = 0.48229549\n",
      "Iteration 266, loss = 0.48177344\n",
      "Iteration 267, loss = 0.48187798\n",
      "Iteration 268, loss = 0.48171984\n",
      "Iteration 269, loss = 0.48151871\n",
      "Iteration 270, loss = 0.48136172\n",
      "Iteration 271, loss = 0.48133693\n",
      "Iteration 272, loss = 0.48144459\n",
      "Iteration 273, loss = 0.48137186\n",
      "Iteration 274, loss = 0.48144448\n",
      "Iteration 275, loss = 0.48110519\n",
      "Iteration 276, loss = 0.48129351\n",
      "Iteration 277, loss = 0.48107481\n",
      "Iteration 278, loss = 0.48074478\n",
      "Iteration 279, loss = 0.48080510\n",
      "Iteration 280, loss = 0.48086994\n",
      "Iteration 281, loss = 0.48059961\n",
      "Iteration 282, loss = 0.48074733\n",
      "Iteration 283, loss = 0.48074950\n",
      "Iteration 284, loss = 0.48081389\n",
      "Iteration 285, loss = 0.48047658\n",
      "Iteration 286, loss = 0.48021601\n",
      "Iteration 287, loss = 0.48037086\n",
      "Iteration 288, loss = 0.48006681\n",
      "Iteration 289, loss = 0.48013142\n",
      "Iteration 290, loss = 0.48006997\n",
      "Iteration 291, loss = 0.47997871\n",
      "Iteration 292, loss = 0.48014795\n",
      "Iteration 293, loss = 0.48030908\n",
      "Iteration 294, loss = 0.47977898\n",
      "Iteration 295, loss = 0.47975562\n",
      "Iteration 296, loss = 0.47986317\n",
      "Iteration 297, loss = 0.47954212\n",
      "Iteration 298, loss = 0.47963666\n",
      "Iteration 299, loss = 0.48000086\n",
      "Iteration 300, loss = 0.47961607\n",
      "Iteration 301, loss = 0.47938246\n",
      "Iteration 302, loss = 0.47958342\n",
      "Iteration 303, loss = 0.47987719\n",
      "Iteration 304, loss = 0.47969829\n",
      "Iteration 305, loss = 0.47927321\n",
      "Iteration 306, loss = 0.47935378\n",
      "Iteration 307, loss = 0.47941150\n",
      "Iteration 308, loss = 0.47907965\n",
      "Iteration 309, loss = 0.47937190\n",
      "Iteration 310, loss = 0.47902374\n",
      "Iteration 311, loss = 0.47944798\n",
      "Iteration 312, loss = 0.47937894\n",
      "Iteration 313, loss = 0.47890678\n",
      "Iteration 314, loss = 0.47906919\n",
      "Iteration 315, loss = 0.47876009\n",
      "Iteration 316, loss = 0.47873672\n",
      "Iteration 317, loss = 0.47880208\n",
      "Iteration 318, loss = 0.47893524\n",
      "Iteration 319, loss = 0.47862961\n",
      "Iteration 320, loss = 0.47876602\n",
      "Iteration 321, loss = 0.47847400\n",
      "Iteration 322, loss = 0.47844109\n",
      "Iteration 323, loss = 0.47845081\n",
      "Iteration 324, loss = 0.47809347\n",
      "Iteration 325, loss = 0.47814693\n",
      "Iteration 326, loss = 0.47803831\n",
      "Iteration 327, loss = 0.47821853\n",
      "Iteration 328, loss = 0.47803280\n",
      "Iteration 329, loss = 0.47805280\n",
      "Iteration 330, loss = 0.47796527\n",
      "Iteration 331, loss = 0.47806313\n",
      "Iteration 332, loss = 0.47786239\n",
      "Iteration 333, loss = 0.47776470\n",
      "Iteration 334, loss = 0.47788280\n",
      "Iteration 335, loss = 0.47775021\n",
      "Iteration 336, loss = 0.47762813\n",
      "Iteration 337, loss = 0.47765376\n",
      "Iteration 338, loss = 0.47747484\n",
      "Iteration 339, loss = 0.47738673\n",
      "Iteration 340, loss = 0.47734616\n",
      "Iteration 341, loss = 0.47722368\n",
      "Iteration 342, loss = 0.47723489\n",
      "Iteration 343, loss = 0.47726852\n",
      "Iteration 344, loss = 0.47703604\n",
      "Iteration 345, loss = 0.47693548\n",
      "Iteration 346, loss = 0.47691765\n",
      "Iteration 347, loss = 0.47697557\n",
      "Iteration 348, loss = 0.47682423\n",
      "Iteration 349, loss = 0.47694926\n",
      "Iteration 350, loss = 0.47681207\n",
      "Iteration 351, loss = 0.47681215\n",
      "Iteration 352, loss = 0.47680857\n",
      "Iteration 353, loss = 0.47668118\n",
      "Iteration 354, loss = 0.47656968\n",
      "Iteration 355, loss = 0.47686603\n",
      "Iteration 356, loss = 0.47674544\n",
      "Iteration 357, loss = 0.47643595\n",
      "Iteration 358, loss = 0.47632328\n",
      "Iteration 359, loss = 0.47617497\n",
      "Iteration 360, loss = 0.47626920\n",
      "Iteration 361, loss = 0.47618891\n",
      "Iteration 362, loss = 0.47603181\n",
      "Iteration 363, loss = 0.47616055\n",
      "Iteration 364, loss = 0.47615854\n",
      "Iteration 365, loss = 0.47618825\n",
      "Iteration 366, loss = 0.47616719\n",
      "Iteration 367, loss = 0.47604955\n",
      "Iteration 368, loss = 0.47590453\n",
      "Iteration 369, loss = 0.47616533\n",
      "Iteration 370, loss = 0.47587541\n",
      "Iteration 371, loss = 0.47589486\n",
      "Iteration 372, loss = 0.47600277\n",
      "Iteration 373, loss = 0.47579974\n",
      "Iteration 374, loss = 0.47584968\n",
      "Iteration 375, loss = 0.47609185\n",
      "Iteration 376, loss = 0.47542249\n",
      "Iteration 377, loss = 0.47572958\n",
      "Iteration 378, loss = 0.47561978\n",
      "Iteration 379, loss = 0.47553141\n",
      "Iteration 380, loss = 0.47533852\n",
      "Iteration 381, loss = 0.47538974\n",
      "Iteration 382, loss = 0.47542897\n",
      "Iteration 383, loss = 0.47510099\n",
      "Iteration 384, loss = 0.47552077\n",
      "Iteration 385, loss = 0.47512164\n",
      "Iteration 386, loss = 0.47526279\n",
      "Iteration 387, loss = 0.47513561\n",
      "Iteration 388, loss = 0.47506107\n",
      "Iteration 389, loss = 0.47486708\n",
      "Iteration 390, loss = 0.47499880\n",
      "Iteration 391, loss = 0.47485861\n",
      "Iteration 392, loss = 0.47501912\n",
      "Iteration 393, loss = 0.47500292\n",
      "Iteration 394, loss = 0.47475930\n",
      "Iteration 395, loss = 0.47500914\n",
      "Iteration 396, loss = 0.47482572\n",
      "Iteration 397, loss = 0.47467148\n",
      "Iteration 398, loss = 0.47487280\n",
      "Iteration 399, loss = 0.47465138\n",
      "Iteration 400, loss = 0.47451301\n",
      "Iteration 401, loss = 0.47467376\n",
      "Iteration 402, loss = 0.47438149\n",
      "Iteration 403, loss = 0.47474713\n",
      "Iteration 404, loss = 0.47457851\n",
      "Iteration 405, loss = 0.47436603\n",
      "Iteration 406, loss = 0.47446087\n",
      "Iteration 407, loss = 0.47445488\n",
      "Iteration 408, loss = 0.47428985\n",
      "Iteration 409, loss = 0.47444448\n",
      "Iteration 410, loss = 0.47386169\n",
      "Iteration 411, loss = 0.47417815\n",
      "Iteration 412, loss = 0.47401103\n",
      "Iteration 413, loss = 0.47404205\n",
      "Iteration 414, loss = 0.47378884\n",
      "Iteration 415, loss = 0.47425765\n",
      "Iteration 416, loss = 0.47375870\n",
      "Iteration 417, loss = 0.47394937\n",
      "Iteration 418, loss = 0.47365034\n",
      "Iteration 419, loss = 0.47387299\n",
      "Iteration 420, loss = 0.47373159\n",
      "Iteration 421, loss = 0.47341093\n",
      "Iteration 422, loss = 0.47344379\n",
      "Iteration 423, loss = 0.47351735\n",
      "Iteration 424, loss = 0.47346039\n",
      "Iteration 425, loss = 0.47322619\n",
      "Iteration 426, loss = 0.47330237\n",
      "Iteration 427, loss = 0.47319595\n",
      "Iteration 428, loss = 0.47307204\n",
      "Iteration 429, loss = 0.47293500\n",
      "Iteration 430, loss = 0.47301452\n",
      "Iteration 431, loss = 0.47291724\n",
      "Iteration 432, loss = 0.47276988\n",
      "Iteration 433, loss = 0.47271968\n",
      "Iteration 434, loss = 0.47300089\n",
      "Iteration 435, loss = 0.47286528\n",
      "Iteration 436, loss = 0.47274837\n",
      "Iteration 437, loss = 0.47270885\n",
      "Iteration 438, loss = 0.47259730\n",
      "Iteration 439, loss = 0.47250476\n",
      "Iteration 440, loss = 0.47245275\n",
      "Iteration 441, loss = 0.47239647\n",
      "Iteration 442, loss = 0.47246152\n",
      "Iteration 443, loss = 0.47215665\n",
      "Iteration 444, loss = 0.47244416\n",
      "Iteration 445, loss = 0.47237365\n",
      "Iteration 446, loss = 0.47203500\n",
      "Iteration 447, loss = 0.47157830\n",
      "Iteration 448, loss = 0.47183595\n",
      "Iteration 449, loss = 0.47218886\n",
      "Iteration 450, loss = 0.47183098\n",
      "Iteration 451, loss = 0.47149922\n",
      "Iteration 452, loss = 0.47135101\n",
      "Iteration 453, loss = 0.47124452\n",
      "Iteration 454, loss = 0.47105158\n",
      "Iteration 455, loss = 0.47174663\n",
      "Iteration 456, loss = 0.47147877\n",
      "Iteration 457, loss = 0.47133441\n",
      "Iteration 458, loss = 0.47082666\n",
      "Iteration 459, loss = 0.47076786\n",
      "Iteration 460, loss = 0.47067773\n",
      "Iteration 461, loss = 0.47055234\n",
      "Iteration 462, loss = 0.47073384\n",
      "Iteration 463, loss = 0.47047497\n",
      "Iteration 464, loss = 0.47069338\n",
      "Iteration 465, loss = 0.47031517\n",
      "Iteration 466, loss = 0.47020042\n",
      "Iteration 467, loss = 0.47017065\n",
      "Iteration 468, loss = 0.47038288\n",
      "Iteration 469, loss = 0.47010174\n",
      "Iteration 470, loss = 0.47021201\n",
      "Iteration 471, loss = 0.46984665\n",
      "Iteration 472, loss = 0.46994799\n",
      "Iteration 473, loss = 0.46988343\n",
      "Iteration 474, loss = 0.46997833\n",
      "Iteration 475, loss = 0.47002784\n",
      "Iteration 476, loss = 0.46943922\n",
      "Iteration 477, loss = 0.46952795\n",
      "Iteration 478, loss = 0.46942809\n",
      "Iteration 479, loss = 0.46927257\n",
      "Iteration 480, loss = 0.46969695\n",
      "Iteration 481, loss = 0.46935279\n",
      "Iteration 482, loss = 0.46928886\n",
      "Iteration 483, loss = 0.46915631\n",
      "Iteration 484, loss = 0.46888348\n",
      "Iteration 485, loss = 0.46910820\n",
      "Iteration 486, loss = 0.46908164\n",
      "Iteration 487, loss = 0.46896287\n",
      "Iteration 488, loss = 0.46877314\n",
      "Iteration 489, loss = 0.46883181\n",
      "Iteration 490, loss = 0.46879015\n",
      "Iteration 491, loss = 0.46860030\n",
      "Iteration 492, loss = 0.46870056\n",
      "Iteration 493, loss = 0.46876343\n",
      "Iteration 494, loss = 0.46854180\n",
      "Iteration 495, loss = 0.46839797\n",
      "Iteration 496, loss = 0.46852410\n",
      "Iteration 497, loss = 0.46838125\n",
      "Iteration 498, loss = 0.46832676\n",
      "Iteration 499, loss = 0.46830023\n",
      "Iteration 500, loss = 0.46832841\n",
      "Iteration 501, loss = 0.46821170\n",
      "Iteration 502, loss = 0.46796511\n",
      "Iteration 503, loss = 0.46810055\n",
      "Iteration 504, loss = 0.46794821\n",
      "Iteration 505, loss = 0.46788897\n",
      "Iteration 506, loss = 0.46812672\n",
      "Iteration 507, loss = 0.46796571\n",
      "Iteration 508, loss = 0.46791073\n",
      "Iteration 509, loss = 0.46797762\n",
      "Iteration 510, loss = 0.46774374\n",
      "Iteration 511, loss = 0.46761723\n",
      "Iteration 512, loss = 0.46748271\n",
      "Iteration 513, loss = 0.46744014\n",
      "Iteration 514, loss = 0.46752448\n",
      "Iteration 515, loss = 0.46743171\n",
      "Iteration 516, loss = 0.46742174\n",
      "Iteration 517, loss = 0.46765825\n",
      "Iteration 518, loss = 0.46756901\n",
      "Iteration 519, loss = 0.46736703\n",
      "Iteration 520, loss = 0.46701337\n",
      "Iteration 521, loss = 0.46697415\n",
      "Iteration 522, loss = 0.46710534\n",
      "Iteration 523, loss = 0.46695826\n",
      "Iteration 524, loss = 0.46685426\n",
      "Iteration 525, loss = 0.46705414\n",
      "Iteration 526, loss = 0.46697523\n",
      "Iteration 527, loss = 0.46685664\n",
      "Iteration 528, loss = 0.46680707\n",
      "Iteration 529, loss = 0.46698331\n",
      "Iteration 530, loss = 0.46677563\n",
      "Iteration 531, loss = 0.46640473\n",
      "Iteration 532, loss = 0.46645645\n",
      "Iteration 533, loss = 0.46643637\n",
      "Iteration 534, loss = 0.46669065\n",
      "Iteration 535, loss = 0.46616456\n",
      "Iteration 536, loss = 0.46651601\n",
      "Iteration 537, loss = 0.46639274\n",
      "Iteration 538, loss = 0.46611459\n",
      "Iteration 539, loss = 0.46591389\n",
      "Iteration 540, loss = 0.46607549\n",
      "Iteration 541, loss = 0.46587209\n",
      "Iteration 542, loss = 0.46579695\n",
      "Iteration 543, loss = 0.46570539\n",
      "Iteration 544, loss = 0.46578300\n",
      "Iteration 545, loss = 0.46567678\n",
      "Iteration 546, loss = 0.46545930\n",
      "Iteration 547, loss = 0.46562128\n",
      "Iteration 548, loss = 0.46563714\n",
      "Iteration 549, loss = 0.46536263\n",
      "Iteration 550, loss = 0.46605483\n",
      "Iteration 551, loss = 0.46581700\n",
      "Iteration 552, loss = 0.46577470\n",
      "Iteration 553, loss = 0.46546308\n",
      "Iteration 554, loss = 0.46525896\n",
      "Iteration 555, loss = 0.46519176\n",
      "Iteration 556, loss = 0.46529706\n",
      "Iteration 557, loss = 0.46527619\n",
      "Iteration 558, loss = 0.46492346\n",
      "Iteration 559, loss = 0.46479959\n",
      "Iteration 560, loss = 0.46464506\n",
      "Iteration 561, loss = 0.46494570\n",
      "Iteration 562, loss = 0.46470086\n",
      "Iteration 563, loss = 0.46461695\n",
      "Iteration 564, loss = 0.46492000\n",
      "Iteration 565, loss = 0.46470934\n",
      "Iteration 566, loss = 0.46454465\n",
      "Iteration 567, loss = 0.46468899\n",
      "Iteration 568, loss = 0.46428567\n",
      "Iteration 569, loss = 0.46462063\n",
      "Iteration 570, loss = 0.46425678\n",
      "Iteration 571, loss = 0.46442395\n",
      "Iteration 572, loss = 0.46440662\n",
      "Iteration 573, loss = 0.46482438\n",
      "Iteration 574, loss = 0.46425343\n",
      "Iteration 575, loss = 0.46440192\n",
      "Iteration 576, loss = 0.46401869\n",
      "Iteration 577, loss = 0.46409263\n",
      "Iteration 578, loss = 0.46394380\n",
      "Iteration 579, loss = 0.46401911\n",
      "Iteration 580, loss = 0.46395443\n",
      "Iteration 581, loss = 0.46441094\n",
      "Iteration 582, loss = 0.46367607\n",
      "Iteration 583, loss = 0.46365997\n",
      "Iteration 584, loss = 0.46383928\n",
      "Iteration 585, loss = 0.46398450\n",
      "Iteration 586, loss = 0.46367362\n",
      "Iteration 587, loss = 0.46382841\n",
      "Iteration 588, loss = 0.46392205\n",
      "Iteration 589, loss = 0.46334743\n",
      "Iteration 590, loss = 0.46362034\n",
      "Iteration 591, loss = 0.46336822\n",
      "Iteration 592, loss = 0.46344674\n",
      "Iteration 593, loss = 0.46325388\n",
      "Iteration 594, loss = 0.46340798\n",
      "Iteration 595, loss = 0.46372347\n",
      "Iteration 596, loss = 0.46294702\n",
      "Iteration 597, loss = 0.46310202\n",
      "Iteration 598, loss = 0.46332507\n",
      "Iteration 599, loss = 0.46309335\n",
      "Iteration 600, loss = 0.46292563\n",
      "Iteration 601, loss = 0.46301184\n",
      "Iteration 602, loss = 0.46298006\n",
      "Iteration 603, loss = 0.46284996\n",
      "Iteration 604, loss = 0.46307105\n",
      "Iteration 605, loss = 0.46306221\n",
      "Iteration 606, loss = 0.46285108\n",
      "Iteration 607, loss = 0.46282197\n",
      "Iteration 608, loss = 0.46297226\n",
      "Iteration 609, loss = 0.46323559\n",
      "Iteration 610, loss = 0.46248386\n",
      "Iteration 611, loss = 0.46287959\n",
      "Iteration 612, loss = 0.46272426\n",
      "Iteration 613, loss = 0.46270397\n",
      "Iteration 614, loss = 0.46235812\n",
      "Iteration 615, loss = 0.46250570\n",
      "Iteration 616, loss = 0.46240033\n",
      "Iteration 617, loss = 0.46248161\n",
      "Iteration 618, loss = 0.46231460\n",
      "Iteration 619, loss = 0.46299325\n",
      "Iteration 620, loss = 0.46270810\n",
      "Iteration 621, loss = 0.46253728\n",
      "Iteration 622, loss = 0.46236070\n",
      "Iteration 623, loss = 0.46242124\n",
      "Iteration 624, loss = 0.46205075\n",
      "Iteration 625, loss = 0.46207240\n",
      "Iteration 626, loss = 0.46235252\n",
      "Iteration 627, loss = 0.46201685\n",
      "Iteration 628, loss = 0.46206956\n",
      "Iteration 629, loss = 0.46188198\n",
      "Iteration 630, loss = 0.46197040\n",
      "Iteration 631, loss = 0.46188605\n",
      "Iteration 632, loss = 0.46175004\n",
      "Iteration 633, loss = 0.46184755\n",
      "Iteration 634, loss = 0.46176867\n",
      "Iteration 635, loss = 0.46237294\n",
      "Iteration 636, loss = 0.46201446\n",
      "Iteration 637, loss = 0.46198657\n",
      "Iteration 638, loss = 0.46175178\n",
      "Iteration 639, loss = 0.46189958\n",
      "Iteration 640, loss = 0.46171601\n",
      "Iteration 641, loss = 0.46190777\n",
      "Iteration 642, loss = 0.46157494\n",
      "Iteration 643, loss = 0.46195278\n",
      "Iteration 644, loss = 0.46168572\n",
      "Iteration 645, loss = 0.46171789\n",
      "Iteration 646, loss = 0.46183242\n",
      "Iteration 647, loss = 0.46166418\n",
      "Iteration 648, loss = 0.46176340\n",
      "Iteration 649, loss = 0.46242875\n",
      "Iteration 650, loss = 0.46254364\n",
      "Iteration 651, loss = 0.46222480\n",
      "Iteration 652, loss = 0.46148193\n",
      "Iteration 653, loss = 0.46146522\n",
      "Iteration 654, loss = 0.46143214\n",
      "Iteration 655, loss = 0.46152395\n",
      "Iteration 656, loss = 0.46178644\n",
      "Iteration 657, loss = 0.46191241\n",
      "Iteration 658, loss = 0.46123588\n",
      "Iteration 659, loss = 0.46110673\n",
      "Iteration 660, loss = 0.46124904\n",
      "Iteration 661, loss = 0.46103280\n",
      "Iteration 662, loss = 0.46111594\n",
      "Iteration 663, loss = 0.46117424\n",
      "Iteration 664, loss = 0.46110482\n",
      "Iteration 665, loss = 0.46113956\n",
      "Iteration 666, loss = 0.46115223\n",
      "Iteration 667, loss = 0.46089307\n",
      "Iteration 668, loss = 0.46080439\n",
      "Iteration 669, loss = 0.46089509\n",
      "Iteration 670, loss = 0.46090686\n",
      "Iteration 671, loss = 0.46089251\n",
      "Iteration 672, loss = 0.46068077\n",
      "Iteration 673, loss = 0.46095194\n",
      "Iteration 674, loss = 0.46078715\n",
      "Iteration 675, loss = 0.46078064\n",
      "Iteration 676, loss = 0.46069672\n",
      "Iteration 677, loss = 0.46070071\n",
      "Iteration 678, loss = 0.46059125\n",
      "Iteration 679, loss = 0.46057137\n",
      "Iteration 680, loss = 0.46116059\n",
      "Iteration 681, loss = 0.46067299\n",
      "Iteration 682, loss = 0.46076433\n",
      "Iteration 683, loss = 0.46050975\n",
      "Iteration 684, loss = 0.46030076\n",
      "Iteration 685, loss = 0.46040511\n",
      "Iteration 686, loss = 0.46046799\n",
      "Iteration 687, loss = 0.46026867\n",
      "Iteration 688, loss = 0.46043714\n",
      "Iteration 689, loss = 0.46051274\n",
      "Iteration 690, loss = 0.46073203\n",
      "Iteration 691, loss = 0.46011066\n",
      "Iteration 692, loss = 0.46026687\n",
      "Iteration 693, loss = 0.46054963\n",
      "Iteration 694, loss = 0.46037308\n",
      "Iteration 695, loss = 0.46079015\n",
      "Iteration 696, loss = 0.46011663\n",
      "Iteration 697, loss = 0.46012035\n",
      "Iteration 698, loss = 0.46029663\n",
      "Iteration 699, loss = 0.46027214\n",
      "Iteration 700, loss = 0.46027776\n",
      "Iteration 701, loss = 0.46004069\n",
      "Iteration 702, loss = 0.46002945\n",
      "Iteration 703, loss = 0.45999988\n",
      "Iteration 704, loss = 0.45987636\n",
      "Iteration 705, loss = 0.46024077\n",
      "Iteration 706, loss = 0.46039120\n",
      "Iteration 707, loss = 0.45984682\n",
      "Iteration 708, loss = 0.45991307\n",
      "Iteration 709, loss = 0.46033197\n",
      "Iteration 710, loss = 0.45995851\n",
      "Iteration 711, loss = 0.45958356\n",
      "Iteration 712, loss = 0.45974931\n",
      "Iteration 713, loss = 0.45974749\n",
      "Iteration 714, loss = 0.46008523\n",
      "Iteration 715, loss = 0.46016965\n",
      "Iteration 716, loss = 0.45969758\n",
      "Iteration 717, loss = 0.45964739\n",
      "Iteration 718, loss = 0.45991660\n",
      "Iteration 719, loss = 0.45946011\n",
      "Iteration 720, loss = 0.45980869\n",
      "Iteration 721, loss = 0.45974168\n",
      "Iteration 722, loss = 0.45952968\n",
      "Iteration 723, loss = 0.45958726\n",
      "Iteration 724, loss = 0.45938684\n",
      "Iteration 725, loss = 0.45947885\n",
      "Iteration 726, loss = 0.45955464\n",
      "Iteration 727, loss = 0.45964874\n",
      "Iteration 728, loss = 0.45943273\n",
      "Iteration 729, loss = 0.45928919\n",
      "Iteration 730, loss = 0.45929201\n",
      "Iteration 731, loss = 0.45936046\n",
      "Iteration 732, loss = 0.45933047\n",
      "Iteration 733, loss = 0.45929629\n",
      "Iteration 734, loss = 0.45923286\n",
      "Iteration 735, loss = 0.45929886\n",
      "Iteration 736, loss = 0.45921559\n",
      "Iteration 737, loss = 0.45948356\n",
      "Iteration 738, loss = 0.45922598\n",
      "Iteration 739, loss = 0.45915302\n",
      "Iteration 740, loss = 0.45918300\n",
      "Iteration 741, loss = 0.45910912\n",
      "Iteration 742, loss = 0.45917028\n",
      "Iteration 743, loss = 0.45907405\n",
      "Iteration 744, loss = 0.45915907\n",
      "Iteration 745, loss = 0.45948726\n",
      "Iteration 746, loss = 0.45957809\n",
      "Iteration 747, loss = 0.45890483\n",
      "Iteration 748, loss = 0.45881814\n",
      "Iteration 749, loss = 0.45907339\n",
      "Iteration 750, loss = 0.45903418\n",
      "Iteration 751, loss = 0.45971711\n",
      "Iteration 752, loss = 0.45915778\n",
      "Iteration 753, loss = 0.45859695\n",
      "Iteration 754, loss = 0.45869421\n",
      "Iteration 755, loss = 0.45879062\n",
      "Iteration 756, loss = 0.45918558\n",
      "Iteration 757, loss = 0.45873661\n",
      "Iteration 758, loss = 0.45854894\n",
      "Iteration 759, loss = 0.45856714\n",
      "Iteration 760, loss = 0.45880154\n",
      "Iteration 761, loss = 0.45892949\n",
      "Iteration 762, loss = 0.45879208\n",
      "Iteration 763, loss = 0.45836293\n",
      "Iteration 764, loss = 0.45856024\n",
      "Iteration 765, loss = 0.45849967\n",
      "Iteration 766, loss = 0.45831939\n",
      "Iteration 767, loss = 0.45835328\n",
      "Iteration 768, loss = 0.45817968\n",
      "Iteration 769, loss = 0.45809046\n",
      "Iteration 770, loss = 0.45827166\n",
      "Iteration 771, loss = 0.45865349\n",
      "Iteration 772, loss = 0.45821025\n",
      "Iteration 773, loss = 0.45805321\n",
      "Iteration 774, loss = 0.45843084\n",
      "Iteration 775, loss = 0.45830655\n",
      "Iteration 776, loss = 0.45818653\n",
      "Iteration 777, loss = 0.45809290\n",
      "Iteration 778, loss = 0.45820781\n",
      "Iteration 779, loss = 0.45870914\n",
      "Iteration 780, loss = 0.45818807\n",
      "Iteration 781, loss = 0.45824692\n",
      "Iteration 782, loss = 0.45793107\n",
      "Iteration 783, loss = 0.45843707\n",
      "Iteration 784, loss = 0.45827982\n",
      "Iteration 785, loss = 0.45808823\n",
      "Iteration 786, loss = 0.45819018\n",
      "Iteration 787, loss = 0.45813767\n",
      "Iteration 788, loss = 0.45834147\n",
      "Iteration 789, loss = 0.45805909\n",
      "Iteration 790, loss = 0.45767036\n",
      "Iteration 791, loss = 0.45783277\n",
      "Iteration 792, loss = 0.45806556\n",
      "Iteration 793, loss = 0.45803938\n",
      "Iteration 794, loss = 0.45807334\n",
      "Iteration 795, loss = 0.45796986\n",
      "Iteration 796, loss = 0.45834623\n",
      "Iteration 797, loss = 0.45845547\n",
      "Iteration 798, loss = 0.45781666\n",
      "Iteration 799, loss = 0.45801520\n",
      "Iteration 800, loss = 0.45775556\n",
      "Iteration 801, loss = 0.45784505\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82578442\n",
      "Iteration 2, loss = 0.75170279\n",
      "Iteration 3, loss = 0.70851169\n",
      "Iteration 4, loss = 0.68510076\n",
      "Iteration 5, loss = 0.66839351\n",
      "Iteration 6, loss = 0.65496140\n",
      "Iteration 7, loss = 0.64304562\n",
      "Iteration 8, loss = 0.63292363\n",
      "Iteration 9, loss = 0.62279922\n",
      "Iteration 10, loss = 0.61397206\n",
      "Iteration 11, loss = 0.60563001\n",
      "Iteration 12, loss = 0.59817953\n",
      "Iteration 13, loss = 0.59193395\n",
      "Iteration 14, loss = 0.58595067\n",
      "Iteration 15, loss = 0.58115173\n",
      "Iteration 16, loss = 0.57693723\n",
      "Iteration 17, loss = 0.57292576\n",
      "Iteration 18, loss = 0.56991567\n",
      "Iteration 19, loss = 0.56687694\n",
      "Iteration 20, loss = 0.56436234\n",
      "Iteration 21, loss = 0.56213029\n",
      "Iteration 22, loss = 0.56029909\n",
      "Iteration 23, loss = 0.55794382\n",
      "Iteration 24, loss = 0.55628503\n",
      "Iteration 25, loss = 0.55470139\n",
      "Iteration 26, loss = 0.55268005\n",
      "Iteration 27, loss = 0.55098573\n",
      "Iteration 28, loss = 0.54944776\n",
      "Iteration 29, loss = 0.54799165\n",
      "Iteration 30, loss = 0.54672961\n",
      "Iteration 31, loss = 0.54496636\n",
      "Iteration 32, loss = 0.54360052\n",
      "Iteration 33, loss = 0.54263002\n",
      "Iteration 34, loss = 0.54087084\n",
      "Iteration 35, loss = 0.53943670\n",
      "Iteration 36, loss = 0.53832484\n",
      "Iteration 37, loss = 0.53687047\n",
      "Iteration 38, loss = 0.53554662\n",
      "Iteration 39, loss = 0.53419948\n",
      "Iteration 40, loss = 0.53313881\n",
      "Iteration 41, loss = 0.53204595\n",
      "Iteration 42, loss = 0.53096722\n",
      "Iteration 43, loss = 0.53010645\n",
      "Iteration 44, loss = 0.52862305\n",
      "Iteration 45, loss = 0.52760477\n",
      "Iteration 46, loss = 0.52651080\n",
      "Iteration 47, loss = 0.52567397\n",
      "Iteration 48, loss = 0.52475478\n",
      "Iteration 49, loss = 0.52411918\n",
      "Iteration 50, loss = 0.52311429\n",
      "Iteration 51, loss = 0.52256013\n",
      "Iteration 52, loss = 0.52173210\n",
      "Iteration 53, loss = 0.52104883\n",
      "Iteration 54, loss = 0.52061141\n",
      "Iteration 55, loss = 0.51972079\n",
      "Iteration 56, loss = 0.51902908\n",
      "Iteration 57, loss = 0.51859279\n",
      "Iteration 58, loss = 0.51794082\n",
      "Iteration 59, loss = 0.51741997\n",
      "Iteration 60, loss = 0.51690722\n",
      "Iteration 61, loss = 0.51644965\n",
      "Iteration 62, loss = 0.51597347\n",
      "Iteration 63, loss = 0.51558421\n",
      "Iteration 64, loss = 0.51493138\n",
      "Iteration 65, loss = 0.51464614\n",
      "Iteration 66, loss = 0.51427891\n",
      "Iteration 67, loss = 0.51366477\n",
      "Iteration 68, loss = 0.51339508\n",
      "Iteration 69, loss = 0.51295750\n",
      "Iteration 70, loss = 0.51250416\n",
      "Iteration 71, loss = 0.51196310\n",
      "Iteration 72, loss = 0.51199518\n",
      "Iteration 73, loss = 0.51158589\n",
      "Iteration 74, loss = 0.51209050\n",
      "Iteration 75, loss = 0.51084214\n",
      "Iteration 76, loss = 0.51054742\n",
      "Iteration 77, loss = 0.51012372\n",
      "Iteration 78, loss = 0.50975140\n",
      "Iteration 79, loss = 0.50959707\n",
      "Iteration 80, loss = 0.50940038\n",
      "Iteration 81, loss = 0.50910226\n",
      "Iteration 82, loss = 0.50876981\n",
      "Iteration 83, loss = 0.50868580\n",
      "Iteration 84, loss = 0.50876214\n",
      "Iteration 85, loss = 0.50826662\n",
      "Iteration 86, loss = 0.50798148\n",
      "Iteration 87, loss = 0.50778888\n",
      "Iteration 88, loss = 0.50762267\n",
      "Iteration 89, loss = 0.50754195\n",
      "Iteration 90, loss = 0.50749631\n",
      "Iteration 91, loss = 0.50685168\n",
      "Iteration 92, loss = 0.50670721\n",
      "Iteration 93, loss = 0.50671555\n",
      "Iteration 94, loss = 0.50673451\n",
      "Iteration 95, loss = 0.50621773\n",
      "Iteration 96, loss = 0.50592122\n",
      "Iteration 97, loss = 0.50572222\n",
      "Iteration 98, loss = 0.50550759\n",
      "Iteration 99, loss = 0.50537559\n",
      "Iteration 100, loss = 0.50527210\n",
      "Iteration 101, loss = 0.50497438\n",
      "Iteration 102, loss = 0.50512304\n",
      "Iteration 103, loss = 0.50445358\n",
      "Iteration 104, loss = 0.50433137\n",
      "Iteration 105, loss = 0.50410195\n",
      "Iteration 106, loss = 0.50406302\n",
      "Iteration 107, loss = 0.50385311\n",
      "Iteration 108, loss = 0.50409116\n",
      "Iteration 109, loss = 0.50365137\n",
      "Iteration 110, loss = 0.50328659\n",
      "Iteration 111, loss = 0.50347023\n",
      "Iteration 112, loss = 0.50345963\n",
      "Iteration 113, loss = 0.50294958\n",
      "Iteration 114, loss = 0.50286283\n",
      "Iteration 115, loss = 0.50265465\n",
      "Iteration 116, loss = 0.50253984\n",
      "Iteration 117, loss = 0.50249131\n",
      "Iteration 118, loss = 0.50204410\n",
      "Iteration 119, loss = 0.50230402\n",
      "Iteration 120, loss = 0.50222707\n",
      "Iteration 121, loss = 0.50233468\n",
      "Iteration 122, loss = 0.50165566\n",
      "Iteration 123, loss = 0.50185890\n",
      "Iteration 124, loss = 0.50151757\n",
      "Iteration 125, loss = 0.50166109\n",
      "Iteration 126, loss = 0.50113979\n",
      "Iteration 127, loss = 0.50097384\n",
      "Iteration 128, loss = 0.50102057\n",
      "Iteration 129, loss = 0.50101844\n",
      "Iteration 130, loss = 0.50075364\n",
      "Iteration 131, loss = 0.50063684\n",
      "Iteration 132, loss = 0.50037896\n",
      "Iteration 133, loss = 0.50034936\n",
      "Iteration 134, loss = 0.50040815\n",
      "Iteration 135, loss = 0.50063352\n",
      "Iteration 136, loss = 0.49974776\n",
      "Iteration 137, loss = 0.50011322\n",
      "Iteration 138, loss = 0.49980031\n",
      "Iteration 139, loss = 0.49954750\n",
      "Iteration 140, loss = 0.49955436\n",
      "Iteration 141, loss = 0.49956578\n",
      "Iteration 142, loss = 0.49904614\n",
      "Iteration 143, loss = 0.49910058\n",
      "Iteration 144, loss = 0.49880188\n",
      "Iteration 145, loss = 0.49889131\n",
      "Iteration 146, loss = 0.49875120\n",
      "Iteration 147, loss = 0.49876052\n",
      "Iteration 148, loss = 0.49844945\n",
      "Iteration 149, loss = 0.49837857\n",
      "Iteration 150, loss = 0.49836473\n",
      "Iteration 151, loss = 0.49841864\n",
      "Iteration 152, loss = 0.49818659\n",
      "Iteration 153, loss = 0.49783541\n",
      "Iteration 154, loss = 0.49779539\n",
      "Iteration 155, loss = 0.49771407\n",
      "Iteration 156, loss = 0.49750257\n",
      "Iteration 157, loss = 0.49755405\n",
      "Iteration 158, loss = 0.49725891\n",
      "Iteration 159, loss = 0.49783720\n",
      "Iteration 160, loss = 0.49727918\n",
      "Iteration 161, loss = 0.49714014\n",
      "Iteration 162, loss = 0.49676146\n",
      "Iteration 163, loss = 0.49679774\n",
      "Iteration 164, loss = 0.49673854\n",
      "Iteration 165, loss = 0.49685307\n",
      "Iteration 166, loss = 0.49655918\n",
      "Iteration 167, loss = 0.49635378\n",
      "Iteration 168, loss = 0.49658493\n",
      "Iteration 169, loss = 0.49630222\n",
      "Iteration 170, loss = 0.49611890\n",
      "Iteration 171, loss = 0.49608115\n",
      "Iteration 172, loss = 0.49590820\n",
      "Iteration 173, loss = 0.49613506\n",
      "Iteration 174, loss = 0.49596463\n",
      "Iteration 175, loss = 0.49550765\n",
      "Iteration 176, loss = 0.49540512\n",
      "Iteration 177, loss = 0.49530388\n",
      "Iteration 178, loss = 0.49533799\n",
      "Iteration 179, loss = 0.49503199\n",
      "Iteration 180, loss = 0.49504637\n",
      "Iteration 181, loss = 0.49484204\n",
      "Iteration 182, loss = 0.49493212\n",
      "Iteration 183, loss = 0.49506251\n",
      "Iteration 184, loss = 0.49491620\n",
      "Iteration 185, loss = 0.49456755\n",
      "Iteration 186, loss = 0.49443042\n",
      "Iteration 187, loss = 0.49424073\n",
      "Iteration 188, loss = 0.49443031\n",
      "Iteration 189, loss = 0.49423919\n",
      "Iteration 190, loss = 0.49413647\n",
      "Iteration 191, loss = 0.49413507\n",
      "Iteration 192, loss = 0.49378161\n",
      "Iteration 193, loss = 0.49392355\n",
      "Iteration 194, loss = 0.49359998\n",
      "Iteration 195, loss = 0.49371693\n",
      "Iteration 196, loss = 0.49359983\n",
      "Iteration 197, loss = 0.49350984\n",
      "Iteration 198, loss = 0.49356478\n",
      "Iteration 199, loss = 0.49350660\n",
      "Iteration 200, loss = 0.49360966\n",
      "Iteration 201, loss = 0.49321958\n",
      "Iteration 202, loss = 0.49332645\n",
      "Iteration 203, loss = 0.49318158\n",
      "Iteration 204, loss = 0.49298366\n",
      "Iteration 205, loss = 0.49284429\n",
      "Iteration 206, loss = 0.49282630\n",
      "Iteration 207, loss = 0.49272208\n",
      "Iteration 208, loss = 0.49269081\n",
      "Iteration 209, loss = 0.49274082\n",
      "Iteration 210, loss = 0.49237031\n",
      "Iteration 211, loss = 0.49282065\n",
      "Iteration 212, loss = 0.49258684\n",
      "Iteration 213, loss = 0.49231136\n",
      "Iteration 214, loss = 0.49230740\n",
      "Iteration 215, loss = 0.49213792\n",
      "Iteration 216, loss = 0.49201027\n",
      "Iteration 217, loss = 0.49214400\n",
      "Iteration 218, loss = 0.49198598\n",
      "Iteration 219, loss = 0.49200395\n",
      "Iteration 220, loss = 0.49190077\n",
      "Iteration 221, loss = 0.49194566\n",
      "Iteration 222, loss = 0.49255154\n",
      "Iteration 223, loss = 0.49217705\n",
      "Iteration 224, loss = 0.49183610\n",
      "Iteration 225, loss = 0.49183377\n",
      "Iteration 226, loss = 0.49163118\n",
      "Iteration 227, loss = 0.49152523\n",
      "Iteration 228, loss = 0.49139862\n",
      "Iteration 229, loss = 0.49156913\n",
      "Iteration 230, loss = 0.49154869\n",
      "Iteration 231, loss = 0.49134766\n",
      "Iteration 232, loss = 0.49142564\n",
      "Iteration 233, loss = 0.49123987\n",
      "Iteration 234, loss = 0.49113348\n",
      "Iteration 235, loss = 0.49091361\n",
      "Iteration 236, loss = 0.49091228\n",
      "Iteration 237, loss = 0.49086760\n",
      "Iteration 238, loss = 0.49078002\n",
      "Iteration 239, loss = 0.49107748\n",
      "Iteration 240, loss = 0.49068103\n",
      "Iteration 241, loss = 0.49069274\n",
      "Iteration 242, loss = 0.49063049\n",
      "Iteration 243, loss = 0.49049688\n",
      "Iteration 244, loss = 0.49072744\n",
      "Iteration 245, loss = 0.49062811\n",
      "Iteration 246, loss = 0.49041816\n",
      "Iteration 247, loss = 0.49048008\n",
      "Iteration 248, loss = 0.49047781\n",
      "Iteration 249, loss = 0.49035976\n",
      "Iteration 250, loss = 0.49016294\n",
      "Iteration 251, loss = 0.49018922\n",
      "Iteration 252, loss = 0.49044307\n",
      "Iteration 253, loss = 0.49030028\n",
      "Iteration 254, loss = 0.49032328\n",
      "Iteration 255, loss = 0.49023597\n",
      "Iteration 256, loss = 0.49018231\n",
      "Iteration 257, loss = 0.49034158\n",
      "Iteration 258, loss = 0.48976944\n",
      "Iteration 259, loss = 0.49004230\n",
      "Iteration 260, loss = 0.48976290\n",
      "Iteration 261, loss = 0.48975736\n",
      "Iteration 262, loss = 0.48949859\n",
      "Iteration 263, loss = 0.48993961\n",
      "Iteration 264, loss = 0.48969556\n",
      "Iteration 265, loss = 0.48968817\n",
      "Iteration 266, loss = 0.48943257\n",
      "Iteration 267, loss = 0.48938740\n",
      "Iteration 268, loss = 0.48949813\n",
      "Iteration 269, loss = 0.48952417\n",
      "Iteration 270, loss = 0.48939678\n",
      "Iteration 271, loss = 0.48910770\n",
      "Iteration 272, loss = 0.48906277\n",
      "Iteration 273, loss = 0.48910522\n",
      "Iteration 274, loss = 0.48927303\n",
      "Iteration 275, loss = 0.48888109\n",
      "Iteration 276, loss = 0.48915390\n",
      "Iteration 277, loss = 0.48921567\n",
      "Iteration 278, loss = 0.48906757\n",
      "Iteration 279, loss = 0.48914020\n",
      "Iteration 280, loss = 0.48874782\n",
      "Iteration 281, loss = 0.48888812\n",
      "Iteration 282, loss = 0.48868060\n",
      "Iteration 283, loss = 0.48898335\n",
      "Iteration 284, loss = 0.48891780\n",
      "Iteration 285, loss = 0.48856177\n",
      "Iteration 286, loss = 0.48855257\n",
      "Iteration 287, loss = 0.48825791\n",
      "Iteration 288, loss = 0.48830572\n",
      "Iteration 289, loss = 0.48815252\n",
      "Iteration 290, loss = 0.48850831\n",
      "Iteration 291, loss = 0.48833472\n",
      "Iteration 292, loss = 0.48818034\n",
      "Iteration 293, loss = 0.48809951\n",
      "Iteration 294, loss = 0.48800966\n",
      "Iteration 295, loss = 0.48809728\n",
      "Iteration 296, loss = 0.48807428\n",
      "Iteration 297, loss = 0.48782485\n",
      "Iteration 298, loss = 0.48796891\n",
      "Iteration 299, loss = 0.48810216\n",
      "Iteration 300, loss = 0.48762810\n",
      "Iteration 301, loss = 0.48782149\n",
      "Iteration 302, loss = 0.48768482\n",
      "Iteration 303, loss = 0.48767406\n",
      "Iteration 304, loss = 0.48744901\n",
      "Iteration 305, loss = 0.48786456\n",
      "Iteration 306, loss = 0.48738884\n",
      "Iteration 307, loss = 0.48747526\n",
      "Iteration 308, loss = 0.48734656\n",
      "Iteration 309, loss = 0.48763638\n",
      "Iteration 310, loss = 0.48747130\n",
      "Iteration 311, loss = 0.48751229\n",
      "Iteration 312, loss = 0.48729348\n",
      "Iteration 313, loss = 0.48730421\n",
      "Iteration 314, loss = 0.48738895\n",
      "Iteration 315, loss = 0.48745312\n",
      "Iteration 316, loss = 0.48759898\n",
      "Iteration 317, loss = 0.48703792\n",
      "Iteration 318, loss = 0.48687100\n",
      "Iteration 319, loss = 0.48761612\n",
      "Iteration 320, loss = 0.48725211\n",
      "Iteration 321, loss = 0.48712920\n",
      "Iteration 322, loss = 0.48728515\n",
      "Iteration 323, loss = 0.48688536\n",
      "Iteration 324, loss = 0.48675593\n",
      "Iteration 325, loss = 0.48675667\n",
      "Iteration 326, loss = 0.48695631\n",
      "Iteration 327, loss = 0.48667648\n",
      "Iteration 328, loss = 0.48681744\n",
      "Iteration 329, loss = 0.48672618\n",
      "Iteration 330, loss = 0.48698053\n",
      "Iteration 331, loss = 0.48662716\n",
      "Iteration 332, loss = 0.48656708\n",
      "Iteration 333, loss = 0.48651504\n",
      "Iteration 334, loss = 0.48635289\n",
      "Iteration 335, loss = 0.48633401\n",
      "Iteration 336, loss = 0.48641272\n",
      "Iteration 337, loss = 0.48643751\n",
      "Iteration 338, loss = 0.48658632\n",
      "Iteration 339, loss = 0.48626002\n",
      "Iteration 340, loss = 0.48628619\n",
      "Iteration 341, loss = 0.48627540\n",
      "Iteration 342, loss = 0.48628805\n",
      "Iteration 343, loss = 0.48640492\n",
      "Iteration 344, loss = 0.48615052\n",
      "Iteration 345, loss = 0.48610160\n",
      "Iteration 346, loss = 0.48590046\n",
      "Iteration 347, loss = 0.48586902\n",
      "Iteration 348, loss = 0.48590965\n",
      "Iteration 349, loss = 0.48578649\n",
      "Iteration 350, loss = 0.48577878\n",
      "Iteration 351, loss = 0.48598428\n",
      "Iteration 352, loss = 0.48572662\n",
      "Iteration 353, loss = 0.48612836\n",
      "Iteration 354, loss = 0.48572631\n",
      "Iteration 355, loss = 0.48549320\n",
      "Iteration 356, loss = 0.48566241\n",
      "Iteration 357, loss = 0.48560031\n",
      "Iteration 358, loss = 0.48556445\n",
      "Iteration 359, loss = 0.48557561\n",
      "Iteration 360, loss = 0.48587426\n",
      "Iteration 361, loss = 0.48545514\n",
      "Iteration 362, loss = 0.48539354\n",
      "Iteration 363, loss = 0.48535108\n",
      "Iteration 364, loss = 0.48542206\n",
      "Iteration 365, loss = 0.48552506\n",
      "Iteration 366, loss = 0.48567876\n",
      "Iteration 367, loss = 0.48531958\n",
      "Iteration 368, loss = 0.48533908\n",
      "Iteration 369, loss = 0.48538018\n",
      "Iteration 370, loss = 0.48501333\n",
      "Iteration 371, loss = 0.48507950\n",
      "Iteration 372, loss = 0.48556662\n",
      "Iteration 373, loss = 0.48553093\n",
      "Iteration 374, loss = 0.48498207\n",
      "Iteration 375, loss = 0.48510938\n",
      "Iteration 376, loss = 0.48502326\n",
      "Iteration 377, loss = 0.48486990\n",
      "Iteration 378, loss = 0.48492491\n",
      "Iteration 379, loss = 0.48483737\n",
      "Iteration 380, loss = 0.48562975\n",
      "Iteration 381, loss = 0.48551598\n",
      "Iteration 382, loss = 0.48470348\n",
      "Iteration 383, loss = 0.48458619\n",
      "Iteration 384, loss = 0.48453042\n",
      "Iteration 385, loss = 0.48492383\n",
      "Iteration 386, loss = 0.48464784\n",
      "Iteration 387, loss = 0.48466431\n",
      "Iteration 388, loss = 0.48470654\n",
      "Iteration 389, loss = 0.48466684\n",
      "Iteration 390, loss = 0.48482981\n",
      "Iteration 391, loss = 0.48429155\n",
      "Iteration 392, loss = 0.48461113\n",
      "Iteration 393, loss = 0.48467623\n",
      "Iteration 394, loss = 0.48461279\n",
      "Iteration 395, loss = 0.48443513\n",
      "Iteration 396, loss = 0.48427246\n",
      "Iteration 397, loss = 0.48456523\n",
      "Iteration 398, loss = 0.48443420\n",
      "Iteration 399, loss = 0.48440666\n",
      "Iteration 400, loss = 0.48436843\n",
      "Iteration 401, loss = 0.48424883\n",
      "Iteration 402, loss = 0.48404402\n",
      "Iteration 403, loss = 0.48454076\n",
      "Iteration 404, loss = 0.48435834\n",
      "Iteration 405, loss = 0.48385175\n",
      "Iteration 406, loss = 0.48392754\n",
      "Iteration 407, loss = 0.48407190\n",
      "Iteration 408, loss = 0.48384280\n",
      "Iteration 409, loss = 0.48380155\n",
      "Iteration 410, loss = 0.48383988\n",
      "Iteration 411, loss = 0.48376615\n",
      "Iteration 412, loss = 0.48410791\n",
      "Iteration 413, loss = 0.48395357\n",
      "Iteration 414, loss = 0.48379468\n",
      "Iteration 415, loss = 0.48359709\n",
      "Iteration 416, loss = 0.48359658\n",
      "Iteration 417, loss = 0.48379851\n",
      "Iteration 418, loss = 0.48386684\n",
      "Iteration 419, loss = 0.48419628\n",
      "Iteration 420, loss = 0.48383793\n",
      "Iteration 421, loss = 0.48373167\n",
      "Iteration 422, loss = 0.48321227\n",
      "Iteration 423, loss = 0.48350210\n",
      "Iteration 424, loss = 0.48331195\n",
      "Iteration 425, loss = 0.48362756\n",
      "Iteration 426, loss = 0.48343877\n",
      "Iteration 427, loss = 0.48350925\n",
      "Iteration 428, loss = 0.48340759\n",
      "Iteration 429, loss = 0.48302310\n",
      "Iteration 430, loss = 0.48351561\n",
      "Iteration 431, loss = 0.48313200\n",
      "Iteration 432, loss = 0.48333178\n",
      "Iteration 433, loss = 0.48346329\n",
      "Iteration 434, loss = 0.48318886\n",
      "Iteration 435, loss = 0.48319847\n",
      "Iteration 436, loss = 0.48296001\n",
      "Iteration 437, loss = 0.48294390\n",
      "Iteration 438, loss = 0.48298536\n",
      "Iteration 439, loss = 0.48280712\n",
      "Iteration 440, loss = 0.48312049\n",
      "Iteration 441, loss = 0.48348004\n",
      "Iteration 442, loss = 0.48357391\n",
      "Iteration 443, loss = 0.48302476\n",
      "Iteration 444, loss = 0.48261262\n",
      "Iteration 445, loss = 0.48297773\n",
      "Iteration 446, loss = 0.48249317\n",
      "Iteration 447, loss = 0.48266060\n",
      "Iteration 448, loss = 0.48287295\n",
      "Iteration 449, loss = 0.48271261\n",
      "Iteration 450, loss = 0.48286832\n",
      "Iteration 451, loss = 0.48246340\n",
      "Iteration 452, loss = 0.48253134\n",
      "Iteration 453, loss = 0.48283102\n",
      "Iteration 454, loss = 0.48280982\n",
      "Iteration 455, loss = 0.48252448\n",
      "Iteration 456, loss = 0.48240128\n",
      "Iteration 457, loss = 0.48277576\n",
      "Iteration 458, loss = 0.48248099\n",
      "Iteration 459, loss = 0.48241291\n",
      "Iteration 460, loss = 0.48260546\n",
      "Iteration 461, loss = 0.48270763\n",
      "Iteration 462, loss = 0.48249305\n",
      "Iteration 463, loss = 0.48258151\n",
      "Iteration 464, loss = 0.48258966\n",
      "Iteration 465, loss = 0.48256934\n",
      "Iteration 466, loss = 0.48255557\n",
      "Iteration 467, loss = 0.48247396\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72735353\n",
      "Iteration 2, loss = 0.68744850\n",
      "Iteration 3, loss = 0.66592372\n",
      "Iteration 4, loss = 0.65182601\n",
      "Iteration 5, loss = 0.64048474\n",
      "Iteration 6, loss = 0.62935329\n",
      "Iteration 7, loss = 0.61959613\n",
      "Iteration 8, loss = 0.61005062\n",
      "Iteration 9, loss = 0.60121091\n",
      "Iteration 10, loss = 0.59351415\n",
      "Iteration 11, loss = 0.58685578\n",
      "Iteration 12, loss = 0.58094593\n",
      "Iteration 13, loss = 0.57624200\n",
      "Iteration 14, loss = 0.57244628\n",
      "Iteration 15, loss = 0.56938091\n",
      "Iteration 16, loss = 0.56699862\n",
      "Iteration 17, loss = 0.56440747\n",
      "Iteration 18, loss = 0.56222553\n",
      "Iteration 19, loss = 0.56028393\n",
      "Iteration 20, loss = 0.55848970\n",
      "Iteration 21, loss = 0.55718054\n",
      "Iteration 22, loss = 0.55539361\n",
      "Iteration 23, loss = 0.55409593\n",
      "Iteration 24, loss = 0.55265836\n",
      "Iteration 25, loss = 0.55141259\n",
      "Iteration 26, loss = 0.54993472\n",
      "Iteration 27, loss = 0.54861362\n",
      "Iteration 28, loss = 0.54748468\n",
      "Iteration 29, loss = 0.54608355\n",
      "Iteration 30, loss = 0.54500239\n",
      "Iteration 31, loss = 0.54391968\n",
      "Iteration 32, loss = 0.54249772\n",
      "Iteration 33, loss = 0.54138282\n",
      "Iteration 34, loss = 0.54011433\n",
      "Iteration 35, loss = 0.53893909\n",
      "Iteration 36, loss = 0.53782390\n",
      "Iteration 37, loss = 0.53675298\n",
      "Iteration 38, loss = 0.53555077\n",
      "Iteration 39, loss = 0.53446517\n",
      "Iteration 40, loss = 0.53351611\n",
      "Iteration 41, loss = 0.53237464\n",
      "Iteration 42, loss = 0.53127156\n",
      "Iteration 43, loss = 0.52994234\n",
      "Iteration 44, loss = 0.52896622\n",
      "Iteration 45, loss = 0.52769819\n",
      "Iteration 46, loss = 0.52673441\n",
      "Iteration 47, loss = 0.52580083\n",
      "Iteration 48, loss = 0.52478184\n",
      "Iteration 49, loss = 0.52369610\n",
      "Iteration 50, loss = 0.52233708\n",
      "Iteration 51, loss = 0.52125499\n",
      "Iteration 52, loss = 0.52041786\n",
      "Iteration 53, loss = 0.51919394\n",
      "Iteration 54, loss = 0.51824765\n",
      "Iteration 55, loss = 0.51746545\n",
      "Iteration 56, loss = 0.51709990\n",
      "Iteration 57, loss = 0.51582034\n",
      "Iteration 58, loss = 0.51455188\n",
      "Iteration 59, loss = 0.51350233\n",
      "Iteration 60, loss = 0.51285281\n",
      "Iteration 61, loss = 0.51203933\n",
      "Iteration 62, loss = 0.51103724\n",
      "Iteration 63, loss = 0.51028360\n",
      "Iteration 64, loss = 0.50964187\n",
      "Iteration 65, loss = 0.50911978\n",
      "Iteration 66, loss = 0.50825979\n",
      "Iteration 67, loss = 0.50778620\n",
      "Iteration 68, loss = 0.50705441\n",
      "Iteration 69, loss = 0.50675191\n",
      "Iteration 70, loss = 0.50604985\n",
      "Iteration 71, loss = 0.50576859\n",
      "Iteration 72, loss = 0.50496733\n",
      "Iteration 73, loss = 0.50456178\n",
      "Iteration 74, loss = 0.50410031\n",
      "Iteration 75, loss = 0.50355270\n",
      "Iteration 76, loss = 0.50280573\n",
      "Iteration 77, loss = 0.50257193\n",
      "Iteration 78, loss = 0.50240535\n",
      "Iteration 79, loss = 0.50155134\n",
      "Iteration 80, loss = 0.50133600\n",
      "Iteration 81, loss = 0.50134809\n",
      "Iteration 82, loss = 0.50062724\n",
      "Iteration 83, loss = 0.50026465\n",
      "Iteration 84, loss = 0.50001813\n",
      "Iteration 85, loss = 0.49972321\n",
      "Iteration 86, loss = 0.49960586\n",
      "Iteration 87, loss = 0.49900629\n",
      "Iteration 88, loss = 0.49888812\n",
      "Iteration 89, loss = 0.49891744\n",
      "Iteration 90, loss = 0.49847298\n",
      "Iteration 91, loss = 0.49834676\n",
      "Iteration 92, loss = 0.49817958\n",
      "Iteration 93, loss = 0.49843929\n",
      "Iteration 94, loss = 0.49772002\n",
      "Iteration 95, loss = 0.49732783\n",
      "Iteration 96, loss = 0.49778866\n",
      "Iteration 97, loss = 0.49710501\n",
      "Iteration 98, loss = 0.49671605\n",
      "Iteration 99, loss = 0.49681425\n",
      "Iteration 100, loss = 0.49657857\n",
      "Iteration 101, loss = 0.49629463\n",
      "Iteration 102, loss = 0.49698879\n",
      "Iteration 103, loss = 0.49567290\n",
      "Iteration 104, loss = 0.49557513\n",
      "Iteration 105, loss = 0.49593287\n",
      "Iteration 106, loss = 0.49540395\n",
      "Iteration 107, loss = 0.49537518\n",
      "Iteration 108, loss = 0.49486764\n",
      "Iteration 109, loss = 0.49473555\n",
      "Iteration 110, loss = 0.49494986\n",
      "Iteration 111, loss = 0.49464911\n",
      "Iteration 112, loss = 0.49488962\n",
      "Iteration 113, loss = 0.49425793\n",
      "Iteration 114, loss = 0.49416696\n",
      "Iteration 115, loss = 0.49404825\n",
      "Iteration 116, loss = 0.49400110\n",
      "Iteration 117, loss = 0.49363418\n",
      "Iteration 118, loss = 0.49395222\n",
      "Iteration 119, loss = 0.49364910\n",
      "Iteration 120, loss = 0.49336853\n",
      "Iteration 121, loss = 0.49350607\n",
      "Iteration 122, loss = 0.49352093\n",
      "Iteration 123, loss = 0.49326119\n",
      "Iteration 124, loss = 0.49303418\n",
      "Iteration 125, loss = 0.49285451\n",
      "Iteration 126, loss = 0.49311379\n",
      "Iteration 127, loss = 0.49260133\n",
      "Iteration 128, loss = 0.49276401\n",
      "Iteration 129, loss = 0.49251297\n",
      "Iteration 130, loss = 0.49231235\n",
      "Iteration 131, loss = 0.49205188\n",
      "Iteration 132, loss = 0.49198437\n",
      "Iteration 133, loss = 0.49180873\n",
      "Iteration 134, loss = 0.49190457\n",
      "Iteration 135, loss = 0.49164449\n",
      "Iteration 136, loss = 0.49166857\n",
      "Iteration 137, loss = 0.49133276\n",
      "Iteration 138, loss = 0.49108993\n",
      "Iteration 139, loss = 0.49146730\n",
      "Iteration 140, loss = 0.49156895\n",
      "Iteration 141, loss = 0.49103121\n",
      "Iteration 142, loss = 0.49079464\n",
      "Iteration 143, loss = 0.49074068\n",
      "Iteration 144, loss = 0.49064494\n",
      "Iteration 145, loss = 0.49062294\n",
      "Iteration 146, loss = 0.49057625\n",
      "Iteration 147, loss = 0.48997442\n",
      "Iteration 148, loss = 0.49036509\n",
      "Iteration 149, loss = 0.49003961\n",
      "Iteration 150, loss = 0.49019461\n",
      "Iteration 151, loss = 0.48984487\n",
      "Iteration 152, loss = 0.49012647\n",
      "Iteration 153, loss = 0.48998299\n",
      "Iteration 154, loss = 0.49029254\n",
      "Iteration 155, loss = 0.48985385\n",
      "Iteration 156, loss = 0.48970973\n",
      "Iteration 157, loss = 0.48961646\n",
      "Iteration 158, loss = 0.48944952\n",
      "Iteration 159, loss = 0.48922471\n",
      "Iteration 160, loss = 0.48898494\n",
      "Iteration 161, loss = 0.48925263\n",
      "Iteration 162, loss = 0.48874048\n",
      "Iteration 163, loss = 0.48872308\n",
      "Iteration 164, loss = 0.48893858\n",
      "Iteration 165, loss = 0.48905888\n",
      "Iteration 166, loss = 0.48857077\n",
      "Iteration 167, loss = 0.48876423\n",
      "Iteration 168, loss = 0.48817176\n",
      "Iteration 169, loss = 0.48821726\n",
      "Iteration 170, loss = 0.48796818\n",
      "Iteration 171, loss = 0.48807771\n",
      "Iteration 172, loss = 0.48803445\n",
      "Iteration 173, loss = 0.48803950\n",
      "Iteration 174, loss = 0.48775004\n",
      "Iteration 175, loss = 0.48791641\n",
      "Iteration 176, loss = 0.48764496\n",
      "Iteration 177, loss = 0.48735114\n",
      "Iteration 178, loss = 0.48760830\n",
      "Iteration 179, loss = 0.48751213\n",
      "Iteration 180, loss = 0.48823234\n",
      "Iteration 181, loss = 0.48708977\n",
      "Iteration 182, loss = 0.48703540\n",
      "Iteration 183, loss = 0.48686020\n",
      "Iteration 184, loss = 0.48689643\n",
      "Iteration 185, loss = 0.48689086\n",
      "Iteration 186, loss = 0.48698419\n",
      "Iteration 187, loss = 0.48679119\n",
      "Iteration 188, loss = 0.48660924\n",
      "Iteration 189, loss = 0.48660083\n",
      "Iteration 190, loss = 0.48652399\n",
      "Iteration 191, loss = 0.48623423\n",
      "Iteration 192, loss = 0.48602053\n",
      "Iteration 193, loss = 0.48658677\n",
      "Iteration 194, loss = 0.48640647\n",
      "Iteration 195, loss = 0.48580911\n",
      "Iteration 196, loss = 0.48565781\n",
      "Iteration 197, loss = 0.48540975\n",
      "Iteration 198, loss = 0.48552472\n",
      "Iteration 199, loss = 0.48536823\n",
      "Iteration 200, loss = 0.48510571\n",
      "Iteration 201, loss = 0.48512162\n",
      "Iteration 202, loss = 0.48493292\n",
      "Iteration 203, loss = 0.48497421\n",
      "Iteration 204, loss = 0.48502026\n",
      "Iteration 205, loss = 0.48483095\n",
      "Iteration 206, loss = 0.48468925\n",
      "Iteration 207, loss = 0.48508683\n",
      "Iteration 208, loss = 0.48460218\n",
      "Iteration 209, loss = 0.48456016\n",
      "Iteration 210, loss = 0.48431999\n",
      "Iteration 211, loss = 0.48432152\n",
      "Iteration 212, loss = 0.48408119\n",
      "Iteration 213, loss = 0.48408001\n",
      "Iteration 214, loss = 0.48410147\n",
      "Iteration 215, loss = 0.48373726\n",
      "Iteration 216, loss = 0.48383655\n",
      "Iteration 217, loss = 0.48417351\n",
      "Iteration 218, loss = 0.48403148\n",
      "Iteration 219, loss = 0.48360183\n",
      "Iteration 220, loss = 0.48336161\n",
      "Iteration 221, loss = 0.48326394\n",
      "Iteration 222, loss = 0.48338569\n",
      "Iteration 223, loss = 0.48337704\n",
      "Iteration 224, loss = 0.48306258\n",
      "Iteration 225, loss = 0.48294203\n",
      "Iteration 226, loss = 0.48326401\n",
      "Iteration 227, loss = 0.48313171\n",
      "Iteration 228, loss = 0.48283012\n",
      "Iteration 229, loss = 0.48306792\n",
      "Iteration 230, loss = 0.48270394\n",
      "Iteration 231, loss = 0.48245479\n",
      "Iteration 232, loss = 0.48270002\n",
      "Iteration 233, loss = 0.48279880\n",
      "Iteration 234, loss = 0.48235991\n",
      "Iteration 235, loss = 0.48214452\n",
      "Iteration 236, loss = 0.48297559\n",
      "Iteration 237, loss = 0.48198456\n",
      "Iteration 238, loss = 0.48204813\n",
      "Iteration 239, loss = 0.48197126\n",
      "Iteration 240, loss = 0.48190865\n",
      "Iteration 241, loss = 0.48203616\n",
      "Iteration 242, loss = 0.48220808\n",
      "Iteration 243, loss = 0.48252646\n",
      "Iteration 244, loss = 0.48159103\n",
      "Iteration 245, loss = 0.48165938\n",
      "Iteration 246, loss = 0.48173436\n",
      "Iteration 247, loss = 0.48170439\n",
      "Iteration 248, loss = 0.48150369\n",
      "Iteration 249, loss = 0.48126129\n",
      "Iteration 250, loss = 0.48133478\n",
      "Iteration 251, loss = 0.48118014\n",
      "Iteration 252, loss = 0.48125596\n",
      "Iteration 253, loss = 0.48103473\n",
      "Iteration 254, loss = 0.48120158\n",
      "Iteration 255, loss = 0.48175290\n",
      "Iteration 256, loss = 0.48163775\n",
      "Iteration 257, loss = 0.48096310\n",
      "Iteration 258, loss = 0.48132676\n",
      "Iteration 259, loss = 0.48063626\n",
      "Iteration 260, loss = 0.48094158\n",
      "Iteration 261, loss = 0.48070285\n",
      "Iteration 262, loss = 0.48074984\n",
      "Iteration 263, loss = 0.48072611\n",
      "Iteration 264, loss = 0.48055895\n",
      "Iteration 265, loss = 0.48062747\n",
      "Iteration 266, loss = 0.48066519\n",
      "Iteration 267, loss = 0.48019528\n",
      "Iteration 268, loss = 0.48046223\n",
      "Iteration 269, loss = 0.48009209\n",
      "Iteration 270, loss = 0.48049501\n",
      "Iteration 271, loss = 0.48037461\n",
      "Iteration 272, loss = 0.48020478\n",
      "Iteration 273, loss = 0.48009576\n",
      "Iteration 274, loss = 0.48023796\n",
      "Iteration 275, loss = 0.48011865\n",
      "Iteration 276, loss = 0.48003596\n",
      "Iteration 277, loss = 0.47999111\n",
      "Iteration 278, loss = 0.47996510\n",
      "Iteration 279, loss = 0.48002615\n",
      "Iteration 280, loss = 0.47987985\n",
      "Iteration 281, loss = 0.47957421\n",
      "Iteration 282, loss = 0.47965164\n",
      "Iteration 283, loss = 0.47975788\n",
      "Iteration 284, loss = 0.47970461\n",
      "Iteration 285, loss = 0.47974968\n",
      "Iteration 286, loss = 0.47967820\n",
      "Iteration 287, loss = 0.48001483\n",
      "Iteration 288, loss = 0.47949459\n",
      "Iteration 289, loss = 0.47932634\n",
      "Iteration 290, loss = 0.47943813\n",
      "Iteration 291, loss = 0.47915669\n",
      "Iteration 292, loss = 0.47916269\n",
      "Iteration 293, loss = 0.47955788\n",
      "Iteration 294, loss = 0.47925020\n",
      "Iteration 295, loss = 0.47929750\n",
      "Iteration 296, loss = 0.47863880\n",
      "Iteration 297, loss = 0.47915683\n",
      "Iteration 298, loss = 0.47901155\n",
      "Iteration 299, loss = 0.47907258\n",
      "Iteration 300, loss = 0.47852514\n",
      "Iteration 301, loss = 0.47874367\n",
      "Iteration 302, loss = 0.47861070\n",
      "Iteration 303, loss = 0.47843801\n",
      "Iteration 304, loss = 0.47846256\n",
      "Iteration 305, loss = 0.47835436\n",
      "Iteration 306, loss = 0.47851638\n",
      "Iteration 307, loss = 0.47823757\n",
      "Iteration 308, loss = 0.47865813\n",
      "Iteration 309, loss = 0.47814810\n",
      "Iteration 310, loss = 0.47882065\n",
      "Iteration 311, loss = 0.47891023\n",
      "Iteration 312, loss = 0.47793700\n",
      "Iteration 313, loss = 0.47819891\n",
      "Iteration 314, loss = 0.47766950\n",
      "Iteration 315, loss = 0.47811635\n",
      "Iteration 316, loss = 0.47774877\n",
      "Iteration 317, loss = 0.47808001\n",
      "Iteration 318, loss = 0.47877255\n",
      "Iteration 319, loss = 0.47804816\n",
      "Iteration 320, loss = 0.47794010\n",
      "Iteration 321, loss = 0.47797697\n",
      "Iteration 322, loss = 0.47782741\n",
      "Iteration 323, loss = 0.47767935\n",
      "Iteration 324, loss = 0.47790140\n",
      "Iteration 325, loss = 0.47771395\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67980410\n",
      "Iteration 2, loss = 0.66649483\n",
      "Iteration 3, loss = 0.65346431\n",
      "Iteration 4, loss = 0.63985644\n",
      "Iteration 5, loss = 0.62676883\n",
      "Iteration 6, loss = 0.61437166\n",
      "Iteration 7, loss = 0.60287205\n",
      "Iteration 8, loss = 0.59310884\n",
      "Iteration 9, loss = 0.58484761\n",
      "Iteration 10, loss = 0.57785393\n",
      "Iteration 11, loss = 0.57255997\n",
      "Iteration 12, loss = 0.56781217\n",
      "Iteration 13, loss = 0.56418929\n",
      "Iteration 14, loss = 0.56107383\n",
      "Iteration 15, loss = 0.55833995\n",
      "Iteration 16, loss = 0.55588630\n",
      "Iteration 17, loss = 0.55364243\n",
      "Iteration 18, loss = 0.55143342\n",
      "Iteration 19, loss = 0.54945479\n",
      "Iteration 20, loss = 0.54731210\n",
      "Iteration 21, loss = 0.54546042\n",
      "Iteration 22, loss = 0.54327817\n",
      "Iteration 23, loss = 0.54143244\n",
      "Iteration 24, loss = 0.53921306\n",
      "Iteration 25, loss = 0.53707587\n",
      "Iteration 26, loss = 0.53506566\n",
      "Iteration 27, loss = 0.53297955\n",
      "Iteration 28, loss = 0.53078118\n",
      "Iteration 29, loss = 0.52902999\n",
      "Iteration 30, loss = 0.52723429\n",
      "Iteration 31, loss = 0.52500819\n",
      "Iteration 32, loss = 0.52353871\n",
      "Iteration 33, loss = 0.52172250\n",
      "Iteration 34, loss = 0.52009041\n",
      "Iteration 35, loss = 0.51857201\n",
      "Iteration 36, loss = 0.51714382\n",
      "Iteration 37, loss = 0.51598885\n",
      "Iteration 38, loss = 0.51430462\n",
      "Iteration 39, loss = 0.51304248\n",
      "Iteration 40, loss = 0.51187644\n",
      "Iteration 41, loss = 0.51072718\n",
      "Iteration 42, loss = 0.50963741\n",
      "Iteration 43, loss = 0.50881309\n",
      "Iteration 44, loss = 0.50793558\n",
      "Iteration 45, loss = 0.50716393\n",
      "Iteration 46, loss = 0.50636300\n",
      "Iteration 47, loss = 0.50560689\n",
      "Iteration 48, loss = 0.50493126\n",
      "Iteration 49, loss = 0.50423716\n",
      "Iteration 50, loss = 0.50346796\n",
      "Iteration 51, loss = 0.50285965\n",
      "Iteration 52, loss = 0.50236811\n",
      "Iteration 53, loss = 0.50177805\n",
      "Iteration 54, loss = 0.50176334\n",
      "Iteration 55, loss = 0.50067082\n",
      "Iteration 56, loss = 0.50042357\n",
      "Iteration 57, loss = 0.49986370\n",
      "Iteration 58, loss = 0.49956407\n",
      "Iteration 59, loss = 0.49928431\n",
      "Iteration 60, loss = 0.49870984\n",
      "Iteration 61, loss = 0.49858875\n",
      "Iteration 62, loss = 0.49831195\n",
      "Iteration 63, loss = 0.49793182\n",
      "Iteration 64, loss = 0.49782566\n",
      "Iteration 65, loss = 0.49735931\n",
      "Iteration 66, loss = 0.49750852\n",
      "Iteration 67, loss = 0.49669031\n",
      "Iteration 68, loss = 0.49661193\n",
      "Iteration 69, loss = 0.49655183\n",
      "Iteration 70, loss = 0.49647336\n",
      "Iteration 71, loss = 0.49588467\n",
      "Iteration 72, loss = 0.49566711\n",
      "Iteration 73, loss = 0.49545405\n",
      "Iteration 74, loss = 0.49560095\n",
      "Iteration 75, loss = 0.49503663\n",
      "Iteration 76, loss = 0.49525925\n",
      "Iteration 77, loss = 0.49484036\n",
      "Iteration 78, loss = 0.49453923\n",
      "Iteration 79, loss = 0.49425640\n",
      "Iteration 80, loss = 0.49417414\n",
      "Iteration 81, loss = 0.49413350\n",
      "Iteration 82, loss = 0.49374939\n",
      "Iteration 83, loss = 0.49364047\n",
      "Iteration 84, loss = 0.49364424\n",
      "Iteration 85, loss = 0.49340191\n",
      "Iteration 86, loss = 0.49346437\n",
      "Iteration 87, loss = 0.49320404\n",
      "Iteration 88, loss = 0.49291606\n",
      "Iteration 89, loss = 0.49313749\n",
      "Iteration 90, loss = 0.49297803\n",
      "Iteration 91, loss = 0.49269449\n",
      "Iteration 92, loss = 0.49259404\n",
      "Iteration 93, loss = 0.49255489\n",
      "Iteration 94, loss = 0.49254831\n",
      "Iteration 95, loss = 0.49234925\n",
      "Iteration 96, loss = 0.49236427\n",
      "Iteration 97, loss = 0.49206287\n",
      "Iteration 98, loss = 0.49195021\n",
      "Iteration 99, loss = 0.49189931\n",
      "Iteration 100, loss = 0.49177928\n",
      "Iteration 101, loss = 0.49165407\n",
      "Iteration 102, loss = 0.49180880\n",
      "Iteration 103, loss = 0.49161651\n",
      "Iteration 104, loss = 0.49142827\n",
      "Iteration 105, loss = 0.49143078\n",
      "Iteration 106, loss = 0.49130842\n",
      "Iteration 107, loss = 0.49142214\n",
      "Iteration 108, loss = 0.49126549\n",
      "Iteration 109, loss = 0.49123301\n",
      "Iteration 110, loss = 0.49127976\n",
      "Iteration 111, loss = 0.49126967\n",
      "Iteration 112, loss = 0.49076706\n",
      "Iteration 113, loss = 0.49102597\n",
      "Iteration 114, loss = 0.49082722\n",
      "Iteration 115, loss = 0.49068830\n",
      "Iteration 116, loss = 0.49060406\n",
      "Iteration 117, loss = 0.49048705\n",
      "Iteration 118, loss = 0.49075117\n",
      "Iteration 119, loss = 0.49048641\n",
      "Iteration 120, loss = 0.48980317\n",
      "Iteration 121, loss = 0.49000812\n",
      "Iteration 122, loss = 0.49016140\n",
      "Iteration 123, loss = 0.48970948\n",
      "Iteration 124, loss = 0.48987237\n",
      "Iteration 125, loss = 0.48952033\n",
      "Iteration 126, loss = 0.48964394\n",
      "Iteration 127, loss = 0.48952145\n",
      "Iteration 128, loss = 0.48920282\n",
      "Iteration 129, loss = 0.48944078\n",
      "Iteration 130, loss = 0.48931384\n",
      "Iteration 131, loss = 0.48919378\n",
      "Iteration 132, loss = 0.48899440\n",
      "Iteration 133, loss = 0.48895667\n",
      "Iteration 134, loss = 0.48874427\n",
      "Iteration 135, loss = 0.48864904\n",
      "Iteration 136, loss = 0.48868138\n",
      "Iteration 137, loss = 0.48850428\n",
      "Iteration 138, loss = 0.48856077\n",
      "Iteration 139, loss = 0.48838722\n",
      "Iteration 140, loss = 0.48824378\n",
      "Iteration 141, loss = 0.48843755\n",
      "Iteration 142, loss = 0.48851777\n",
      "Iteration 143, loss = 0.48819210\n",
      "Iteration 144, loss = 0.48816999\n",
      "Iteration 145, loss = 0.48803986\n",
      "Iteration 146, loss = 0.48801063\n",
      "Iteration 147, loss = 0.48788183\n",
      "Iteration 148, loss = 0.48776411\n",
      "Iteration 149, loss = 0.48776544\n",
      "Iteration 150, loss = 0.48761709\n",
      "Iteration 151, loss = 0.48756485\n",
      "Iteration 152, loss = 0.48741726\n",
      "Iteration 153, loss = 0.48761459\n",
      "Iteration 154, loss = 0.48740709\n",
      "Iteration 155, loss = 0.48760959\n",
      "Iteration 156, loss = 0.48720075\n",
      "Iteration 157, loss = 0.48711536\n",
      "Iteration 158, loss = 0.48709194\n",
      "Iteration 159, loss = 0.48701591\n",
      "Iteration 160, loss = 0.48699899\n",
      "Iteration 161, loss = 0.48707146\n",
      "Iteration 162, loss = 0.48683813\n",
      "Iteration 163, loss = 0.48673520\n",
      "Iteration 164, loss = 0.48668353\n",
      "Iteration 165, loss = 0.48689485\n",
      "Iteration 166, loss = 0.48703120\n",
      "Iteration 167, loss = 0.48653408\n",
      "Iteration 168, loss = 0.48653052\n",
      "Iteration 169, loss = 0.48648515\n",
      "Iteration 170, loss = 0.48637054\n",
      "Iteration 171, loss = 0.48636180\n",
      "Iteration 172, loss = 0.48617154\n",
      "Iteration 173, loss = 0.48627733\n",
      "Iteration 174, loss = 0.48606151\n",
      "Iteration 175, loss = 0.48611892\n",
      "Iteration 176, loss = 0.48603742\n",
      "Iteration 177, loss = 0.48601684\n",
      "Iteration 178, loss = 0.48608039\n",
      "Iteration 179, loss = 0.48625595\n",
      "Iteration 180, loss = 0.48583805\n",
      "Iteration 181, loss = 0.48599722\n",
      "Iteration 182, loss = 0.48580777\n",
      "Iteration 183, loss = 0.48583692\n",
      "Iteration 184, loss = 0.48581397\n",
      "Iteration 185, loss = 0.48588565\n",
      "Iteration 186, loss = 0.48562728\n",
      "Iteration 187, loss = 0.48539113\n",
      "Iteration 188, loss = 0.48533713\n",
      "Iteration 189, loss = 0.48544082\n",
      "Iteration 190, loss = 0.48546246\n",
      "Iteration 191, loss = 0.48544871\n",
      "Iteration 192, loss = 0.48561587\n",
      "Iteration 193, loss = 0.48511740\n",
      "Iteration 194, loss = 0.48536671\n",
      "Iteration 195, loss = 0.48508085\n",
      "Iteration 196, loss = 0.48560091\n",
      "Iteration 197, loss = 0.48516364\n",
      "Iteration 198, loss = 0.48499556\n",
      "Iteration 199, loss = 0.48518224\n",
      "Iteration 200, loss = 0.48482974\n",
      "Iteration 201, loss = 0.48495728\n",
      "Iteration 202, loss = 0.48509329\n",
      "Iteration 203, loss = 0.48461415\n",
      "Iteration 204, loss = 0.48467406\n",
      "Iteration 205, loss = 0.48479725\n",
      "Iteration 206, loss = 0.48512539\n",
      "Iteration 207, loss = 0.48463128\n",
      "Iteration 208, loss = 0.48493044\n",
      "Iteration 209, loss = 0.48443118\n",
      "Iteration 210, loss = 0.48427540\n",
      "Iteration 211, loss = 0.48507665\n",
      "Iteration 212, loss = 0.48436520\n",
      "Iteration 213, loss = 0.48441183\n",
      "Iteration 214, loss = 0.48441476\n",
      "Iteration 215, loss = 0.48436000\n",
      "Iteration 216, loss = 0.48402835\n",
      "Iteration 217, loss = 0.48415917\n",
      "Iteration 218, loss = 0.48411946\n",
      "Iteration 219, loss = 0.48411906\n",
      "Iteration 220, loss = 0.48391816\n",
      "Iteration 221, loss = 0.48404450\n",
      "Iteration 222, loss = 0.48416734\n",
      "Iteration 223, loss = 0.48390311\n",
      "Iteration 224, loss = 0.48372299\n",
      "Iteration 225, loss = 0.48380740\n",
      "Iteration 226, loss = 0.48370218\n",
      "Iteration 227, loss = 0.48377724\n",
      "Iteration 228, loss = 0.48373114\n",
      "Iteration 229, loss = 0.48358486\n",
      "Iteration 230, loss = 0.48358506\n",
      "Iteration 231, loss = 0.48362881\n",
      "Iteration 232, loss = 0.48336229\n",
      "Iteration 233, loss = 0.48328041\n",
      "Iteration 234, loss = 0.48327655\n",
      "Iteration 235, loss = 0.48331140\n",
      "Iteration 236, loss = 0.48330936\n",
      "Iteration 237, loss = 0.48312983\n",
      "Iteration 238, loss = 0.48312483\n",
      "Iteration 239, loss = 0.48330740\n",
      "Iteration 240, loss = 0.48298455\n",
      "Iteration 241, loss = 0.48309491\n",
      "Iteration 242, loss = 0.48303950\n",
      "Iteration 243, loss = 0.48290064\n",
      "Iteration 244, loss = 0.48317366\n",
      "Iteration 245, loss = 0.48304788\n",
      "Iteration 246, loss = 0.48285729\n",
      "Iteration 247, loss = 0.48283585\n",
      "Iteration 248, loss = 0.48256881\n",
      "Iteration 249, loss = 0.48247866\n",
      "Iteration 250, loss = 0.48244877\n",
      "Iteration 251, loss = 0.48240731\n",
      "Iteration 252, loss = 0.48259804\n",
      "Iteration 253, loss = 0.48256595\n",
      "Iteration 254, loss = 0.48229119\n",
      "Iteration 255, loss = 0.48221972\n",
      "Iteration 256, loss = 0.48248326\n",
      "Iteration 257, loss = 0.48209504\n",
      "Iteration 258, loss = 0.48227174\n",
      "Iteration 259, loss = 0.48188890\n",
      "Iteration 260, loss = 0.48225834\n",
      "Iteration 261, loss = 0.48188967\n",
      "Iteration 262, loss = 0.48179305\n",
      "Iteration 263, loss = 0.48164939\n",
      "Iteration 264, loss = 0.48180172\n",
      "Iteration 265, loss = 0.48158417\n",
      "Iteration 266, loss = 0.48151882\n",
      "Iteration 267, loss = 0.48140907\n",
      "Iteration 268, loss = 0.48150573\n",
      "Iteration 269, loss = 0.48161005\n",
      "Iteration 270, loss = 0.48159058\n",
      "Iteration 271, loss = 0.48143206\n",
      "Iteration 272, loss = 0.48107500\n",
      "Iteration 273, loss = 0.48120338\n",
      "Iteration 274, loss = 0.48104549\n",
      "Iteration 275, loss = 0.48100058\n",
      "Iteration 276, loss = 0.48121007\n",
      "Iteration 277, loss = 0.48085789\n",
      "Iteration 278, loss = 0.48093838\n",
      "Iteration 279, loss = 0.48094329\n",
      "Iteration 280, loss = 0.48072165\n",
      "Iteration 281, loss = 0.48076023\n",
      "Iteration 282, loss = 0.48060971\n",
      "Iteration 283, loss = 0.48085367\n",
      "Iteration 284, loss = 0.48083231\n",
      "Iteration 285, loss = 0.48079109\n",
      "Iteration 286, loss = 0.48069615\n",
      "Iteration 287, loss = 0.48024673\n",
      "Iteration 288, loss = 0.48052050\n",
      "Iteration 289, loss = 0.48028877\n",
      "Iteration 290, loss = 0.48031702\n",
      "Iteration 291, loss = 0.48032188\n",
      "Iteration 292, loss = 0.48019893\n",
      "Iteration 293, loss = 0.48091054\n",
      "Iteration 294, loss = 0.48041782\n",
      "Iteration 295, loss = 0.48039971\n",
      "Iteration 296, loss = 0.48036308\n",
      "Iteration 297, loss = 0.48002426\n",
      "Iteration 298, loss = 0.48010665\n",
      "Iteration 299, loss = 0.47971317\n",
      "Iteration 300, loss = 0.47990046\n",
      "Iteration 301, loss = 0.47987716\n",
      "Iteration 302, loss = 0.47973631\n",
      "Iteration 303, loss = 0.47972497\n",
      "Iteration 304, loss = 0.47984318\n",
      "Iteration 305, loss = 0.47977360\n",
      "Iteration 306, loss = 0.47949533\n",
      "Iteration 307, loss = 0.47954802\n",
      "Iteration 308, loss = 0.47953382\n",
      "Iteration 309, loss = 0.47951542\n",
      "Iteration 310, loss = 0.47938079\n",
      "Iteration 311, loss = 0.47951940\n",
      "Iteration 312, loss = 0.47938955\n",
      "Iteration 313, loss = 0.47966553\n",
      "Iteration 314, loss = 0.47933128\n",
      "Iteration 315, loss = 0.47928714\n",
      "Iteration 316, loss = 0.47913464\n",
      "Iteration 317, loss = 0.47911531\n",
      "Iteration 318, loss = 0.47910483\n",
      "Iteration 319, loss = 0.47915155\n",
      "Iteration 320, loss = 0.47909903\n",
      "Iteration 321, loss = 0.47932304\n",
      "Iteration 322, loss = 0.47925259\n",
      "Iteration 323, loss = 0.47908036\n",
      "Iteration 324, loss = 0.47886831\n",
      "Iteration 325, loss = 0.47938255\n",
      "Iteration 326, loss = 0.47881878\n",
      "Iteration 327, loss = 0.47886215\n",
      "Iteration 328, loss = 0.47875125\n",
      "Iteration 329, loss = 0.47873047\n",
      "Iteration 330, loss = 0.47889351\n",
      "Iteration 331, loss = 0.47868788\n",
      "Iteration 332, loss = 0.47870069\n",
      "Iteration 333, loss = 0.47858113\n",
      "Iteration 334, loss = 0.47845979\n",
      "Iteration 335, loss = 0.47855493\n",
      "Iteration 336, loss = 0.47843504\n",
      "Iteration 337, loss = 0.47855948\n",
      "Iteration 338, loss = 0.47848665\n",
      "Iteration 339, loss = 0.47837868\n",
      "Iteration 340, loss = 0.47857706\n",
      "Iteration 341, loss = 0.47845657\n",
      "Iteration 342, loss = 0.47820278\n",
      "Iteration 343, loss = 0.47819219\n",
      "Iteration 344, loss = 0.47833058\n",
      "Iteration 345, loss = 0.47803977\n",
      "Iteration 346, loss = 0.47831990\n",
      "Iteration 347, loss = 0.47805504\n",
      "Iteration 348, loss = 0.47793062\n",
      "Iteration 349, loss = 0.47801427\n",
      "Iteration 350, loss = 0.47780761\n",
      "Iteration 351, loss = 0.47785780\n",
      "Iteration 352, loss = 0.47786254\n",
      "Iteration 353, loss = 0.47798011\n",
      "Iteration 354, loss = 0.47759521\n",
      "Iteration 355, loss = 0.47768446\n",
      "Iteration 356, loss = 0.47802303\n",
      "Iteration 357, loss = 0.47791664\n",
      "Iteration 358, loss = 0.47752389\n",
      "Iteration 359, loss = 0.47787347\n",
      "Iteration 360, loss = 0.47740907\n",
      "Iteration 361, loss = 0.47759899\n",
      "Iteration 362, loss = 0.47772539\n",
      "Iteration 363, loss = 0.47732299\n",
      "Iteration 364, loss = 0.47759886\n",
      "Iteration 365, loss = 0.47726851\n",
      "Iteration 366, loss = 0.47719997\n",
      "Iteration 367, loss = 0.47720124\n",
      "Iteration 368, loss = 0.47720958\n",
      "Iteration 369, loss = 0.47741946\n",
      "Iteration 370, loss = 0.47721931\n",
      "Iteration 371, loss = 0.47676540\n",
      "Iteration 372, loss = 0.47705513\n",
      "Iteration 373, loss = 0.47714050\n",
      "Iteration 374, loss = 0.47684216\n",
      "Iteration 375, loss = 0.47691152\n",
      "Iteration 376, loss = 0.47681447\n",
      "Iteration 377, loss = 0.47667258\n",
      "Iteration 378, loss = 0.47686536\n",
      "Iteration 379, loss = 0.47662651\n",
      "Iteration 380, loss = 0.47671272\n",
      "Iteration 381, loss = 0.47654451\n",
      "Iteration 382, loss = 0.47638822\n",
      "Iteration 383, loss = 0.47662985\n",
      "Iteration 384, loss = 0.47637224\n",
      "Iteration 385, loss = 0.47653319\n",
      "Iteration 386, loss = 0.47620214\n",
      "Iteration 387, loss = 0.47624788\n",
      "Iteration 388, loss = 0.47626230\n",
      "Iteration 389, loss = 0.47612517\n",
      "Iteration 390, loss = 0.47606551\n",
      "Iteration 391, loss = 0.47592252\n",
      "Iteration 392, loss = 0.47607194\n",
      "Iteration 393, loss = 0.47578272\n",
      "Iteration 394, loss = 0.47581244\n",
      "Iteration 395, loss = 0.47572554\n",
      "Iteration 396, loss = 0.47589703\n",
      "Iteration 397, loss = 0.47552781\n",
      "Iteration 398, loss = 0.47552486\n",
      "Iteration 399, loss = 0.47571717\n",
      "Iteration 400, loss = 0.47540954\n",
      "Iteration 401, loss = 0.47556573\n",
      "Iteration 402, loss = 0.47533009\n",
      "Iteration 403, loss = 0.47530466\n",
      "Iteration 404, loss = 0.47548049\n",
      "Iteration 405, loss = 0.47534535\n",
      "Iteration 406, loss = 0.47495550\n",
      "Iteration 407, loss = 0.47491574\n",
      "Iteration 408, loss = 0.47516956\n",
      "Iteration 409, loss = 0.47485264\n",
      "Iteration 410, loss = 0.47491243\n",
      "Iteration 411, loss = 0.47485806\n",
      "Iteration 412, loss = 0.47488933\n",
      "Iteration 413, loss = 0.47467750\n",
      "Iteration 414, loss = 0.47497720\n",
      "Iteration 415, loss = 0.47480684\n",
      "Iteration 416, loss = 0.47484977\n",
      "Iteration 417, loss = 0.47453165\n",
      "Iteration 418, loss = 0.47446364\n",
      "Iteration 419, loss = 0.47461642\n",
      "Iteration 420, loss = 0.47440596\n",
      "Iteration 421, loss = 0.47418001\n",
      "Iteration 422, loss = 0.47415507\n",
      "Iteration 423, loss = 0.47426701\n",
      "Iteration 424, loss = 0.47415700\n",
      "Iteration 425, loss = 0.47419244\n",
      "Iteration 426, loss = 0.47386420\n",
      "Iteration 427, loss = 0.47409983\n",
      "Iteration 428, loss = 0.47402902\n",
      "Iteration 429, loss = 0.47380710\n",
      "Iteration 430, loss = 0.47390156\n",
      "Iteration 431, loss = 0.47393383\n",
      "Iteration 432, loss = 0.47394835\n",
      "Iteration 433, loss = 0.47381804\n",
      "Iteration 434, loss = 0.47349684\n",
      "Iteration 435, loss = 0.47357081\n",
      "Iteration 436, loss = 0.47357996\n",
      "Iteration 437, loss = 0.47350935\n",
      "Iteration 438, loss = 0.47341733\n",
      "Iteration 439, loss = 0.47340057\n",
      "Iteration 440, loss = 0.47388902\n",
      "Iteration 441, loss = 0.47340614\n",
      "Iteration 442, loss = 0.47322154\n",
      "Iteration 443, loss = 0.47333004\n",
      "Iteration 444, loss = 0.47325233\n",
      "Iteration 445, loss = 0.47313087\n",
      "Iteration 446, loss = 0.47299892\n",
      "Iteration 447, loss = 0.47303896\n",
      "Iteration 448, loss = 0.47288926\n",
      "Iteration 449, loss = 0.47288801\n",
      "Iteration 450, loss = 0.47278141\n",
      "Iteration 451, loss = 0.47270888\n",
      "Iteration 452, loss = 0.47320778\n",
      "Iteration 453, loss = 0.47269532\n",
      "Iteration 454, loss = 0.47256029\n",
      "Iteration 455, loss = 0.47254899\n",
      "Iteration 456, loss = 0.47275193\n",
      "Iteration 457, loss = 0.47241197\n",
      "Iteration 458, loss = 0.47243336\n",
      "Iteration 459, loss = 0.47222177\n",
      "Iteration 460, loss = 0.47233744\n",
      "Iteration 461, loss = 0.47226075\n",
      "Iteration 462, loss = 0.47232386\n",
      "Iteration 463, loss = 0.47241988\n",
      "Iteration 464, loss = 0.47207724\n",
      "Iteration 465, loss = 0.47213009\n",
      "Iteration 466, loss = 0.47202702\n",
      "Iteration 467, loss = 0.47224329\n",
      "Iteration 468, loss = 0.47200628\n",
      "Iteration 469, loss = 0.47198158\n",
      "Iteration 470, loss = 0.47199916\n",
      "Iteration 471, loss = 0.47201615\n",
      "Iteration 472, loss = 0.47179174\n",
      "Iteration 473, loss = 0.47184809\n",
      "Iteration 474, loss = 0.47175250\n",
      "Iteration 475, loss = 0.47169515\n",
      "Iteration 476, loss = 0.47173369\n",
      "Iteration 477, loss = 0.47181718\n",
      "Iteration 478, loss = 0.47169178\n",
      "Iteration 479, loss = 0.47161802\n",
      "Iteration 480, loss = 0.47154508\n",
      "Iteration 481, loss = 0.47158439\n",
      "Iteration 482, loss = 0.47158448\n",
      "Iteration 483, loss = 0.47141126\n",
      "Iteration 484, loss = 0.47137065\n",
      "Iteration 485, loss = 0.47176977\n",
      "Iteration 486, loss = 0.47157929\n",
      "Iteration 487, loss = 0.47125176\n",
      "Iteration 488, loss = 0.47141196\n",
      "Iteration 489, loss = 0.47116712\n",
      "Iteration 490, loss = 0.47132698\n",
      "Iteration 491, loss = 0.47136795\n",
      "Iteration 492, loss = 0.47103927\n",
      "Iteration 493, loss = 0.47122959\n",
      "Iteration 494, loss = 0.47120443\n",
      "Iteration 495, loss = 0.47120349\n",
      "Iteration 496, loss = 0.47107045\n",
      "Iteration 497, loss = 0.47088307\n",
      "Iteration 498, loss = 0.47091047\n",
      "Iteration 499, loss = 0.47101148\n",
      "Iteration 500, loss = 0.47097183\n",
      "Iteration 501, loss = 0.47109543\n",
      "Iteration 502, loss = 0.47094549\n",
      "Iteration 503, loss = 0.47127650\n",
      "Iteration 504, loss = 0.47092280\n",
      "Iteration 505, loss = 0.47087116\n",
      "Iteration 506, loss = 0.47101239\n",
      "Iteration 507, loss = 0.47058693\n",
      "Iteration 508, loss = 0.47083628\n",
      "Iteration 509, loss = 0.47054921\n",
      "Iteration 510, loss = 0.47090343\n",
      "Iteration 511, loss = 0.47109487\n",
      "Iteration 512, loss = 0.47056249\n",
      "Iteration 513, loss = 0.47048396\n",
      "Iteration 514, loss = 0.47074886\n",
      "Iteration 515, loss = 0.47052165\n",
      "Iteration 516, loss = 0.47041242\n",
      "Iteration 517, loss = 0.47054845\n",
      "Iteration 518, loss = 0.47030195\n",
      "Iteration 519, loss = 0.47049162\n",
      "Iteration 520, loss = 0.47023851\n",
      "Iteration 521, loss = 0.47030300\n",
      "Iteration 522, loss = 0.47075874\n",
      "Iteration 523, loss = 0.47020154\n",
      "Iteration 524, loss = 0.47025795\n",
      "Iteration 525, loss = 0.47066210\n",
      "Iteration 526, loss = 0.46997213\n",
      "Iteration 527, loss = 0.47035953\n",
      "Iteration 528, loss = 0.47004567\n",
      "Iteration 529, loss = 0.47037866\n",
      "Iteration 530, loss = 0.47008832\n",
      "Iteration 531, loss = 0.47001575\n",
      "Iteration 532, loss = 0.47021175\n",
      "Iteration 533, loss = 0.46969715\n",
      "Iteration 534, loss = 0.47013547\n",
      "Iteration 535, loss = 0.46957673\n",
      "Iteration 536, loss = 0.46978568\n",
      "Iteration 537, loss = 0.47007288\n",
      "Iteration 538, loss = 0.46995650\n",
      "Iteration 539, loss = 0.46969075\n",
      "Iteration 540, loss = 0.46980665\n",
      "Iteration 541, loss = 0.46984966\n",
      "Iteration 542, loss = 0.46947041\n",
      "Iteration 543, loss = 0.46949948\n",
      "Iteration 544, loss = 0.46976495\n",
      "Iteration 545, loss = 0.46954711\n",
      "Iteration 546, loss = 0.46956537\n",
      "Iteration 547, loss = 0.46931952\n",
      "Iteration 548, loss = 0.46943244\n",
      "Iteration 549, loss = 0.46930155\n",
      "Iteration 550, loss = 0.46970297\n",
      "Iteration 551, loss = 0.46954776\n",
      "Iteration 552, loss = 0.46947172\n",
      "Iteration 553, loss = 0.46944430\n",
      "Iteration 554, loss = 0.46900111\n",
      "Iteration 555, loss = 0.46913852\n",
      "Iteration 556, loss = 0.46908506\n",
      "Iteration 557, loss = 0.46897897\n",
      "Iteration 558, loss = 0.46905833\n",
      "Iteration 559, loss = 0.46912179\n",
      "Iteration 560, loss = 0.46929637\n",
      "Iteration 561, loss = 0.46889118\n",
      "Iteration 562, loss = 0.46890859\n",
      "Iteration 563, loss = 0.46898972\n",
      "Iteration 564, loss = 0.46916028\n",
      "Iteration 565, loss = 0.46875969\n",
      "Iteration 566, loss = 0.46896112\n",
      "Iteration 567, loss = 0.46867630\n",
      "Iteration 568, loss = 0.46870118\n",
      "Iteration 569, loss = 0.46956947\n",
      "Iteration 570, loss = 0.46879342\n",
      "Iteration 571, loss = 0.46861729\n",
      "Iteration 572, loss = 0.46873568\n",
      "Iteration 573, loss = 0.46854033\n",
      "Iteration 574, loss = 0.46873776\n",
      "Iteration 575, loss = 0.46866645\n",
      "Iteration 576, loss = 0.46857401\n",
      "Iteration 577, loss = 0.46848307\n",
      "Iteration 578, loss = 0.46876168\n",
      "Iteration 579, loss = 0.46867325\n",
      "Iteration 580, loss = 0.46841877\n",
      "Iteration 581, loss = 0.46834038\n",
      "Iteration 582, loss = 0.46871996\n",
      "Iteration 583, loss = 0.46818919\n",
      "Iteration 584, loss = 0.46836496\n",
      "Iteration 585, loss = 0.46833553\n",
      "Iteration 586, loss = 0.46837477\n",
      "Iteration 587, loss = 0.46829343\n",
      "Iteration 588, loss = 0.46853890\n",
      "Iteration 589, loss = 0.46870930\n",
      "Iteration 590, loss = 0.46823325\n",
      "Iteration 591, loss = 0.46817806\n",
      "Iteration 592, loss = 0.46814250\n",
      "Iteration 593, loss = 0.46810722\n",
      "Iteration 594, loss = 0.46833666\n",
      "Iteration 595, loss = 0.46781426\n",
      "Iteration 596, loss = 0.46785871\n",
      "Iteration 597, loss = 0.46817298\n",
      "Iteration 598, loss = 0.46810080\n",
      "Iteration 599, loss = 0.46803638\n",
      "Iteration 600, loss = 0.46803218\n",
      "Iteration 601, loss = 0.46813639\n",
      "Iteration 602, loss = 0.46766858\n",
      "Iteration 603, loss = 0.46817252\n",
      "Iteration 604, loss = 0.46778267\n",
      "Iteration 605, loss = 0.46822565\n",
      "Iteration 606, loss = 0.46774786\n",
      "Iteration 607, loss = 0.46804606\n",
      "Iteration 608, loss = 0.46788300\n",
      "Iteration 609, loss = 0.46794586\n",
      "Iteration 610, loss = 0.46778613\n",
      "Iteration 611, loss = 0.46755908\n",
      "Iteration 612, loss = 0.46757550\n",
      "Iteration 613, loss = 0.46765404\n",
      "Iteration 614, loss = 0.46780064\n",
      "Iteration 615, loss = 0.46770881\n",
      "Iteration 616, loss = 0.46744694\n",
      "Iteration 617, loss = 0.46778707\n",
      "Iteration 618, loss = 0.46815789\n",
      "Iteration 619, loss = 0.46800241\n",
      "Iteration 620, loss = 0.46751275\n",
      "Iteration 621, loss = 0.46772646\n",
      "Iteration 622, loss = 0.46758152\n",
      "Iteration 623, loss = 0.46756179\n",
      "Iteration 624, loss = 0.46741364\n",
      "Iteration 625, loss = 0.46782477\n",
      "Iteration 626, loss = 0.46786068\n",
      "Iteration 627, loss = 0.46762753\n",
      "Iteration 628, loss = 0.46767196\n",
      "Iteration 629, loss = 0.46778164\n",
      "Iteration 630, loss = 0.46746533\n",
      "Iteration 631, loss = 0.46758586\n",
      "Iteration 632, loss = 0.46782260\n",
      "Iteration 633, loss = 0.46777311\n",
      "Iteration 634, loss = 0.46745991\n",
      "Iteration 635, loss = 0.46737302\n",
      "Iteration 636, loss = 0.46755884\n",
      "Iteration 637, loss = 0.46753677\n",
      "Iteration 638, loss = 0.46731906\n",
      "Iteration 639, loss = 0.46759341\n",
      "Iteration 640, loss = 0.46828905\n",
      "Iteration 641, loss = 0.46761396\n",
      "Iteration 642, loss = 0.46747842\n",
      "Iteration 643, loss = 0.46709628\n",
      "Iteration 644, loss = 0.46737903\n",
      "Iteration 645, loss = 0.46757925\n",
      "Iteration 646, loss = 0.46747110\n",
      "Iteration 647, loss = 0.46748634\n",
      "Iteration 648, loss = 0.46736151\n",
      "Iteration 649, loss = 0.46708625\n",
      "Iteration 650, loss = 0.46738834\n",
      "Iteration 651, loss = 0.46744870\n",
      "Iteration 652, loss = 0.46820275\n",
      "Iteration 653, loss = 0.46726719\n",
      "Iteration 654, loss = 0.46713939\n",
      "Iteration 655, loss = 0.46712070\n",
      "Iteration 656, loss = 0.46712374\n",
      "Iteration 657, loss = 0.46714689\n",
      "Iteration 658, loss = 0.46711786\n",
      "Iteration 659, loss = 0.46729953\n",
      "Iteration 660, loss = 0.46709046\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "scores_ann = cross_val_score(model_ann, X_customer_balanced, Y_customer_balanced, cv=kf_ann, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.57917201\n",
      "Iteration 2, loss = 0.53336200\n",
      "Iteration 3, loss = 0.51877426\n",
      "Iteration 4, loss = 0.50900382\n",
      "Iteration 5, loss = 0.50376499\n",
      "Iteration 6, loss = 0.49795483\n",
      "Iteration 7, loss = 0.49370219\n",
      "Iteration 8, loss = 0.48853555\n",
      "Iteration 9, loss = 0.48499957\n",
      "Iteration 10, loss = 0.48242712\n",
      "Iteration 11, loss = 0.47838781\n",
      "Iteration 12, loss = 0.47602647\n",
      "Iteration 13, loss = 0.47229559\n",
      "Iteration 14, loss = 0.46846872\n",
      "Iteration 15, loss = 0.46710006\n",
      "Iteration 16, loss = 0.46285702\n",
      "Iteration 17, loss = 0.45926173\n",
      "Iteration 18, loss = 0.45866379\n",
      "Iteration 19, loss = 0.45229094\n",
      "Iteration 20, loss = 0.45261414\n",
      "Iteration 21, loss = 0.44733448\n",
      "Iteration 22, loss = 0.44469941\n",
      "Iteration 23, loss = 0.44168155\n",
      "Iteration 24, loss = 0.43799381\n",
      "Iteration 25, loss = 0.43615839\n",
      "Iteration 26, loss = 0.43366463\n",
      "Iteration 27, loss = 0.43265938\n",
      "Iteration 28, loss = 0.42898259\n",
      "Iteration 29, loss = 0.42531318\n",
      "Iteration 30, loss = 0.42138341\n",
      "Iteration 31, loss = 0.41931917\n",
      "Iteration 32, loss = 0.41572089\n",
      "Iteration 33, loss = 0.41431055\n",
      "Iteration 34, loss = 0.41277270\n",
      "Iteration 35, loss = 0.40508788\n",
      "Iteration 36, loss = 0.40488563\n",
      "Iteration 37, loss = 0.40134563\n",
      "Iteration 38, loss = 0.39981677\n",
      "Iteration 39, loss = 0.39788194\n",
      "Iteration 40, loss = 0.39491376\n",
      "Iteration 41, loss = 0.39112486\n",
      "Iteration 42, loss = 0.38561089\n",
      "Iteration 43, loss = 0.38599943\n",
      "Iteration 44, loss = 0.38286251\n",
      "Iteration 45, loss = 0.37986064\n",
      "Iteration 46, loss = 0.37990908\n",
      "Iteration 47, loss = 0.37388811\n",
      "Iteration 48, loss = 0.37467782\n",
      "Iteration 49, loss = 0.36940410\n",
      "Iteration 50, loss = 0.36857131\n",
      "Iteration 51, loss = 0.36564379\n",
      "Iteration 52, loss = 0.36432467\n",
      "Iteration 53, loss = 0.36092506\n",
      "Iteration 54, loss = 0.35974792\n",
      "Iteration 55, loss = 0.35560148\n",
      "Iteration 56, loss = 0.35107027\n",
      "Iteration 57, loss = 0.34821341\n",
      "Iteration 58, loss = 0.34982451\n",
      "Iteration 59, loss = 0.34759109\n",
      "Iteration 60, loss = 0.34003848\n",
      "Iteration 61, loss = 0.33973718\n",
      "Iteration 62, loss = 0.34008379\n",
      "Iteration 63, loss = 0.33602009\n",
      "Iteration 64, loss = 0.33423792\n",
      "Iteration 65, loss = 0.33466070\n",
      "Iteration 66, loss = 0.32845054\n",
      "Iteration 67, loss = 0.32709605\n",
      "Iteration 68, loss = 0.32661932\n",
      "Iteration 69, loss = 0.32637299\n",
      "Iteration 70, loss = 0.32097658\n",
      "Iteration 71, loss = 0.32173954\n",
      "Iteration 72, loss = 0.31841977\n",
      "Iteration 73, loss = 0.31584648\n",
      "Iteration 74, loss = 0.31253695\n",
      "Iteration 75, loss = 0.30958501\n",
      "Iteration 76, loss = 0.30848955\n",
      "Iteration 77, loss = 0.30614224\n",
      "Iteration 78, loss = 0.30691561\n",
      "Iteration 79, loss = 0.30031260\n",
      "Iteration 80, loss = 0.30321379\n",
      "Iteration 81, loss = 0.30115811\n",
      "Iteration 82, loss = 0.29604009\n",
      "Iteration 83, loss = 0.29738915\n",
      "Iteration 84, loss = 0.29915964\n",
      "Iteration 85, loss = 0.29250042\n",
      "Iteration 86, loss = 0.29226443\n",
      "Iteration 87, loss = 0.29099281\n",
      "Iteration 88, loss = 0.28918622\n",
      "Iteration 89, loss = 0.28408614\n",
      "Iteration 90, loss = 0.28544363\n",
      "Iteration 91, loss = 0.28241487\n",
      "Iteration 92, loss = 0.27898505\n",
      "Iteration 93, loss = 0.28105431\n",
      "Iteration 94, loss = 0.27965473\n",
      "Iteration 95, loss = 0.27625671\n",
      "Iteration 96, loss = 0.27283862\n",
      "Iteration 97, loss = 0.26940396\n",
      "Iteration 98, loss = 0.26916783\n",
      "Iteration 99, loss = 0.26992047\n",
      "Iteration 100, loss = 0.26585235\n",
      "Iteration 101, loss = 0.26006605\n",
      "Iteration 102, loss = 0.26754756\n",
      "Iteration 103, loss = 0.26295124\n",
      "Iteration 104, loss = 0.25786220\n",
      "Iteration 105, loss = 0.26036919\n",
      "Iteration 106, loss = 0.25387622\n",
      "Iteration 107, loss = 0.25395339\n",
      "Iteration 108, loss = 0.25239047\n",
      "Iteration 109, loss = 0.25002691\n",
      "Iteration 110, loss = 0.24803759\n",
      "Iteration 111, loss = 0.24410889\n",
      "Iteration 112, loss = 0.24586529\n",
      "Iteration 113, loss = 0.24275602\n",
      "Iteration 114, loss = 0.24213528\n",
      "Iteration 115, loss = 0.24137791\n",
      "Iteration 116, loss = 0.24228035\n",
      "Iteration 117, loss = 0.23687129\n",
      "Iteration 118, loss = 0.23356169\n",
      "Iteration 119, loss = 0.23714969\n",
      "Iteration 120, loss = 0.23307998\n",
      "Iteration 121, loss = 0.23146054\n",
      "Iteration 122, loss = 0.22517216\n",
      "Iteration 123, loss = 0.22888614\n",
      "Iteration 124, loss = 0.22439914\n",
      "Iteration 125, loss = 0.22284721\n",
      "Iteration 126, loss = 0.22269458\n",
      "Iteration 127, loss = 0.21679126\n",
      "Iteration 128, loss = 0.22767326\n",
      "Iteration 129, loss = 0.21456176\n",
      "Iteration 130, loss = 0.21793495\n",
      "Iteration 131, loss = 0.21379286\n",
      "Iteration 132, loss = 0.21124939\n",
      "Iteration 133, loss = 0.20956016\n",
      "Iteration 134, loss = 0.21029373\n",
      "Iteration 135, loss = 0.20776084\n",
      "Iteration 136, loss = 0.20677482\n",
      "Iteration 137, loss = 0.20306802\n",
      "Iteration 138, loss = 0.20719364\n",
      "Iteration 139, loss = 0.19942842\n",
      "Iteration 140, loss = 0.19964615\n",
      "Iteration 141, loss = 0.19679941\n",
      "Iteration 142, loss = 0.19842085\n",
      "Iteration 143, loss = 0.19122165\n",
      "Iteration 144, loss = 0.19536994\n",
      "Iteration 145, loss = 0.19352319\n",
      "Iteration 146, loss = 0.18734623\n",
      "Iteration 147, loss = 0.19361222\n",
      "Iteration 148, loss = 0.18835432\n",
      "Iteration 149, loss = 0.17945744\n",
      "Iteration 150, loss = 0.18498705\n",
      "Iteration 151, loss = 0.18247684\n",
      "Iteration 152, loss = 0.17605032\n",
      "Iteration 153, loss = 0.18474952\n",
      "Iteration 154, loss = 0.17764154\n",
      "Iteration 155, loss = 0.17776468\n",
      "Iteration 156, loss = 0.17329392\n",
      "Iteration 157, loss = 0.17311457\n",
      "Iteration 158, loss = 0.17189111\n",
      "Iteration 159, loss = 0.17332260\n",
      "Iteration 160, loss = 0.16796371\n",
      "Iteration 161, loss = 0.16907589\n",
      "Iteration 162, loss = 0.16339351\n",
      "Iteration 163, loss = 0.16812274\n",
      "Iteration 164, loss = 0.16681828\n",
      "Iteration 165, loss = 0.16114543\n",
      "Iteration 166, loss = 0.15908109\n",
      "Iteration 167, loss = 0.16197980\n",
      "Iteration 168, loss = 0.16409510\n",
      "Iteration 169, loss = 0.15795704\n",
      "Iteration 170, loss = 0.15488856\n",
      "Iteration 171, loss = 0.15317275\n",
      "Iteration 172, loss = 0.15696228\n",
      "Iteration 173, loss = 0.15105878\n",
      "Iteration 174, loss = 0.15219040\n",
      "Iteration 175, loss = 0.14799679\n",
      "Iteration 176, loss = 0.14845143\n",
      "Iteration 177, loss = 0.14622949\n",
      "Iteration 178, loss = 0.14474238\n",
      "Iteration 179, loss = 0.13857271\n",
      "Iteration 180, loss = 0.14704299\n",
      "Iteration 181, loss = 0.14049765\n",
      "Iteration 182, loss = 0.13479137\n",
      "Iteration 183, loss = 0.13705831\n",
      "Iteration 184, loss = 0.14232469\n",
      "Iteration 185, loss = 0.14334998\n",
      "Iteration 186, loss = 0.13158816\n",
      "Iteration 187, loss = 0.13606526\n",
      "Iteration 188, loss = 0.13196448\n",
      "Iteration 189, loss = 0.13210527\n",
      "Iteration 190, loss = 0.13083726\n",
      "Iteration 191, loss = 0.13093198\n",
      "Iteration 192, loss = 0.12862284\n",
      "Iteration 193, loss = 0.12216031\n",
      "Iteration 194, loss = 0.12807231\n",
      "Iteration 195, loss = 0.11819921\n",
      "Iteration 196, loss = 0.12659537\n",
      "Iteration 197, loss = 0.12088909\n",
      "Iteration 198, loss = 0.12205385\n",
      "Iteration 199, loss = 0.11937160\n",
      "Iteration 200, loss = 0.12271298\n",
      "Iteration 201, loss = 0.11359158\n",
      "Iteration 202, loss = 0.11190565\n",
      "Iteration 203, loss = 0.11502087\n",
      "Iteration 204, loss = 0.11984451\n",
      "Iteration 205, loss = 0.11245697\n",
      "Iteration 206, loss = 0.11347261\n",
      "Iteration 207, loss = 0.11488322\n",
      "Iteration 208, loss = 0.11472331\n",
      "Iteration 209, loss = 0.10778316\n",
      "Iteration 210, loss = 0.10560398\n",
      "Iteration 211, loss = 0.10973330\n",
      "Iteration 212, loss = 0.10622245\n",
      "Iteration 213, loss = 0.10214683\n",
      "Iteration 214, loss = 0.10430375\n",
      "Iteration 215, loss = 0.09851510\n",
      "Iteration 216, loss = 0.10301950\n",
      "Iteration 217, loss = 0.09897632\n",
      "Iteration 218, loss = 0.10206377\n",
      "Iteration 219, loss = 0.09972599\n",
      "Iteration 220, loss = 0.09697265\n",
      "Iteration 221, loss = 0.09272523\n",
      "Iteration 222, loss = 0.09924693\n",
      "Iteration 223, loss = 0.10264999\n",
      "Iteration 224, loss = 0.09240515\n",
      "Iteration 225, loss = 0.09382917\n",
      "Iteration 226, loss = 0.09532337\n",
      "Iteration 227, loss = 0.09030703\n",
      "Iteration 228, loss = 0.08682716\n",
      "Iteration 229, loss = 0.08467203\n",
      "Iteration 230, loss = 0.09543956\n",
      "Iteration 231, loss = 0.08296441\n",
      "Iteration 232, loss = 0.09448456\n",
      "Iteration 233, loss = 0.08323856\n",
      "Iteration 234, loss = 0.08439461\n",
      "Iteration 235, loss = 0.08524579\n",
      "Iteration 236, loss = 0.09473242\n",
      "Iteration 237, loss = 0.08074412\n",
      "Iteration 238, loss = 0.07585074\n",
      "Iteration 239, loss = 0.09198379\n",
      "Iteration 240, loss = 0.08376764\n",
      "Iteration 241, loss = 0.08698166\n",
      "Iteration 242, loss = 0.07727791\n",
      "Iteration 243, loss = 0.07976920\n",
      "Iteration 244, loss = 0.07596163\n",
      "Iteration 245, loss = 0.07603045\n",
      "Iteration 246, loss = 0.07571740\n",
      "Iteration 247, loss = 0.07571560\n",
      "Iteration 248, loss = 0.07775321\n",
      "Iteration 249, loss = 0.07569689\n",
      "Iteration 250, loss = 0.08647817\n",
      "Iteration 251, loss = 0.07250198\n",
      "Iteration 252, loss = 0.07192659\n",
      "Iteration 253, loss = 0.07164251\n",
      "Iteration 254, loss = 0.07344435\n",
      "Iteration 255, loss = 0.06392819\n",
      "Iteration 256, loss = 0.06564945\n",
      "Iteration 257, loss = 0.07780158\n",
      "Iteration 258, loss = 0.06743665\n",
      "Iteration 259, loss = 0.06599497\n",
      "Iteration 260, loss = 0.06662745\n",
      "Iteration 261, loss = 0.06903490\n",
      "Iteration 262, loss = 0.06738357\n",
      "Iteration 263, loss = 0.06655706\n",
      "Iteration 264, loss = 0.06595314\n",
      "Iteration 265, loss = 0.05959494\n",
      "Iteration 266, loss = 0.06431708\n",
      "Iteration 267, loss = 0.06302548\n",
      "Iteration 268, loss = 0.07018895\n",
      "Iteration 269, loss = 0.05548313\n",
      "Iteration 270, loss = 0.06966153\n",
      "Iteration 271, loss = 0.06307398\n",
      "Iteration 272, loss = 0.06327802\n",
      "Iteration 273, loss = 0.06230474\n",
      "Iteration 274, loss = 0.05571954\n",
      "Iteration 275, loss = 0.05836919\n",
      "Iteration 276, loss = 0.06656729\n",
      "Iteration 277, loss = 0.05558477\n",
      "Iteration 278, loss = 0.06283018\n",
      "Iteration 279, loss = 0.06045475\n",
      "Iteration 280, loss = 0.05911058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58459576\n",
      "Iteration 2, loss = 0.54502924\n",
      "Iteration 3, loss = 0.53093021\n",
      "Iteration 4, loss = 0.52129352\n",
      "Iteration 5, loss = 0.51056621\n",
      "Iteration 6, loss = 0.50252701\n",
      "Iteration 7, loss = 0.49855459\n",
      "Iteration 8, loss = 0.49424140\n",
      "Iteration 9, loss = 0.48981329\n",
      "Iteration 10, loss = 0.48967942\n",
      "Iteration 11, loss = 0.48407896\n",
      "Iteration 12, loss = 0.47948148\n",
      "Iteration 13, loss = 0.48095183\n",
      "Iteration 14, loss = 0.47771787\n",
      "Iteration 15, loss = 0.47265122\n",
      "Iteration 16, loss = 0.47075515\n",
      "Iteration 17, loss = 0.46737537\n",
      "Iteration 18, loss = 0.46523494\n",
      "Iteration 19, loss = 0.46222740\n",
      "Iteration 20, loss = 0.45863248\n",
      "Iteration 21, loss = 0.45674947\n",
      "Iteration 22, loss = 0.45501337\n",
      "Iteration 23, loss = 0.45010690\n",
      "Iteration 24, loss = 0.44623075\n",
      "Iteration 25, loss = 0.44479731\n",
      "Iteration 26, loss = 0.44284547\n",
      "Iteration 27, loss = 0.44205505\n",
      "Iteration 28, loss = 0.43735536\n",
      "Iteration 29, loss = 0.43653209\n",
      "Iteration 30, loss = 0.43091714\n",
      "Iteration 31, loss = 0.43099554\n",
      "Iteration 32, loss = 0.42769665\n",
      "Iteration 33, loss = 0.42755766\n",
      "Iteration 34, loss = 0.42083724\n",
      "Iteration 35, loss = 0.41893424\n",
      "Iteration 36, loss = 0.41769201\n",
      "Iteration 37, loss = 0.41294205\n",
      "Iteration 38, loss = 0.41199701\n",
      "Iteration 39, loss = 0.40837844\n",
      "Iteration 40, loss = 0.40407312\n",
      "Iteration 41, loss = 0.40147882\n",
      "Iteration 42, loss = 0.39857862\n",
      "Iteration 43, loss = 0.39677310\n",
      "Iteration 44, loss = 0.39597768\n",
      "Iteration 45, loss = 0.38902089\n",
      "Iteration 46, loss = 0.38880328\n",
      "Iteration 47, loss = 0.38478572\n",
      "Iteration 48, loss = 0.38155612\n",
      "Iteration 49, loss = 0.37685846\n",
      "Iteration 50, loss = 0.37639253\n",
      "Iteration 51, loss = 0.37573395\n",
      "Iteration 52, loss = 0.37369557\n",
      "Iteration 53, loss = 0.36657491\n",
      "Iteration 54, loss = 0.36854161\n",
      "Iteration 55, loss = 0.36116975\n",
      "Iteration 56, loss = 0.35942571\n",
      "Iteration 57, loss = 0.35564411\n",
      "Iteration 58, loss = 0.35762150\n",
      "Iteration 59, loss = 0.34736361\n",
      "Iteration 60, loss = 0.35166521\n",
      "Iteration 61, loss = 0.34493200\n",
      "Iteration 62, loss = 0.34573933\n",
      "Iteration 63, loss = 0.34099423\n",
      "Iteration 64, loss = 0.33801977\n",
      "Iteration 65, loss = 0.33530000\n",
      "Iteration 66, loss = 0.33203364\n",
      "Iteration 67, loss = 0.33330217\n",
      "Iteration 68, loss = 0.32916612\n",
      "Iteration 69, loss = 0.32792039\n",
      "Iteration 70, loss = 0.32666819\n",
      "Iteration 71, loss = 0.32181688\n",
      "Iteration 72, loss = 0.31735039\n",
      "Iteration 73, loss = 0.31484496\n",
      "Iteration 74, loss = 0.31518299\n",
      "Iteration 75, loss = 0.31122462\n",
      "Iteration 76, loss = 0.30733771\n",
      "Iteration 77, loss = 0.31010092\n",
      "Iteration 78, loss = 0.30821777\n",
      "Iteration 79, loss = 0.30091973\n",
      "Iteration 80, loss = 0.30046711\n",
      "Iteration 81, loss = 0.30273586\n",
      "Iteration 82, loss = 0.29847057\n",
      "Iteration 83, loss = 0.29649730\n",
      "Iteration 84, loss = 0.29314524\n",
      "Iteration 85, loss = 0.29372997\n",
      "Iteration 86, loss = 0.29010313\n",
      "Iteration 87, loss = 0.29004742\n",
      "Iteration 88, loss = 0.28521874\n",
      "Iteration 89, loss = 0.28049349\n",
      "Iteration 90, loss = 0.27996955\n",
      "Iteration 91, loss = 0.27950459\n",
      "Iteration 92, loss = 0.27844766\n",
      "Iteration 93, loss = 0.27695459\n",
      "Iteration 94, loss = 0.27182467\n",
      "Iteration 95, loss = 0.26914340\n",
      "Iteration 96, loss = 0.27093741\n",
      "Iteration 97, loss = 0.26987828\n",
      "Iteration 98, loss = 0.26244387\n",
      "Iteration 99, loss = 0.26467933\n",
      "Iteration 100, loss = 0.26477885\n",
      "Iteration 101, loss = 0.26215453\n",
      "Iteration 102, loss = 0.26294812\n",
      "Iteration 103, loss = 0.25486674\n",
      "Iteration 104, loss = 0.25433680\n",
      "Iteration 105, loss = 0.25660342\n",
      "Iteration 106, loss = 0.25009668\n",
      "Iteration 107, loss = 0.25182860\n",
      "Iteration 108, loss = 0.24485850\n",
      "Iteration 109, loss = 0.24945678\n",
      "Iteration 110, loss = 0.24533495\n",
      "Iteration 111, loss = 0.24236913\n",
      "Iteration 112, loss = 0.24157597\n",
      "Iteration 113, loss = 0.23880230\n",
      "Iteration 114, loss = 0.23862687\n",
      "Iteration 115, loss = 0.23785661\n",
      "Iteration 116, loss = 0.23762140\n",
      "Iteration 117, loss = 0.23166472\n",
      "Iteration 118, loss = 0.22971861\n",
      "Iteration 119, loss = 0.22779010\n",
      "Iteration 120, loss = 0.23193574\n",
      "Iteration 121, loss = 0.22549850\n",
      "Iteration 122, loss = 0.22335748\n",
      "Iteration 123, loss = 0.22355208\n",
      "Iteration 124, loss = 0.22753101\n",
      "Iteration 125, loss = 0.22113708\n",
      "Iteration 126, loss = 0.21712005\n",
      "Iteration 127, loss = 0.22220701\n",
      "Iteration 128, loss = 0.21146217\n",
      "Iteration 129, loss = 0.21401510\n",
      "Iteration 130, loss = 0.21389567\n",
      "Iteration 131, loss = 0.21093463\n",
      "Iteration 132, loss = 0.20856254\n",
      "Iteration 133, loss = 0.20625988\n",
      "Iteration 134, loss = 0.21444827\n",
      "Iteration 135, loss = 0.20507194\n",
      "Iteration 136, loss = 0.20471424\n",
      "Iteration 137, loss = 0.20133582\n",
      "Iteration 138, loss = 0.20341578\n",
      "Iteration 139, loss = 0.20038408\n",
      "Iteration 140, loss = 0.20379574\n",
      "Iteration 141, loss = 0.19509731\n",
      "Iteration 142, loss = 0.19426522\n",
      "Iteration 143, loss = 0.19500198\n",
      "Iteration 144, loss = 0.19523396\n",
      "Iteration 145, loss = 0.19003126\n",
      "Iteration 146, loss = 0.18973875\n",
      "Iteration 147, loss = 0.18735377\n",
      "Iteration 148, loss = 0.18781934\n",
      "Iteration 149, loss = 0.18964965\n",
      "Iteration 150, loss = 0.18765915\n",
      "Iteration 151, loss = 0.18181647\n",
      "Iteration 152, loss = 0.17734597\n",
      "Iteration 153, loss = 0.18491983\n",
      "Iteration 154, loss = 0.18051200\n",
      "Iteration 155, loss = 0.17671927\n",
      "Iteration 156, loss = 0.18026562\n",
      "Iteration 157, loss = 0.17735091\n",
      "Iteration 158, loss = 0.16970487\n",
      "Iteration 159, loss = 0.17297073\n",
      "Iteration 160, loss = 0.16505888\n",
      "Iteration 161, loss = 0.17281981\n",
      "Iteration 162, loss = 0.16670293\n",
      "Iteration 163, loss = 0.16852405\n",
      "Iteration 164, loss = 0.16609315\n",
      "Iteration 165, loss = 0.16609677\n",
      "Iteration 166, loss = 0.16220239\n",
      "Iteration 167, loss = 0.15838199\n",
      "Iteration 168, loss = 0.15912524\n",
      "Iteration 169, loss = 0.16647116\n",
      "Iteration 170, loss = 0.15262677\n",
      "Iteration 171, loss = 0.15720656\n",
      "Iteration 172, loss = 0.15681313\n",
      "Iteration 173, loss = 0.15091364\n",
      "Iteration 174, loss = 0.15426262\n",
      "Iteration 175, loss = 0.15219747\n",
      "Iteration 176, loss = 0.15326960\n",
      "Iteration 177, loss = 0.14674141\n",
      "Iteration 178, loss = 0.14903525\n",
      "Iteration 179, loss = 0.14395390\n",
      "Iteration 180, loss = 0.15613916\n",
      "Iteration 181, loss = 0.14351341\n",
      "Iteration 182, loss = 0.14653077\n",
      "Iteration 183, loss = 0.14674560\n",
      "Iteration 184, loss = 0.13957298\n",
      "Iteration 185, loss = 0.13998134\n",
      "Iteration 186, loss = 0.14099477\n",
      "Iteration 187, loss = 0.14072782\n",
      "Iteration 188, loss = 0.13827069\n",
      "Iteration 189, loss = 0.14143404\n",
      "Iteration 190, loss = 0.13439279\n",
      "Iteration 191, loss = 0.12986976\n",
      "Iteration 192, loss = 0.13323583\n",
      "Iteration 193, loss = 0.12912206\n",
      "Iteration 194, loss = 0.13301036\n",
      "Iteration 195, loss = 0.12951922\n",
      "Iteration 196, loss = 0.12147949\n",
      "Iteration 197, loss = 0.12975427\n",
      "Iteration 198, loss = 0.12182513\n",
      "Iteration 199, loss = 0.12488349\n",
      "Iteration 200, loss = 0.12778255\n",
      "Iteration 201, loss = 0.12494618\n",
      "Iteration 202, loss = 0.11802210\n",
      "Iteration 203, loss = 0.11997742\n",
      "Iteration 204, loss = 0.12134375\n",
      "Iteration 205, loss = 0.11927443\n",
      "Iteration 206, loss = 0.12083544\n",
      "Iteration 207, loss = 0.12123054\n",
      "Iteration 208, loss = 0.11783095\n",
      "Iteration 209, loss = 0.12314903\n",
      "Iteration 210, loss = 0.11962302\n",
      "Iteration 211, loss = 0.11206908\n",
      "Iteration 212, loss = 0.10992911\n",
      "Iteration 213, loss = 0.11278026\n",
      "Iteration 214, loss = 0.11161857\n",
      "Iteration 215, loss = 0.11087985\n",
      "Iteration 216, loss = 0.10728330\n",
      "Iteration 217, loss = 0.10666712\n",
      "Iteration 218, loss = 0.10315788\n",
      "Iteration 219, loss = 0.11681768\n",
      "Iteration 220, loss = 0.10055032\n",
      "Iteration 221, loss = 0.10341343\n",
      "Iteration 222, loss = 0.10778607\n",
      "Iteration 223, loss = 0.09809882\n",
      "Iteration 224, loss = 0.10378084\n",
      "Iteration 225, loss = 0.10688324\n",
      "Iteration 226, loss = 0.09647434\n",
      "Iteration 227, loss = 0.09981328\n",
      "Iteration 228, loss = 0.10365027\n",
      "Iteration 229, loss = 0.09472095\n",
      "Iteration 230, loss = 0.10373340\n",
      "Iteration 231, loss = 0.09669712\n",
      "Iteration 232, loss = 0.09346976\n",
      "Iteration 233, loss = 0.09960441\n",
      "Iteration 234, loss = 0.10126208\n",
      "Iteration 235, loss = 0.09505296\n",
      "Iteration 236, loss = 0.09401205\n",
      "Iteration 237, loss = 0.08471123\n",
      "Iteration 238, loss = 0.09151044\n",
      "Iteration 239, loss = 0.09387877\n",
      "Iteration 240, loss = 0.08717665\n",
      "Iteration 241, loss = 0.09204041\n",
      "Iteration 242, loss = 0.08464067\n",
      "Iteration 243, loss = 0.08727892\n",
      "Iteration 244, loss = 0.08568906\n",
      "Iteration 245, loss = 0.09389581\n",
      "Iteration 246, loss = 0.08514389\n",
      "Iteration 247, loss = 0.08995208\n",
      "Iteration 248, loss = 0.08860198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59032551\n",
      "Iteration 2, loss = 0.53792141\n",
      "Iteration 3, loss = 0.51910882\n",
      "Iteration 4, loss = 0.50945822\n",
      "Iteration 5, loss = 0.50297230\n",
      "Iteration 6, loss = 0.49865749\n",
      "Iteration 7, loss = 0.49360720\n",
      "Iteration 8, loss = 0.49034510\n",
      "Iteration 9, loss = 0.48653058\n",
      "Iteration 10, loss = 0.48387649\n",
      "Iteration 11, loss = 0.48151479\n",
      "Iteration 12, loss = 0.47770839\n",
      "Iteration 13, loss = 0.47525862\n",
      "Iteration 14, loss = 0.47099349\n",
      "Iteration 15, loss = 0.46784641\n",
      "Iteration 16, loss = 0.46556241\n",
      "Iteration 17, loss = 0.46029411\n",
      "Iteration 18, loss = 0.45843692\n",
      "Iteration 19, loss = 0.45587340\n",
      "Iteration 20, loss = 0.45020458\n",
      "Iteration 21, loss = 0.44774383\n",
      "Iteration 22, loss = 0.44498246\n",
      "Iteration 23, loss = 0.44285894\n",
      "Iteration 24, loss = 0.43713956\n",
      "Iteration 25, loss = 0.43429814\n",
      "Iteration 26, loss = 0.43152917\n",
      "Iteration 27, loss = 0.42864342\n",
      "Iteration 28, loss = 0.42610576\n",
      "Iteration 29, loss = 0.42365582\n",
      "Iteration 30, loss = 0.41991650\n",
      "Iteration 31, loss = 0.41613218\n",
      "Iteration 32, loss = 0.41242369\n",
      "Iteration 33, loss = 0.40852267\n",
      "Iteration 34, loss = 0.40679933\n",
      "Iteration 35, loss = 0.40118557\n",
      "Iteration 36, loss = 0.40210141\n",
      "Iteration 37, loss = 0.39553895\n",
      "Iteration 38, loss = 0.39471986\n",
      "Iteration 39, loss = 0.39419988\n",
      "Iteration 40, loss = 0.38790454\n",
      "Iteration 41, loss = 0.38619794\n",
      "Iteration 42, loss = 0.38024499\n",
      "Iteration 43, loss = 0.37808952\n",
      "Iteration 44, loss = 0.37676049\n",
      "Iteration 45, loss = 0.37554217\n",
      "Iteration 46, loss = 0.37102510\n",
      "Iteration 47, loss = 0.36667670\n",
      "Iteration 48, loss = 0.36735573\n",
      "Iteration 49, loss = 0.35733645\n",
      "Iteration 50, loss = 0.36044434\n",
      "Iteration 51, loss = 0.35608978\n",
      "Iteration 52, loss = 0.35500542\n",
      "Iteration 53, loss = 0.35226664\n",
      "Iteration 54, loss = 0.34711481\n",
      "Iteration 55, loss = 0.34325548\n",
      "Iteration 56, loss = 0.34019610\n",
      "Iteration 57, loss = 0.34236086\n",
      "Iteration 58, loss = 0.33677526\n",
      "Iteration 59, loss = 0.33686716\n",
      "Iteration 60, loss = 0.33836603\n",
      "Iteration 61, loss = 0.32977612\n",
      "Iteration 62, loss = 0.32636361\n",
      "Iteration 63, loss = 0.32560847\n",
      "Iteration 64, loss = 0.32591156\n",
      "Iteration 65, loss = 0.32071282\n",
      "Iteration 66, loss = 0.31370571\n",
      "Iteration 67, loss = 0.31341216\n",
      "Iteration 68, loss = 0.31108460\n",
      "Iteration 69, loss = 0.30894385\n",
      "Iteration 70, loss = 0.30807561\n",
      "Iteration 71, loss = 0.30380857\n",
      "Iteration 72, loss = 0.30447028\n",
      "Iteration 73, loss = 0.30144275\n",
      "Iteration 74, loss = 0.29319039\n",
      "Iteration 75, loss = 0.29472550\n",
      "Iteration 76, loss = 0.29268277\n",
      "Iteration 77, loss = 0.28776702\n",
      "Iteration 78, loss = 0.28780415\n",
      "Iteration 79, loss = 0.28530185\n",
      "Iteration 80, loss = 0.28670074\n",
      "Iteration 81, loss = 0.27900865\n",
      "Iteration 82, loss = 0.27546577\n",
      "Iteration 83, loss = 0.27135114\n",
      "Iteration 84, loss = 0.26755128\n",
      "Iteration 85, loss = 0.26690067\n",
      "Iteration 86, loss = 0.26462614\n",
      "Iteration 87, loss = 0.26215175\n",
      "Iteration 88, loss = 0.25859789\n",
      "Iteration 89, loss = 0.26265645\n",
      "Iteration 90, loss = 0.25388568\n",
      "Iteration 91, loss = 0.25698553\n",
      "Iteration 92, loss = 0.25252920\n",
      "Iteration 93, loss = 0.25147519\n",
      "Iteration 94, loss = 0.24606122\n",
      "Iteration 95, loss = 0.24353120\n",
      "Iteration 96, loss = 0.24309251\n",
      "Iteration 97, loss = 0.24195770\n",
      "Iteration 98, loss = 0.24057362\n",
      "Iteration 99, loss = 0.23766602\n",
      "Iteration 100, loss = 0.23888632\n",
      "Iteration 101, loss = 0.23294290\n",
      "Iteration 102, loss = 0.22983179\n",
      "Iteration 103, loss = 0.23338115\n",
      "Iteration 104, loss = 0.22371233\n",
      "Iteration 105, loss = 0.22572063\n",
      "Iteration 106, loss = 0.22093347\n",
      "Iteration 107, loss = 0.22372380\n",
      "Iteration 108, loss = 0.22389292\n",
      "Iteration 109, loss = 0.21481771\n",
      "Iteration 110, loss = 0.21522918\n",
      "Iteration 111, loss = 0.20906794\n",
      "Iteration 112, loss = 0.21285191\n",
      "Iteration 113, loss = 0.21043398\n",
      "Iteration 114, loss = 0.20783690\n",
      "Iteration 115, loss = 0.20509936\n",
      "Iteration 116, loss = 0.20162376\n",
      "Iteration 117, loss = 0.19929734\n",
      "Iteration 118, loss = 0.20045485\n",
      "Iteration 119, loss = 0.19354813\n",
      "Iteration 120, loss = 0.19422239\n",
      "Iteration 121, loss = 0.19300270\n",
      "Iteration 122, loss = 0.18693662\n",
      "Iteration 123, loss = 0.19415268\n",
      "Iteration 124, loss = 0.18988393\n",
      "Iteration 125, loss = 0.19009177\n",
      "Iteration 126, loss = 0.18760821\n",
      "Iteration 127, loss = 0.18504410\n",
      "Iteration 128, loss = 0.17942153\n",
      "Iteration 129, loss = 0.17740547\n",
      "Iteration 130, loss = 0.17930946\n",
      "Iteration 131, loss = 0.17864401\n",
      "Iteration 132, loss = 0.17681672\n",
      "Iteration 133, loss = 0.17169255\n",
      "Iteration 134, loss = 0.16970199\n",
      "Iteration 135, loss = 0.16724199\n",
      "Iteration 136, loss = 0.16775428\n",
      "Iteration 137, loss = 0.16360196\n",
      "Iteration 138, loss = 0.16740319\n",
      "Iteration 139, loss = 0.16853562\n",
      "Iteration 140, loss = 0.16008440\n",
      "Iteration 141, loss = 0.16555023\n",
      "Iteration 142, loss = 0.15805054\n",
      "Iteration 143, loss = 0.15728386\n",
      "Iteration 144, loss = 0.15022700\n",
      "Iteration 145, loss = 0.15657164\n",
      "Iteration 146, loss = 0.15292232\n",
      "Iteration 147, loss = 0.14995262\n",
      "Iteration 148, loss = 0.14754083\n",
      "Iteration 149, loss = 0.14714123\n",
      "Iteration 150, loss = 0.15669089\n",
      "Iteration 151, loss = 0.14307679\n",
      "Iteration 152, loss = 0.14501762\n",
      "Iteration 153, loss = 0.14414970\n",
      "Iteration 154, loss = 0.13833604\n",
      "Iteration 155, loss = 0.14379278\n",
      "Iteration 156, loss = 0.13201642\n",
      "Iteration 157, loss = 0.14670601\n",
      "Iteration 158, loss = 0.13355366\n",
      "Iteration 159, loss = 0.13421763\n",
      "Iteration 160, loss = 0.13122506\n",
      "Iteration 161, loss = 0.13122560\n",
      "Iteration 162, loss = 0.12935201\n",
      "Iteration 163, loss = 0.12622056\n",
      "Iteration 164, loss = 0.13037874\n",
      "Iteration 165, loss = 0.12577858\n",
      "Iteration 166, loss = 0.12096990\n",
      "Iteration 167, loss = 0.13024150\n",
      "Iteration 168, loss = 0.12198901\n",
      "Iteration 169, loss = 0.12253641\n",
      "Iteration 170, loss = 0.11998006\n",
      "Iteration 171, loss = 0.11492592\n",
      "Iteration 172, loss = 0.11555831\n",
      "Iteration 173, loss = 0.11730395\n",
      "Iteration 174, loss = 0.11582594\n",
      "Iteration 175, loss = 0.11503656\n",
      "Iteration 176, loss = 0.11467595\n",
      "Iteration 177, loss = 0.11299552\n",
      "Iteration 178, loss = 0.10773234\n",
      "Iteration 179, loss = 0.10939086\n",
      "Iteration 180, loss = 0.10753732\n",
      "Iteration 181, loss = 0.10819812\n",
      "Iteration 182, loss = 0.10466006\n",
      "Iteration 183, loss = 0.10464295\n",
      "Iteration 184, loss = 0.10787887\n",
      "Iteration 185, loss = 0.10010838\n",
      "Iteration 186, loss = 0.10574534\n",
      "Iteration 187, loss = 0.10000086\n",
      "Iteration 188, loss = 0.09618893\n",
      "Iteration 189, loss = 0.10617867\n",
      "Iteration 190, loss = 0.09350101\n",
      "Iteration 191, loss = 0.09443127\n",
      "Iteration 192, loss = 0.10296326\n",
      "Iteration 193, loss = 0.10067843\n",
      "Iteration 194, loss = 0.09369128\n",
      "Iteration 195, loss = 0.09066848\n",
      "Iteration 196, loss = 0.08596173\n",
      "Iteration 197, loss = 0.09164396\n",
      "Iteration 198, loss = 0.08470913\n",
      "Iteration 199, loss = 0.08970261\n",
      "Iteration 200, loss = 0.08598466\n",
      "Iteration 201, loss = 0.09195518\n",
      "Iteration 202, loss = 0.08263954\n",
      "Iteration 203, loss = 0.08455364\n",
      "Iteration 204, loss = 0.08208268\n",
      "Iteration 205, loss = 0.08943712\n",
      "Iteration 206, loss = 0.08056095\n",
      "Iteration 207, loss = 0.08139935\n",
      "Iteration 208, loss = 0.07602835\n",
      "Iteration 209, loss = 0.07494894\n",
      "Iteration 210, loss = 0.07984699\n",
      "Iteration 211, loss = 0.08201411\n",
      "Iteration 212, loss = 0.06946545\n",
      "Iteration 213, loss = 0.07741977\n",
      "Iteration 214, loss = 0.07733173\n",
      "Iteration 215, loss = 0.07236739\n",
      "Iteration 216, loss = 0.07953046\n",
      "Iteration 217, loss = 0.07375801\n",
      "Iteration 218, loss = 0.07020966\n",
      "Iteration 219, loss = 0.07285163\n",
      "Iteration 220, loss = 0.07132054\n",
      "Iteration 221, loss = 0.06788504\n",
      "Iteration 222, loss = 0.07762263\n",
      "Iteration 223, loss = 0.07602785\n",
      "Iteration 224, loss = 0.05934248\n",
      "Iteration 225, loss = 0.06437579\n",
      "Iteration 226, loss = 0.06394143\n",
      "Iteration 227, loss = 0.06854906\n",
      "Iteration 228, loss = 0.06492234\n",
      "Iteration 229, loss = 0.06574078\n",
      "Iteration 230, loss = 0.05879503\n",
      "Iteration 231, loss = 0.06610482\n",
      "Iteration 232, loss = 0.08034951\n",
      "Iteration 233, loss = 0.06662839\n",
      "Iteration 234, loss = 0.06316910\n",
      "Iteration 235, loss = 0.06220442\n",
      "Iteration 236, loss = 0.07334737\n",
      "Iteration 237, loss = 0.05851107\n",
      "Iteration 238, loss = 0.06105730\n",
      "Iteration 239, loss = 0.05455518\n",
      "Iteration 240, loss = 0.07142612\n",
      "Iteration 241, loss = 0.06068939\n",
      "Iteration 242, loss = 0.05761678\n",
      "Iteration 243, loss = 0.05712137\n",
      "Iteration 244, loss = 0.04705132\n",
      "Iteration 245, loss = 0.05179242\n",
      "Iteration 246, loss = 0.05476232\n",
      "Iteration 247, loss = 0.05191604\n",
      "Iteration 248, loss = 0.06068963\n",
      "Iteration 249, loss = 0.05477005\n",
      "Iteration 250, loss = 0.05180729\n",
      "Iteration 251, loss = 0.05879562\n",
      "Iteration 252, loss = 0.05872745\n",
      "Iteration 253, loss = 0.04850285\n",
      "Iteration 254, loss = 0.05171160\n",
      "Iteration 255, loss = 0.04249080\n",
      "Iteration 256, loss = 0.05173642\n",
      "Iteration 257, loss = 0.04934122\n",
      "Iteration 258, loss = 0.05960778\n",
      "Iteration 259, loss = 0.04728804\n",
      "Iteration 260, loss = 0.04691372\n",
      "Iteration 261, loss = 0.04495517\n",
      "Iteration 262, loss = 0.04190290\n",
      "Iteration 263, loss = 0.05662823\n",
      "Iteration 264, loss = 0.06490640\n",
      "Iteration 265, loss = 0.05062598\n",
      "Iteration 266, loss = 0.03915269\n",
      "Iteration 267, loss = 0.03481253\n",
      "Iteration 268, loss = 0.03725565\n",
      "Iteration 269, loss = 0.04180749\n",
      "Iteration 270, loss = 0.06623436\n",
      "Iteration 271, loss = 0.07390671\n",
      "Iteration 272, loss = 0.04569618\n",
      "Iteration 273, loss = 0.03639650\n",
      "Iteration 274, loss = 0.03085094\n",
      "Iteration 275, loss = 0.02999870\n",
      "Iteration 276, loss = 0.07283465\n",
      "Iteration 277, loss = 0.04930514\n",
      "Iteration 278, loss = 0.03712321\n",
      "Iteration 279, loss = 0.04582770\n",
      "Iteration 280, loss = 0.02985882\n",
      "Iteration 281, loss = 0.04467107\n",
      "Iteration 282, loss = 0.04001323\n",
      "Iteration 283, loss = 0.03309169\n",
      "Iteration 284, loss = 0.03872821\n",
      "Iteration 285, loss = 0.06174476\n",
      "Iteration 286, loss = 0.04959720\n",
      "Iteration 287, loss = 0.03709341\n",
      "Iteration 288, loss = 0.02981918\n",
      "Iteration 289, loss = 0.03179819\n",
      "Iteration 290, loss = 0.04233378\n",
      "Iteration 291, loss = 0.04436590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58759935\n",
      "Iteration 2, loss = 0.54230615\n",
      "Iteration 3, loss = 0.52394904\n",
      "Iteration 4, loss = 0.51235761\n",
      "Iteration 5, loss = 0.50640384\n",
      "Iteration 6, loss = 0.50025360\n",
      "Iteration 7, loss = 0.49581359\n",
      "Iteration 8, loss = 0.49139136\n",
      "Iteration 9, loss = 0.48716263\n",
      "Iteration 10, loss = 0.48467718\n",
      "Iteration 11, loss = 0.48222650\n",
      "Iteration 12, loss = 0.47968490\n",
      "Iteration 13, loss = 0.47511734\n",
      "Iteration 14, loss = 0.47222150\n",
      "Iteration 15, loss = 0.46946700\n",
      "Iteration 16, loss = 0.46409666\n",
      "Iteration 17, loss = 0.46167391\n",
      "Iteration 18, loss = 0.45930312\n",
      "Iteration 19, loss = 0.45851074\n",
      "Iteration 20, loss = 0.45130800\n",
      "Iteration 21, loss = 0.45011808\n",
      "Iteration 22, loss = 0.44499117\n",
      "Iteration 23, loss = 0.44151240\n",
      "Iteration 24, loss = 0.43970450\n",
      "Iteration 25, loss = 0.43759469\n",
      "Iteration 26, loss = 0.43247552\n",
      "Iteration 27, loss = 0.43012263\n",
      "Iteration 28, loss = 0.42850082\n",
      "Iteration 29, loss = 0.42637600\n",
      "Iteration 30, loss = 0.41896406\n",
      "Iteration 31, loss = 0.41979307\n",
      "Iteration 32, loss = 0.41491092\n",
      "Iteration 33, loss = 0.41236605\n",
      "Iteration 34, loss = 0.40845890\n",
      "Iteration 35, loss = 0.40738046\n",
      "Iteration 36, loss = 0.40370736\n",
      "Iteration 37, loss = 0.40167995\n",
      "Iteration 38, loss = 0.39512081\n",
      "Iteration 39, loss = 0.39614377\n",
      "Iteration 40, loss = 0.39217612\n",
      "Iteration 41, loss = 0.38977288\n",
      "Iteration 42, loss = 0.38630295\n",
      "Iteration 43, loss = 0.38319079\n",
      "Iteration 44, loss = 0.38510489\n",
      "Iteration 45, loss = 0.37819599\n",
      "Iteration 46, loss = 0.37602008\n",
      "Iteration 47, loss = 0.37463418\n",
      "Iteration 48, loss = 0.36812300\n",
      "Iteration 49, loss = 0.37036661\n",
      "Iteration 50, loss = 0.36622734\n",
      "Iteration 51, loss = 0.36661712\n",
      "Iteration 52, loss = 0.36144760\n",
      "Iteration 53, loss = 0.35701541\n",
      "Iteration 54, loss = 0.35584126\n",
      "Iteration 55, loss = 0.35531439\n",
      "Iteration 56, loss = 0.35146620\n",
      "Iteration 57, loss = 0.34753845\n",
      "Iteration 58, loss = 0.34481292\n",
      "Iteration 59, loss = 0.34318670\n",
      "Iteration 60, loss = 0.33912596\n",
      "Iteration 61, loss = 0.34044398\n",
      "Iteration 62, loss = 0.33489339\n",
      "Iteration 63, loss = 0.33359839\n",
      "Iteration 64, loss = 0.32744311\n",
      "Iteration 65, loss = 0.32739822\n",
      "Iteration 66, loss = 0.32536850\n",
      "Iteration 67, loss = 0.32297640\n",
      "Iteration 68, loss = 0.31944447\n",
      "Iteration 69, loss = 0.31582998\n",
      "Iteration 70, loss = 0.31948746\n",
      "Iteration 71, loss = 0.31334254\n",
      "Iteration 72, loss = 0.31265244\n",
      "Iteration 73, loss = 0.30676601\n",
      "Iteration 74, loss = 0.30613272\n",
      "Iteration 75, loss = 0.30464267\n",
      "Iteration 76, loss = 0.30204053\n",
      "Iteration 77, loss = 0.29673158\n",
      "Iteration 78, loss = 0.29718336\n",
      "Iteration 79, loss = 0.29381268\n",
      "Iteration 80, loss = 0.29085606\n",
      "Iteration 81, loss = 0.29031711\n",
      "Iteration 82, loss = 0.28919033\n",
      "Iteration 83, loss = 0.28529602\n",
      "Iteration 84, loss = 0.28405441\n",
      "Iteration 85, loss = 0.27971723\n",
      "Iteration 86, loss = 0.27693259\n",
      "Iteration 87, loss = 0.27484343\n",
      "Iteration 88, loss = 0.27315869\n",
      "Iteration 89, loss = 0.27354632\n",
      "Iteration 90, loss = 0.26488891\n",
      "Iteration 91, loss = 0.26568014\n",
      "Iteration 92, loss = 0.26386620\n",
      "Iteration 93, loss = 0.26470906\n",
      "Iteration 94, loss = 0.25627833\n",
      "Iteration 95, loss = 0.25699240\n",
      "Iteration 96, loss = 0.25607453\n",
      "Iteration 97, loss = 0.25471865\n",
      "Iteration 98, loss = 0.25140336\n",
      "Iteration 99, loss = 0.24481120\n",
      "Iteration 100, loss = 0.24979265\n",
      "Iteration 101, loss = 0.24602342\n",
      "Iteration 102, loss = 0.24161739\n",
      "Iteration 103, loss = 0.24137749\n",
      "Iteration 104, loss = 0.23910395\n",
      "Iteration 105, loss = 0.23813870\n",
      "Iteration 106, loss = 0.23673823\n",
      "Iteration 107, loss = 0.23458463\n",
      "Iteration 108, loss = 0.23031661\n",
      "Iteration 109, loss = 0.22834205\n",
      "Iteration 110, loss = 0.22940433\n",
      "Iteration 111, loss = 0.22527347\n",
      "Iteration 112, loss = 0.22764968\n",
      "Iteration 113, loss = 0.22106979\n",
      "Iteration 114, loss = 0.21867258\n",
      "Iteration 115, loss = 0.21730472\n",
      "Iteration 116, loss = 0.21230765\n",
      "Iteration 117, loss = 0.21251510\n",
      "Iteration 118, loss = 0.20710431\n",
      "Iteration 119, loss = 0.21089342\n",
      "Iteration 120, loss = 0.20930982\n",
      "Iteration 121, loss = 0.20320138\n",
      "Iteration 122, loss = 0.20199163\n",
      "Iteration 123, loss = 0.19900290\n",
      "Iteration 124, loss = 0.20651713\n",
      "Iteration 125, loss = 0.20253786\n",
      "Iteration 126, loss = 0.19464517\n",
      "Iteration 127, loss = 0.19380214\n",
      "Iteration 128, loss = 0.18931890\n",
      "Iteration 129, loss = 0.19203437\n",
      "Iteration 130, loss = 0.18724335\n",
      "Iteration 131, loss = 0.18574888\n",
      "Iteration 132, loss = 0.18346123\n",
      "Iteration 133, loss = 0.18314784\n",
      "Iteration 134, loss = 0.18033261\n",
      "Iteration 135, loss = 0.17956764\n",
      "Iteration 136, loss = 0.17694709\n",
      "Iteration 137, loss = 0.17606462\n",
      "Iteration 138, loss = 0.18237379\n",
      "Iteration 139, loss = 0.17583515\n",
      "Iteration 140, loss = 0.17106033\n",
      "Iteration 141, loss = 0.17232036\n",
      "Iteration 142, loss = 0.16893980\n",
      "Iteration 143, loss = 0.16347740\n",
      "Iteration 144, loss = 0.16940316\n",
      "Iteration 145, loss = 0.16062739\n",
      "Iteration 146, loss = 0.16017239\n",
      "Iteration 147, loss = 0.16606597\n",
      "Iteration 148, loss = 0.15513144\n",
      "Iteration 149, loss = 0.16241381\n",
      "Iteration 150, loss = 0.15914341\n",
      "Iteration 151, loss = 0.15254118\n",
      "Iteration 152, loss = 0.15207588\n",
      "Iteration 153, loss = 0.15545317\n",
      "Iteration 154, loss = 0.15020177\n",
      "Iteration 155, loss = 0.14519237\n",
      "Iteration 156, loss = 0.14593697\n",
      "Iteration 157, loss = 0.14581011\n",
      "Iteration 158, loss = 0.14421833\n",
      "Iteration 159, loss = 0.14326774\n",
      "Iteration 160, loss = 0.14459989\n",
      "Iteration 161, loss = 0.14333805\n",
      "Iteration 162, loss = 0.13616616\n",
      "Iteration 163, loss = 0.13150193\n",
      "Iteration 164, loss = 0.13616117\n",
      "Iteration 165, loss = 0.13485515\n",
      "Iteration 166, loss = 0.13861306\n",
      "Iteration 167, loss = 0.13512218\n",
      "Iteration 168, loss = 0.12423807\n",
      "Iteration 169, loss = 0.12946949\n",
      "Iteration 170, loss = 0.13063601\n",
      "Iteration 171, loss = 0.13241951\n",
      "Iteration 172, loss = 0.13045196\n",
      "Iteration 173, loss = 0.12283815\n",
      "Iteration 174, loss = 0.12532611\n",
      "Iteration 175, loss = 0.12400852\n",
      "Iteration 176, loss = 0.12592189\n",
      "Iteration 177, loss = 0.11796511\n",
      "Iteration 178, loss = 0.11599386\n",
      "Iteration 179, loss = 0.11404497\n",
      "Iteration 180, loss = 0.11573151\n",
      "Iteration 181, loss = 0.11513294\n",
      "Iteration 182, loss = 0.11577138\n",
      "Iteration 183, loss = 0.11439312\n",
      "Iteration 184, loss = 0.11014110\n",
      "Iteration 185, loss = 0.11252798\n",
      "Iteration 186, loss = 0.11234563\n",
      "Iteration 187, loss = 0.10817200\n",
      "Iteration 188, loss = 0.11231161\n",
      "Iteration 189, loss = 0.10310726\n",
      "Iteration 190, loss = 0.10519334\n",
      "Iteration 191, loss = 0.10223321\n",
      "Iteration 192, loss = 0.10609217\n",
      "Iteration 193, loss = 0.10430311\n",
      "Iteration 194, loss = 0.10018270\n",
      "Iteration 195, loss = 0.10416803\n",
      "Iteration 196, loss = 0.09916839\n",
      "Iteration 197, loss = 0.10088068\n",
      "Iteration 198, loss = 0.09586617\n",
      "Iteration 199, loss = 0.09349963\n",
      "Iteration 200, loss = 0.09145777\n",
      "Iteration 201, loss = 0.09531109\n",
      "Iteration 202, loss = 0.09474599\n",
      "Iteration 203, loss = 0.09276044\n",
      "Iteration 204, loss = 0.08767723\n",
      "Iteration 205, loss = 0.09505046\n",
      "Iteration 206, loss = 0.08602277\n",
      "Iteration 207, loss = 0.09137657\n",
      "Iteration 208, loss = 0.08435009\n",
      "Iteration 209, loss = 0.08609310\n",
      "Iteration 210, loss = 0.08965551\n",
      "Iteration 211, loss = 0.08024000\n",
      "Iteration 212, loss = 0.07943659\n",
      "Iteration 213, loss = 0.08675034\n",
      "Iteration 214, loss = 0.08662813\n",
      "Iteration 215, loss = 0.07829002\n",
      "Iteration 216, loss = 0.07960602\n",
      "Iteration 217, loss = 0.07576541\n",
      "Iteration 218, loss = 0.08905645\n",
      "Iteration 219, loss = 0.07838856\n",
      "Iteration 220, loss = 0.07794211\n",
      "Iteration 221, loss = 0.07184824\n",
      "Iteration 222, loss = 0.06859762\n",
      "Iteration 223, loss = 0.07634467\n",
      "Iteration 224, loss = 0.07935867\n",
      "Iteration 225, loss = 0.07388231\n",
      "Iteration 226, loss = 0.06496601\n",
      "Iteration 227, loss = 0.07584635\n",
      "Iteration 228, loss = 0.07671409\n",
      "Iteration 229, loss = 0.06664857\n",
      "Iteration 230, loss = 0.06997678\n",
      "Iteration 231, loss = 0.06896882\n",
      "Iteration 232, loss = 0.06683847\n",
      "Iteration 233, loss = 0.06795700\n",
      "Iteration 234, loss = 0.06934075\n",
      "Iteration 235, loss = 0.06759628\n",
      "Iteration 236, loss = 0.05876238\n",
      "Iteration 237, loss = 0.07212929\n",
      "Iteration 238, loss = 0.06480253\n",
      "Iteration 239, loss = 0.06686988\n",
      "Iteration 240, loss = 0.06031538\n",
      "Iteration 241, loss = 0.05242158\n",
      "Iteration 242, loss = 0.05932446\n",
      "Iteration 243, loss = 0.06421532\n",
      "Iteration 244, loss = 0.07020590\n",
      "Iteration 245, loss = 0.05763505\n",
      "Iteration 246, loss = 0.06292694\n",
      "Iteration 247, loss = 0.06395842\n",
      "Iteration 248, loss = 0.06123102\n",
      "Iteration 249, loss = 0.05454335\n",
      "Iteration 250, loss = 0.05241026\n",
      "Iteration 251, loss = 0.05423539\n",
      "Iteration 252, loss = 0.07117942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57936315\n",
      "Iteration 2, loss = 0.53068618\n",
      "Iteration 3, loss = 0.51437533\n",
      "Iteration 4, loss = 0.50473140\n",
      "Iteration 5, loss = 0.49560522\n",
      "Iteration 6, loss = 0.49119557\n",
      "Iteration 7, loss = 0.48600724\n",
      "Iteration 8, loss = 0.48095555\n",
      "Iteration 9, loss = 0.47577243\n",
      "Iteration 10, loss = 0.47511828\n",
      "Iteration 11, loss = 0.46971096\n",
      "Iteration 12, loss = 0.46790615\n",
      "Iteration 13, loss = 0.46518887\n",
      "Iteration 14, loss = 0.46129611\n",
      "Iteration 15, loss = 0.45748828\n",
      "Iteration 16, loss = 0.45465855\n",
      "Iteration 17, loss = 0.44844575\n",
      "Iteration 18, loss = 0.44612647\n",
      "Iteration 19, loss = 0.44508143\n",
      "Iteration 20, loss = 0.43930412\n",
      "Iteration 21, loss = 0.44029805\n",
      "Iteration 22, loss = 0.43344862\n",
      "Iteration 23, loss = 0.42953404\n",
      "Iteration 24, loss = 0.42871058\n",
      "Iteration 25, loss = 0.42505990\n",
      "Iteration 26, loss = 0.42293761\n",
      "Iteration 27, loss = 0.41784997\n",
      "Iteration 28, loss = 0.41689618\n",
      "Iteration 29, loss = 0.41164530\n",
      "Iteration 30, loss = 0.40996753\n",
      "Iteration 31, loss = 0.40385464\n",
      "Iteration 32, loss = 0.40481584\n",
      "Iteration 33, loss = 0.40083678\n",
      "Iteration 34, loss = 0.39550519\n",
      "Iteration 35, loss = 0.39551089\n",
      "Iteration 36, loss = 0.38823832\n",
      "Iteration 37, loss = 0.38505766\n",
      "Iteration 38, loss = 0.38309929\n",
      "Iteration 39, loss = 0.37707139\n",
      "Iteration 40, loss = 0.37847852\n",
      "Iteration 41, loss = 0.37463140\n",
      "Iteration 42, loss = 0.36782615\n",
      "Iteration 43, loss = 0.36474510\n",
      "Iteration 44, loss = 0.36326137\n",
      "Iteration 45, loss = 0.36100470\n",
      "Iteration 46, loss = 0.35697568\n",
      "Iteration 47, loss = 0.35263954\n",
      "Iteration 48, loss = 0.35145999\n",
      "Iteration 49, loss = 0.34946113\n",
      "Iteration 50, loss = 0.34581889\n",
      "Iteration 51, loss = 0.34171477\n",
      "Iteration 52, loss = 0.33760417\n",
      "Iteration 53, loss = 0.33762658\n",
      "Iteration 54, loss = 0.33155232\n",
      "Iteration 55, loss = 0.33391131\n",
      "Iteration 56, loss = 0.32659999\n",
      "Iteration 57, loss = 0.33100549\n",
      "Iteration 58, loss = 0.32214861\n",
      "Iteration 59, loss = 0.31757821\n",
      "Iteration 60, loss = 0.31598595\n",
      "Iteration 61, loss = 0.30977588\n",
      "Iteration 62, loss = 0.30963563\n",
      "Iteration 63, loss = 0.30842053\n",
      "Iteration 64, loss = 0.30350554\n",
      "Iteration 65, loss = 0.29994938\n",
      "Iteration 66, loss = 0.29717546\n",
      "Iteration 67, loss = 0.29793292\n",
      "Iteration 68, loss = 0.29298880\n",
      "Iteration 69, loss = 0.28756104\n",
      "Iteration 70, loss = 0.28776338\n",
      "Iteration 71, loss = 0.28357470\n",
      "Iteration 72, loss = 0.28168109\n",
      "Iteration 73, loss = 0.27830002\n",
      "Iteration 74, loss = 0.27609265\n",
      "Iteration 75, loss = 0.27378924\n",
      "Iteration 76, loss = 0.27405506\n",
      "Iteration 77, loss = 0.26803259\n",
      "Iteration 78, loss = 0.26959286\n",
      "Iteration 79, loss = 0.26394270\n",
      "Iteration 80, loss = 0.26387186\n",
      "Iteration 81, loss = 0.25866287\n",
      "Iteration 82, loss = 0.25533082\n",
      "Iteration 83, loss = 0.25643460\n",
      "Iteration 84, loss = 0.25175233\n",
      "Iteration 85, loss = 0.25377294\n",
      "Iteration 86, loss = 0.24994240\n",
      "Iteration 87, loss = 0.24626353\n",
      "Iteration 88, loss = 0.24729205\n",
      "Iteration 89, loss = 0.23958413\n",
      "Iteration 90, loss = 0.23855675\n",
      "Iteration 91, loss = 0.23715006\n",
      "Iteration 92, loss = 0.23259886\n",
      "Iteration 93, loss = 0.23153093\n",
      "Iteration 94, loss = 0.22604903\n",
      "Iteration 95, loss = 0.22682127\n",
      "Iteration 96, loss = 0.22379591\n",
      "Iteration 97, loss = 0.22545633\n",
      "Iteration 98, loss = 0.22082532\n",
      "Iteration 99, loss = 0.21504903\n",
      "Iteration 100, loss = 0.21726869\n",
      "Iteration 101, loss = 0.21604790\n",
      "Iteration 102, loss = 0.21471190\n",
      "Iteration 103, loss = 0.21093990\n",
      "Iteration 104, loss = 0.20761318\n",
      "Iteration 105, loss = 0.20512762\n",
      "Iteration 106, loss = 0.20367651\n",
      "Iteration 107, loss = 0.19932467\n",
      "Iteration 108, loss = 0.19910783\n",
      "Iteration 109, loss = 0.19925499\n",
      "Iteration 110, loss = 0.19524004\n",
      "Iteration 111, loss = 0.19162467\n",
      "Iteration 112, loss = 0.19428249\n",
      "Iteration 113, loss = 0.19152432\n",
      "Iteration 114, loss = 0.19070248\n",
      "Iteration 115, loss = 0.18694423\n",
      "Iteration 116, loss = 0.18441105\n",
      "Iteration 117, loss = 0.18175940\n",
      "Iteration 118, loss = 0.17797354\n",
      "Iteration 119, loss = 0.17591148\n",
      "Iteration 120, loss = 0.17759781\n",
      "Iteration 121, loss = 0.17335301\n",
      "Iteration 122, loss = 0.17746817\n",
      "Iteration 123, loss = 0.17299474\n",
      "Iteration 124, loss = 0.16973478\n",
      "Iteration 125, loss = 0.17024231\n",
      "Iteration 126, loss = 0.16371242\n",
      "Iteration 127, loss = 0.16656576\n",
      "Iteration 128, loss = 0.16619005\n",
      "Iteration 129, loss = 0.16643154\n",
      "Iteration 130, loss = 0.16536152\n",
      "Iteration 131, loss = 0.16035250\n",
      "Iteration 132, loss = 0.15745147\n",
      "Iteration 133, loss = 0.15321628\n",
      "Iteration 134, loss = 0.15712907\n",
      "Iteration 135, loss = 0.15255477\n",
      "Iteration 136, loss = 0.15075607\n",
      "Iteration 137, loss = 0.15397846\n",
      "Iteration 138, loss = 0.14786294\n",
      "Iteration 139, loss = 0.14506107\n",
      "Iteration 140, loss = 0.14381815\n",
      "Iteration 141, loss = 0.14446471\n",
      "Iteration 142, loss = 0.14836071\n",
      "Iteration 143, loss = 0.13937694\n",
      "Iteration 144, loss = 0.13428043\n",
      "Iteration 145, loss = 0.13685430\n",
      "Iteration 146, loss = 0.13145469\n",
      "Iteration 147, loss = 0.13395765\n",
      "Iteration 148, loss = 0.13553613\n",
      "Iteration 149, loss = 0.12722973\n",
      "Iteration 150, loss = 0.13168702\n",
      "Iteration 151, loss = 0.12561464\n",
      "Iteration 152, loss = 0.12598929\n",
      "Iteration 153, loss = 0.12489390\n",
      "Iteration 154, loss = 0.11946065\n",
      "Iteration 155, loss = 0.12584075\n",
      "Iteration 156, loss = 0.11909655\n",
      "Iteration 157, loss = 0.11895546\n",
      "Iteration 158, loss = 0.11739485\n",
      "Iteration 159, loss = 0.12053495\n",
      "Iteration 160, loss = 0.11451973\n",
      "Iteration 161, loss = 0.11581735\n",
      "Iteration 162, loss = 0.11285755\n",
      "Iteration 163, loss = 0.10806694\n",
      "Iteration 164, loss = 0.11056125\n",
      "Iteration 165, loss = 0.11154388\n",
      "Iteration 166, loss = 0.10605201\n",
      "Iteration 167, loss = 0.10816296\n",
      "Iteration 168, loss = 0.10212601\n",
      "Iteration 169, loss = 0.10019685\n",
      "Iteration 170, loss = 0.10354103\n",
      "Iteration 171, loss = 0.10272054\n",
      "Iteration 172, loss = 0.10067089\n",
      "Iteration 173, loss = 0.09863314\n",
      "Iteration 174, loss = 0.09742105\n",
      "Iteration 175, loss = 0.09541924\n",
      "Iteration 176, loss = 0.09757918\n",
      "Iteration 177, loss = 0.09208455\n",
      "Iteration 178, loss = 0.09161337\n",
      "Iteration 179, loss = 0.09112358\n",
      "Iteration 180, loss = 0.09189370\n",
      "Iteration 181, loss = 0.08636097\n",
      "Iteration 182, loss = 0.08689267\n",
      "Iteration 183, loss = 0.09398386\n",
      "Iteration 184, loss = 0.08530859\n",
      "Iteration 185, loss = 0.09131368\n",
      "Iteration 186, loss = 0.08170618\n",
      "Iteration 187, loss = 0.08146533\n",
      "Iteration 188, loss = 0.08584012\n",
      "Iteration 189, loss = 0.09050891\n",
      "Iteration 190, loss = 0.08029073\n",
      "Iteration 191, loss = 0.08020100\n",
      "Iteration 192, loss = 0.07765550\n",
      "Iteration 193, loss = 0.08370660\n",
      "Iteration 194, loss = 0.07952975\n",
      "Iteration 195, loss = 0.07656794\n",
      "Iteration 196, loss = 0.07378202\n",
      "Iteration 197, loss = 0.06876889\n",
      "Iteration 198, loss = 0.07210401\n",
      "Iteration 199, loss = 0.07020395\n",
      "Iteration 200, loss = 0.07303336\n",
      "Iteration 201, loss = 0.07191771\n",
      "Iteration 202, loss = 0.06474494\n",
      "Iteration 203, loss = 0.06936984\n",
      "Iteration 204, loss = 0.07091633\n",
      "Iteration 205, loss = 0.07404101\n",
      "Iteration 206, loss = 0.06658210\n",
      "Iteration 207, loss = 0.06723450\n",
      "Iteration 208, loss = 0.05856045\n",
      "Iteration 209, loss = 0.06247197\n",
      "Iteration 210, loss = 0.06358695\n",
      "Iteration 211, loss = 0.06218207\n",
      "Iteration 212, loss = 0.06603514\n",
      "Iteration 213, loss = 0.06369931\n",
      "Iteration 214, loss = 0.06201423\n",
      "Iteration 215, loss = 0.06105280\n",
      "Iteration 216, loss = 0.06061416\n",
      "Iteration 217, loss = 0.05606388\n",
      "Iteration 218, loss = 0.05533663\n",
      "Iteration 219, loss = 0.06345294\n",
      "Iteration 220, loss = 0.06705992\n",
      "Iteration 221, loss = 0.05058645\n",
      "Iteration 222, loss = 0.05872683\n",
      "Iteration 223, loss = 0.05814150\n",
      "Iteration 224, loss = 0.05137504\n",
      "Iteration 225, loss = 0.05352525\n",
      "Iteration 226, loss = 0.05108495\n",
      "Iteration 227, loss = 0.05083962\n",
      "Iteration 228, loss = 0.05228105\n",
      "Iteration 229, loss = 0.06211638\n",
      "Iteration 230, loss = 0.05060929\n",
      "Iteration 231, loss = 0.04733919\n",
      "Iteration 232, loss = 0.05130925\n",
      "Iteration 233, loss = 0.06200201\n",
      "Iteration 234, loss = 0.04591204\n",
      "Iteration 235, loss = 0.04629673\n",
      "Iteration 236, loss = 0.05330077\n",
      "Iteration 237, loss = 0.04824014\n",
      "Iteration 238, loss = 0.05930144\n",
      "Iteration 239, loss = 0.04167372\n",
      "Iteration 240, loss = 0.05787185\n",
      "Iteration 241, loss = 0.04643268\n",
      "Iteration 242, loss = 0.04855054\n",
      "Iteration 243, loss = 0.04164489\n",
      "Iteration 244, loss = 0.04070935\n",
      "Iteration 245, loss = 0.03543096\n",
      "Iteration 246, loss = 0.03568304\n",
      "Iteration 247, loss = 0.05888157\n",
      "Iteration 248, loss = 0.04965179\n",
      "Iteration 249, loss = 0.05477699\n",
      "Iteration 250, loss = 0.03981725\n",
      "Iteration 251, loss = 0.03527787\n",
      "Iteration 252, loss = 0.03776545\n",
      "Iteration 253, loss = 0.05186515\n",
      "Iteration 254, loss = 0.04521642\n",
      "Iteration 255, loss = 0.03997129\n",
      "Iteration 256, loss = 0.03516355\n",
      "Iteration 257, loss = 0.03427036\n",
      "Iteration 258, loss = 0.03841197\n",
      "Iteration 259, loss = 0.06539029\n",
      "Iteration 260, loss = 0.04881964\n",
      "Iteration 261, loss = 0.03818018\n",
      "Iteration 262, loss = 0.02875226\n",
      "Iteration 263, loss = 0.02815655\n",
      "Iteration 264, loss = 0.02874584\n",
      "Iteration 265, loss = 0.03921389\n",
      "Iteration 266, loss = 0.05818075\n",
      "Iteration 267, loss = 0.04110602\n",
      "Iteration 268, loss = 0.03080930\n",
      "Iteration 269, loss = 0.03408913\n",
      "Iteration 270, loss = 0.03461100\n",
      "Iteration 271, loss = 0.03588191\n",
      "Iteration 272, loss = 0.04044298\n",
      "Iteration 273, loss = 0.04131893\n",
      "Iteration 274, loss = 0.03297028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58070506\n",
      "Iteration 2, loss = 0.53120501\n",
      "Iteration 3, loss = 0.51261124\n",
      "Iteration 4, loss = 0.50107728\n",
      "Iteration 5, loss = 0.49309583\n",
      "Iteration 6, loss = 0.49028631\n",
      "Iteration 7, loss = 0.48357691\n",
      "Iteration 8, loss = 0.48119822\n",
      "Iteration 9, loss = 0.47939255\n",
      "Iteration 10, loss = 0.47217726\n",
      "Iteration 11, loss = 0.47046125\n",
      "Iteration 12, loss = 0.46811513\n",
      "Iteration 13, loss = 0.46362687\n",
      "Iteration 14, loss = 0.46084496\n",
      "Iteration 15, loss = 0.45771083\n",
      "Iteration 16, loss = 0.45254186\n",
      "Iteration 17, loss = 0.44896860\n",
      "Iteration 18, loss = 0.44683210\n",
      "Iteration 19, loss = 0.44244807\n",
      "Iteration 20, loss = 0.43880629\n",
      "Iteration 21, loss = 0.43654185\n",
      "Iteration 22, loss = 0.43300987\n",
      "Iteration 23, loss = 0.42894152\n",
      "Iteration 24, loss = 0.42515157\n",
      "Iteration 25, loss = 0.42085849\n",
      "Iteration 26, loss = 0.41759394\n",
      "Iteration 27, loss = 0.41653391\n",
      "Iteration 28, loss = 0.40948658\n",
      "Iteration 29, loss = 0.40601133\n",
      "Iteration 30, loss = 0.40639092\n",
      "Iteration 31, loss = 0.40001125\n",
      "Iteration 32, loss = 0.39577917\n",
      "Iteration 33, loss = 0.39443516\n",
      "Iteration 34, loss = 0.39060101\n",
      "Iteration 35, loss = 0.38534230\n",
      "Iteration 36, loss = 0.38347330\n",
      "Iteration 37, loss = 0.37825440\n",
      "Iteration 38, loss = 0.37344074\n",
      "Iteration 39, loss = 0.37097539\n",
      "Iteration 40, loss = 0.36961502\n",
      "Iteration 41, loss = 0.36787947\n",
      "Iteration 42, loss = 0.36345878\n",
      "Iteration 43, loss = 0.35942079\n",
      "Iteration 44, loss = 0.35621752\n",
      "Iteration 45, loss = 0.35102355\n",
      "Iteration 46, loss = 0.35118860\n",
      "Iteration 47, loss = 0.34672855\n",
      "Iteration 48, loss = 0.34257025\n",
      "Iteration 49, loss = 0.34043494\n",
      "Iteration 50, loss = 0.33575424\n",
      "Iteration 51, loss = 0.33353811\n",
      "Iteration 52, loss = 0.33020145\n",
      "Iteration 53, loss = 0.32765428\n",
      "Iteration 54, loss = 0.32556019\n",
      "Iteration 55, loss = 0.32054224\n",
      "Iteration 56, loss = 0.31833559\n",
      "Iteration 57, loss = 0.31584514\n",
      "Iteration 58, loss = 0.31224274\n",
      "Iteration 59, loss = 0.30812750\n",
      "Iteration 60, loss = 0.30660460\n",
      "Iteration 61, loss = 0.30155074\n",
      "Iteration 62, loss = 0.30012574\n",
      "Iteration 63, loss = 0.29807310\n",
      "Iteration 64, loss = 0.29342033\n",
      "Iteration 65, loss = 0.29409592\n",
      "Iteration 66, loss = 0.28749437\n",
      "Iteration 67, loss = 0.28693984\n",
      "Iteration 68, loss = 0.28651064\n",
      "Iteration 69, loss = 0.28376062\n",
      "Iteration 70, loss = 0.28011063\n",
      "Iteration 71, loss = 0.27675237\n",
      "Iteration 72, loss = 0.27483482\n",
      "Iteration 73, loss = 0.26969363\n",
      "Iteration 74, loss = 0.26742908\n",
      "Iteration 75, loss = 0.26472846\n",
      "Iteration 76, loss = 0.26246598\n",
      "Iteration 77, loss = 0.26383950\n",
      "Iteration 78, loss = 0.25960028\n",
      "Iteration 79, loss = 0.25770170\n",
      "Iteration 80, loss = 0.25402426\n",
      "Iteration 81, loss = 0.24737026\n",
      "Iteration 82, loss = 0.25047750\n",
      "Iteration 83, loss = 0.25107400\n",
      "Iteration 84, loss = 0.24383448\n",
      "Iteration 85, loss = 0.24214268\n",
      "Iteration 86, loss = 0.23752977\n",
      "Iteration 87, loss = 0.23844770\n",
      "Iteration 88, loss = 0.23737216\n",
      "Iteration 89, loss = 0.23087057\n",
      "Iteration 90, loss = 0.23177254\n",
      "Iteration 91, loss = 0.22856780\n",
      "Iteration 92, loss = 0.22359832\n",
      "Iteration 93, loss = 0.22450024\n",
      "Iteration 94, loss = 0.22444858\n",
      "Iteration 95, loss = 0.21530382\n",
      "Iteration 96, loss = 0.21672421\n",
      "Iteration 97, loss = 0.21124191\n",
      "Iteration 98, loss = 0.21820978\n",
      "Iteration 99, loss = 0.21126730\n",
      "Iteration 100, loss = 0.20456857\n",
      "Iteration 101, loss = 0.20429860\n",
      "Iteration 102, loss = 0.20663272\n",
      "Iteration 103, loss = 0.20064029\n",
      "Iteration 104, loss = 0.20111106\n",
      "Iteration 105, loss = 0.19407668\n",
      "Iteration 106, loss = 0.19841963\n",
      "Iteration 107, loss = 0.19203002\n",
      "Iteration 108, loss = 0.18820655\n",
      "Iteration 109, loss = 0.18602016\n",
      "Iteration 110, loss = 0.18744287\n",
      "Iteration 111, loss = 0.18239315\n",
      "Iteration 112, loss = 0.18680391\n",
      "Iteration 113, loss = 0.17935548\n",
      "Iteration 114, loss = 0.17811724\n",
      "Iteration 115, loss = 0.17407771\n",
      "Iteration 116, loss = 0.17394233\n",
      "Iteration 117, loss = 0.17250940\n",
      "Iteration 118, loss = 0.17225330\n",
      "Iteration 119, loss = 0.16878978\n",
      "Iteration 120, loss = 0.16797270\n",
      "Iteration 121, loss = 0.16471327\n",
      "Iteration 122, loss = 0.16268645\n",
      "Iteration 123, loss = 0.15743409\n",
      "Iteration 124, loss = 0.15741455\n",
      "Iteration 125, loss = 0.15961943\n",
      "Iteration 126, loss = 0.15566369\n",
      "Iteration 127, loss = 0.15276995\n",
      "Iteration 128, loss = 0.15136799\n",
      "Iteration 129, loss = 0.15230142\n",
      "Iteration 130, loss = 0.14540327\n",
      "Iteration 131, loss = 0.15145505\n",
      "Iteration 132, loss = 0.14508610\n",
      "Iteration 133, loss = 0.13834393\n",
      "Iteration 134, loss = 0.14386806\n",
      "Iteration 135, loss = 0.13649369\n",
      "Iteration 136, loss = 0.14152209\n",
      "Iteration 137, loss = 0.13242786\n",
      "Iteration 138, loss = 0.13278498\n",
      "Iteration 139, loss = 0.13636631\n",
      "Iteration 140, loss = 0.13228071\n",
      "Iteration 141, loss = 0.13109313\n",
      "Iteration 142, loss = 0.12766303\n",
      "Iteration 143, loss = 0.12614597\n",
      "Iteration 144, loss = 0.12779534\n",
      "Iteration 145, loss = 0.12273715\n",
      "Iteration 146, loss = 0.11957027\n",
      "Iteration 147, loss = 0.12131203\n",
      "Iteration 148, loss = 0.11811219\n",
      "Iteration 149, loss = 0.11755272\n",
      "Iteration 150, loss = 0.11939931\n",
      "Iteration 151, loss = 0.11476850\n",
      "Iteration 152, loss = 0.11428319\n",
      "Iteration 153, loss = 0.10961205\n",
      "Iteration 154, loss = 0.11046849\n",
      "Iteration 155, loss = 0.11138004\n",
      "Iteration 156, loss = 0.11055463\n",
      "Iteration 157, loss = 0.10338057\n",
      "Iteration 158, loss = 0.10876984\n",
      "Iteration 159, loss = 0.10579604\n",
      "Iteration 160, loss = 0.10887765\n",
      "Iteration 161, loss = 0.10099931\n",
      "Iteration 162, loss = 0.09914006\n",
      "Iteration 163, loss = 0.09434158\n",
      "Iteration 164, loss = 0.09884809\n",
      "Iteration 165, loss = 0.09671748\n",
      "Iteration 166, loss = 0.09684474\n",
      "Iteration 167, loss = 0.09178056\n",
      "Iteration 168, loss = 0.09390130\n",
      "Iteration 169, loss = 0.09186818\n",
      "Iteration 170, loss = 0.09322944\n",
      "Iteration 171, loss = 0.09716476\n",
      "Iteration 172, loss = 0.09178868\n",
      "Iteration 173, loss = 0.09207356\n",
      "Iteration 174, loss = 0.08467658\n",
      "Iteration 175, loss = 0.08008963\n",
      "Iteration 176, loss = 0.08621321\n",
      "Iteration 177, loss = 0.08210391\n",
      "Iteration 178, loss = 0.08284554\n",
      "Iteration 179, loss = 0.08450499\n",
      "Iteration 180, loss = 0.07518170\n",
      "Iteration 181, loss = 0.07641483\n",
      "Iteration 182, loss = 0.07939590\n",
      "Iteration 183, loss = 0.07863183\n",
      "Iteration 184, loss = 0.07620979\n",
      "Iteration 185, loss = 0.07370338\n",
      "Iteration 186, loss = 0.06991390\n",
      "Iteration 187, loss = 0.07507423\n",
      "Iteration 188, loss = 0.07252135\n",
      "Iteration 189, loss = 0.07643645\n",
      "Iteration 190, loss = 0.07385389\n",
      "Iteration 191, loss = 0.07176148\n",
      "Iteration 192, loss = 0.07055787\n",
      "Iteration 193, loss = 0.06739279\n",
      "Iteration 194, loss = 0.06584348\n",
      "Iteration 195, loss = 0.06489097\n",
      "Iteration 196, loss = 0.06724140\n",
      "Iteration 197, loss = 0.06415475\n",
      "Iteration 198, loss = 0.06947665\n",
      "Iteration 199, loss = 0.07357914\n",
      "Iteration 200, loss = 0.06635985\n",
      "Iteration 201, loss = 0.05853001\n",
      "Iteration 202, loss = 0.05806160\n",
      "Iteration 203, loss = 0.06386438\n",
      "Iteration 204, loss = 0.05192732\n",
      "Iteration 205, loss = 0.05707188\n",
      "Iteration 206, loss = 0.05519205\n",
      "Iteration 207, loss = 0.06460708\n",
      "Iteration 208, loss = 0.06291127\n",
      "Iteration 209, loss = 0.05202288\n",
      "Iteration 210, loss = 0.05226477\n",
      "Iteration 211, loss = 0.05943771\n",
      "Iteration 212, loss = 0.05332725\n",
      "Iteration 213, loss = 0.05755009\n",
      "Iteration 214, loss = 0.05579145\n",
      "Iteration 215, loss = 0.04855787\n",
      "Iteration 216, loss = 0.05047633\n",
      "Iteration 217, loss = 0.06033440\n",
      "Iteration 218, loss = 0.05278947\n",
      "Iteration 219, loss = 0.05229392\n",
      "Iteration 220, loss = 0.05016061\n",
      "Iteration 221, loss = 0.05018040\n",
      "Iteration 222, loss = 0.04517889\n",
      "Iteration 223, loss = 0.04272798\n",
      "Iteration 224, loss = 0.04856705\n",
      "Iteration 225, loss = 0.04686959\n",
      "Iteration 226, loss = 0.04611805\n",
      "Iteration 227, loss = 0.04449164\n",
      "Iteration 228, loss = 0.05249177\n",
      "Iteration 229, loss = 0.05279025\n",
      "Iteration 230, loss = 0.04677503\n",
      "Iteration 231, loss = 0.04219588\n",
      "Iteration 232, loss = 0.03689175\n",
      "Iteration 233, loss = 0.04859832\n",
      "Iteration 234, loss = 0.05340531\n",
      "Iteration 235, loss = 0.04488131\n",
      "Iteration 236, loss = 0.03685320\n",
      "Iteration 237, loss = 0.03456620\n",
      "Iteration 238, loss = 0.04330747\n",
      "Iteration 239, loss = 0.04691458\n",
      "Iteration 240, loss = 0.03687236\n",
      "Iteration 241, loss = 0.04305220\n",
      "Iteration 242, loss = 0.03638828\n",
      "Iteration 243, loss = 0.04242576\n",
      "Iteration 244, loss = 0.03541397\n",
      "Iteration 245, loss = 0.04752169\n",
      "Iteration 246, loss = 0.03697826\n",
      "Iteration 247, loss = 0.03993558\n",
      "Iteration 248, loss = 0.04172520\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59330752\n",
      "Iteration 2, loss = 0.54195148\n",
      "Iteration 3, loss = 0.52424855\n",
      "Iteration 4, loss = 0.51239430\n",
      "Iteration 5, loss = 0.50751120\n",
      "Iteration 6, loss = 0.50085964\n",
      "Iteration 7, loss = 0.49431894\n",
      "Iteration 8, loss = 0.49191782\n",
      "Iteration 9, loss = 0.48710131\n",
      "Iteration 10, loss = 0.48625153\n",
      "Iteration 11, loss = 0.48178209\n",
      "Iteration 12, loss = 0.47883190\n",
      "Iteration 13, loss = 0.47591209\n",
      "Iteration 14, loss = 0.47474073\n",
      "Iteration 15, loss = 0.46933139\n",
      "Iteration 16, loss = 0.46541139\n",
      "Iteration 17, loss = 0.46274075\n",
      "Iteration 18, loss = 0.45929536\n",
      "Iteration 19, loss = 0.45531554\n",
      "Iteration 20, loss = 0.45478679\n",
      "Iteration 21, loss = 0.45164429\n",
      "Iteration 22, loss = 0.44840292\n",
      "Iteration 23, loss = 0.44461810\n",
      "Iteration 24, loss = 0.44301461\n",
      "Iteration 25, loss = 0.43746878\n",
      "Iteration 26, loss = 0.43569509\n",
      "Iteration 27, loss = 0.43101069\n",
      "Iteration 28, loss = 0.43092591\n",
      "Iteration 29, loss = 0.42492950\n",
      "Iteration 30, loss = 0.42280725\n",
      "Iteration 31, loss = 0.42188608\n",
      "Iteration 32, loss = 0.41612465\n",
      "Iteration 33, loss = 0.41415895\n",
      "Iteration 34, loss = 0.40947095\n",
      "Iteration 35, loss = 0.40974829\n",
      "Iteration 36, loss = 0.40404697\n",
      "Iteration 37, loss = 0.40161263\n",
      "Iteration 38, loss = 0.39839337\n",
      "Iteration 39, loss = 0.39614256\n",
      "Iteration 40, loss = 0.39325720\n",
      "Iteration 41, loss = 0.39323093\n",
      "Iteration 42, loss = 0.38944645\n",
      "Iteration 43, loss = 0.38489532\n",
      "Iteration 44, loss = 0.38028737\n",
      "Iteration 45, loss = 0.37744863\n",
      "Iteration 46, loss = 0.37418667\n",
      "Iteration 47, loss = 0.37365307\n",
      "Iteration 48, loss = 0.36984673\n",
      "Iteration 49, loss = 0.36381609\n",
      "Iteration 50, loss = 0.36283603\n",
      "Iteration 51, loss = 0.36178343\n",
      "Iteration 52, loss = 0.35824512\n",
      "Iteration 53, loss = 0.35683273\n",
      "Iteration 54, loss = 0.35171657\n",
      "Iteration 55, loss = 0.35002484\n",
      "Iteration 56, loss = 0.34756782\n",
      "Iteration 57, loss = 0.34637115\n",
      "Iteration 58, loss = 0.34123866\n",
      "Iteration 59, loss = 0.34043570\n",
      "Iteration 60, loss = 0.33714992\n",
      "Iteration 61, loss = 0.33788814\n",
      "Iteration 62, loss = 0.33005347\n",
      "Iteration 63, loss = 0.32641302\n",
      "Iteration 64, loss = 0.32518650\n",
      "Iteration 65, loss = 0.32460397\n",
      "Iteration 66, loss = 0.31953305\n",
      "Iteration 67, loss = 0.31646393\n",
      "Iteration 68, loss = 0.31427799\n",
      "Iteration 69, loss = 0.31060567\n",
      "Iteration 70, loss = 0.31012986\n",
      "Iteration 71, loss = 0.30949569\n",
      "Iteration 72, loss = 0.30292537\n",
      "Iteration 73, loss = 0.30163500\n",
      "Iteration 74, loss = 0.29798787\n",
      "Iteration 75, loss = 0.29495461\n",
      "Iteration 76, loss = 0.29401918\n",
      "Iteration 77, loss = 0.29144989\n",
      "Iteration 78, loss = 0.28848833\n",
      "Iteration 79, loss = 0.28646721\n",
      "Iteration 80, loss = 0.27979591\n",
      "Iteration 81, loss = 0.27835542\n",
      "Iteration 82, loss = 0.27744800\n",
      "Iteration 83, loss = 0.28066427\n",
      "Iteration 84, loss = 0.27398471\n",
      "Iteration 85, loss = 0.27394554\n",
      "Iteration 86, loss = 0.27096750\n",
      "Iteration 87, loss = 0.26832581\n",
      "Iteration 88, loss = 0.26807744\n",
      "Iteration 89, loss = 0.26480491\n",
      "Iteration 90, loss = 0.26199260\n",
      "Iteration 91, loss = 0.25620106\n",
      "Iteration 92, loss = 0.25647170\n",
      "Iteration 93, loss = 0.25620268\n",
      "Iteration 94, loss = 0.25208290\n",
      "Iteration 95, loss = 0.25031911\n",
      "Iteration 96, loss = 0.24739966\n",
      "Iteration 97, loss = 0.24820486\n",
      "Iteration 98, loss = 0.24436506\n",
      "Iteration 99, loss = 0.24011661\n",
      "Iteration 100, loss = 0.24333470\n",
      "Iteration 101, loss = 0.23574244\n",
      "Iteration 102, loss = 0.23450112\n",
      "Iteration 103, loss = 0.23464485\n",
      "Iteration 104, loss = 0.23032617\n",
      "Iteration 105, loss = 0.22614208\n",
      "Iteration 106, loss = 0.22791581\n",
      "Iteration 107, loss = 0.22415222\n",
      "Iteration 108, loss = 0.22194419\n",
      "Iteration 109, loss = 0.22160880\n",
      "Iteration 110, loss = 0.21766168\n",
      "Iteration 111, loss = 0.21682757\n",
      "Iteration 112, loss = 0.21970506\n",
      "Iteration 113, loss = 0.21135280\n",
      "Iteration 114, loss = 0.21070160\n",
      "Iteration 115, loss = 0.20648238\n",
      "Iteration 116, loss = 0.20578710\n",
      "Iteration 117, loss = 0.20260110\n",
      "Iteration 118, loss = 0.20181362\n",
      "Iteration 119, loss = 0.19949995\n",
      "Iteration 120, loss = 0.19978691\n",
      "Iteration 121, loss = 0.19883108\n",
      "Iteration 122, loss = 0.19314661\n",
      "Iteration 123, loss = 0.19583788\n",
      "Iteration 124, loss = 0.19096810\n",
      "Iteration 125, loss = 0.18791630\n",
      "Iteration 126, loss = 0.18769011\n",
      "Iteration 127, loss = 0.18788734\n",
      "Iteration 128, loss = 0.18713320\n",
      "Iteration 129, loss = 0.17854383\n",
      "Iteration 130, loss = 0.17780137\n",
      "Iteration 131, loss = 0.17534288\n",
      "Iteration 132, loss = 0.17493481\n",
      "Iteration 133, loss = 0.17270544\n",
      "Iteration 134, loss = 0.17041571\n",
      "Iteration 135, loss = 0.16911223\n",
      "Iteration 136, loss = 0.17007151\n",
      "Iteration 137, loss = 0.17202614\n",
      "Iteration 138, loss = 0.16468825\n",
      "Iteration 139, loss = 0.16369942\n",
      "Iteration 140, loss = 0.16283760\n",
      "Iteration 141, loss = 0.15651219\n",
      "Iteration 142, loss = 0.15794411\n",
      "Iteration 143, loss = 0.15855152\n",
      "Iteration 144, loss = 0.15255854\n",
      "Iteration 145, loss = 0.15384959\n",
      "Iteration 146, loss = 0.15402236\n",
      "Iteration 147, loss = 0.15015351\n",
      "Iteration 148, loss = 0.15012759\n",
      "Iteration 149, loss = 0.14443945\n",
      "Iteration 150, loss = 0.14193087\n",
      "Iteration 151, loss = 0.14339004\n",
      "Iteration 152, loss = 0.14007104\n",
      "Iteration 153, loss = 0.13947301\n",
      "Iteration 154, loss = 0.14039873\n",
      "Iteration 155, loss = 0.13809686\n",
      "Iteration 156, loss = 0.13563698\n",
      "Iteration 157, loss = 0.12849155\n",
      "Iteration 158, loss = 0.13009585\n",
      "Iteration 159, loss = 0.12947578\n",
      "Iteration 160, loss = 0.12951954\n",
      "Iteration 161, loss = 0.12583033\n",
      "Iteration 162, loss = 0.12705478\n",
      "Iteration 163, loss = 0.12618622\n",
      "Iteration 164, loss = 0.11814164\n",
      "Iteration 165, loss = 0.12124985\n",
      "Iteration 166, loss = 0.11699460\n",
      "Iteration 167, loss = 0.11915601\n",
      "Iteration 168, loss = 0.11490749\n",
      "Iteration 169, loss = 0.12122507\n",
      "Iteration 170, loss = 0.11067708\n",
      "Iteration 171, loss = 0.11061202\n",
      "Iteration 172, loss = 0.11169541\n",
      "Iteration 173, loss = 0.11356469\n",
      "Iteration 174, loss = 0.10988898\n",
      "Iteration 175, loss = 0.10648714\n",
      "Iteration 176, loss = 0.10454079\n",
      "Iteration 177, loss = 0.10102589\n",
      "Iteration 178, loss = 0.10313903\n",
      "Iteration 179, loss = 0.10190778\n",
      "Iteration 180, loss = 0.09937137\n",
      "Iteration 181, loss = 0.10294075\n",
      "Iteration 182, loss = 0.09960241\n",
      "Iteration 183, loss = 0.09340223\n",
      "Iteration 184, loss = 0.09924139\n",
      "Iteration 185, loss = 0.09222971\n",
      "Iteration 186, loss = 0.09423364\n",
      "Iteration 187, loss = 0.08704079\n",
      "Iteration 188, loss = 0.09397751\n",
      "Iteration 189, loss = 0.08464790\n",
      "Iteration 190, loss = 0.09203938\n",
      "Iteration 191, loss = 0.08548886\n",
      "Iteration 192, loss = 0.09475994\n",
      "Iteration 193, loss = 0.08683636\n",
      "Iteration 194, loss = 0.08647715\n",
      "Iteration 195, loss = 0.08711076\n",
      "Iteration 196, loss = 0.07680716\n",
      "Iteration 197, loss = 0.08769212\n",
      "Iteration 198, loss = 0.07476396\n",
      "Iteration 199, loss = 0.07820000\n",
      "Iteration 200, loss = 0.07260403\n",
      "Iteration 201, loss = 0.07266716\n",
      "Iteration 202, loss = 0.07695327\n",
      "Iteration 203, loss = 0.07905786\n",
      "Iteration 204, loss = 0.07276263\n",
      "Iteration 205, loss = 0.07311613\n",
      "Iteration 206, loss = 0.07006961\n",
      "Iteration 207, loss = 0.07401860\n",
      "Iteration 208, loss = 0.07489722\n",
      "Iteration 209, loss = 0.07226506\n",
      "Iteration 210, loss = 0.06282136\n",
      "Iteration 211, loss = 0.07066537\n",
      "Iteration 212, loss = 0.06181009\n",
      "Iteration 213, loss = 0.06107753\n",
      "Iteration 214, loss = 0.06605630\n",
      "Iteration 215, loss = 0.06262629\n",
      "Iteration 216, loss = 0.06086385\n",
      "Iteration 217, loss = 0.06599319\n",
      "Iteration 218, loss = 0.06346504\n",
      "Iteration 219, loss = 0.06359936\n",
      "Iteration 220, loss = 0.06318763\n",
      "Iteration 221, loss = 0.05622727\n",
      "Iteration 222, loss = 0.05820592\n",
      "Iteration 223, loss = 0.06052058\n",
      "Iteration 224, loss = 0.05838675\n",
      "Iteration 225, loss = 0.05209341\n",
      "Iteration 226, loss = 0.06000251\n",
      "Iteration 227, loss = 0.05570361\n",
      "Iteration 228, loss = 0.06108190\n",
      "Iteration 229, loss = 0.05917190\n",
      "Iteration 230, loss = 0.05007890\n",
      "Iteration 231, loss = 0.04813131\n",
      "Iteration 232, loss = 0.04893814\n",
      "Iteration 233, loss = 0.05745711\n",
      "Iteration 234, loss = 0.05259450\n",
      "Iteration 235, loss = 0.06267603\n",
      "Iteration 236, loss = 0.04384653\n",
      "Iteration 237, loss = 0.04732222\n",
      "Iteration 238, loss = 0.04649265\n",
      "Iteration 239, loss = 0.04367480\n",
      "Iteration 240, loss = 0.05392908\n",
      "Iteration 241, loss = 0.05108947\n",
      "Iteration 242, loss = 0.03959282\n",
      "Iteration 243, loss = 0.04753971\n",
      "Iteration 244, loss = 0.04694073\n",
      "Iteration 245, loss = 0.04119084\n",
      "Iteration 246, loss = 0.04108986\n",
      "Iteration 247, loss = 0.05416898\n",
      "Iteration 248, loss = 0.04750346\n",
      "Iteration 249, loss = 0.04692627\n",
      "Iteration 250, loss = 0.03983286\n",
      "Iteration 251, loss = 0.04211603\n",
      "Iteration 252, loss = 0.03732336\n",
      "Iteration 253, loss = 0.03991574\n",
      "Iteration 254, loss = 0.04949033\n",
      "Iteration 255, loss = 0.05676279\n",
      "Iteration 256, loss = 0.04032241\n",
      "Iteration 257, loss = 0.03522852\n",
      "Iteration 258, loss = 0.03284623\n",
      "Iteration 259, loss = 0.03737286\n",
      "Iteration 260, loss = 0.03836984\n",
      "Iteration 261, loss = 0.03372658\n",
      "Iteration 262, loss = 0.04827580\n",
      "Iteration 263, loss = 0.03708519\n",
      "Iteration 264, loss = 0.03518643\n",
      "Iteration 265, loss = 0.04174750\n",
      "Iteration 266, loss = 0.04124371\n",
      "Iteration 267, loss = 0.03299413\n",
      "Iteration 268, loss = 0.02661834\n",
      "Iteration 269, loss = 0.02643079\n",
      "Iteration 270, loss = 0.03343411\n",
      "Iteration 271, loss = 0.06102565\n",
      "Iteration 272, loss = 0.04423085\n",
      "Iteration 273, loss = 0.03561667\n",
      "Iteration 274, loss = 0.02821110\n",
      "Iteration 275, loss = 0.02554969\n",
      "Iteration 276, loss = 0.03421772\n",
      "Iteration 277, loss = 0.04481170\n",
      "Iteration 278, loss = 0.03334351\n",
      "Iteration 279, loss = 0.04395262\n",
      "Iteration 280, loss = 0.02817834\n",
      "Iteration 281, loss = 0.02431933\n",
      "Iteration 282, loss = 0.02062009\n",
      "Iteration 283, loss = 0.03390839\n",
      "Iteration 284, loss = 0.06046391\n",
      "Iteration 285, loss = 0.04383624\n",
      "Iteration 286, loss = 0.02554502\n",
      "Iteration 287, loss = 0.02512742\n",
      "Iteration 288, loss = 0.02259151\n",
      "Iteration 289, loss = 0.02868979\n",
      "Iteration 290, loss = 0.04299134\n",
      "Iteration 291, loss = 0.03198874\n",
      "Iteration 292, loss = 0.02142431\n",
      "Iteration 293, loss = 0.02838645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59570544\n",
      "Iteration 2, loss = 0.53955001\n",
      "Iteration 3, loss = 0.52487419\n",
      "Iteration 4, loss = 0.51335992\n",
      "Iteration 5, loss = 0.50711430\n",
      "Iteration 6, loss = 0.50178669\n",
      "Iteration 7, loss = 0.49966767\n",
      "Iteration 8, loss = 0.49368659\n",
      "Iteration 9, loss = 0.49031380\n",
      "Iteration 10, loss = 0.48518381\n",
      "Iteration 11, loss = 0.48073186\n",
      "Iteration 12, loss = 0.48038582\n",
      "Iteration 13, loss = 0.47722924\n",
      "Iteration 14, loss = 0.47272588\n",
      "Iteration 15, loss = 0.47036091\n",
      "Iteration 16, loss = 0.46861990\n",
      "Iteration 17, loss = 0.46275119\n",
      "Iteration 18, loss = 0.46276064\n",
      "Iteration 19, loss = 0.45807244\n",
      "Iteration 20, loss = 0.45649012\n",
      "Iteration 21, loss = 0.45160922\n",
      "Iteration 22, loss = 0.44751432\n",
      "Iteration 23, loss = 0.44748632\n",
      "Iteration 24, loss = 0.44055292\n",
      "Iteration 25, loss = 0.43893618\n",
      "Iteration 26, loss = 0.43567060\n",
      "Iteration 27, loss = 0.43494898\n",
      "Iteration 28, loss = 0.42935993\n",
      "Iteration 29, loss = 0.42617565\n",
      "Iteration 30, loss = 0.42429585\n",
      "Iteration 31, loss = 0.41804732\n",
      "Iteration 32, loss = 0.41768408\n",
      "Iteration 33, loss = 0.41364348\n",
      "Iteration 34, loss = 0.41022209\n",
      "Iteration 35, loss = 0.40685432\n",
      "Iteration 36, loss = 0.40397582\n",
      "Iteration 37, loss = 0.39892911\n",
      "Iteration 38, loss = 0.39695331\n",
      "Iteration 39, loss = 0.39515893\n",
      "Iteration 40, loss = 0.38793254\n",
      "Iteration 41, loss = 0.38687878\n",
      "Iteration 42, loss = 0.38164206\n",
      "Iteration 43, loss = 0.38036944\n",
      "Iteration 44, loss = 0.37649757\n",
      "Iteration 45, loss = 0.37588492\n",
      "Iteration 46, loss = 0.37040287\n",
      "Iteration 47, loss = 0.36750375\n",
      "Iteration 48, loss = 0.36439837\n",
      "Iteration 49, loss = 0.36102637\n",
      "Iteration 50, loss = 0.36159364\n",
      "Iteration 51, loss = 0.35256659\n",
      "Iteration 52, loss = 0.35632953\n",
      "Iteration 53, loss = 0.35149776\n",
      "Iteration 54, loss = 0.34915309\n",
      "Iteration 55, loss = 0.34642908\n",
      "Iteration 56, loss = 0.34336025\n",
      "Iteration 57, loss = 0.33823214\n",
      "Iteration 58, loss = 0.34002194\n",
      "Iteration 59, loss = 0.33703729\n",
      "Iteration 60, loss = 0.33350778\n",
      "Iteration 61, loss = 0.32886747\n",
      "Iteration 62, loss = 0.32938214\n",
      "Iteration 63, loss = 0.32688320\n",
      "Iteration 64, loss = 0.32109705\n",
      "Iteration 65, loss = 0.32042553\n",
      "Iteration 66, loss = 0.31516933\n",
      "Iteration 67, loss = 0.31187188\n",
      "Iteration 68, loss = 0.31344592\n",
      "Iteration 69, loss = 0.30744504\n",
      "Iteration 70, loss = 0.30391146\n",
      "Iteration 71, loss = 0.30116307\n",
      "Iteration 72, loss = 0.29858299\n",
      "Iteration 73, loss = 0.29963027\n",
      "Iteration 74, loss = 0.29615100\n",
      "Iteration 75, loss = 0.29258289\n",
      "Iteration 76, loss = 0.29043001\n",
      "Iteration 77, loss = 0.28953764\n",
      "Iteration 78, loss = 0.28591196\n",
      "Iteration 79, loss = 0.28097237\n",
      "Iteration 80, loss = 0.27984997\n",
      "Iteration 81, loss = 0.27675801\n",
      "Iteration 82, loss = 0.27451570\n",
      "Iteration 83, loss = 0.27631318\n",
      "Iteration 84, loss = 0.27230689\n",
      "Iteration 85, loss = 0.26605104\n",
      "Iteration 86, loss = 0.26762674\n",
      "Iteration 87, loss = 0.26772720\n",
      "Iteration 88, loss = 0.26569561\n",
      "Iteration 89, loss = 0.26269645\n",
      "Iteration 90, loss = 0.25816266\n",
      "Iteration 91, loss = 0.25378770\n",
      "Iteration 92, loss = 0.25104713\n",
      "Iteration 93, loss = 0.24854415\n",
      "Iteration 94, loss = 0.25241072\n",
      "Iteration 95, loss = 0.24685250\n",
      "Iteration 96, loss = 0.24626718\n",
      "Iteration 97, loss = 0.24134152\n",
      "Iteration 98, loss = 0.24230198\n",
      "Iteration 99, loss = 0.24162105\n",
      "Iteration 100, loss = 0.23460535\n",
      "Iteration 101, loss = 0.23518739\n",
      "Iteration 102, loss = 0.22945980\n",
      "Iteration 103, loss = 0.22745081\n",
      "Iteration 104, loss = 0.22411626\n",
      "Iteration 105, loss = 0.22886475\n",
      "Iteration 106, loss = 0.22761581\n",
      "Iteration 107, loss = 0.22721169\n",
      "Iteration 108, loss = 0.22125807\n",
      "Iteration 109, loss = 0.21410464\n",
      "Iteration 110, loss = 0.21557867\n",
      "Iteration 111, loss = 0.21427761\n",
      "Iteration 112, loss = 0.21020928\n",
      "Iteration 113, loss = 0.21329632\n",
      "Iteration 114, loss = 0.21052272\n",
      "Iteration 115, loss = 0.20779123\n",
      "Iteration 116, loss = 0.20511050\n",
      "Iteration 117, loss = 0.20344728\n",
      "Iteration 118, loss = 0.20229929\n",
      "Iteration 119, loss = 0.19827518\n",
      "Iteration 120, loss = 0.19703432\n",
      "Iteration 121, loss = 0.19439509\n",
      "Iteration 122, loss = 0.19324364\n",
      "Iteration 123, loss = 0.19214349\n",
      "Iteration 124, loss = 0.19295482\n",
      "Iteration 125, loss = 0.18719226\n",
      "Iteration 126, loss = 0.18622224\n",
      "Iteration 127, loss = 0.18523627\n",
      "Iteration 128, loss = 0.18013054\n",
      "Iteration 129, loss = 0.18047077\n",
      "Iteration 130, loss = 0.17487915\n",
      "Iteration 131, loss = 0.17841093\n",
      "Iteration 132, loss = 0.17887518\n",
      "Iteration 133, loss = 0.17816746\n",
      "Iteration 134, loss = 0.17675067\n",
      "Iteration 135, loss = 0.16967578\n",
      "Iteration 136, loss = 0.16838299\n",
      "Iteration 137, loss = 0.16800143\n",
      "Iteration 138, loss = 0.16790673\n",
      "Iteration 139, loss = 0.15930454\n",
      "Iteration 140, loss = 0.16381571\n",
      "Iteration 141, loss = 0.16390577\n",
      "Iteration 142, loss = 0.16491660\n",
      "Iteration 143, loss = 0.15662352\n",
      "Iteration 144, loss = 0.15487131\n",
      "Iteration 145, loss = 0.15938289\n",
      "Iteration 146, loss = 0.15806350\n",
      "Iteration 147, loss = 0.15261394\n",
      "Iteration 148, loss = 0.15029858\n",
      "Iteration 149, loss = 0.14801661\n",
      "Iteration 150, loss = 0.14542844\n",
      "Iteration 151, loss = 0.14576460\n",
      "Iteration 152, loss = 0.14459164\n",
      "Iteration 153, loss = 0.14207099\n",
      "Iteration 154, loss = 0.13927041\n",
      "Iteration 155, loss = 0.14522157\n",
      "Iteration 156, loss = 0.13864676\n",
      "Iteration 157, loss = 0.14013951\n",
      "Iteration 158, loss = 0.13741085\n",
      "Iteration 159, loss = 0.13932212\n",
      "Iteration 160, loss = 0.13232923\n",
      "Iteration 161, loss = 0.13241566\n",
      "Iteration 162, loss = 0.13331935\n",
      "Iteration 163, loss = 0.12870411\n",
      "Iteration 164, loss = 0.12893103\n",
      "Iteration 165, loss = 0.12760002\n",
      "Iteration 166, loss = 0.12202251\n",
      "Iteration 167, loss = 0.12977585\n",
      "Iteration 168, loss = 0.12431263\n",
      "Iteration 169, loss = 0.12225302\n",
      "Iteration 170, loss = 0.12184812\n",
      "Iteration 171, loss = 0.11619119\n",
      "Iteration 172, loss = 0.12077672\n",
      "Iteration 173, loss = 0.11488048\n",
      "Iteration 174, loss = 0.11723675\n",
      "Iteration 175, loss = 0.11634665\n",
      "Iteration 176, loss = 0.11480342\n",
      "Iteration 177, loss = 0.10897957\n",
      "Iteration 178, loss = 0.11309787\n",
      "Iteration 179, loss = 0.11086078\n",
      "Iteration 180, loss = 0.10678408\n",
      "Iteration 181, loss = 0.10197220\n",
      "Iteration 182, loss = 0.10418110\n",
      "Iteration 183, loss = 0.10800748\n",
      "Iteration 184, loss = 0.10645028\n",
      "Iteration 185, loss = 0.10313318\n",
      "Iteration 186, loss = 0.10349248\n",
      "Iteration 187, loss = 0.10233373\n",
      "Iteration 188, loss = 0.09607055\n",
      "Iteration 189, loss = 0.10393500\n",
      "Iteration 190, loss = 0.09914661\n",
      "Iteration 191, loss = 0.09671088\n",
      "Iteration 192, loss = 0.10126202\n",
      "Iteration 193, loss = 0.09324449\n",
      "Iteration 194, loss = 0.09709356\n",
      "Iteration 195, loss = 0.09301473\n",
      "Iteration 196, loss = 0.09032067\n",
      "Iteration 197, loss = 0.08602540\n",
      "Iteration 198, loss = 0.09595889\n",
      "Iteration 199, loss = 0.08598686\n",
      "Iteration 200, loss = 0.08608344\n",
      "Iteration 201, loss = 0.08988906\n",
      "Iteration 202, loss = 0.08542434\n",
      "Iteration 203, loss = 0.08364384\n",
      "Iteration 204, loss = 0.08042440\n",
      "Iteration 205, loss = 0.08579071\n",
      "Iteration 206, loss = 0.08968691\n",
      "Iteration 207, loss = 0.09286515\n",
      "Iteration 208, loss = 0.07622836\n",
      "Iteration 209, loss = 0.07410467\n",
      "Iteration 210, loss = 0.07934746\n",
      "Iteration 211, loss = 0.07460784\n",
      "Iteration 212, loss = 0.07848453\n",
      "Iteration 213, loss = 0.07641145\n",
      "Iteration 214, loss = 0.08657970\n",
      "Iteration 215, loss = 0.07303807\n",
      "Iteration 216, loss = 0.07165348\n",
      "Iteration 217, loss = 0.07206038\n",
      "Iteration 218, loss = 0.07235552\n",
      "Iteration 219, loss = 0.06518276\n",
      "Iteration 220, loss = 0.07412969\n",
      "Iteration 221, loss = 0.07562307\n",
      "Iteration 222, loss = 0.06925392\n",
      "Iteration 223, loss = 0.06685383\n",
      "Iteration 224, loss = 0.07065632\n",
      "Iteration 225, loss = 0.06540312\n",
      "Iteration 226, loss = 0.06510823\n",
      "Iteration 227, loss = 0.06105550\n",
      "Iteration 228, loss = 0.06813863\n",
      "Iteration 229, loss = 0.06751843\n",
      "Iteration 230, loss = 0.06317964\n",
      "Iteration 231, loss = 0.05856493\n",
      "Iteration 232, loss = 0.05869702\n",
      "Iteration 233, loss = 0.06410496\n",
      "Iteration 234, loss = 0.06186776\n",
      "Iteration 235, loss = 0.06235303\n",
      "Iteration 236, loss = 0.06243021\n",
      "Iteration 237, loss = 0.05660182\n",
      "Iteration 238, loss = 0.06426038\n",
      "Iteration 239, loss = 0.06002139\n",
      "Iteration 240, loss = 0.05899947\n",
      "Iteration 241, loss = 0.05042620\n",
      "Iteration 242, loss = 0.05771680\n",
      "Iteration 243, loss = 0.05920285\n",
      "Iteration 244, loss = 0.05363165\n",
      "Iteration 245, loss = 0.05473100\n",
      "Iteration 246, loss = 0.05678530\n",
      "Iteration 247, loss = 0.05723525\n",
      "Iteration 248, loss = 0.04995117\n",
      "Iteration 249, loss = 0.04894278\n",
      "Iteration 250, loss = 0.05598682\n",
      "Iteration 251, loss = 0.05163247\n",
      "Iteration 252, loss = 0.05197626\n",
      "Iteration 253, loss = 0.04356281\n",
      "Iteration 254, loss = 0.04431753\n",
      "Iteration 255, loss = 0.05485578\n",
      "Iteration 256, loss = 0.05102323\n",
      "Iteration 257, loss = 0.05127917\n",
      "Iteration 258, loss = 0.05151757\n",
      "Iteration 259, loss = 0.05454104\n",
      "Iteration 260, loss = 0.04006607\n",
      "Iteration 261, loss = 0.04592550\n",
      "Iteration 262, loss = 0.06241966\n",
      "Iteration 263, loss = 0.04241642\n",
      "Iteration 264, loss = 0.03953692\n",
      "Iteration 265, loss = 0.03742229\n",
      "Iteration 266, loss = 0.04096025\n",
      "Iteration 267, loss = 0.04668642\n",
      "Iteration 268, loss = 0.04539811\n",
      "Iteration 269, loss = 0.05644876\n",
      "Iteration 270, loss = 0.05561805\n",
      "Iteration 271, loss = 0.04473653\n",
      "Iteration 272, loss = 0.03375988\n",
      "Iteration 273, loss = 0.03296737\n",
      "Iteration 274, loss = 0.03802512\n",
      "Iteration 275, loss = 0.05087183\n",
      "Iteration 276, loss = 0.05397988\n",
      "Iteration 277, loss = 0.04474229\n",
      "Iteration 278, loss = 0.03705751\n",
      "Iteration 279, loss = 0.03721862\n",
      "Iteration 280, loss = 0.04018756\n",
      "Iteration 281, loss = 0.03382570\n",
      "Iteration 282, loss = 0.04314999\n",
      "Iteration 283, loss = 0.04160857\n",
      "Iteration 284, loss = 0.03269702\n",
      "Iteration 285, loss = 0.03667632\n",
      "Iteration 286, loss = 0.05604333\n",
      "Iteration 287, loss = 0.04569538\n",
      "Iteration 288, loss = 0.03180290\n",
      "Iteration 289, loss = 0.06112726\n",
      "Iteration 290, loss = 0.03564098\n",
      "Iteration 291, loss = 0.02822275\n",
      "Iteration 292, loss = 0.02415064\n",
      "Iteration 293, loss = 0.02766991\n",
      "Iteration 294, loss = 0.07248853\n",
      "Iteration 295, loss = 0.04552829\n",
      "Iteration 296, loss = 0.03008910\n",
      "Iteration 297, loss = 0.02395885\n",
      "Iteration 298, loss = 0.02134595\n",
      "Iteration 299, loss = 0.03685031\n",
      "Iteration 300, loss = 0.06722707\n",
      "Iteration 301, loss = 0.03612066\n",
      "Iteration 302, loss = 0.03344646\n",
      "Iteration 303, loss = 0.02306326\n",
      "Iteration 304, loss = 0.02142671\n",
      "Iteration 305, loss = 0.02997114\n",
      "Iteration 306, loss = 0.06836012\n",
      "Iteration 307, loss = 0.04009218\n",
      "Iteration 308, loss = 0.02413952\n",
      "Iteration 309, loss = 0.02088659\n",
      "Iteration 310, loss = 0.02326627\n",
      "Iteration 311, loss = 0.05361347\n",
      "Iteration 312, loss = 0.06461242\n",
      "Iteration 313, loss = 0.02763499\n",
      "Iteration 314, loss = 0.02029667\n",
      "Iteration 315, loss = 0.01903190\n",
      "Iteration 316, loss = 0.01803563\n",
      "Iteration 317, loss = 0.04921294\n",
      "Iteration 318, loss = 0.05643083\n",
      "Iteration 319, loss = 0.02637882\n",
      "Iteration 320, loss = 0.02720699\n",
      "Iteration 321, loss = 0.02275920\n",
      "Iteration 322, loss = 0.01794018\n",
      "Iteration 323, loss = 0.01943329\n",
      "Iteration 324, loss = 0.05671045\n",
      "Iteration 325, loss = 0.06643824\n",
      "Iteration 326, loss = 0.02861797\n",
      "Iteration 327, loss = 0.02259572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58361673\n",
      "Iteration 2, loss = 0.53270764\n",
      "Iteration 3, loss = 0.51608164\n",
      "Iteration 4, loss = 0.50466630\n",
      "Iteration 5, loss = 0.50137628\n",
      "Iteration 6, loss = 0.49471593\n",
      "Iteration 7, loss = 0.48982377\n",
      "Iteration 8, loss = 0.48861797\n",
      "Iteration 9, loss = 0.48307962\n",
      "Iteration 10, loss = 0.48027424\n",
      "Iteration 11, loss = 0.47759204\n",
      "Iteration 12, loss = 0.47129786\n",
      "Iteration 13, loss = 0.46768199\n",
      "Iteration 14, loss = 0.46789987\n",
      "Iteration 15, loss = 0.46252283\n",
      "Iteration 16, loss = 0.45980849\n",
      "Iteration 17, loss = 0.45697022\n",
      "Iteration 18, loss = 0.45305780\n",
      "Iteration 19, loss = 0.45030846\n",
      "Iteration 20, loss = 0.44913926\n",
      "Iteration 21, loss = 0.44309030\n",
      "Iteration 22, loss = 0.44154690\n",
      "Iteration 23, loss = 0.43889852\n",
      "Iteration 24, loss = 0.43208984\n",
      "Iteration 25, loss = 0.42999757\n",
      "Iteration 26, loss = 0.42570134\n",
      "Iteration 27, loss = 0.42776554\n",
      "Iteration 28, loss = 0.42082455\n",
      "Iteration 29, loss = 0.41807522\n",
      "Iteration 30, loss = 0.41353360\n",
      "Iteration 31, loss = 0.41167801\n",
      "Iteration 32, loss = 0.40965068\n",
      "Iteration 33, loss = 0.40654608\n",
      "Iteration 34, loss = 0.40384614\n",
      "Iteration 35, loss = 0.39800075\n",
      "Iteration 36, loss = 0.39333302\n",
      "Iteration 37, loss = 0.39136807\n",
      "Iteration 38, loss = 0.39061110\n",
      "Iteration 39, loss = 0.38453253\n",
      "Iteration 40, loss = 0.38403299\n",
      "Iteration 41, loss = 0.37878797\n",
      "Iteration 42, loss = 0.37947949\n",
      "Iteration 43, loss = 0.37077712\n",
      "Iteration 44, loss = 0.37255350\n",
      "Iteration 45, loss = 0.37007563\n",
      "Iteration 46, loss = 0.36668562\n",
      "Iteration 47, loss = 0.36104982\n",
      "Iteration 48, loss = 0.36167285\n",
      "Iteration 49, loss = 0.35793861\n",
      "Iteration 50, loss = 0.35637680\n",
      "Iteration 51, loss = 0.35093364\n",
      "Iteration 52, loss = 0.35059777\n",
      "Iteration 53, loss = 0.34674338\n",
      "Iteration 54, loss = 0.34322821\n",
      "Iteration 55, loss = 0.33986196\n",
      "Iteration 56, loss = 0.33731259\n",
      "Iteration 57, loss = 0.33642011\n",
      "Iteration 58, loss = 0.33164272\n",
      "Iteration 59, loss = 0.33295542\n",
      "Iteration 60, loss = 0.32851596\n",
      "Iteration 61, loss = 0.32534302\n",
      "Iteration 62, loss = 0.32131591\n",
      "Iteration 63, loss = 0.31912709\n",
      "Iteration 64, loss = 0.32239467\n",
      "Iteration 65, loss = 0.31390399\n",
      "Iteration 66, loss = 0.31350108\n",
      "Iteration 67, loss = 0.30832746\n",
      "Iteration 68, loss = 0.30899163\n",
      "Iteration 69, loss = 0.30627212\n",
      "Iteration 70, loss = 0.30189520\n",
      "Iteration 71, loss = 0.30055850\n",
      "Iteration 72, loss = 0.29766886\n",
      "Iteration 73, loss = 0.29655209\n",
      "Iteration 74, loss = 0.29317115\n",
      "Iteration 75, loss = 0.28741791\n",
      "Iteration 76, loss = 0.28927983\n",
      "Iteration 77, loss = 0.28495843\n",
      "Iteration 78, loss = 0.28019158\n",
      "Iteration 79, loss = 0.28048558\n",
      "Iteration 80, loss = 0.27822723\n",
      "Iteration 81, loss = 0.27879142\n",
      "Iteration 82, loss = 0.27130653\n",
      "Iteration 83, loss = 0.27077055\n",
      "Iteration 84, loss = 0.26359677\n",
      "Iteration 85, loss = 0.26414248\n",
      "Iteration 86, loss = 0.26235769\n",
      "Iteration 87, loss = 0.25701343\n",
      "Iteration 88, loss = 0.25556718\n",
      "Iteration 89, loss = 0.25398176\n",
      "Iteration 90, loss = 0.25306947\n",
      "Iteration 91, loss = 0.24842454\n",
      "Iteration 92, loss = 0.24701996\n",
      "Iteration 93, loss = 0.24266875\n",
      "Iteration 94, loss = 0.24573731\n",
      "Iteration 95, loss = 0.24176847\n",
      "Iteration 96, loss = 0.23555154\n",
      "Iteration 97, loss = 0.23590385\n",
      "Iteration 98, loss = 0.23400834\n",
      "Iteration 99, loss = 0.22807793\n",
      "Iteration 100, loss = 0.23306859\n",
      "Iteration 101, loss = 0.23033562\n",
      "Iteration 102, loss = 0.22704609\n",
      "Iteration 103, loss = 0.22023520\n",
      "Iteration 104, loss = 0.21856821\n",
      "Iteration 105, loss = 0.21749780\n",
      "Iteration 106, loss = 0.21780840\n",
      "Iteration 107, loss = 0.21035223\n",
      "Iteration 108, loss = 0.21207263\n",
      "Iteration 109, loss = 0.20980059\n",
      "Iteration 110, loss = 0.20827320\n",
      "Iteration 111, loss = 0.20715449\n",
      "Iteration 112, loss = 0.20029676\n",
      "Iteration 113, loss = 0.20185135\n",
      "Iteration 114, loss = 0.20259628\n",
      "Iteration 115, loss = 0.19510335\n",
      "Iteration 116, loss = 0.19786075\n",
      "Iteration 117, loss = 0.19698450\n",
      "Iteration 118, loss = 0.19080795\n",
      "Iteration 119, loss = 0.18663531\n",
      "Iteration 120, loss = 0.19094631\n",
      "Iteration 121, loss = 0.18770973\n",
      "Iteration 122, loss = 0.18435349\n",
      "Iteration 123, loss = 0.18494200\n",
      "Iteration 124, loss = 0.18181936\n",
      "Iteration 125, loss = 0.17951632\n",
      "Iteration 126, loss = 0.17362781\n",
      "Iteration 127, loss = 0.17276285\n",
      "Iteration 128, loss = 0.17031035\n",
      "Iteration 129, loss = 0.16575239\n",
      "Iteration 130, loss = 0.16909115\n",
      "Iteration 131, loss = 0.17056257\n",
      "Iteration 132, loss = 0.16416260\n",
      "Iteration 133, loss = 0.16117884\n",
      "Iteration 134, loss = 0.15446194\n",
      "Iteration 135, loss = 0.16110939\n",
      "Iteration 136, loss = 0.15912689\n",
      "Iteration 137, loss = 0.15524779\n",
      "Iteration 138, loss = 0.15484965\n",
      "Iteration 139, loss = 0.14922966\n",
      "Iteration 140, loss = 0.15229398\n",
      "Iteration 141, loss = 0.14549765\n",
      "Iteration 142, loss = 0.14142706\n",
      "Iteration 143, loss = 0.14354458\n",
      "Iteration 144, loss = 0.14455357\n",
      "Iteration 145, loss = 0.13698956\n",
      "Iteration 146, loss = 0.13553111\n",
      "Iteration 147, loss = 0.13883140\n",
      "Iteration 148, loss = 0.13240532\n",
      "Iteration 149, loss = 0.13260431\n",
      "Iteration 150, loss = 0.13608720\n",
      "Iteration 151, loss = 0.12861858\n",
      "Iteration 152, loss = 0.13134731\n",
      "Iteration 153, loss = 0.13323405\n",
      "Iteration 154, loss = 0.12433346\n",
      "Iteration 155, loss = 0.12438594\n",
      "Iteration 156, loss = 0.12499936\n",
      "Iteration 157, loss = 0.12241749\n",
      "Iteration 158, loss = 0.12318050\n",
      "Iteration 159, loss = 0.11752899\n",
      "Iteration 160, loss = 0.11978212\n",
      "Iteration 161, loss = 0.11458541\n",
      "Iteration 162, loss = 0.11607689\n",
      "Iteration 163, loss = 0.11494448\n",
      "Iteration 164, loss = 0.11126714\n",
      "Iteration 165, loss = 0.10627955\n",
      "Iteration 166, loss = 0.10725569\n",
      "Iteration 167, loss = 0.10680740\n",
      "Iteration 168, loss = 0.10993226\n",
      "Iteration 169, loss = 0.10686553\n",
      "Iteration 170, loss = 0.10436842\n",
      "Iteration 171, loss = 0.10413393\n",
      "Iteration 172, loss = 0.10169367\n",
      "Iteration 173, loss = 0.09657000\n",
      "Iteration 174, loss = 0.09757619\n",
      "Iteration 175, loss = 0.10153542\n",
      "Iteration 176, loss = 0.09987654\n",
      "Iteration 177, loss = 0.09078277\n",
      "Iteration 178, loss = 0.09549135\n",
      "Iteration 179, loss = 0.09703812\n",
      "Iteration 180, loss = 0.09234427\n",
      "Iteration 181, loss = 0.09230187\n",
      "Iteration 182, loss = 0.08957162\n",
      "Iteration 183, loss = 0.08795524\n",
      "Iteration 184, loss = 0.08707482\n",
      "Iteration 185, loss = 0.09411768\n",
      "Iteration 186, loss = 0.08632459\n",
      "Iteration 187, loss = 0.08890056\n",
      "Iteration 188, loss = 0.07852973\n",
      "Iteration 189, loss = 0.08074772\n",
      "Iteration 190, loss = 0.08115460\n",
      "Iteration 191, loss = 0.08297607\n",
      "Iteration 192, loss = 0.07986829\n",
      "Iteration 193, loss = 0.08561718\n",
      "Iteration 194, loss = 0.07829440\n",
      "Iteration 195, loss = 0.07420949\n",
      "Iteration 196, loss = 0.07463728\n",
      "Iteration 197, loss = 0.07313783\n",
      "Iteration 198, loss = 0.07399586\n",
      "Iteration 199, loss = 0.08181431\n",
      "Iteration 200, loss = 0.07031936\n",
      "Iteration 201, loss = 0.07113394\n",
      "Iteration 202, loss = 0.07067199\n",
      "Iteration 203, loss = 0.07369191\n",
      "Iteration 204, loss = 0.07269161\n",
      "Iteration 205, loss = 0.06967184\n",
      "Iteration 206, loss = 0.06435965\n",
      "Iteration 207, loss = 0.06040781\n",
      "Iteration 208, loss = 0.06799990\n",
      "Iteration 209, loss = 0.07120604\n",
      "Iteration 210, loss = 0.06859206\n",
      "Iteration 211, loss = 0.07379886\n",
      "Iteration 212, loss = 0.05801546\n",
      "Iteration 213, loss = 0.06275383\n",
      "Iteration 214, loss = 0.06219254\n",
      "Iteration 215, loss = 0.06111429\n",
      "Iteration 216, loss = 0.06676117\n",
      "Iteration 217, loss = 0.06518015\n",
      "Iteration 218, loss = 0.05808753\n",
      "Iteration 219, loss = 0.05505594\n",
      "Iteration 220, loss = 0.05539877\n",
      "Iteration 221, loss = 0.05163886\n",
      "Iteration 222, loss = 0.06643566\n",
      "Iteration 223, loss = 0.06536271\n",
      "Iteration 224, loss = 0.04878072\n",
      "Iteration 225, loss = 0.05053934\n",
      "Iteration 226, loss = 0.06548975\n",
      "Iteration 227, loss = 0.05532704\n",
      "Iteration 228, loss = 0.05199270\n",
      "Iteration 229, loss = 0.05531178\n",
      "Iteration 230, loss = 0.05815566\n",
      "Iteration 231, loss = 0.05141119\n",
      "Iteration 232, loss = 0.04987014\n",
      "Iteration 233, loss = 0.04533303\n",
      "Iteration 234, loss = 0.04855625\n",
      "Iteration 235, loss = 0.05231244\n",
      "Iteration 236, loss = 0.05272112\n",
      "Iteration 237, loss = 0.04276229\n",
      "Iteration 238, loss = 0.04487631\n",
      "Iteration 239, loss = 0.05313750\n",
      "Iteration 240, loss = 0.05984204\n",
      "Iteration 241, loss = 0.04686016\n",
      "Iteration 242, loss = 0.03596972\n",
      "Iteration 243, loss = 0.04913462\n",
      "Iteration 244, loss = 0.04746345\n",
      "Iteration 245, loss = 0.05587507\n",
      "Iteration 246, loss = 0.05231889\n",
      "Iteration 247, loss = 0.04461805\n",
      "Iteration 248, loss = 0.04116935\n",
      "Iteration 249, loss = 0.03511141\n",
      "Iteration 250, loss = 0.05195770\n",
      "Iteration 251, loss = 0.07422422\n",
      "Iteration 252, loss = 0.03516103\n",
      "Iteration 253, loss = 0.03418097\n",
      "Iteration 254, loss = 0.04031231\n",
      "Iteration 255, loss = 0.03695121\n",
      "Iteration 256, loss = 0.03900851\n",
      "Iteration 257, loss = 0.05215667\n",
      "Iteration 258, loss = 0.04720372\n",
      "Iteration 259, loss = 0.04568552\n",
      "Iteration 260, loss = 0.03177555\n",
      "Iteration 261, loss = 0.03601967\n",
      "Iteration 262, loss = 0.04196826\n",
      "Iteration 263, loss = 0.03328918\n",
      "Iteration 264, loss = 0.03524782\n",
      "Iteration 265, loss = 0.03966573\n",
      "Iteration 266, loss = 0.09058521\n",
      "Iteration 267, loss = 0.04232652\n",
      "Iteration 268, loss = 0.03149982\n",
      "Iteration 269, loss = 0.02657139\n",
      "Iteration 270, loss = 0.02683504\n",
      "Iteration 271, loss = 0.04485298\n",
      "Iteration 272, loss = 0.06745056\n",
      "Iteration 273, loss = 0.04517892\n",
      "Iteration 274, loss = 0.02810808\n",
      "Iteration 275, loss = 0.02537509\n",
      "Iteration 276, loss = 0.02461836\n",
      "Iteration 277, loss = 0.03239562\n",
      "Iteration 278, loss = 0.06098157\n",
      "Iteration 279, loss = 0.04787987\n",
      "Iteration 280, loss = 0.03092268\n",
      "Iteration 281, loss = 0.02821242\n",
      "Iteration 282, loss = 0.02316124\n",
      "Iteration 283, loss = 0.02170586\n",
      "Iteration 284, loss = 0.04689295\n",
      "Iteration 285, loss = 0.06135946\n",
      "Iteration 286, loss = 0.04311387\n",
      "Iteration 287, loss = 0.02424033\n",
      "Iteration 288, loss = 0.02221707\n",
      "Iteration 289, loss = 0.02052543\n",
      "Iteration 290, loss = 0.02385730\n",
      "Iteration 291, loss = 0.08296804\n",
      "Iteration 292, loss = 0.04043050\n",
      "Iteration 293, loss = 0.02513373\n",
      "Iteration 294, loss = 0.02002865\n",
      "Iteration 295, loss = 0.01803964\n",
      "Iteration 296, loss = 0.01968224\n",
      "Iteration 297, loss = 0.05880068\n",
      "Iteration 298, loss = 0.07018266\n",
      "Iteration 299, loss = 0.03266451\n",
      "Iteration 300, loss = 0.02042728\n",
      "Iteration 301, loss = 0.02268579\n",
      "Iteration 302, loss = 0.01761173\n",
      "Iteration 303, loss = 0.01701340\n",
      "Iteration 304, loss = 0.04045409\n",
      "Iteration 305, loss = 0.09386423\n",
      "Iteration 306, loss = 0.02803588\n",
      "Iteration 307, loss = 0.01811535\n",
      "Iteration 308, loss = 0.01676349\n",
      "Iteration 309, loss = 0.01616817\n",
      "Iteration 310, loss = 0.01763824\n",
      "Iteration 311, loss = 0.03523624\n",
      "Iteration 312, loss = 0.07723821\n",
      "Iteration 313, loss = 0.07785691\n",
      "Iteration 314, loss = 0.03563758\n",
      "Iteration 315, loss = 0.02190847\n",
      "Iteration 316, loss = 0.01587173\n",
      "Iteration 317, loss = 0.01588001\n",
      "Iteration 318, loss = 0.01487736\n",
      "Iteration 319, loss = 0.01496896\n",
      "Iteration 320, loss = 0.02086686\n",
      "Iteration 321, loss = 0.09021829\n",
      "Iteration 322, loss = 0.04433901\n",
      "Iteration 323, loss = 0.02261990\n",
      "Iteration 324, loss = 0.02204485\n",
      "Iteration 325, loss = 0.01871713\n",
      "Iteration 326, loss = 0.01469255\n",
      "Iteration 327, loss = 0.01397750\n",
      "Iteration 328, loss = 0.01565000\n",
      "Iteration 329, loss = 0.05187986\n",
      "Iteration 330, loss = 0.09014487\n",
      "Iteration 331, loss = 0.03500052\n",
      "Iteration 332, loss = 0.01884966\n",
      "Iteration 333, loss = 0.01454963\n",
      "Iteration 334, loss = 0.01367940\n",
      "Iteration 335, loss = 0.01448561\n",
      "Iteration 336, loss = 0.01416114\n",
      "Iteration 337, loss = 0.01446977\n",
      "Iteration 338, loss = 0.04506253\n",
      "Iteration 339, loss = 0.13556453\n",
      "Iteration 340, loss = 0.02949833\n",
      "Iteration 341, loss = 0.01650389\n",
      "Iteration 342, loss = 0.01445805\n",
      "Iteration 343, loss = 0.01347065\n",
      "Iteration 344, loss = 0.01336520\n",
      "Iteration 345, loss = 0.01373251\n",
      "Iteration 346, loss = 0.02825926\n",
      "Iteration 347, loss = 0.09270020\n",
      "Iteration 348, loss = 0.03362159\n",
      "Iteration 349, loss = 0.01663580\n",
      "Iteration 350, loss = 0.01338797\n",
      "Iteration 351, loss = 0.01274244\n",
      "Iteration 352, loss = 0.01290773\n",
      "Iteration 353, loss = 0.01280814\n",
      "Iteration 354, loss = 0.02083611\n",
      "Iteration 355, loss = 0.12660976\n",
      "Iteration 356, loss = 0.04384378\n",
      "Iteration 357, loss = 0.01687105\n",
      "Iteration 358, loss = 0.01417925\n",
      "Iteration 359, loss = 0.01294891\n",
      "Iteration 360, loss = 0.01262943\n",
      "Iteration 361, loss = 0.01224147\n",
      "Iteration 362, loss = 0.01260858\n",
      "Iteration 363, loss = 0.01215333\n",
      "Iteration 364, loss = 0.01355870\n",
      "Iteration 365, loss = 0.12951370\n",
      "Iteration 366, loss = 0.04107067\n",
      "Iteration 367, loss = 0.02436914\n",
      "Iteration 368, loss = 0.01359599\n",
      "Iteration 369, loss = 0.01250778\n",
      "Iteration 370, loss = 0.01216886\n",
      "Iteration 371, loss = 0.01217223\n",
      "Iteration 372, loss = 0.01212039\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58865865\n",
      "Iteration 2, loss = 0.53640914\n",
      "Iteration 3, loss = 0.51815220\n",
      "Iteration 4, loss = 0.51291474\n",
      "Iteration 5, loss = 0.50425226\n",
      "Iteration 6, loss = 0.49900846\n",
      "Iteration 7, loss = 0.49454583\n",
      "Iteration 8, loss = 0.49229422\n",
      "Iteration 9, loss = 0.48972478\n",
      "Iteration 10, loss = 0.48548084\n",
      "Iteration 11, loss = 0.48102582\n",
      "Iteration 12, loss = 0.47882500\n",
      "Iteration 13, loss = 0.47419441\n",
      "Iteration 14, loss = 0.47271619\n",
      "Iteration 15, loss = 0.46927341\n",
      "Iteration 16, loss = 0.46591670\n",
      "Iteration 17, loss = 0.46133052\n",
      "Iteration 18, loss = 0.45924503\n",
      "Iteration 19, loss = 0.45677726\n",
      "Iteration 20, loss = 0.45338948\n",
      "Iteration 21, loss = 0.44941409\n",
      "Iteration 22, loss = 0.44436382\n",
      "Iteration 23, loss = 0.44343668\n",
      "Iteration 24, loss = 0.44250690\n",
      "Iteration 25, loss = 0.43665170\n",
      "Iteration 26, loss = 0.43488902\n",
      "Iteration 27, loss = 0.43499549\n",
      "Iteration 28, loss = 0.42874775\n",
      "Iteration 29, loss = 0.42464049\n",
      "Iteration 30, loss = 0.42399626\n",
      "Iteration 31, loss = 0.41903109\n",
      "Iteration 32, loss = 0.41917579\n",
      "Iteration 33, loss = 0.41257335\n",
      "Iteration 34, loss = 0.41222245\n",
      "Iteration 35, loss = 0.40509136\n",
      "Iteration 36, loss = 0.40527756\n",
      "Iteration 37, loss = 0.40392193\n",
      "Iteration 38, loss = 0.40018705\n",
      "Iteration 39, loss = 0.39500081\n",
      "Iteration 40, loss = 0.39411554\n",
      "Iteration 41, loss = 0.38872372\n",
      "Iteration 42, loss = 0.39004904\n",
      "Iteration 43, loss = 0.38701381\n",
      "Iteration 44, loss = 0.38569265\n",
      "Iteration 45, loss = 0.37696255\n",
      "Iteration 46, loss = 0.37858754\n",
      "Iteration 47, loss = 0.37480089\n",
      "Iteration 48, loss = 0.37284799\n",
      "Iteration 49, loss = 0.37144584\n",
      "Iteration 50, loss = 0.36815541\n",
      "Iteration 51, loss = 0.36188371\n",
      "Iteration 52, loss = 0.36020562\n",
      "Iteration 53, loss = 0.35968037\n",
      "Iteration 54, loss = 0.35558210\n",
      "Iteration 55, loss = 0.35186497\n",
      "Iteration 56, loss = 0.35348685\n",
      "Iteration 57, loss = 0.34811046\n",
      "Iteration 58, loss = 0.34692729\n",
      "Iteration 59, loss = 0.34343210\n",
      "Iteration 60, loss = 0.33948202\n",
      "Iteration 61, loss = 0.33745364\n",
      "Iteration 62, loss = 0.33599293\n",
      "Iteration 63, loss = 0.33067537\n",
      "Iteration 64, loss = 0.32847583\n",
      "Iteration 65, loss = 0.32782267\n",
      "Iteration 66, loss = 0.32251444\n",
      "Iteration 67, loss = 0.31996384\n",
      "Iteration 68, loss = 0.32110284\n",
      "Iteration 69, loss = 0.31550468\n",
      "Iteration 70, loss = 0.31265921\n",
      "Iteration 71, loss = 0.31422834\n",
      "Iteration 72, loss = 0.30813957\n",
      "Iteration 73, loss = 0.30509203\n",
      "Iteration 74, loss = 0.30286914\n",
      "Iteration 75, loss = 0.29960374\n",
      "Iteration 76, loss = 0.29965720\n",
      "Iteration 77, loss = 0.29678040\n",
      "Iteration 78, loss = 0.29281014\n",
      "Iteration 79, loss = 0.29303737\n",
      "Iteration 80, loss = 0.28706871\n",
      "Iteration 81, loss = 0.28557053\n",
      "Iteration 82, loss = 0.28429597\n",
      "Iteration 83, loss = 0.28390266\n",
      "Iteration 84, loss = 0.28171527\n",
      "Iteration 85, loss = 0.27663943\n",
      "Iteration 86, loss = 0.27387189\n",
      "Iteration 87, loss = 0.27271038\n",
      "Iteration 88, loss = 0.26895999\n",
      "Iteration 89, loss = 0.26687413\n",
      "Iteration 90, loss = 0.26508733\n",
      "Iteration 91, loss = 0.26103473\n",
      "Iteration 92, loss = 0.26326993\n",
      "Iteration 93, loss = 0.25873157\n",
      "Iteration 94, loss = 0.25469420\n",
      "Iteration 95, loss = 0.25687556\n",
      "Iteration 96, loss = 0.25273270\n",
      "Iteration 97, loss = 0.24674614\n",
      "Iteration 98, loss = 0.24612312\n",
      "Iteration 99, loss = 0.24331490\n",
      "Iteration 100, loss = 0.24540082\n",
      "Iteration 101, loss = 0.24675377\n",
      "Iteration 102, loss = 0.24239330\n",
      "Iteration 103, loss = 0.23394409\n",
      "Iteration 104, loss = 0.23236722\n",
      "Iteration 105, loss = 0.23317913\n",
      "Iteration 106, loss = 0.23079257\n",
      "Iteration 107, loss = 0.23033020\n",
      "Iteration 108, loss = 0.23200125\n",
      "Iteration 109, loss = 0.23094239\n",
      "Iteration 110, loss = 0.22141780\n",
      "Iteration 111, loss = 0.22123527\n",
      "Iteration 112, loss = 0.21706024\n",
      "Iteration 113, loss = 0.21940451\n",
      "Iteration 114, loss = 0.21276419\n",
      "Iteration 115, loss = 0.21087317\n",
      "Iteration 116, loss = 0.21445065\n",
      "Iteration 117, loss = 0.20938247\n",
      "Iteration 118, loss = 0.21126451\n",
      "Iteration 119, loss = 0.20548165\n",
      "Iteration 120, loss = 0.20536743\n",
      "Iteration 121, loss = 0.20424696\n",
      "Iteration 122, loss = 0.20166738\n",
      "Iteration 123, loss = 0.20133717\n",
      "Iteration 124, loss = 0.19477534\n",
      "Iteration 125, loss = 0.19391311\n",
      "Iteration 126, loss = 0.19082194\n",
      "Iteration 127, loss = 0.19186617\n",
      "Iteration 128, loss = 0.18816670\n",
      "Iteration 129, loss = 0.18982874\n",
      "Iteration 130, loss = 0.18171802\n",
      "Iteration 131, loss = 0.18832379\n",
      "Iteration 132, loss = 0.18505428\n",
      "Iteration 133, loss = 0.17625928\n",
      "Iteration 134, loss = 0.17833190\n",
      "Iteration 135, loss = 0.17590646\n",
      "Iteration 136, loss = 0.17573906\n",
      "Iteration 137, loss = 0.17364671\n",
      "Iteration 138, loss = 0.17264694\n",
      "Iteration 139, loss = 0.17063148\n",
      "Iteration 140, loss = 0.17484071\n",
      "Iteration 141, loss = 0.17229669\n",
      "Iteration 142, loss = 0.16460457\n",
      "Iteration 143, loss = 0.16361788\n",
      "Iteration 144, loss = 0.15786983\n",
      "Iteration 145, loss = 0.15868144\n",
      "Iteration 146, loss = 0.15936790\n",
      "Iteration 147, loss = 0.15576052\n",
      "Iteration 148, loss = 0.15486803\n",
      "Iteration 149, loss = 0.15275619\n",
      "Iteration 150, loss = 0.15556999\n",
      "Iteration 151, loss = 0.15344564\n",
      "Iteration 152, loss = 0.14658980\n",
      "Iteration 153, loss = 0.14347994\n",
      "Iteration 154, loss = 0.15726373\n",
      "Iteration 155, loss = 0.14570658\n",
      "Iteration 156, loss = 0.14941376\n",
      "Iteration 157, loss = 0.14202330\n",
      "Iteration 158, loss = 0.14107626\n",
      "Iteration 159, loss = 0.13873075\n",
      "Iteration 160, loss = 0.14282794\n",
      "Iteration 161, loss = 0.13909193\n",
      "Iteration 162, loss = 0.13718789\n",
      "Iteration 163, loss = 0.13750598\n",
      "Iteration 164, loss = 0.13298236\n",
      "Iteration 165, loss = 0.13831429\n",
      "Iteration 166, loss = 0.12630953\n",
      "Iteration 167, loss = 0.12570348\n",
      "Iteration 168, loss = 0.13333202\n",
      "Iteration 169, loss = 0.12171120\n",
      "Iteration 170, loss = 0.12761415\n",
      "Iteration 171, loss = 0.13037093\n",
      "Iteration 172, loss = 0.12471160\n",
      "Iteration 173, loss = 0.12763468\n",
      "Iteration 174, loss = 0.12459785\n",
      "Iteration 175, loss = 0.12206395\n",
      "Iteration 176, loss = 0.11583495\n",
      "Iteration 177, loss = 0.11962335\n",
      "Iteration 178, loss = 0.11802754\n",
      "Iteration 179, loss = 0.11432962\n",
      "Iteration 180, loss = 0.11488273\n",
      "Iteration 181, loss = 0.11333682\n",
      "Iteration 182, loss = 0.10836233\n",
      "Iteration 183, loss = 0.11111140\n",
      "Iteration 184, loss = 0.11566728\n",
      "Iteration 185, loss = 0.10771628\n",
      "Iteration 186, loss = 0.10951565\n",
      "Iteration 187, loss = 0.10515266\n",
      "Iteration 188, loss = 0.10455052\n",
      "Iteration 189, loss = 0.10783588\n",
      "Iteration 190, loss = 0.10152094\n",
      "Iteration 191, loss = 0.10178117\n",
      "Iteration 192, loss = 0.10432097\n",
      "Iteration 193, loss = 0.10658860\n",
      "Iteration 194, loss = 0.09855138\n",
      "Iteration 195, loss = 0.09728354\n",
      "Iteration 196, loss = 0.10551858\n",
      "Iteration 197, loss = 0.09820318\n",
      "Iteration 198, loss = 0.09386826\n",
      "Iteration 199, loss = 0.09491445\n",
      "Iteration 200, loss = 0.09273190\n",
      "Iteration 201, loss = 0.09313170\n",
      "Iteration 202, loss = 0.08983210\n",
      "Iteration 203, loss = 0.09420427\n",
      "Iteration 204, loss = 0.09214992\n",
      "Iteration 205, loss = 0.09631681\n",
      "Iteration 206, loss = 0.08748705\n",
      "Iteration 207, loss = 0.08917268\n",
      "Iteration 208, loss = 0.08926139\n",
      "Iteration 209, loss = 0.09034181\n",
      "Iteration 210, loss = 0.08512082\n",
      "Iteration 211, loss = 0.08130229\n",
      "Iteration 212, loss = 0.08023209\n",
      "Iteration 213, loss = 0.08584502\n",
      "Iteration 214, loss = 0.08305614\n",
      "Iteration 215, loss = 0.08244443\n",
      "Iteration 216, loss = 0.08046322\n",
      "Iteration 217, loss = 0.08424213\n",
      "Iteration 218, loss = 0.07895695\n",
      "Iteration 219, loss = 0.07801580\n",
      "Iteration 220, loss = 0.07573340\n",
      "Iteration 221, loss = 0.07920144\n",
      "Iteration 222, loss = 0.07586581\n",
      "Iteration 223, loss = 0.08338299\n",
      "Iteration 224, loss = 0.07227461\n",
      "Iteration 225, loss = 0.07388179\n",
      "Iteration 226, loss = 0.07970969\n",
      "Iteration 227, loss = 0.07374325\n",
      "Iteration 228, loss = 0.07352046\n",
      "Iteration 229, loss = 0.07099651\n",
      "Iteration 230, loss = 0.07173591\n",
      "Iteration 231, loss = 0.07331989\n",
      "Iteration 232, loss = 0.07195217\n",
      "Iteration 233, loss = 0.07803832\n",
      "Iteration 234, loss = 0.07436258\n",
      "Iteration 235, loss = 0.06943975\n",
      "Iteration 236, loss = 0.07166134\n",
      "Iteration 237, loss = 0.07117806\n",
      "Iteration 238, loss = 0.06723347\n",
      "Iteration 239, loss = 0.05957279\n",
      "Iteration 240, loss = 0.06596150\n",
      "Iteration 241, loss = 0.06484103\n",
      "Iteration 242, loss = 0.06573352\n",
      "Iteration 243, loss = 0.06849751\n",
      "Iteration 244, loss = 0.06734081\n",
      "Iteration 245, loss = 0.06155282\n",
      "Iteration 246, loss = 0.06039249\n",
      "Iteration 247, loss = 0.06082979\n",
      "Iteration 248, loss = 0.05808395\n",
      "Iteration 249, loss = 0.06289379\n",
      "Iteration 250, loss = 0.06594178\n",
      "Iteration 251, loss = 0.06688998\n",
      "Iteration 252, loss = 0.06544955\n",
      "Iteration 253, loss = 0.05699246\n",
      "Iteration 254, loss = 0.06443885\n",
      "Iteration 255, loss = 0.05657992\n",
      "Iteration 256, loss = 0.04908308\n",
      "Iteration 257, loss = 0.05307845\n",
      "Iteration 258, loss = 0.05171446\n",
      "Iteration 259, loss = 0.05947448\n",
      "Iteration 260, loss = 0.07994854\n",
      "Iteration 261, loss = 0.05802439\n",
      "Iteration 262, loss = 0.06052543\n",
      "Iteration 263, loss = 0.04734155\n",
      "Iteration 264, loss = 0.04568208\n",
      "Iteration 265, loss = 0.04933659\n",
      "Iteration 266, loss = 0.05725074\n",
      "Iteration 267, loss = 0.05517966\n",
      "Iteration 268, loss = 0.05233552\n",
      "Iteration 269, loss = 0.06041562\n",
      "Iteration 270, loss = 0.04585628\n",
      "Iteration 271, loss = 0.05072915\n",
      "Iteration 272, loss = 0.05013688\n",
      "Iteration 273, loss = 0.05545418\n",
      "Iteration 274, loss = 0.06814810\n",
      "Iteration 275, loss = 0.04730823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# scores_ann_boosted = cross_val_score(model_ann_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_ann, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.74234694 0.76020408 0.73979592 0.75       0.69897959 0.71938776\n",
      " 0.74489796 0.77295918 0.7314578  0.7544757 ]\n",
      "Score médio: 0.7414504932407746\n",
      "Desvio padrão: 0.01998265962323871\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_ann}\")\n",
    "print(f\"Score médio: {np.mean(scores_ann)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_ann)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.64030612 0.67857143 0.68367347 0.67091837 0.66836735 0.66581633\n",
      " 0.66581633 0.7244898  0.66496164 0.72122762]\n",
      "Score médio: 0.678414844198549\n",
      "Desvio padrão: 0.024680702695677934\n"
     ]
    }
   ],
   "source": [
    "# # Exibindo os resultados\n",
    "# print(f\"Scores de cada fold: {scores_ann_boosted}\")\n",
    "# print(f\"Score médio: {np.mean(scores_ann_boosted)}\")\n",
    "# print(f\"Desvio padrão: {np.std(scores_ann_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros = {\n",
    "#     'activation': ['relu', 'logistic', 'tanh'],  # Funções de ativação\n",
    "#     'solver': ['adam', 'sgd'],  # Otimizadores\n",
    "#     'batch_size': [32, 64],  # Tamanhos de lote comuns\n",
    "#     'learning_rate': ['constant', 'adaptive'],  # Taxa de aprendizado\n",
    "#     'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Tamanho das camadas ocultas\n",
    "#     'alpha': [0.0001, 0.001]  # Parâmetro de regularização L2\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# grid_search = GridSearchCV(estimator=MLPClassifier(), param_grid=parametros)\n",
    "# grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "# melhores_parametros = grid_search.best_params_\n",
    "# melhor_resultado = grid_search.best_score_\n",
    "# print(melhores_parametros)\n",
    "# print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - 66%(Normal) 72%(Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "model_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted\n",
    "model_tree_boosted = DecisionTreeClassifier(criterion='gini', splitter='random', min_samples_leaf=10, min_samples_split=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_tree = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree = cross_val_score(model_tree, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.6505102  0.65306122 0.66071429 0.67091837 0.62755102 0.63010204\n",
      " 0.68367347 0.69642857 0.69309463 0.68030691]\n",
      "Score médio: 0.6646360718200324\n",
      "Desvio padrão: 0.023205420177937815\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree_boosted = cross_val_score(model_tree_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70918367 0.70153061 0.73979592 0.73469388 0.72193878 0.69897959\n",
      " 0.70918367 0.7627551  0.73401535 0.73657289]\n",
      "Score médio: 0.7248649459783915\n",
      "Desvio padrão: 0.019211770606043753\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "parametros = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.7253707613313524\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculando desvio padrão\n",
    "# std_dev = np.std(scores_tree_boosted)\n",
    "\n",
    "# # Plotando o desvio padrão\n",
    "# plt.bar('Desvio Padrão', std_dev)\n",
    "# plt.ylabel('Valor')\n",
    "# plt.title('Desvio Padrão dos Scores')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - 70% Boosted(72%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn_boosted = KNeighborsClassifier(n_neighbors=25, metric='minkowski', p = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_knn = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn = cross_val_score(model_knn, X_customer_balanced, Y_customer_balanced, cv=kf_knn, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn_boosted = cross_val_score(model_knn_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_knn, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70153061 0.70408163 0.72704082 0.69132653 0.68877551 0.66071429\n",
      " 0.68367347 0.75510204 0.70588235 0.71611253]\n",
      "Score médio: 0.703423978286967\n",
      "Desvio padrão: 0.02444292091659416\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_knn}\")\n",
    "print(f\"Score médio: {np.mean(scores_knn)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_knn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.74489796 0.72704082 0.75255102 0.74234694 0.68877551 0.70918367\n",
      " 0.71428571 0.75765306 0.72122762 0.72890026]\n",
      "Score médio: 0.7286862571115402\n",
      "Desvio padrão: 0.020241083994342945\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_knn_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_knn_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_knn_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "parametros = {\n",
    "    'n_neighbors': range(1, 31),  # Test a range of neighbor values\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan'],  # Different distance metrics\n",
    "    'p': [1, 2]  # Minkowski parameter (1 for manhattan, 2 for euclidean)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'minkowski', 'n_neighbors': 25, 'p': 1}\n",
      "0.7302258451273229\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(random_state=42, max_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_boosted = LogisticRegression(random_state=0, max_iter=100, C=1.0, solver='newton-cg', tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_logistic = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic = cross_val_score(model_logistic, X_customer_balanced, Y_customer_balanced, cv=kf_logistic, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic_boosted = cross_val_score(model_logistic_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_logistic, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.67602041 0.70663265 0.71938776 0.71683673 0.66326531 0.67091837\n",
      " 0.72959184 0.75255102 0.71355499 0.74680307]\n",
      "Score médio: 0.7095562137898638\n",
      "Desvio padrão: 0.029277793572685756\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_logistic}\")\n",
    "print(f\"Score médio: {np.mean(scores_logistic)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_logistic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.67602041 0.70663265 0.71938776 0.71683673 0.66326531 0.67091837\n",
      " 0.72959184 0.75255102 0.71355499 0.74680307]\n",
      "Score médio: 0.7095562137898638\n",
      "Desvio padrão: 0.029277793572685756\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_logistic_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_logistic_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_logistic_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "parametros = {\n",
    "    'C': [1.0, 1.5, 2.0],  # Regularization strength\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # Solvers\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'tol': [0.0001, 0.00001, 0.000001]# Number of iterations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'max_iter': 100, 'solver': 'newton-cg', 'tol': 0.0001}\n",
      "0.7064893007011234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = GaussianNB()\n",
    "kf_naive = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = cross_val_score(model_naive, X_customer_balanced, Y_customer_balanced, cv=kf_naive, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.68877551 0.7244898  0.70918367 0.73214286 0.68877551 0.68112245\n",
      " 0.72193878 0.77295918 0.72634271 0.75959079]\n",
      "Score médio: 0.7205321258938358\n",
      "Desvio padrão: 0.028564427059807447\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_naive}\")\n",
    "print(f\"Score médio: {np.mean(scores_naive)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_naive)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - 75% - 76% Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(n_estimators=80, criterion='entropy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest_boosted = RandomForestClassifier(n_estimators=150, criterion='gini', random_state=0, min_samples_leaf=10, min_samples_split=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_forest = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_forest_boosted = cross_val_score(model_forest_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_forest, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_forest = cross_val_score(model_forest, X_customer_balanced, Y_customer_balanced, cv=kf_forest, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.75       0.75765306 0.7627551  0.75       0.7372449  0.72193878\n",
      " 0.73469388 0.78316327 0.76470588 0.76982097]\n",
      "Score médio: 0.7531975833811785\n",
      "Desvio padrão: 0.01735613718267566\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_forest}\")\n",
    "print(f\"Score médio: {np.mean(scores_forest)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_forest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.74234694 0.78571429 0.75510204 0.76020408 0.7372449  0.70153061\n",
      " 0.76020408 0.78571429 0.76726343 0.78772379]\n",
      "Score médio: 0.7583048436766011\n",
      "Desvio padrão: 0.025273003212994954\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_forest_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_forest_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_forest_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [10, 40, 100, 150],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruno\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 150}\n",
      "0.7616187087861964\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
