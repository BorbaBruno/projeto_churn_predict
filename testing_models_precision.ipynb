{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('customer.pkl', 'rb') as f:\n",
    "    X_customer_balanced, Y_customer_balanced = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *SVM - 74%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3918, 10), (3918,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_customer_balanced.shape, Y_customer_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', random_state=42, C=2.0)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_customer_balanced, Y_customer_balanced, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.7372449  0.75       0.75255102 0.73469388 0.72704082 0.69897959\n",
      " 0.73469388 0.78316327 0.72890026 0.76982097]\n",
      "Score médio: 0.7417088574560259\n",
      "Desvio padrão: 0.022401792602462955\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores}\")\n",
    "print(f\"Score médio: {np.mean(scores)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Artificial Neural Network - 74,5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = MLPClassifier(max_iter=1500, verbose=True, tol=0.000000, solver='adam', activation='relu', hidden_layer_sizes=(10,10))\n",
    "kf_ann = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68006787\n",
      "Iteration 2, loss = 0.66744869\n",
      "Iteration 3, loss = 0.65479591\n",
      "Iteration 4, loss = 0.64228197\n",
      "Iteration 5, loss = 0.63008621\n",
      "Iteration 6, loss = 0.61811929\n",
      "Iteration 7, loss = 0.60715317\n",
      "Iteration 8, loss = 0.59721503\n",
      "Iteration 9, loss = 0.58852141\n",
      "Iteration 10, loss = 0.58094276\n",
      "Iteration 11, loss = 0.57488409\n",
      "Iteration 12, loss = 0.56991148\n",
      "Iteration 13, loss = 0.56559369\n",
      "Iteration 14, loss = 0.56233973\n",
      "Iteration 15, loss = 0.55988566\n",
      "Iteration 16, loss = 0.55770978\n",
      "Iteration 17, loss = 0.55579933\n",
      "Iteration 18, loss = 0.55432800\n",
      "Iteration 19, loss = 0.55299230\n",
      "Iteration 20, loss = 0.55148849\n",
      "Iteration 21, loss = 0.55028195\n",
      "Iteration 22, loss = 0.54907014\n",
      "Iteration 23, loss = 0.54780897\n",
      "Iteration 24, loss = 0.54664640\n",
      "Iteration 25, loss = 0.54547789\n",
      "Iteration 26, loss = 0.54447886\n",
      "Iteration 27, loss = 0.54354144\n",
      "Iteration 28, loss = 0.54242787\n",
      "Iteration 29, loss = 0.54150461\n",
      "Iteration 30, loss = 0.54038880\n",
      "Iteration 31, loss = 0.53925920\n",
      "Iteration 32, loss = 0.53840468\n",
      "Iteration 33, loss = 0.53734952\n",
      "Iteration 34, loss = 0.53636088\n",
      "Iteration 35, loss = 0.53522788\n",
      "Iteration 36, loss = 0.53425451\n",
      "Iteration 37, loss = 0.53316595\n",
      "Iteration 38, loss = 0.53227689\n",
      "Iteration 39, loss = 0.53124456\n",
      "Iteration 40, loss = 0.53023846\n",
      "Iteration 41, loss = 0.52914978\n",
      "Iteration 42, loss = 0.52797334\n",
      "Iteration 43, loss = 0.52696728\n",
      "Iteration 44, loss = 0.52600326\n",
      "Iteration 45, loss = 0.52473331\n",
      "Iteration 46, loss = 0.52374361\n",
      "Iteration 47, loss = 0.52266494\n",
      "Iteration 48, loss = 0.52154831\n",
      "Iteration 49, loss = 0.52067999\n",
      "Iteration 50, loss = 0.51960934\n",
      "Iteration 51, loss = 0.51850147\n",
      "Iteration 52, loss = 0.51760652\n",
      "Iteration 53, loss = 0.51672950\n",
      "Iteration 54, loss = 0.51607033\n",
      "Iteration 55, loss = 0.51507752\n",
      "Iteration 56, loss = 0.51451940\n",
      "Iteration 57, loss = 0.51333674\n",
      "Iteration 58, loss = 0.51249676\n",
      "Iteration 59, loss = 0.51146673\n",
      "Iteration 60, loss = 0.51113204\n",
      "Iteration 61, loss = 0.51010063\n",
      "Iteration 62, loss = 0.50907744\n",
      "Iteration 63, loss = 0.50827195\n",
      "Iteration 64, loss = 0.50731610\n",
      "Iteration 65, loss = 0.50662178\n",
      "Iteration 66, loss = 0.50584313\n",
      "Iteration 67, loss = 0.50528687\n",
      "Iteration 68, loss = 0.50448460\n",
      "Iteration 69, loss = 0.50364725\n",
      "Iteration 70, loss = 0.50357062\n",
      "Iteration 71, loss = 0.50260798\n",
      "Iteration 72, loss = 0.50193631\n",
      "Iteration 73, loss = 0.50111892\n",
      "Iteration 74, loss = 0.50071519\n",
      "Iteration 75, loss = 0.50010883\n",
      "Iteration 76, loss = 0.49947480\n",
      "Iteration 77, loss = 0.49899086\n",
      "Iteration 78, loss = 0.49820051\n",
      "Iteration 79, loss = 0.49770605\n",
      "Iteration 80, loss = 0.49727585\n",
      "Iteration 81, loss = 0.49646034\n",
      "Iteration 82, loss = 0.49633651\n",
      "Iteration 83, loss = 0.49550573\n",
      "Iteration 84, loss = 0.49510318\n",
      "Iteration 85, loss = 0.49480318\n",
      "Iteration 86, loss = 0.49400700\n",
      "Iteration 87, loss = 0.49377621\n",
      "Iteration 88, loss = 0.49309214\n",
      "Iteration 89, loss = 0.49302716\n",
      "Iteration 90, loss = 0.49217496\n",
      "Iteration 91, loss = 0.49178597\n",
      "Iteration 92, loss = 0.49155641\n",
      "Iteration 93, loss = 0.49126347\n",
      "Iteration 94, loss = 0.49069650\n",
      "Iteration 95, loss = 0.49055759\n",
      "Iteration 96, loss = 0.48989329\n",
      "Iteration 97, loss = 0.48958764\n",
      "Iteration 98, loss = 0.48897038\n",
      "Iteration 99, loss = 0.48880204\n",
      "Iteration 100, loss = 0.48852950\n",
      "Iteration 101, loss = 0.48794236\n",
      "Iteration 102, loss = 0.48762803\n",
      "Iteration 103, loss = 0.48745884\n",
      "Iteration 104, loss = 0.48711969\n",
      "Iteration 105, loss = 0.48667296\n",
      "Iteration 106, loss = 0.48625739\n",
      "Iteration 107, loss = 0.48653305\n",
      "Iteration 108, loss = 0.48581425\n",
      "Iteration 109, loss = 0.48580950\n",
      "Iteration 110, loss = 0.48514211\n",
      "Iteration 111, loss = 0.48511228\n",
      "Iteration 112, loss = 0.48477984\n",
      "Iteration 113, loss = 0.48451831\n",
      "Iteration 114, loss = 0.48423038\n",
      "Iteration 115, loss = 0.48392059\n",
      "Iteration 116, loss = 0.48362654\n",
      "Iteration 117, loss = 0.48324016\n",
      "Iteration 118, loss = 0.48324167\n",
      "Iteration 119, loss = 0.48283757\n",
      "Iteration 120, loss = 0.48274319\n",
      "Iteration 121, loss = 0.48252533\n",
      "Iteration 122, loss = 0.48254162\n",
      "Iteration 123, loss = 0.48174504\n",
      "Iteration 124, loss = 0.48161585\n",
      "Iteration 125, loss = 0.48138509\n",
      "Iteration 126, loss = 0.48127097\n",
      "Iteration 127, loss = 0.48088394\n",
      "Iteration 128, loss = 0.48093486\n",
      "Iteration 129, loss = 0.48066776\n",
      "Iteration 130, loss = 0.48097774\n",
      "Iteration 131, loss = 0.48004703\n",
      "Iteration 132, loss = 0.47988909\n",
      "Iteration 133, loss = 0.47971129\n",
      "Iteration 134, loss = 0.47957284\n",
      "Iteration 135, loss = 0.47941097\n",
      "Iteration 136, loss = 0.47932394\n",
      "Iteration 137, loss = 0.47901075\n",
      "Iteration 138, loss = 0.47882299\n",
      "Iteration 139, loss = 0.47871044\n",
      "Iteration 140, loss = 0.47876388\n",
      "Iteration 141, loss = 0.47822391\n",
      "Iteration 142, loss = 0.47807819\n",
      "Iteration 143, loss = 0.47761459\n",
      "Iteration 144, loss = 0.47778234\n",
      "Iteration 145, loss = 0.47728196\n",
      "Iteration 146, loss = 0.47725292\n",
      "Iteration 147, loss = 0.47719302\n",
      "Iteration 148, loss = 0.47724097\n",
      "Iteration 149, loss = 0.47691944\n",
      "Iteration 150, loss = 0.47655302\n",
      "Iteration 151, loss = 0.47638644\n",
      "Iteration 152, loss = 0.47618634\n",
      "Iteration 153, loss = 0.47619854\n",
      "Iteration 154, loss = 0.47611333\n",
      "Iteration 155, loss = 0.47592776\n",
      "Iteration 156, loss = 0.47574932\n",
      "Iteration 157, loss = 0.47558512\n",
      "Iteration 158, loss = 0.47562100\n",
      "Iteration 159, loss = 0.47554270\n",
      "Iteration 160, loss = 0.47480553\n",
      "Iteration 161, loss = 0.47491127\n",
      "Iteration 162, loss = 0.47489820\n",
      "Iteration 163, loss = 0.47466123\n",
      "Iteration 164, loss = 0.47457925\n",
      "Iteration 165, loss = 0.47440434\n",
      "Iteration 166, loss = 0.47399558\n",
      "Iteration 167, loss = 0.47419917\n",
      "Iteration 168, loss = 0.47419138\n",
      "Iteration 169, loss = 0.47376367\n",
      "Iteration 170, loss = 0.47391988\n",
      "Iteration 171, loss = 0.47366388\n",
      "Iteration 172, loss = 0.47361617\n",
      "Iteration 173, loss = 0.47339949\n",
      "Iteration 174, loss = 0.47344642\n",
      "Iteration 175, loss = 0.47324020\n",
      "Iteration 176, loss = 0.47327853\n",
      "Iteration 177, loss = 0.47312645\n",
      "Iteration 178, loss = 0.47303501\n",
      "Iteration 179, loss = 0.47293109\n",
      "Iteration 180, loss = 0.47284367\n",
      "Iteration 181, loss = 0.47296354\n",
      "Iteration 182, loss = 0.47274706\n",
      "Iteration 183, loss = 0.47269553\n",
      "Iteration 184, loss = 0.47263736\n",
      "Iteration 185, loss = 0.47229992\n",
      "Iteration 186, loss = 0.47262167\n",
      "Iteration 187, loss = 0.47232195\n",
      "Iteration 188, loss = 0.47225736\n",
      "Iteration 189, loss = 0.47234054\n",
      "Iteration 190, loss = 0.47213251\n",
      "Iteration 191, loss = 0.47246627\n",
      "Iteration 192, loss = 0.47180799\n",
      "Iteration 193, loss = 0.47202842\n",
      "Iteration 194, loss = 0.47196615\n",
      "Iteration 195, loss = 0.47176340\n",
      "Iteration 196, loss = 0.47174573\n",
      "Iteration 197, loss = 0.47162384\n",
      "Iteration 198, loss = 0.47142707\n",
      "Iteration 199, loss = 0.47169366\n",
      "Iteration 200, loss = 0.47157790\n",
      "Iteration 201, loss = 0.47151700\n",
      "Iteration 202, loss = 0.47131651\n",
      "Iteration 203, loss = 0.47111400\n",
      "Iteration 204, loss = 0.47098840\n",
      "Iteration 205, loss = 0.47123682\n",
      "Iteration 206, loss = 0.47121900\n",
      "Iteration 207, loss = 0.47098796\n",
      "Iteration 208, loss = 0.47119175\n",
      "Iteration 209, loss = 0.47130657\n",
      "Iteration 210, loss = 0.47090385\n",
      "Iteration 211, loss = 0.47086632\n",
      "Iteration 212, loss = 0.47066140\n",
      "Iteration 213, loss = 0.47045175\n",
      "Iteration 214, loss = 0.47045617\n",
      "Iteration 215, loss = 0.47081318\n",
      "Iteration 216, loss = 0.47050681\n",
      "Iteration 217, loss = 0.47019995\n",
      "Iteration 218, loss = 0.47032654\n",
      "Iteration 219, loss = 0.47020256\n",
      "Iteration 220, loss = 0.47005052\n",
      "Iteration 221, loss = 0.47002493\n",
      "Iteration 222, loss = 0.46987040\n",
      "Iteration 223, loss = 0.47008530\n",
      "Iteration 224, loss = 0.46986018\n",
      "Iteration 225, loss = 0.46982481\n",
      "Iteration 226, loss = 0.46991431\n",
      "Iteration 227, loss = 0.46964445\n",
      "Iteration 228, loss = 0.46983937\n",
      "Iteration 229, loss = 0.46941524\n",
      "Iteration 230, loss = 0.46955128\n",
      "Iteration 231, loss = 0.46929737\n",
      "Iteration 232, loss = 0.46940903\n",
      "Iteration 233, loss = 0.46970751\n",
      "Iteration 234, loss = 0.46921769\n",
      "Iteration 235, loss = 0.46895873\n",
      "Iteration 236, loss = 0.46911681\n",
      "Iteration 237, loss = 0.46887406\n",
      "Iteration 238, loss = 0.46885343\n",
      "Iteration 239, loss = 0.46879188\n",
      "Iteration 240, loss = 0.46849552\n",
      "Iteration 241, loss = 0.46845124\n",
      "Iteration 242, loss = 0.46859638\n",
      "Iteration 243, loss = 0.46836996\n",
      "Iteration 244, loss = 0.46830976\n",
      "Iteration 245, loss = 0.46833195\n",
      "Iteration 246, loss = 0.46812373\n",
      "Iteration 247, loss = 0.46815292\n",
      "Iteration 248, loss = 0.46817012\n",
      "Iteration 249, loss = 0.46787187\n",
      "Iteration 250, loss = 0.46799853\n",
      "Iteration 251, loss = 0.46785902\n",
      "Iteration 252, loss = 0.46787741\n",
      "Iteration 253, loss = 0.46747241\n",
      "Iteration 254, loss = 0.46758007\n",
      "Iteration 255, loss = 0.46756482\n",
      "Iteration 256, loss = 0.46736529\n",
      "Iteration 257, loss = 0.46739144\n",
      "Iteration 258, loss = 0.46744188\n",
      "Iteration 259, loss = 0.46711121\n",
      "Iteration 260, loss = 0.46744512\n",
      "Iteration 261, loss = 0.46699858\n",
      "Iteration 262, loss = 0.46712732\n",
      "Iteration 263, loss = 0.46715957\n",
      "Iteration 264, loss = 0.46691695\n",
      "Iteration 265, loss = 0.46700621\n",
      "Iteration 266, loss = 0.46687847\n",
      "Iteration 267, loss = 0.46691964\n",
      "Iteration 268, loss = 0.46666887\n",
      "Iteration 269, loss = 0.46650113\n",
      "Iteration 270, loss = 0.46660639\n",
      "Iteration 271, loss = 0.46671297\n",
      "Iteration 272, loss = 0.46654554\n",
      "Iteration 273, loss = 0.46648698\n",
      "Iteration 274, loss = 0.46625772\n",
      "Iteration 275, loss = 0.46623472\n",
      "Iteration 276, loss = 0.46646282\n",
      "Iteration 277, loss = 0.46632809\n",
      "Iteration 278, loss = 0.46630844\n",
      "Iteration 279, loss = 0.46611360\n",
      "Iteration 280, loss = 0.46613385\n",
      "Iteration 281, loss = 0.46647043\n",
      "Iteration 282, loss = 0.46603718\n",
      "Iteration 283, loss = 0.46611097\n",
      "Iteration 284, loss = 0.46584849\n",
      "Iteration 285, loss = 0.46583620\n",
      "Iteration 286, loss = 0.46575832\n",
      "Iteration 287, loss = 0.46596516\n",
      "Iteration 288, loss = 0.46596852\n",
      "Iteration 289, loss = 0.46552100\n",
      "Iteration 290, loss = 0.46555428\n",
      "Iteration 291, loss = 0.46573094\n",
      "Iteration 292, loss = 0.46554478\n",
      "Iteration 293, loss = 0.46529559\n",
      "Iteration 294, loss = 0.46545176\n",
      "Iteration 295, loss = 0.46541536\n",
      "Iteration 296, loss = 0.46536545\n",
      "Iteration 297, loss = 0.46545135\n",
      "Iteration 298, loss = 0.46527154\n",
      "Iteration 299, loss = 0.46557266\n",
      "Iteration 300, loss = 0.46539771\n",
      "Iteration 301, loss = 0.46526513\n",
      "Iteration 302, loss = 0.46538437\n",
      "Iteration 303, loss = 0.46489612\n",
      "Iteration 304, loss = 0.46499833\n",
      "Iteration 305, loss = 0.46499796\n",
      "Iteration 306, loss = 0.46498313\n",
      "Iteration 307, loss = 0.46472986\n",
      "Iteration 308, loss = 0.46500396\n",
      "Iteration 309, loss = 0.46503544\n",
      "Iteration 310, loss = 0.46513672\n",
      "Iteration 311, loss = 0.46471449\n",
      "Iteration 312, loss = 0.46480319\n",
      "Iteration 313, loss = 0.46478790\n",
      "Iteration 314, loss = 0.46466351\n",
      "Iteration 315, loss = 0.46446393\n",
      "Iteration 316, loss = 0.46446565\n",
      "Iteration 317, loss = 0.46448819\n",
      "Iteration 318, loss = 0.46448912\n",
      "Iteration 319, loss = 0.46467342\n",
      "Iteration 320, loss = 0.46456935\n",
      "Iteration 321, loss = 0.46444532\n",
      "Iteration 322, loss = 0.46454204\n",
      "Iteration 323, loss = 0.46457899\n",
      "Iteration 324, loss = 0.46429229\n",
      "Iteration 325, loss = 0.46430229\n",
      "Iteration 326, loss = 0.46440625\n",
      "Iteration 327, loss = 0.46420023\n",
      "Iteration 328, loss = 0.46427385\n",
      "Iteration 329, loss = 0.46414516\n",
      "Iteration 330, loss = 0.46426773\n",
      "Iteration 331, loss = 0.46439549\n",
      "Iteration 332, loss = 0.46401870\n",
      "Iteration 333, loss = 0.46394576\n",
      "Iteration 334, loss = 0.46391727\n",
      "Iteration 335, loss = 0.46399792\n",
      "Iteration 336, loss = 0.46384949\n",
      "Iteration 337, loss = 0.46376114\n",
      "Iteration 338, loss = 0.46374338\n",
      "Iteration 339, loss = 0.46373469\n",
      "Iteration 340, loss = 0.46377913\n",
      "Iteration 341, loss = 0.46368700\n",
      "Iteration 342, loss = 0.46376167\n",
      "Iteration 343, loss = 0.46364622\n",
      "Iteration 344, loss = 0.46378619\n",
      "Iteration 345, loss = 0.46382872\n",
      "Iteration 346, loss = 0.46357187\n",
      "Iteration 347, loss = 0.46355957\n",
      "Iteration 348, loss = 0.46348258\n",
      "Iteration 349, loss = 0.46343495\n",
      "Iteration 350, loss = 0.46339461\n",
      "Iteration 351, loss = 0.46356210\n",
      "Iteration 352, loss = 0.46363048\n",
      "Iteration 353, loss = 0.46334954\n",
      "Iteration 354, loss = 0.46362771\n",
      "Iteration 355, loss = 0.46341290\n",
      "Iteration 356, loss = 0.46326377\n",
      "Iteration 357, loss = 0.46345014\n",
      "Iteration 358, loss = 0.46317014\n",
      "Iteration 359, loss = 0.46314960\n",
      "Iteration 360, loss = 0.46341869\n",
      "Iteration 361, loss = 0.46299612\n",
      "Iteration 362, loss = 0.46302168\n",
      "Iteration 363, loss = 0.46299963\n",
      "Iteration 364, loss = 0.46320506\n",
      "Iteration 365, loss = 0.46323814\n",
      "Iteration 366, loss = 0.46303497\n",
      "Iteration 367, loss = 0.46300045\n",
      "Iteration 368, loss = 0.46291846\n",
      "Iteration 369, loss = 0.46291197\n",
      "Iteration 370, loss = 0.46311587\n",
      "Iteration 371, loss = 0.46298832\n",
      "Iteration 372, loss = 0.46294757\n",
      "Iteration 373, loss = 0.46299847\n",
      "Iteration 374, loss = 0.46331551\n",
      "Iteration 375, loss = 0.46310055\n",
      "Iteration 376, loss = 0.46310493\n",
      "Iteration 377, loss = 0.46269250\n",
      "Iteration 378, loss = 0.46260028\n",
      "Iteration 379, loss = 0.46253519\n",
      "Iteration 380, loss = 0.46287419\n",
      "Iteration 381, loss = 0.46280643\n",
      "Iteration 382, loss = 0.46287113\n",
      "Iteration 383, loss = 0.46267339\n",
      "Iteration 384, loss = 0.46284912\n",
      "Iteration 385, loss = 0.46277687\n",
      "Iteration 386, loss = 0.46280233\n",
      "Iteration 387, loss = 0.46258614\n",
      "Iteration 388, loss = 0.46244642\n",
      "Iteration 389, loss = 0.46263298\n",
      "Iteration 390, loss = 0.46256804\n",
      "Iteration 391, loss = 0.46237288\n",
      "Iteration 392, loss = 0.46234375\n",
      "Iteration 393, loss = 0.46244203\n",
      "Iteration 394, loss = 0.46223717\n",
      "Iteration 395, loss = 0.46284861\n",
      "Iteration 396, loss = 0.46259035\n",
      "Iteration 397, loss = 0.46228149\n",
      "Iteration 398, loss = 0.46257617\n",
      "Iteration 399, loss = 0.46231043\n",
      "Iteration 400, loss = 0.46223452\n",
      "Iteration 401, loss = 0.46230589\n",
      "Iteration 402, loss = 0.46242131\n",
      "Iteration 403, loss = 0.46208284\n",
      "Iteration 404, loss = 0.46217668\n",
      "Iteration 405, loss = 0.46213714\n",
      "Iteration 406, loss = 0.46210226\n",
      "Iteration 407, loss = 0.46191384\n",
      "Iteration 408, loss = 0.46210693\n",
      "Iteration 409, loss = 0.46219302\n",
      "Iteration 410, loss = 0.46205493\n",
      "Iteration 411, loss = 0.46186148\n",
      "Iteration 412, loss = 0.46187352\n",
      "Iteration 413, loss = 0.46198817\n",
      "Iteration 414, loss = 0.46173430\n",
      "Iteration 415, loss = 0.46209752\n",
      "Iteration 416, loss = 0.46180699\n",
      "Iteration 417, loss = 0.46186808\n",
      "Iteration 418, loss = 0.46168436\n",
      "Iteration 419, loss = 0.46170611\n",
      "Iteration 420, loss = 0.46187898\n",
      "Iteration 421, loss = 0.46166672\n",
      "Iteration 422, loss = 0.46171345\n",
      "Iteration 423, loss = 0.46169304\n",
      "Iteration 424, loss = 0.46159836\n",
      "Iteration 425, loss = 0.46162443\n",
      "Iteration 426, loss = 0.46151910\n",
      "Iteration 427, loss = 0.46138852\n",
      "Iteration 428, loss = 0.46153910\n",
      "Iteration 429, loss = 0.46130590\n",
      "Iteration 430, loss = 0.46128955\n",
      "Iteration 431, loss = 0.46130982\n",
      "Iteration 432, loss = 0.46133371\n",
      "Iteration 433, loss = 0.46132366\n",
      "Iteration 434, loss = 0.46126810\n",
      "Iteration 435, loss = 0.46161316\n",
      "Iteration 436, loss = 0.46126408\n",
      "Iteration 437, loss = 0.46103788\n",
      "Iteration 438, loss = 0.46099850\n",
      "Iteration 439, loss = 0.46108465\n",
      "Iteration 440, loss = 0.46107521\n",
      "Iteration 441, loss = 0.46097744\n",
      "Iteration 442, loss = 0.46099827\n",
      "Iteration 443, loss = 0.46073856\n",
      "Iteration 444, loss = 0.46092707\n",
      "Iteration 445, loss = 0.46086304\n",
      "Iteration 446, loss = 0.46066356\n",
      "Iteration 447, loss = 0.46090864\n",
      "Iteration 448, loss = 0.46058900\n",
      "Iteration 449, loss = 0.46069017\n",
      "Iteration 450, loss = 0.46062595\n",
      "Iteration 451, loss = 0.46063433\n",
      "Iteration 452, loss = 0.46045278\n",
      "Iteration 453, loss = 0.46052255\n",
      "Iteration 454, loss = 0.46038400\n",
      "Iteration 455, loss = 0.46037887\n",
      "Iteration 456, loss = 0.46042332\n",
      "Iteration 457, loss = 0.46012694\n",
      "Iteration 458, loss = 0.46019588\n",
      "Iteration 459, loss = 0.46031063\n",
      "Iteration 460, loss = 0.46065096\n",
      "Iteration 461, loss = 0.46022711\n",
      "Iteration 462, loss = 0.46018678\n",
      "Iteration 463, loss = 0.46001777\n",
      "Iteration 464, loss = 0.46022603\n",
      "Iteration 465, loss = 0.46012623\n",
      "Iteration 466, loss = 0.46004904\n",
      "Iteration 467, loss = 0.46015352\n",
      "Iteration 468, loss = 0.46024145\n",
      "Iteration 469, loss = 0.45995124\n",
      "Iteration 470, loss = 0.46011874\n",
      "Iteration 471, loss = 0.46022625\n",
      "Iteration 472, loss = 0.45999862\n",
      "Iteration 473, loss = 0.46000925\n",
      "Iteration 474, loss = 0.46020445\n",
      "Iteration 475, loss = 0.45987282\n",
      "Iteration 476, loss = 0.46015780\n",
      "Iteration 477, loss = 0.45997990\n",
      "Iteration 478, loss = 0.46010379\n",
      "Iteration 479, loss = 0.46007158\n",
      "Iteration 480, loss = 0.45988906\n",
      "Iteration 481, loss = 0.45973187\n",
      "Iteration 482, loss = 0.45965769\n",
      "Iteration 483, loss = 0.45976438\n",
      "Iteration 484, loss = 0.46005882\n",
      "Iteration 485, loss = 0.45947140\n",
      "Iteration 486, loss = 0.45978558\n",
      "Iteration 487, loss = 0.45942703\n",
      "Iteration 488, loss = 0.45943649\n",
      "Iteration 489, loss = 0.45945516\n",
      "Iteration 490, loss = 0.45940569\n",
      "Iteration 491, loss = 0.45962504\n",
      "Iteration 492, loss = 0.45957024\n",
      "Iteration 493, loss = 0.45933345\n",
      "Iteration 494, loss = 0.45949685\n",
      "Iteration 495, loss = 0.45953659\n",
      "Iteration 496, loss = 0.45960575\n",
      "Iteration 497, loss = 0.45922314\n",
      "Iteration 498, loss = 0.45931900\n",
      "Iteration 499, loss = 0.45923961\n",
      "Iteration 500, loss = 0.45912287\n",
      "Iteration 501, loss = 0.45919322\n",
      "Iteration 502, loss = 0.45907106\n",
      "Iteration 503, loss = 0.45914578\n",
      "Iteration 504, loss = 0.45939167\n",
      "Iteration 505, loss = 0.45907930\n",
      "Iteration 506, loss = 0.45921693\n",
      "Iteration 507, loss = 0.45911987\n",
      "Iteration 508, loss = 0.45931658\n",
      "Iteration 509, loss = 0.45890229\n",
      "Iteration 510, loss = 0.45880501\n",
      "Iteration 511, loss = 0.45880169\n",
      "Iteration 512, loss = 0.45881649\n",
      "Iteration 513, loss = 0.45888072\n",
      "Iteration 514, loss = 0.45898345\n",
      "Iteration 515, loss = 0.45897547\n",
      "Iteration 516, loss = 0.45893652\n",
      "Iteration 517, loss = 0.45868132\n",
      "Iteration 518, loss = 0.45870319\n",
      "Iteration 519, loss = 0.45882084\n",
      "Iteration 520, loss = 0.45857286\n",
      "Iteration 521, loss = 0.45865210\n",
      "Iteration 522, loss = 0.45894363\n",
      "Iteration 523, loss = 0.45866350\n",
      "Iteration 524, loss = 0.45868374\n",
      "Iteration 525, loss = 0.45846972\n",
      "Iteration 526, loss = 0.45851020\n",
      "Iteration 527, loss = 0.45841282\n",
      "Iteration 528, loss = 0.45845335\n",
      "Iteration 529, loss = 0.45830943\n",
      "Iteration 530, loss = 0.45829950\n",
      "Iteration 531, loss = 0.45832424\n",
      "Iteration 532, loss = 0.45845050\n",
      "Iteration 533, loss = 0.45823313\n",
      "Iteration 534, loss = 0.45848960\n",
      "Iteration 535, loss = 0.45848531\n",
      "Iteration 536, loss = 0.45809146\n",
      "Iteration 537, loss = 0.45826528\n",
      "Iteration 538, loss = 0.45871458\n",
      "Iteration 539, loss = 0.45804809\n",
      "Iteration 540, loss = 0.45826000\n",
      "Iteration 541, loss = 0.45806209\n",
      "Iteration 542, loss = 0.45821753\n",
      "Iteration 543, loss = 0.45805054\n",
      "Iteration 544, loss = 0.45811392\n",
      "Iteration 545, loss = 0.45814649\n",
      "Iteration 546, loss = 0.45791051\n",
      "Iteration 547, loss = 0.45791098\n",
      "Iteration 548, loss = 0.45797215\n",
      "Iteration 549, loss = 0.45783011\n",
      "Iteration 550, loss = 0.45807437\n",
      "Iteration 551, loss = 0.45778608\n",
      "Iteration 552, loss = 0.45812767\n",
      "Iteration 553, loss = 0.45797378\n",
      "Iteration 554, loss = 0.45769896\n",
      "Iteration 555, loss = 0.45779932\n",
      "Iteration 556, loss = 0.45776505\n",
      "Iteration 557, loss = 0.45758654\n",
      "Iteration 558, loss = 0.45766580\n",
      "Iteration 559, loss = 0.45782905\n",
      "Iteration 560, loss = 0.45791575\n",
      "Iteration 561, loss = 0.45772158\n",
      "Iteration 562, loss = 0.45772766\n",
      "Iteration 563, loss = 0.45748367\n",
      "Iteration 564, loss = 0.45764970\n",
      "Iteration 565, loss = 0.45757543\n",
      "Iteration 566, loss = 0.45769249\n",
      "Iteration 567, loss = 0.45754205\n",
      "Iteration 568, loss = 0.45762993\n",
      "Iteration 569, loss = 0.45748308\n",
      "Iteration 570, loss = 0.45722825\n",
      "Iteration 571, loss = 0.45718661\n",
      "Iteration 572, loss = 0.45743570\n",
      "Iteration 573, loss = 0.45735511\n",
      "Iteration 574, loss = 0.45736177\n",
      "Iteration 575, loss = 0.45717336\n",
      "Iteration 576, loss = 0.45721728\n",
      "Iteration 577, loss = 0.45726364\n",
      "Iteration 578, loss = 0.45715680\n",
      "Iteration 579, loss = 0.45727439\n",
      "Iteration 580, loss = 0.45714728\n",
      "Iteration 581, loss = 0.45721849\n",
      "Iteration 582, loss = 0.45726740\n",
      "Iteration 583, loss = 0.45708491\n",
      "Iteration 584, loss = 0.45718209\n",
      "Iteration 585, loss = 0.45699709\n",
      "Iteration 586, loss = 0.45709551\n",
      "Iteration 587, loss = 0.45686707\n",
      "Iteration 588, loss = 0.45692944\n",
      "Iteration 589, loss = 0.45705703\n",
      "Iteration 590, loss = 0.45716850\n",
      "Iteration 591, loss = 0.45707095\n",
      "Iteration 592, loss = 0.45711181\n",
      "Iteration 593, loss = 0.45702555\n",
      "Iteration 594, loss = 0.45697749\n",
      "Iteration 595, loss = 0.45683270\n",
      "Iteration 596, loss = 0.45699740\n",
      "Iteration 597, loss = 0.45674060\n",
      "Iteration 598, loss = 0.45691448\n",
      "Iteration 599, loss = 0.45685866\n",
      "Iteration 600, loss = 0.45672108\n",
      "Iteration 601, loss = 0.45670845\n",
      "Iteration 602, loss = 0.45692678\n",
      "Iteration 603, loss = 0.45655326\n",
      "Iteration 604, loss = 0.45667680\n",
      "Iteration 605, loss = 0.45664782\n",
      "Iteration 606, loss = 0.45650621\n",
      "Iteration 607, loss = 0.45679152\n",
      "Iteration 608, loss = 0.45657642\n",
      "Iteration 609, loss = 0.45676450\n",
      "Iteration 610, loss = 0.45653901\n",
      "Iteration 611, loss = 0.45663414\n",
      "Iteration 612, loss = 0.45653766\n",
      "Iteration 613, loss = 0.45648755\n",
      "Iteration 614, loss = 0.45667257\n",
      "Iteration 615, loss = 0.45673542\n",
      "Iteration 616, loss = 0.45653734\n",
      "Iteration 617, loss = 0.45646733\n",
      "Iteration 618, loss = 0.45647249\n",
      "Iteration 619, loss = 0.45643749\n",
      "Iteration 620, loss = 0.45640158\n",
      "Iteration 621, loss = 0.45686006\n",
      "Iteration 622, loss = 0.45651197\n",
      "Iteration 623, loss = 0.45654860\n",
      "Iteration 624, loss = 0.45650907\n",
      "Iteration 625, loss = 0.45624890\n",
      "Iteration 626, loss = 0.45639670\n",
      "Iteration 627, loss = 0.45636638\n",
      "Iteration 628, loss = 0.45651338\n",
      "Iteration 629, loss = 0.45637915\n",
      "Iteration 630, loss = 0.45651713\n",
      "Iteration 631, loss = 0.45707568\n",
      "Iteration 632, loss = 0.45635260\n",
      "Iteration 633, loss = 0.45649704\n",
      "Iteration 634, loss = 0.45634478\n",
      "Iteration 635, loss = 0.45629991\n",
      "Iteration 636, loss = 0.45643283\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73221695\n",
      "Iteration 2, loss = 0.69542917\n",
      "Iteration 3, loss = 0.67258521\n",
      "Iteration 4, loss = 0.65805450\n",
      "Iteration 5, loss = 0.64660673\n",
      "Iteration 6, loss = 0.63696321\n",
      "Iteration 7, loss = 0.62812616\n",
      "Iteration 8, loss = 0.62002753\n",
      "Iteration 9, loss = 0.61207482\n",
      "Iteration 10, loss = 0.60450423\n",
      "Iteration 11, loss = 0.59686621\n",
      "Iteration 12, loss = 0.58987983\n",
      "Iteration 13, loss = 0.58319753\n",
      "Iteration 14, loss = 0.57730978\n",
      "Iteration 15, loss = 0.57183474\n",
      "Iteration 16, loss = 0.56759488\n",
      "Iteration 17, loss = 0.56373346\n",
      "Iteration 18, loss = 0.56045985\n",
      "Iteration 19, loss = 0.55794169\n",
      "Iteration 20, loss = 0.55565261\n",
      "Iteration 21, loss = 0.55367658\n",
      "Iteration 22, loss = 0.55132528\n",
      "Iteration 23, loss = 0.54936070\n",
      "Iteration 24, loss = 0.54736549\n",
      "Iteration 25, loss = 0.54509937\n",
      "Iteration 26, loss = 0.54303797\n",
      "Iteration 27, loss = 0.54107704\n",
      "Iteration 28, loss = 0.53870084\n",
      "Iteration 29, loss = 0.53675312\n",
      "Iteration 30, loss = 0.53472176\n",
      "Iteration 31, loss = 0.53269115\n",
      "Iteration 32, loss = 0.53076689\n",
      "Iteration 33, loss = 0.52844546\n",
      "Iteration 34, loss = 0.52636998\n",
      "Iteration 35, loss = 0.52441465\n",
      "Iteration 36, loss = 0.52254189\n",
      "Iteration 37, loss = 0.52076088\n",
      "Iteration 38, loss = 0.51888119\n",
      "Iteration 39, loss = 0.51732811\n",
      "Iteration 40, loss = 0.51607349\n",
      "Iteration 41, loss = 0.51471327\n",
      "Iteration 42, loss = 0.51330497\n",
      "Iteration 43, loss = 0.51239581\n",
      "Iteration 44, loss = 0.51087391\n",
      "Iteration 45, loss = 0.50985211\n",
      "Iteration 46, loss = 0.50915682\n",
      "Iteration 47, loss = 0.50805775\n",
      "Iteration 48, loss = 0.50711144\n",
      "Iteration 49, loss = 0.50655355\n",
      "Iteration 50, loss = 0.50543632\n",
      "Iteration 51, loss = 0.50476586\n",
      "Iteration 52, loss = 0.50392139\n",
      "Iteration 53, loss = 0.50349682\n",
      "Iteration 54, loss = 0.50265622\n",
      "Iteration 55, loss = 0.50210212\n",
      "Iteration 56, loss = 0.50184022\n",
      "Iteration 57, loss = 0.50096725\n",
      "Iteration 58, loss = 0.50075935\n",
      "Iteration 59, loss = 0.50039836\n",
      "Iteration 60, loss = 0.49969120\n",
      "Iteration 61, loss = 0.49924486\n",
      "Iteration 62, loss = 0.49880692\n",
      "Iteration 63, loss = 0.49846862\n",
      "Iteration 64, loss = 0.49807863\n",
      "Iteration 65, loss = 0.49773879\n",
      "Iteration 66, loss = 0.49734104\n",
      "Iteration 67, loss = 0.49698156\n",
      "Iteration 68, loss = 0.49674737\n",
      "Iteration 69, loss = 0.49647167\n",
      "Iteration 70, loss = 0.49627073\n",
      "Iteration 71, loss = 0.49600584\n",
      "Iteration 72, loss = 0.49554081\n",
      "Iteration 73, loss = 0.49543565\n",
      "Iteration 74, loss = 0.49499183\n",
      "Iteration 75, loss = 0.49495223\n",
      "Iteration 76, loss = 0.49478607\n",
      "Iteration 77, loss = 0.49472005\n",
      "Iteration 78, loss = 0.49415952\n",
      "Iteration 79, loss = 0.49387320\n",
      "Iteration 80, loss = 0.49386516\n",
      "Iteration 81, loss = 0.49349153\n",
      "Iteration 82, loss = 0.49342587\n",
      "Iteration 83, loss = 0.49282774\n",
      "Iteration 84, loss = 0.49298230\n",
      "Iteration 85, loss = 0.49277950\n",
      "Iteration 86, loss = 0.49238381\n",
      "Iteration 87, loss = 0.49221470\n",
      "Iteration 88, loss = 0.49221091\n",
      "Iteration 89, loss = 0.49173207\n",
      "Iteration 90, loss = 0.49146417\n",
      "Iteration 91, loss = 0.49132451\n",
      "Iteration 92, loss = 0.49174196\n",
      "Iteration 93, loss = 0.49090875\n",
      "Iteration 94, loss = 0.49073087\n",
      "Iteration 95, loss = 0.49048427\n",
      "Iteration 96, loss = 0.49003988\n",
      "Iteration 97, loss = 0.48988988\n",
      "Iteration 98, loss = 0.48981465\n",
      "Iteration 99, loss = 0.48970022\n",
      "Iteration 100, loss = 0.48929025\n",
      "Iteration 101, loss = 0.48930648\n",
      "Iteration 102, loss = 0.48900142\n",
      "Iteration 103, loss = 0.48888866\n",
      "Iteration 104, loss = 0.48876308\n",
      "Iteration 105, loss = 0.48864451\n",
      "Iteration 106, loss = 0.48844248\n",
      "Iteration 107, loss = 0.48822274\n",
      "Iteration 108, loss = 0.48815562\n",
      "Iteration 109, loss = 0.48775230\n",
      "Iteration 110, loss = 0.48771992\n",
      "Iteration 111, loss = 0.48755872\n",
      "Iteration 112, loss = 0.48741366\n",
      "Iteration 113, loss = 0.48746713\n",
      "Iteration 114, loss = 0.48697015\n",
      "Iteration 115, loss = 0.48710963\n",
      "Iteration 116, loss = 0.48672316\n",
      "Iteration 117, loss = 0.48678039\n",
      "Iteration 118, loss = 0.48635820\n",
      "Iteration 119, loss = 0.48624709\n",
      "Iteration 120, loss = 0.48614367\n",
      "Iteration 121, loss = 0.48609035\n",
      "Iteration 122, loss = 0.48617373\n",
      "Iteration 123, loss = 0.48558000\n",
      "Iteration 124, loss = 0.48578672\n",
      "Iteration 125, loss = 0.48557157\n",
      "Iteration 126, loss = 0.48569765\n",
      "Iteration 127, loss = 0.48537314\n",
      "Iteration 128, loss = 0.48501600\n",
      "Iteration 129, loss = 0.48480876\n",
      "Iteration 130, loss = 0.48453850\n",
      "Iteration 131, loss = 0.48456199\n",
      "Iteration 132, loss = 0.48428412\n",
      "Iteration 133, loss = 0.48424577\n",
      "Iteration 134, loss = 0.48437949\n",
      "Iteration 135, loss = 0.48399692\n",
      "Iteration 136, loss = 0.48371439\n",
      "Iteration 137, loss = 0.48384993\n",
      "Iteration 138, loss = 0.48354806\n",
      "Iteration 139, loss = 0.48333107\n",
      "Iteration 140, loss = 0.48354064\n",
      "Iteration 141, loss = 0.48317122\n",
      "Iteration 142, loss = 0.48364090\n",
      "Iteration 143, loss = 0.48319157\n",
      "Iteration 144, loss = 0.48284429\n",
      "Iteration 145, loss = 0.48268945\n",
      "Iteration 146, loss = 0.48282589\n",
      "Iteration 147, loss = 0.48236346\n",
      "Iteration 148, loss = 0.48220069\n",
      "Iteration 149, loss = 0.48216530\n",
      "Iteration 150, loss = 0.48202931\n",
      "Iteration 151, loss = 0.48186704\n",
      "Iteration 152, loss = 0.48177922\n",
      "Iteration 153, loss = 0.48154352\n",
      "Iteration 154, loss = 0.48159801\n",
      "Iteration 155, loss = 0.48144986\n",
      "Iteration 156, loss = 0.48128937\n",
      "Iteration 157, loss = 0.48141923\n",
      "Iteration 158, loss = 0.48120176\n",
      "Iteration 159, loss = 0.48104529\n",
      "Iteration 160, loss = 0.48098422\n",
      "Iteration 161, loss = 0.48074681\n",
      "Iteration 162, loss = 0.48093553\n",
      "Iteration 163, loss = 0.48083278\n",
      "Iteration 164, loss = 0.48039977\n",
      "Iteration 165, loss = 0.48104634\n",
      "Iteration 166, loss = 0.48027933\n",
      "Iteration 167, loss = 0.48015945\n",
      "Iteration 168, loss = 0.48001110\n",
      "Iteration 169, loss = 0.47997761\n",
      "Iteration 170, loss = 0.47995556\n",
      "Iteration 171, loss = 0.48005292\n",
      "Iteration 172, loss = 0.47982030\n",
      "Iteration 173, loss = 0.47973002\n",
      "Iteration 174, loss = 0.47958994\n",
      "Iteration 175, loss = 0.47931741\n",
      "Iteration 176, loss = 0.47941986\n",
      "Iteration 177, loss = 0.47912737\n",
      "Iteration 178, loss = 0.47919247\n",
      "Iteration 179, loss = 0.47906173\n",
      "Iteration 180, loss = 0.47877812\n",
      "Iteration 181, loss = 0.47887098\n",
      "Iteration 182, loss = 0.47871527\n",
      "Iteration 183, loss = 0.47859357\n",
      "Iteration 184, loss = 0.47860531\n",
      "Iteration 185, loss = 0.47885405\n",
      "Iteration 186, loss = 0.47838427\n",
      "Iteration 187, loss = 0.47867547\n",
      "Iteration 188, loss = 0.47822677\n",
      "Iteration 189, loss = 0.47823715\n",
      "Iteration 190, loss = 0.47804036\n",
      "Iteration 191, loss = 0.47809001\n",
      "Iteration 192, loss = 0.47785152\n",
      "Iteration 193, loss = 0.47800674\n",
      "Iteration 194, loss = 0.47780809\n",
      "Iteration 195, loss = 0.47771706\n",
      "Iteration 196, loss = 0.47762935\n",
      "Iteration 197, loss = 0.47755634\n",
      "Iteration 198, loss = 0.47735214\n",
      "Iteration 199, loss = 0.47712698\n",
      "Iteration 200, loss = 0.47764340\n",
      "Iteration 201, loss = 0.47701660\n",
      "Iteration 202, loss = 0.47719786\n",
      "Iteration 203, loss = 0.47722928\n",
      "Iteration 204, loss = 0.47708640\n",
      "Iteration 205, loss = 0.47711031\n",
      "Iteration 206, loss = 0.47708048\n",
      "Iteration 207, loss = 0.47685231\n",
      "Iteration 208, loss = 0.47681089\n",
      "Iteration 209, loss = 0.47654277\n",
      "Iteration 210, loss = 0.47684163\n",
      "Iteration 211, loss = 0.47635744\n",
      "Iteration 212, loss = 0.47649642\n",
      "Iteration 213, loss = 0.47641619\n",
      "Iteration 214, loss = 0.47646809\n",
      "Iteration 215, loss = 0.47638515\n",
      "Iteration 216, loss = 0.47635961\n",
      "Iteration 217, loss = 0.47609922\n",
      "Iteration 218, loss = 0.47621974\n",
      "Iteration 219, loss = 0.47613036\n",
      "Iteration 220, loss = 0.47607445\n",
      "Iteration 221, loss = 0.47592543\n",
      "Iteration 222, loss = 0.47607180\n",
      "Iteration 223, loss = 0.47629945\n",
      "Iteration 224, loss = 0.47603301\n",
      "Iteration 225, loss = 0.47580863\n",
      "Iteration 226, loss = 0.47575464\n",
      "Iteration 227, loss = 0.47571530\n",
      "Iteration 228, loss = 0.47551592\n",
      "Iteration 229, loss = 0.47528010\n",
      "Iteration 230, loss = 0.47553765\n",
      "Iteration 231, loss = 0.47551751\n",
      "Iteration 232, loss = 0.47530485\n",
      "Iteration 233, loss = 0.47560749\n",
      "Iteration 234, loss = 0.47535213\n",
      "Iteration 235, loss = 0.47518796\n",
      "Iteration 236, loss = 0.47530694\n",
      "Iteration 237, loss = 0.47490593\n",
      "Iteration 238, loss = 0.47475742\n",
      "Iteration 239, loss = 0.47457486\n",
      "Iteration 240, loss = 0.47451994\n",
      "Iteration 241, loss = 0.47442105\n",
      "Iteration 242, loss = 0.47417922\n",
      "Iteration 243, loss = 0.47414452\n",
      "Iteration 244, loss = 0.47405977\n",
      "Iteration 245, loss = 0.47409265\n",
      "Iteration 246, loss = 0.47394587\n",
      "Iteration 247, loss = 0.47377344\n",
      "Iteration 248, loss = 0.47391303\n",
      "Iteration 249, loss = 0.47440873\n",
      "Iteration 250, loss = 0.47368630\n",
      "Iteration 251, loss = 0.47373587\n",
      "Iteration 252, loss = 0.47387519\n",
      "Iteration 253, loss = 0.47394932\n",
      "Iteration 254, loss = 0.47354288\n",
      "Iteration 255, loss = 0.47336801\n",
      "Iteration 256, loss = 0.47354042\n",
      "Iteration 257, loss = 0.47327346\n",
      "Iteration 258, loss = 0.47315342\n",
      "Iteration 259, loss = 0.47304461\n",
      "Iteration 260, loss = 0.47330808\n",
      "Iteration 261, loss = 0.47333138\n",
      "Iteration 262, loss = 0.47295192\n",
      "Iteration 263, loss = 0.47310538\n",
      "Iteration 264, loss = 0.47310298\n",
      "Iteration 265, loss = 0.47277084\n",
      "Iteration 266, loss = 0.47276624\n",
      "Iteration 267, loss = 0.47288693\n",
      "Iteration 268, loss = 0.47257787\n",
      "Iteration 269, loss = 0.47261555\n",
      "Iteration 270, loss = 0.47239470\n",
      "Iteration 271, loss = 0.47227631\n",
      "Iteration 272, loss = 0.47224113\n",
      "Iteration 273, loss = 0.47224692\n",
      "Iteration 274, loss = 0.47211455\n",
      "Iteration 275, loss = 0.47226569\n",
      "Iteration 276, loss = 0.47191301\n",
      "Iteration 277, loss = 0.47175133\n",
      "Iteration 278, loss = 0.47202976\n",
      "Iteration 279, loss = 0.47198610\n",
      "Iteration 280, loss = 0.47184224\n",
      "Iteration 281, loss = 0.47152023\n",
      "Iteration 282, loss = 0.47186932\n",
      "Iteration 283, loss = 0.47180464\n",
      "Iteration 284, loss = 0.47150798\n",
      "Iteration 285, loss = 0.47164018\n",
      "Iteration 286, loss = 0.47141523\n",
      "Iteration 287, loss = 0.47127879\n",
      "Iteration 288, loss = 0.47129206\n",
      "Iteration 289, loss = 0.47129253\n",
      "Iteration 290, loss = 0.47117306\n",
      "Iteration 291, loss = 0.47100837\n",
      "Iteration 292, loss = 0.47110895\n",
      "Iteration 293, loss = 0.47102573\n",
      "Iteration 294, loss = 0.47111816\n",
      "Iteration 295, loss = 0.47080466\n",
      "Iteration 296, loss = 0.47079012\n",
      "Iteration 297, loss = 0.47114497\n",
      "Iteration 298, loss = 0.47090606\n",
      "Iteration 299, loss = 0.47061553\n",
      "Iteration 300, loss = 0.47104440\n",
      "Iteration 301, loss = 0.47070610\n",
      "Iteration 302, loss = 0.47080817\n",
      "Iteration 303, loss = 0.47066898\n",
      "Iteration 304, loss = 0.47047374\n",
      "Iteration 305, loss = 0.47074734\n",
      "Iteration 306, loss = 0.47034118\n",
      "Iteration 307, loss = 0.47035342\n",
      "Iteration 308, loss = 0.47036490\n",
      "Iteration 309, loss = 0.47019121\n",
      "Iteration 310, loss = 0.47024067\n",
      "Iteration 311, loss = 0.47016907\n",
      "Iteration 312, loss = 0.47008811\n",
      "Iteration 313, loss = 0.47006633\n",
      "Iteration 314, loss = 0.47002436\n",
      "Iteration 315, loss = 0.47014164\n",
      "Iteration 316, loss = 0.47012644\n",
      "Iteration 317, loss = 0.47010880\n",
      "Iteration 318, loss = 0.46965727\n",
      "Iteration 319, loss = 0.46988760\n",
      "Iteration 320, loss = 0.46987035\n",
      "Iteration 321, loss = 0.46972046\n",
      "Iteration 322, loss = 0.46956913\n",
      "Iteration 323, loss = 0.47022792\n",
      "Iteration 324, loss = 0.46960524\n",
      "Iteration 325, loss = 0.46954041\n",
      "Iteration 326, loss = 0.46942253\n",
      "Iteration 327, loss = 0.46925838\n",
      "Iteration 328, loss = 0.46950303\n",
      "Iteration 329, loss = 0.46930600\n",
      "Iteration 330, loss = 0.46927210\n",
      "Iteration 331, loss = 0.46932188\n",
      "Iteration 332, loss = 0.47011112\n",
      "Iteration 333, loss = 0.46935776\n",
      "Iteration 334, loss = 0.46907309\n",
      "Iteration 335, loss = 0.46912966\n",
      "Iteration 336, loss = 0.46950347\n",
      "Iteration 337, loss = 0.46891341\n",
      "Iteration 338, loss = 0.46919729\n",
      "Iteration 339, loss = 0.46875365\n",
      "Iteration 340, loss = 0.46904702\n",
      "Iteration 341, loss = 0.46859419\n",
      "Iteration 342, loss = 0.46868436\n",
      "Iteration 343, loss = 0.46877009\n",
      "Iteration 344, loss = 0.46873526\n",
      "Iteration 345, loss = 0.46874966\n",
      "Iteration 346, loss = 0.46894473\n",
      "Iteration 347, loss = 0.46851521\n",
      "Iteration 348, loss = 0.46854308\n",
      "Iteration 349, loss = 0.46844674\n",
      "Iteration 350, loss = 0.46884338\n",
      "Iteration 351, loss = 0.46822867\n",
      "Iteration 352, loss = 0.46811985\n",
      "Iteration 353, loss = 0.46830727\n",
      "Iteration 354, loss = 0.46856915\n",
      "Iteration 355, loss = 0.46837922\n",
      "Iteration 356, loss = 0.46851851\n",
      "Iteration 357, loss = 0.46833256\n",
      "Iteration 358, loss = 0.46820332\n",
      "Iteration 359, loss = 0.46800670\n",
      "Iteration 360, loss = 0.46823369\n",
      "Iteration 361, loss = 0.46815817\n",
      "Iteration 362, loss = 0.46803757\n",
      "Iteration 363, loss = 0.46816552\n",
      "Iteration 364, loss = 0.46822999\n",
      "Iteration 365, loss = 0.46817749\n",
      "Iteration 366, loss = 0.46804637\n",
      "Iteration 367, loss = 0.46798149\n",
      "Iteration 368, loss = 0.46808740\n",
      "Iteration 369, loss = 0.46777577\n",
      "Iteration 370, loss = 0.46782165\n",
      "Iteration 371, loss = 0.46761738\n",
      "Iteration 372, loss = 0.46818943\n",
      "Iteration 373, loss = 0.46818024\n",
      "Iteration 374, loss = 0.46773855\n",
      "Iteration 375, loss = 0.46766726\n",
      "Iteration 376, loss = 0.46763882\n",
      "Iteration 377, loss = 0.46758044\n",
      "Iteration 378, loss = 0.46743572\n",
      "Iteration 379, loss = 0.46743677\n",
      "Iteration 380, loss = 0.46751956\n",
      "Iteration 381, loss = 0.46761581\n",
      "Iteration 382, loss = 0.46736699\n",
      "Iteration 383, loss = 0.46748422\n",
      "Iteration 384, loss = 0.46721233\n",
      "Iteration 385, loss = 0.46766586\n",
      "Iteration 386, loss = 0.46719538\n",
      "Iteration 387, loss = 0.46712614\n",
      "Iteration 388, loss = 0.46747366\n",
      "Iteration 389, loss = 0.46708622\n",
      "Iteration 390, loss = 0.46699558\n",
      "Iteration 391, loss = 0.46707363\n",
      "Iteration 392, loss = 0.46692394\n",
      "Iteration 393, loss = 0.46683929\n",
      "Iteration 394, loss = 0.46680191\n",
      "Iteration 395, loss = 0.46676744\n",
      "Iteration 396, loss = 0.46673748\n",
      "Iteration 397, loss = 0.46681437\n",
      "Iteration 398, loss = 0.46656855\n",
      "Iteration 399, loss = 0.46701578\n",
      "Iteration 400, loss = 0.46648572\n",
      "Iteration 401, loss = 0.46679754\n",
      "Iteration 402, loss = 0.46647752\n",
      "Iteration 403, loss = 0.46636022\n",
      "Iteration 404, loss = 0.46681301\n",
      "Iteration 405, loss = 0.46700918\n",
      "Iteration 406, loss = 0.46634418\n",
      "Iteration 407, loss = 0.46643938\n",
      "Iteration 408, loss = 0.46628063\n",
      "Iteration 409, loss = 0.46639578\n",
      "Iteration 410, loss = 0.46636598\n",
      "Iteration 411, loss = 0.46633231\n",
      "Iteration 412, loss = 0.46649854\n",
      "Iteration 413, loss = 0.46604376\n",
      "Iteration 414, loss = 0.46626728\n",
      "Iteration 415, loss = 0.46611482\n",
      "Iteration 416, loss = 0.46634236\n",
      "Iteration 417, loss = 0.46623269\n",
      "Iteration 418, loss = 0.46599501\n",
      "Iteration 419, loss = 0.46604836\n",
      "Iteration 420, loss = 0.46590841\n",
      "Iteration 421, loss = 0.46573919\n",
      "Iteration 422, loss = 0.46596102\n",
      "Iteration 423, loss = 0.46574254\n",
      "Iteration 424, loss = 0.46591617\n",
      "Iteration 425, loss = 0.46567161\n",
      "Iteration 426, loss = 0.46559002\n",
      "Iteration 427, loss = 0.46574468\n",
      "Iteration 428, loss = 0.46546703\n",
      "Iteration 429, loss = 0.46571470\n",
      "Iteration 430, loss = 0.46583098\n",
      "Iteration 431, loss = 0.46547302\n",
      "Iteration 432, loss = 0.46566452\n",
      "Iteration 433, loss = 0.46570851\n",
      "Iteration 434, loss = 0.46536858\n",
      "Iteration 435, loss = 0.46527251\n",
      "Iteration 436, loss = 0.46527007\n",
      "Iteration 437, loss = 0.46517041\n",
      "Iteration 438, loss = 0.46525319\n",
      "Iteration 439, loss = 0.46553731\n",
      "Iteration 440, loss = 0.46538103\n",
      "Iteration 441, loss = 0.46514819\n",
      "Iteration 442, loss = 0.46504107\n",
      "Iteration 443, loss = 0.46503249\n",
      "Iteration 444, loss = 0.46504816\n",
      "Iteration 445, loss = 0.46501979\n",
      "Iteration 446, loss = 0.46488547\n",
      "Iteration 447, loss = 0.46479158\n",
      "Iteration 448, loss = 0.46499197\n",
      "Iteration 449, loss = 0.46498127\n",
      "Iteration 450, loss = 0.46479592\n",
      "Iteration 451, loss = 0.46493663\n",
      "Iteration 452, loss = 0.46484123\n",
      "Iteration 453, loss = 0.46479527\n",
      "Iteration 454, loss = 0.46470607\n",
      "Iteration 455, loss = 0.46470124\n",
      "Iteration 456, loss = 0.46459187\n",
      "Iteration 457, loss = 0.46467769\n",
      "Iteration 458, loss = 0.46474936\n",
      "Iteration 459, loss = 0.46499305\n",
      "Iteration 460, loss = 0.46488100\n",
      "Iteration 461, loss = 0.46466093\n",
      "Iteration 462, loss = 0.46462463\n",
      "Iteration 463, loss = 0.46449086\n",
      "Iteration 464, loss = 0.46458829\n",
      "Iteration 465, loss = 0.46446440\n",
      "Iteration 466, loss = 0.46454507\n",
      "Iteration 467, loss = 0.46421467\n",
      "Iteration 468, loss = 0.46452968\n",
      "Iteration 469, loss = 0.46429054\n",
      "Iteration 470, loss = 0.46423452\n",
      "Iteration 471, loss = 0.46419688\n",
      "Iteration 472, loss = 0.46407015\n",
      "Iteration 473, loss = 0.46427263\n",
      "Iteration 474, loss = 0.46405750\n",
      "Iteration 475, loss = 0.46404547\n",
      "Iteration 476, loss = 0.46400950\n",
      "Iteration 477, loss = 0.46413237\n",
      "Iteration 478, loss = 0.46395777\n",
      "Iteration 479, loss = 0.46413757\n",
      "Iteration 480, loss = 0.46381853\n",
      "Iteration 481, loss = 0.46402837\n",
      "Iteration 482, loss = 0.46397506\n",
      "Iteration 483, loss = 0.46370258\n",
      "Iteration 484, loss = 0.46367770\n",
      "Iteration 485, loss = 0.46365873\n",
      "Iteration 486, loss = 0.46394408\n",
      "Iteration 487, loss = 0.46416060\n",
      "Iteration 488, loss = 0.46358165\n",
      "Iteration 489, loss = 0.46383687\n",
      "Iteration 490, loss = 0.46378901\n",
      "Iteration 491, loss = 0.46366071\n",
      "Iteration 492, loss = 0.46361928\n",
      "Iteration 493, loss = 0.46366488\n",
      "Iteration 494, loss = 0.46345227\n",
      "Iteration 495, loss = 0.46439313\n",
      "Iteration 496, loss = 0.46330418\n",
      "Iteration 497, loss = 0.46369784\n",
      "Iteration 498, loss = 0.46318955\n",
      "Iteration 499, loss = 0.46318681\n",
      "Iteration 500, loss = 0.46314053\n",
      "Iteration 501, loss = 0.46318488\n",
      "Iteration 502, loss = 0.46348834\n",
      "Iteration 503, loss = 0.46317006\n",
      "Iteration 504, loss = 0.46328471\n",
      "Iteration 505, loss = 0.46313920\n",
      "Iteration 506, loss = 0.46320622\n",
      "Iteration 507, loss = 0.46317217\n",
      "Iteration 508, loss = 0.46351299\n",
      "Iteration 509, loss = 0.46307335\n",
      "Iteration 510, loss = 0.46314689\n",
      "Iteration 511, loss = 0.46295083\n",
      "Iteration 512, loss = 0.46292358\n",
      "Iteration 513, loss = 0.46303254\n",
      "Iteration 514, loss = 0.46277689\n",
      "Iteration 515, loss = 0.46282580\n",
      "Iteration 516, loss = 0.46301405\n",
      "Iteration 517, loss = 0.46256133\n",
      "Iteration 518, loss = 0.46271476\n",
      "Iteration 519, loss = 0.46292938\n",
      "Iteration 520, loss = 0.46271724\n",
      "Iteration 521, loss = 0.46276286\n",
      "Iteration 522, loss = 0.46286393\n",
      "Iteration 523, loss = 0.46279078\n",
      "Iteration 524, loss = 0.46259082\n",
      "Iteration 525, loss = 0.46245955\n",
      "Iteration 526, loss = 0.46277514\n",
      "Iteration 527, loss = 0.46255319\n",
      "Iteration 528, loss = 0.46275835\n",
      "Iteration 529, loss = 0.46276181\n",
      "Iteration 530, loss = 0.46248040\n",
      "Iteration 531, loss = 0.46241967\n",
      "Iteration 532, loss = 0.46274857\n",
      "Iteration 533, loss = 0.46237555\n",
      "Iteration 534, loss = 0.46218508\n",
      "Iteration 535, loss = 0.46244248\n",
      "Iteration 536, loss = 0.46232878\n",
      "Iteration 537, loss = 0.46230632\n",
      "Iteration 538, loss = 0.46222912\n",
      "Iteration 539, loss = 0.46234934\n",
      "Iteration 540, loss = 0.46223888\n",
      "Iteration 541, loss = 0.46220612\n",
      "Iteration 542, loss = 0.46210044\n",
      "Iteration 543, loss = 0.46212741\n",
      "Iteration 544, loss = 0.46233534\n",
      "Iteration 545, loss = 0.46248102\n",
      "Iteration 546, loss = 0.46191503\n",
      "Iteration 547, loss = 0.46197908\n",
      "Iteration 548, loss = 0.46187976\n",
      "Iteration 549, loss = 0.46226366\n",
      "Iteration 550, loss = 0.46197010\n",
      "Iteration 551, loss = 0.46208104\n",
      "Iteration 552, loss = 0.46216779\n",
      "Iteration 553, loss = 0.46212187\n",
      "Iteration 554, loss = 0.46173779\n",
      "Iteration 555, loss = 0.46181615\n",
      "Iteration 556, loss = 0.46167983\n",
      "Iteration 557, loss = 0.46198022\n",
      "Iteration 558, loss = 0.46153787\n",
      "Iteration 559, loss = 0.46154676\n",
      "Iteration 560, loss = 0.46183215\n",
      "Iteration 561, loss = 0.46167447\n",
      "Iteration 562, loss = 0.46161594\n",
      "Iteration 563, loss = 0.46147674\n",
      "Iteration 564, loss = 0.46145488\n",
      "Iteration 565, loss = 0.46121803\n",
      "Iteration 566, loss = 0.46131169\n",
      "Iteration 567, loss = 0.46143215\n",
      "Iteration 568, loss = 0.46136912\n",
      "Iteration 569, loss = 0.46136054\n",
      "Iteration 570, loss = 0.46128805\n",
      "Iteration 571, loss = 0.46124386\n",
      "Iteration 572, loss = 0.46128212\n",
      "Iteration 573, loss = 0.46160825\n",
      "Iteration 574, loss = 0.46150654\n",
      "Iteration 575, loss = 0.46114715\n",
      "Iteration 576, loss = 0.46144565\n",
      "Iteration 577, loss = 0.46166531\n",
      "Iteration 578, loss = 0.46123259\n",
      "Iteration 579, loss = 0.46124973\n",
      "Iteration 580, loss = 0.46107473\n",
      "Iteration 581, loss = 0.46126993\n",
      "Iteration 582, loss = 0.46105653\n",
      "Iteration 583, loss = 0.46115055\n",
      "Iteration 584, loss = 0.46104786\n",
      "Iteration 585, loss = 0.46140629\n",
      "Iteration 586, loss = 0.46089326\n",
      "Iteration 587, loss = 0.46178283\n",
      "Iteration 588, loss = 0.46114984\n",
      "Iteration 589, loss = 0.46120133\n",
      "Iteration 590, loss = 0.46146564\n",
      "Iteration 591, loss = 0.46079262\n",
      "Iteration 592, loss = 0.46083929\n",
      "Iteration 593, loss = 0.46085125\n",
      "Iteration 594, loss = 0.46074220\n",
      "Iteration 595, loss = 0.46098828\n",
      "Iteration 596, loss = 0.46078866\n",
      "Iteration 597, loss = 0.46072084\n",
      "Iteration 598, loss = 0.46117756\n",
      "Iteration 599, loss = 0.46087844\n",
      "Iteration 600, loss = 0.46057109\n",
      "Iteration 601, loss = 0.46114768\n",
      "Iteration 602, loss = 0.46114808\n",
      "Iteration 603, loss = 0.46065357\n",
      "Iteration 604, loss = 0.46052663\n",
      "Iteration 605, loss = 0.46091795\n",
      "Iteration 606, loss = 0.46114083\n",
      "Iteration 607, loss = 0.46053131\n",
      "Iteration 608, loss = 0.46061795\n",
      "Iteration 609, loss = 0.46026874\n",
      "Iteration 610, loss = 0.46038158\n",
      "Iteration 611, loss = 0.46079473\n",
      "Iteration 612, loss = 0.46007637\n",
      "Iteration 613, loss = 0.46044256\n",
      "Iteration 614, loss = 0.46042632\n",
      "Iteration 615, loss = 0.46024739\n",
      "Iteration 616, loss = 0.45997187\n",
      "Iteration 617, loss = 0.46033534\n",
      "Iteration 618, loss = 0.46062708\n",
      "Iteration 619, loss = 0.46146075\n",
      "Iteration 620, loss = 0.46049619\n",
      "Iteration 621, loss = 0.46014573\n",
      "Iteration 622, loss = 0.46032989\n",
      "Iteration 623, loss = 0.46008036\n",
      "Iteration 624, loss = 0.46003463\n",
      "Iteration 625, loss = 0.46004215\n",
      "Iteration 626, loss = 0.46059208\n",
      "Iteration 627, loss = 0.46009835\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84424906\n",
      "Iteration 2, loss = 0.78343606\n",
      "Iteration 3, loss = 0.74160938\n",
      "Iteration 4, loss = 0.70944597\n",
      "Iteration 5, loss = 0.68271124\n",
      "Iteration 6, loss = 0.66098103\n",
      "Iteration 7, loss = 0.64224765\n",
      "Iteration 8, loss = 0.62584243\n",
      "Iteration 9, loss = 0.61183376\n",
      "Iteration 10, loss = 0.59988249\n",
      "Iteration 11, loss = 0.59004470\n",
      "Iteration 12, loss = 0.58230002\n",
      "Iteration 13, loss = 0.57602830\n",
      "Iteration 14, loss = 0.57088825\n",
      "Iteration 15, loss = 0.56657033\n",
      "Iteration 16, loss = 0.56280670\n",
      "Iteration 17, loss = 0.56005345\n",
      "Iteration 18, loss = 0.55709719\n",
      "Iteration 19, loss = 0.55465407\n",
      "Iteration 20, loss = 0.55267746\n",
      "Iteration 21, loss = 0.55048149\n",
      "Iteration 22, loss = 0.54852669\n",
      "Iteration 23, loss = 0.54663010\n",
      "Iteration 24, loss = 0.54512968\n",
      "Iteration 25, loss = 0.54397943\n",
      "Iteration 26, loss = 0.54197091\n",
      "Iteration 27, loss = 0.54048872\n",
      "Iteration 28, loss = 0.53913914\n",
      "Iteration 29, loss = 0.53806188\n",
      "Iteration 30, loss = 0.53687573\n",
      "Iteration 31, loss = 0.53578649\n",
      "Iteration 32, loss = 0.53508791\n",
      "Iteration 33, loss = 0.53423428\n",
      "Iteration 34, loss = 0.53348815\n",
      "Iteration 35, loss = 0.53247379\n",
      "Iteration 36, loss = 0.53191674\n",
      "Iteration 37, loss = 0.53124609\n",
      "Iteration 38, loss = 0.53050056\n",
      "Iteration 39, loss = 0.52995144\n",
      "Iteration 40, loss = 0.52931562\n",
      "Iteration 41, loss = 0.52893927\n",
      "Iteration 42, loss = 0.52791369\n",
      "Iteration 43, loss = 0.52727371\n",
      "Iteration 44, loss = 0.52667631\n",
      "Iteration 45, loss = 0.52606784\n",
      "Iteration 46, loss = 0.52529612\n",
      "Iteration 47, loss = 0.52473902\n",
      "Iteration 48, loss = 0.52424924\n",
      "Iteration 49, loss = 0.52337181\n",
      "Iteration 50, loss = 0.52255018\n",
      "Iteration 51, loss = 0.52208195\n",
      "Iteration 52, loss = 0.52123131\n",
      "Iteration 53, loss = 0.52045900\n",
      "Iteration 54, loss = 0.51961308\n",
      "Iteration 55, loss = 0.51921588\n",
      "Iteration 56, loss = 0.51810176\n",
      "Iteration 57, loss = 0.51744149\n",
      "Iteration 58, loss = 0.51686791\n",
      "Iteration 59, loss = 0.51621624\n",
      "Iteration 60, loss = 0.51563001\n",
      "Iteration 61, loss = 0.51479705\n",
      "Iteration 62, loss = 0.51415513\n",
      "Iteration 63, loss = 0.51361292\n",
      "Iteration 64, loss = 0.51282089\n",
      "Iteration 65, loss = 0.51202079\n",
      "Iteration 66, loss = 0.51129154\n",
      "Iteration 67, loss = 0.51048545\n",
      "Iteration 68, loss = 0.50981902\n",
      "Iteration 69, loss = 0.50961470\n",
      "Iteration 70, loss = 0.50826534\n",
      "Iteration 71, loss = 0.50770204\n",
      "Iteration 72, loss = 0.50714259\n",
      "Iteration 73, loss = 0.50653066\n",
      "Iteration 74, loss = 0.50577861\n",
      "Iteration 75, loss = 0.50554147\n",
      "Iteration 76, loss = 0.50488892\n",
      "Iteration 77, loss = 0.50491214\n",
      "Iteration 78, loss = 0.50413882\n",
      "Iteration 79, loss = 0.50368344\n",
      "Iteration 80, loss = 0.50325345\n",
      "Iteration 81, loss = 0.50285021\n",
      "Iteration 82, loss = 0.50239258\n",
      "Iteration 83, loss = 0.50215857\n",
      "Iteration 84, loss = 0.50209935\n",
      "Iteration 85, loss = 0.50157136\n",
      "Iteration 86, loss = 0.50119414\n",
      "Iteration 87, loss = 0.50070568\n",
      "Iteration 88, loss = 0.50074984\n",
      "Iteration 89, loss = 0.50052120\n",
      "Iteration 90, loss = 0.50003857\n",
      "Iteration 91, loss = 0.50001156\n",
      "Iteration 92, loss = 0.49962333\n",
      "Iteration 93, loss = 0.49938361\n",
      "Iteration 94, loss = 0.49919773\n",
      "Iteration 95, loss = 0.49930086\n",
      "Iteration 96, loss = 0.49916828\n",
      "Iteration 97, loss = 0.49846932\n",
      "Iteration 98, loss = 0.49858205\n",
      "Iteration 99, loss = 0.49862829\n",
      "Iteration 100, loss = 0.49860855\n",
      "Iteration 101, loss = 0.49759601\n",
      "Iteration 102, loss = 0.49771605\n",
      "Iteration 103, loss = 0.49752616\n",
      "Iteration 104, loss = 0.49729552\n",
      "Iteration 105, loss = 0.49747176\n",
      "Iteration 106, loss = 0.49706292\n",
      "Iteration 107, loss = 0.49696755\n",
      "Iteration 108, loss = 0.49689783\n",
      "Iteration 109, loss = 0.49664960\n",
      "Iteration 110, loss = 0.49656587\n",
      "Iteration 111, loss = 0.49629823\n",
      "Iteration 112, loss = 0.49616009\n",
      "Iteration 113, loss = 0.49611843\n",
      "Iteration 114, loss = 0.49596706\n",
      "Iteration 115, loss = 0.49593952\n",
      "Iteration 116, loss = 0.49546738\n",
      "Iteration 117, loss = 0.49554282\n",
      "Iteration 118, loss = 0.49559534\n",
      "Iteration 119, loss = 0.49520158\n",
      "Iteration 120, loss = 0.49518193\n",
      "Iteration 121, loss = 0.49534333\n",
      "Iteration 122, loss = 0.49501223\n",
      "Iteration 123, loss = 0.49494874\n",
      "Iteration 124, loss = 0.49483126\n",
      "Iteration 125, loss = 0.49441916\n",
      "Iteration 126, loss = 0.49441090\n",
      "Iteration 127, loss = 0.49459397\n",
      "Iteration 128, loss = 0.49416904\n",
      "Iteration 129, loss = 0.49417787\n",
      "Iteration 130, loss = 0.49401306\n",
      "Iteration 131, loss = 0.49406146\n",
      "Iteration 132, loss = 0.49385506\n",
      "Iteration 133, loss = 0.49360655\n",
      "Iteration 134, loss = 0.49363511\n",
      "Iteration 135, loss = 0.49349488\n",
      "Iteration 136, loss = 0.49353080\n",
      "Iteration 137, loss = 0.49324810\n",
      "Iteration 138, loss = 0.49335891\n",
      "Iteration 139, loss = 0.49311234\n",
      "Iteration 140, loss = 0.49310155\n",
      "Iteration 141, loss = 0.49316319\n",
      "Iteration 142, loss = 0.49293636\n",
      "Iteration 143, loss = 0.49273699\n",
      "Iteration 144, loss = 0.49255558\n",
      "Iteration 145, loss = 0.49266097\n",
      "Iteration 146, loss = 0.49258391\n",
      "Iteration 147, loss = 0.49236841\n",
      "Iteration 148, loss = 0.49230841\n",
      "Iteration 149, loss = 0.49225291\n",
      "Iteration 150, loss = 0.49233260\n",
      "Iteration 151, loss = 0.49187871\n",
      "Iteration 152, loss = 0.49194360\n",
      "Iteration 153, loss = 0.49210397\n",
      "Iteration 154, loss = 0.49163718\n",
      "Iteration 155, loss = 0.49191355\n",
      "Iteration 156, loss = 0.49181383\n",
      "Iteration 157, loss = 0.49179662\n",
      "Iteration 158, loss = 0.49148858\n",
      "Iteration 159, loss = 0.49143204\n",
      "Iteration 160, loss = 0.49137648\n",
      "Iteration 161, loss = 0.49115772\n",
      "Iteration 162, loss = 0.49118979\n",
      "Iteration 163, loss = 0.49115116\n",
      "Iteration 164, loss = 0.49085989\n",
      "Iteration 165, loss = 0.49130997\n",
      "Iteration 166, loss = 0.49127577\n",
      "Iteration 167, loss = 0.49120473\n",
      "Iteration 168, loss = 0.49088374\n",
      "Iteration 169, loss = 0.49070995\n",
      "Iteration 170, loss = 0.49049524\n",
      "Iteration 171, loss = 0.49036624\n",
      "Iteration 172, loss = 0.49068276\n",
      "Iteration 173, loss = 0.49067645\n",
      "Iteration 174, loss = 0.49015187\n",
      "Iteration 175, loss = 0.49018090\n",
      "Iteration 176, loss = 0.49003004\n",
      "Iteration 177, loss = 0.49037243\n",
      "Iteration 178, loss = 0.49016890\n",
      "Iteration 179, loss = 0.49011675\n",
      "Iteration 180, loss = 0.48983110\n",
      "Iteration 181, loss = 0.48996898\n",
      "Iteration 182, loss = 0.48975105\n",
      "Iteration 183, loss = 0.48984602\n",
      "Iteration 184, loss = 0.48987791\n",
      "Iteration 185, loss = 0.48966166\n",
      "Iteration 186, loss = 0.48976949\n",
      "Iteration 187, loss = 0.48948320\n",
      "Iteration 188, loss = 0.48994565\n",
      "Iteration 189, loss = 0.48921892\n",
      "Iteration 190, loss = 0.48940495\n",
      "Iteration 191, loss = 0.48932604\n",
      "Iteration 192, loss = 0.48940902\n",
      "Iteration 193, loss = 0.48927156\n",
      "Iteration 194, loss = 0.48945863\n",
      "Iteration 195, loss = 0.48937254\n",
      "Iteration 196, loss = 0.48922665\n",
      "Iteration 197, loss = 0.48905149\n",
      "Iteration 198, loss = 0.48902794\n",
      "Iteration 199, loss = 0.48892737\n",
      "Iteration 200, loss = 0.48893115\n",
      "Iteration 201, loss = 0.48924299\n",
      "Iteration 202, loss = 0.48952871\n",
      "Iteration 203, loss = 0.48857791\n",
      "Iteration 204, loss = 0.48868107\n",
      "Iteration 205, loss = 0.48880914\n",
      "Iteration 206, loss = 0.48846609\n",
      "Iteration 207, loss = 0.48866984\n",
      "Iteration 208, loss = 0.48845364\n",
      "Iteration 209, loss = 0.48838322\n",
      "Iteration 210, loss = 0.48826180\n",
      "Iteration 211, loss = 0.48830680\n",
      "Iteration 212, loss = 0.48858868\n",
      "Iteration 213, loss = 0.48807599\n",
      "Iteration 214, loss = 0.48949338\n",
      "Iteration 215, loss = 0.48847235\n",
      "Iteration 216, loss = 0.48818886\n",
      "Iteration 217, loss = 0.48832081\n",
      "Iteration 218, loss = 0.48847374\n",
      "Iteration 219, loss = 0.48813297\n",
      "Iteration 220, loss = 0.48779540\n",
      "Iteration 221, loss = 0.48774502\n",
      "Iteration 222, loss = 0.48780768\n",
      "Iteration 223, loss = 0.48760914\n",
      "Iteration 224, loss = 0.48762689\n",
      "Iteration 225, loss = 0.48750758\n",
      "Iteration 226, loss = 0.48771477\n",
      "Iteration 227, loss = 0.48768601\n",
      "Iteration 228, loss = 0.48778710\n",
      "Iteration 229, loss = 0.48759203\n",
      "Iteration 230, loss = 0.48780425\n",
      "Iteration 231, loss = 0.48732817\n",
      "Iteration 232, loss = 0.48736658\n",
      "Iteration 233, loss = 0.48737500\n",
      "Iteration 234, loss = 0.48713509\n",
      "Iteration 235, loss = 0.48724150\n",
      "Iteration 236, loss = 0.48705711\n",
      "Iteration 237, loss = 0.48718592\n",
      "Iteration 238, loss = 0.48712627\n",
      "Iteration 239, loss = 0.48713485\n",
      "Iteration 240, loss = 0.48788450\n",
      "Iteration 241, loss = 0.48700289\n",
      "Iteration 242, loss = 0.48683232\n",
      "Iteration 243, loss = 0.48679069\n",
      "Iteration 244, loss = 0.48686475\n",
      "Iteration 245, loss = 0.48687009\n",
      "Iteration 246, loss = 0.48657262\n",
      "Iteration 247, loss = 0.48660637\n",
      "Iteration 248, loss = 0.48648239\n",
      "Iteration 249, loss = 0.48642546\n",
      "Iteration 250, loss = 0.48656719\n",
      "Iteration 251, loss = 0.48665257\n",
      "Iteration 252, loss = 0.48640335\n",
      "Iteration 253, loss = 0.48629568\n",
      "Iteration 254, loss = 0.48641706\n",
      "Iteration 255, loss = 0.48645517\n",
      "Iteration 256, loss = 0.48638114\n",
      "Iteration 257, loss = 0.48682481\n",
      "Iteration 258, loss = 0.48615233\n",
      "Iteration 259, loss = 0.48610346\n",
      "Iteration 260, loss = 0.48617406\n",
      "Iteration 261, loss = 0.48612673\n",
      "Iteration 262, loss = 0.48591123\n",
      "Iteration 263, loss = 0.48595927\n",
      "Iteration 264, loss = 0.48561657\n",
      "Iteration 265, loss = 0.48573278\n",
      "Iteration 266, loss = 0.48565012\n",
      "Iteration 267, loss = 0.48588284\n",
      "Iteration 268, loss = 0.48554341\n",
      "Iteration 269, loss = 0.48573663\n",
      "Iteration 270, loss = 0.48537508\n",
      "Iteration 271, loss = 0.48566495\n",
      "Iteration 272, loss = 0.48574555\n",
      "Iteration 273, loss = 0.48536068\n",
      "Iteration 274, loss = 0.48559245\n",
      "Iteration 275, loss = 0.48514596\n",
      "Iteration 276, loss = 0.48571017\n",
      "Iteration 277, loss = 0.48550073\n",
      "Iteration 278, loss = 0.48636473\n",
      "Iteration 279, loss = 0.48580673\n",
      "Iteration 280, loss = 0.48521852\n",
      "Iteration 281, loss = 0.48533665\n",
      "Iteration 282, loss = 0.48501984\n",
      "Iteration 283, loss = 0.48490284\n",
      "Iteration 284, loss = 0.48534000\n",
      "Iteration 285, loss = 0.48507806\n",
      "Iteration 286, loss = 0.48479197\n",
      "Iteration 287, loss = 0.48486645\n",
      "Iteration 288, loss = 0.48492732\n",
      "Iteration 289, loss = 0.48469966\n",
      "Iteration 290, loss = 0.48467385\n",
      "Iteration 291, loss = 0.48454261\n",
      "Iteration 292, loss = 0.48465289\n",
      "Iteration 293, loss = 0.48463078\n",
      "Iteration 294, loss = 0.48461246\n",
      "Iteration 295, loss = 0.48473067\n",
      "Iteration 296, loss = 0.48467440\n",
      "Iteration 297, loss = 0.48469211\n",
      "Iteration 298, loss = 0.48444691\n",
      "Iteration 299, loss = 0.48484220\n",
      "Iteration 300, loss = 0.48426401\n",
      "Iteration 301, loss = 0.48451876\n",
      "Iteration 302, loss = 0.48450914\n",
      "Iteration 303, loss = 0.48460975\n",
      "Iteration 304, loss = 0.48498617\n",
      "Iteration 305, loss = 0.48448769\n",
      "Iteration 306, loss = 0.48439121\n",
      "Iteration 307, loss = 0.48424165\n",
      "Iteration 308, loss = 0.48406532\n",
      "Iteration 309, loss = 0.48406891\n",
      "Iteration 310, loss = 0.48410090\n",
      "Iteration 311, loss = 0.48405441\n",
      "Iteration 312, loss = 0.48396331\n",
      "Iteration 313, loss = 0.48438974\n",
      "Iteration 314, loss = 0.48400101\n",
      "Iteration 315, loss = 0.48374770\n",
      "Iteration 316, loss = 0.48407197\n",
      "Iteration 317, loss = 0.48447006\n",
      "Iteration 318, loss = 0.48406011\n",
      "Iteration 319, loss = 0.48361993\n",
      "Iteration 320, loss = 0.48384952\n",
      "Iteration 321, loss = 0.48405641\n",
      "Iteration 322, loss = 0.48378102\n",
      "Iteration 323, loss = 0.48380049\n",
      "Iteration 324, loss = 0.48366662\n",
      "Iteration 325, loss = 0.48380312\n",
      "Iteration 326, loss = 0.48358693\n",
      "Iteration 327, loss = 0.48365281\n",
      "Iteration 328, loss = 0.48345665\n",
      "Iteration 329, loss = 0.48373622\n",
      "Iteration 330, loss = 0.48356989\n",
      "Iteration 331, loss = 0.48402083\n",
      "Iteration 332, loss = 0.48361881\n",
      "Iteration 333, loss = 0.48414005\n",
      "Iteration 334, loss = 0.48369114\n",
      "Iteration 335, loss = 0.48353434\n",
      "Iteration 336, loss = 0.48374687\n",
      "Iteration 337, loss = 0.48352710\n",
      "Iteration 338, loss = 0.48326829\n",
      "Iteration 339, loss = 0.48335443\n",
      "Iteration 340, loss = 0.48314432\n",
      "Iteration 341, loss = 0.48316898\n",
      "Iteration 342, loss = 0.48349168\n",
      "Iteration 343, loss = 0.48380438\n",
      "Iteration 344, loss = 0.48303178\n",
      "Iteration 345, loss = 0.48330474\n",
      "Iteration 346, loss = 0.48328551\n",
      "Iteration 347, loss = 0.48299326\n",
      "Iteration 348, loss = 0.48312110\n",
      "Iteration 349, loss = 0.48304573\n",
      "Iteration 350, loss = 0.48305511\n",
      "Iteration 351, loss = 0.48331937\n",
      "Iteration 352, loss = 0.48339880\n",
      "Iteration 353, loss = 0.48311610\n",
      "Iteration 354, loss = 0.48310336\n",
      "Iteration 355, loss = 0.48310930\n",
      "Iteration 356, loss = 0.48312159\n",
      "Iteration 357, loss = 0.48272939\n",
      "Iteration 358, loss = 0.48297334\n",
      "Iteration 359, loss = 0.48295844\n",
      "Iteration 360, loss = 0.48264371\n",
      "Iteration 361, loss = 0.48299938\n",
      "Iteration 362, loss = 0.48291346\n",
      "Iteration 363, loss = 0.48356824\n",
      "Iteration 364, loss = 0.48274794\n",
      "Iteration 365, loss = 0.48270975\n",
      "Iteration 366, loss = 0.48307519\n",
      "Iteration 367, loss = 0.48237438\n",
      "Iteration 368, loss = 0.48279665\n",
      "Iteration 369, loss = 0.48264254\n",
      "Iteration 370, loss = 0.48240065\n",
      "Iteration 371, loss = 0.48281559\n",
      "Iteration 372, loss = 0.48247176\n",
      "Iteration 373, loss = 0.48225433\n",
      "Iteration 374, loss = 0.48240565\n",
      "Iteration 375, loss = 0.48272299\n",
      "Iteration 376, loss = 0.48237280\n",
      "Iteration 377, loss = 0.48258405\n",
      "Iteration 378, loss = 0.48222478\n",
      "Iteration 379, loss = 0.48216736\n",
      "Iteration 380, loss = 0.48236583\n",
      "Iteration 381, loss = 0.48245528\n",
      "Iteration 382, loss = 0.48210650\n",
      "Iteration 383, loss = 0.48209001\n",
      "Iteration 384, loss = 0.48197473\n",
      "Iteration 385, loss = 0.48179250\n",
      "Iteration 386, loss = 0.48203632\n",
      "Iteration 387, loss = 0.48229543\n",
      "Iteration 388, loss = 0.48181551\n",
      "Iteration 389, loss = 0.48187858\n",
      "Iteration 390, loss = 0.48220119\n",
      "Iteration 391, loss = 0.48197013\n",
      "Iteration 392, loss = 0.48201493\n",
      "Iteration 393, loss = 0.48185415\n",
      "Iteration 394, loss = 0.48173897\n",
      "Iteration 395, loss = 0.48153245\n",
      "Iteration 396, loss = 0.48148208\n",
      "Iteration 397, loss = 0.48148267\n",
      "Iteration 398, loss = 0.48164186\n",
      "Iteration 399, loss = 0.48175237\n",
      "Iteration 400, loss = 0.48144819\n",
      "Iteration 401, loss = 0.48166481\n",
      "Iteration 402, loss = 0.48166479\n",
      "Iteration 403, loss = 0.48150636\n",
      "Iteration 404, loss = 0.48140555\n",
      "Iteration 405, loss = 0.48135072\n",
      "Iteration 406, loss = 0.48145766\n",
      "Iteration 407, loss = 0.48155450\n",
      "Iteration 408, loss = 0.48168563\n",
      "Iteration 409, loss = 0.48187679\n",
      "Iteration 410, loss = 0.48116955\n",
      "Iteration 411, loss = 0.48140615\n",
      "Iteration 412, loss = 0.48121235\n",
      "Iteration 413, loss = 0.48131165\n",
      "Iteration 414, loss = 0.48105130\n",
      "Iteration 415, loss = 0.48120750\n",
      "Iteration 416, loss = 0.48117746\n",
      "Iteration 417, loss = 0.48110619\n",
      "Iteration 418, loss = 0.48132429\n",
      "Iteration 419, loss = 0.48105883\n",
      "Iteration 420, loss = 0.48093146\n",
      "Iteration 421, loss = 0.48095238\n",
      "Iteration 422, loss = 0.48098815\n",
      "Iteration 423, loss = 0.48082172\n",
      "Iteration 424, loss = 0.48113119\n",
      "Iteration 425, loss = 0.48097408\n",
      "Iteration 426, loss = 0.48071307\n",
      "Iteration 427, loss = 0.48081607\n",
      "Iteration 428, loss = 0.48060701\n",
      "Iteration 429, loss = 0.48056582\n",
      "Iteration 430, loss = 0.48057077\n",
      "Iteration 431, loss = 0.48046522\n",
      "Iteration 432, loss = 0.48056775\n",
      "Iteration 433, loss = 0.48050660\n",
      "Iteration 434, loss = 0.48069262\n",
      "Iteration 435, loss = 0.48103455\n",
      "Iteration 436, loss = 0.48024157\n",
      "Iteration 437, loss = 0.48056649\n",
      "Iteration 438, loss = 0.48015985\n",
      "Iteration 439, loss = 0.48021440\n",
      "Iteration 440, loss = 0.48049582\n",
      "Iteration 441, loss = 0.48028542\n",
      "Iteration 442, loss = 0.48046165\n",
      "Iteration 443, loss = 0.48018749\n",
      "Iteration 444, loss = 0.48005946\n",
      "Iteration 445, loss = 0.48043569\n",
      "Iteration 446, loss = 0.48001477\n",
      "Iteration 447, loss = 0.48029204\n",
      "Iteration 448, loss = 0.48028357\n",
      "Iteration 449, loss = 0.48027135\n",
      "Iteration 450, loss = 0.48047031\n",
      "Iteration 451, loss = 0.48013421\n",
      "Iteration 452, loss = 0.48015928\n",
      "Iteration 453, loss = 0.48053494\n",
      "Iteration 454, loss = 0.48059330\n",
      "Iteration 455, loss = 0.48065625\n",
      "Iteration 456, loss = 0.48059837\n",
      "Iteration 457, loss = 0.48003086\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85421952\n",
      "Iteration 2, loss = 0.78008361\n",
      "Iteration 3, loss = 0.73105039\n",
      "Iteration 4, loss = 0.69698983\n",
      "Iteration 5, loss = 0.67412767\n",
      "Iteration 6, loss = 0.65657667\n",
      "Iteration 7, loss = 0.64190564\n",
      "Iteration 8, loss = 0.62911408\n",
      "Iteration 9, loss = 0.61788777\n",
      "Iteration 10, loss = 0.60766122\n",
      "Iteration 11, loss = 0.59866404\n",
      "Iteration 12, loss = 0.59042717\n",
      "Iteration 13, loss = 0.58374593\n",
      "Iteration 14, loss = 0.57832446\n",
      "Iteration 15, loss = 0.57356054\n",
      "Iteration 16, loss = 0.56996178\n",
      "Iteration 17, loss = 0.56719981\n",
      "Iteration 18, loss = 0.56462901\n",
      "Iteration 19, loss = 0.56276470\n",
      "Iteration 20, loss = 0.56120355\n",
      "Iteration 21, loss = 0.55956300\n",
      "Iteration 22, loss = 0.55821644\n",
      "Iteration 23, loss = 0.55693531\n",
      "Iteration 24, loss = 0.55557150\n",
      "Iteration 25, loss = 0.55416212\n",
      "Iteration 26, loss = 0.55288568\n",
      "Iteration 27, loss = 0.55143705\n",
      "Iteration 28, loss = 0.54997996\n",
      "Iteration 29, loss = 0.54859297\n",
      "Iteration 30, loss = 0.54752102\n",
      "Iteration 31, loss = 0.54605431\n",
      "Iteration 32, loss = 0.54486796\n",
      "Iteration 33, loss = 0.54358298\n",
      "Iteration 34, loss = 0.54242111\n",
      "Iteration 35, loss = 0.54136685\n",
      "Iteration 36, loss = 0.54014384\n",
      "Iteration 37, loss = 0.53903086\n",
      "Iteration 38, loss = 0.53812887\n",
      "Iteration 39, loss = 0.53713390\n",
      "Iteration 40, loss = 0.53588652\n",
      "Iteration 41, loss = 0.53499118\n",
      "Iteration 42, loss = 0.53396299\n",
      "Iteration 43, loss = 0.53308963\n",
      "Iteration 44, loss = 0.53174495\n",
      "Iteration 45, loss = 0.53067727\n",
      "Iteration 46, loss = 0.52963007\n",
      "Iteration 47, loss = 0.52841563\n",
      "Iteration 48, loss = 0.52754713\n",
      "Iteration 49, loss = 0.52664751\n",
      "Iteration 50, loss = 0.52582405\n",
      "Iteration 51, loss = 0.52496843\n",
      "Iteration 52, loss = 0.52443133\n",
      "Iteration 53, loss = 0.52351109\n",
      "Iteration 54, loss = 0.52270913\n",
      "Iteration 55, loss = 0.52186411\n",
      "Iteration 56, loss = 0.52136221\n",
      "Iteration 57, loss = 0.52049119\n",
      "Iteration 58, loss = 0.51986750\n",
      "Iteration 59, loss = 0.51919858\n",
      "Iteration 60, loss = 0.51892473\n",
      "Iteration 61, loss = 0.51790899\n",
      "Iteration 62, loss = 0.51735582\n",
      "Iteration 63, loss = 0.51668948\n",
      "Iteration 64, loss = 0.51613476\n",
      "Iteration 65, loss = 0.51548553\n",
      "Iteration 66, loss = 0.51486465\n",
      "Iteration 67, loss = 0.51459783\n",
      "Iteration 68, loss = 0.51376332\n",
      "Iteration 69, loss = 0.51310317\n",
      "Iteration 70, loss = 0.51253161\n",
      "Iteration 71, loss = 0.51217904\n",
      "Iteration 72, loss = 0.51142857\n",
      "Iteration 73, loss = 0.51107640\n",
      "Iteration 74, loss = 0.51064603\n",
      "Iteration 75, loss = 0.51042241\n",
      "Iteration 76, loss = 0.50968349\n",
      "Iteration 77, loss = 0.50895105\n",
      "Iteration 78, loss = 0.50882668\n",
      "Iteration 79, loss = 0.50822041\n",
      "Iteration 80, loss = 0.50809495\n",
      "Iteration 81, loss = 0.50775643\n",
      "Iteration 82, loss = 0.50721655\n",
      "Iteration 83, loss = 0.50696181\n",
      "Iteration 84, loss = 0.50677641\n",
      "Iteration 85, loss = 0.50637299\n",
      "Iteration 86, loss = 0.50613310\n",
      "Iteration 87, loss = 0.50570338\n",
      "Iteration 88, loss = 0.50536366\n",
      "Iteration 89, loss = 0.50506225\n",
      "Iteration 90, loss = 0.50462124\n",
      "Iteration 91, loss = 0.50431236\n",
      "Iteration 92, loss = 0.50407768\n",
      "Iteration 93, loss = 0.50406343\n",
      "Iteration 94, loss = 0.50338527\n",
      "Iteration 95, loss = 0.50330291\n",
      "Iteration 96, loss = 0.50311445\n",
      "Iteration 97, loss = 0.50253307\n",
      "Iteration 98, loss = 0.50250282\n",
      "Iteration 99, loss = 0.50221150\n",
      "Iteration 100, loss = 0.50191419\n",
      "Iteration 101, loss = 0.50172506\n",
      "Iteration 102, loss = 0.50170663\n",
      "Iteration 103, loss = 0.50137016\n",
      "Iteration 104, loss = 0.50097435\n",
      "Iteration 105, loss = 0.50116623\n",
      "Iteration 106, loss = 0.50061160\n",
      "Iteration 107, loss = 0.50073768\n",
      "Iteration 108, loss = 0.50012650\n",
      "Iteration 109, loss = 0.49995788\n",
      "Iteration 110, loss = 0.49985760\n",
      "Iteration 111, loss = 0.49980398\n",
      "Iteration 112, loss = 0.49964950\n",
      "Iteration 113, loss = 0.49946391\n",
      "Iteration 114, loss = 0.49964363\n",
      "Iteration 115, loss = 0.49936323\n",
      "Iteration 116, loss = 0.49907690\n",
      "Iteration 117, loss = 0.49906414\n",
      "Iteration 118, loss = 0.49903210\n",
      "Iteration 119, loss = 0.49900455\n",
      "Iteration 120, loss = 0.49837535\n",
      "Iteration 121, loss = 0.49844565\n",
      "Iteration 122, loss = 0.49823394\n",
      "Iteration 123, loss = 0.49800056\n",
      "Iteration 124, loss = 0.49805804\n",
      "Iteration 125, loss = 0.49803335\n",
      "Iteration 126, loss = 0.49807417\n",
      "Iteration 127, loss = 0.49800889\n",
      "Iteration 128, loss = 0.49750766\n",
      "Iteration 129, loss = 0.49739272\n",
      "Iteration 130, loss = 0.49717501\n",
      "Iteration 131, loss = 0.49700459\n",
      "Iteration 132, loss = 0.49741584\n",
      "Iteration 133, loss = 0.49703019\n",
      "Iteration 134, loss = 0.49691938\n",
      "Iteration 135, loss = 0.49635946\n",
      "Iteration 136, loss = 0.49662020\n",
      "Iteration 137, loss = 0.49637601\n",
      "Iteration 138, loss = 0.49615554\n",
      "Iteration 139, loss = 0.49621155\n",
      "Iteration 140, loss = 0.49605958\n",
      "Iteration 141, loss = 0.49596143\n",
      "Iteration 142, loss = 0.49572039\n",
      "Iteration 143, loss = 0.49599869\n",
      "Iteration 144, loss = 0.49543953\n",
      "Iteration 145, loss = 0.49548320\n",
      "Iteration 146, loss = 0.49525086\n",
      "Iteration 147, loss = 0.49525434\n",
      "Iteration 148, loss = 0.49530038\n",
      "Iteration 149, loss = 0.49480264\n",
      "Iteration 150, loss = 0.49498930\n",
      "Iteration 151, loss = 0.49475773\n",
      "Iteration 152, loss = 0.49482192\n",
      "Iteration 153, loss = 0.49451761\n",
      "Iteration 154, loss = 0.49443696\n",
      "Iteration 155, loss = 0.49428688\n",
      "Iteration 156, loss = 0.49446972\n",
      "Iteration 157, loss = 0.49411593\n",
      "Iteration 158, loss = 0.49407936\n",
      "Iteration 159, loss = 0.49389256\n",
      "Iteration 160, loss = 0.49394818\n",
      "Iteration 161, loss = 0.49395058\n",
      "Iteration 162, loss = 0.49454216\n",
      "Iteration 163, loss = 0.49376830\n",
      "Iteration 164, loss = 0.49401392\n",
      "Iteration 165, loss = 0.49336073\n",
      "Iteration 166, loss = 0.49360458\n",
      "Iteration 167, loss = 0.49312395\n",
      "Iteration 168, loss = 0.49299304\n",
      "Iteration 169, loss = 0.49303787\n",
      "Iteration 170, loss = 0.49273708\n",
      "Iteration 171, loss = 0.49328777\n",
      "Iteration 172, loss = 0.49322401\n",
      "Iteration 173, loss = 0.49275267\n",
      "Iteration 174, loss = 0.49262363\n",
      "Iteration 175, loss = 0.49264219\n",
      "Iteration 176, loss = 0.49245521\n",
      "Iteration 177, loss = 0.49229095\n",
      "Iteration 178, loss = 0.49276803\n",
      "Iteration 179, loss = 0.49327986\n",
      "Iteration 180, loss = 0.49383622\n",
      "Iteration 181, loss = 0.49243298\n",
      "Iteration 182, loss = 0.49198974\n",
      "Iteration 183, loss = 0.49213701\n",
      "Iteration 184, loss = 0.49171842\n",
      "Iteration 185, loss = 0.49149447\n",
      "Iteration 186, loss = 0.49140312\n",
      "Iteration 187, loss = 0.49142479\n",
      "Iteration 188, loss = 0.49134306\n",
      "Iteration 189, loss = 0.49142054\n",
      "Iteration 190, loss = 0.49153214\n",
      "Iteration 191, loss = 0.49118182\n",
      "Iteration 192, loss = 0.49104282\n",
      "Iteration 193, loss = 0.49106884\n",
      "Iteration 194, loss = 0.49070124\n",
      "Iteration 195, loss = 0.49064105\n",
      "Iteration 196, loss = 0.49058482\n",
      "Iteration 197, loss = 0.49045038\n",
      "Iteration 198, loss = 0.49020100\n",
      "Iteration 199, loss = 0.49031379\n",
      "Iteration 200, loss = 0.49029707\n",
      "Iteration 201, loss = 0.49027870\n",
      "Iteration 202, loss = 0.49002055\n",
      "Iteration 203, loss = 0.49014667\n",
      "Iteration 204, loss = 0.49062362\n",
      "Iteration 205, loss = 0.48993633\n",
      "Iteration 206, loss = 0.48969990\n",
      "Iteration 207, loss = 0.48955089\n",
      "Iteration 208, loss = 0.48959682\n",
      "Iteration 209, loss = 0.48943351\n",
      "Iteration 210, loss = 0.48918374\n",
      "Iteration 211, loss = 0.48953964\n",
      "Iteration 212, loss = 0.48911707\n",
      "Iteration 213, loss = 0.48907369\n",
      "Iteration 214, loss = 0.48917538\n",
      "Iteration 215, loss = 0.48883405\n",
      "Iteration 216, loss = 0.48897226\n",
      "Iteration 217, loss = 0.48860832\n",
      "Iteration 218, loss = 0.48839228\n",
      "Iteration 219, loss = 0.48854774\n",
      "Iteration 220, loss = 0.48834355\n",
      "Iteration 221, loss = 0.48846931\n",
      "Iteration 222, loss = 0.48861159\n",
      "Iteration 223, loss = 0.48795517\n",
      "Iteration 224, loss = 0.48793894\n",
      "Iteration 225, loss = 0.48778744\n",
      "Iteration 226, loss = 0.48770085\n",
      "Iteration 227, loss = 0.48752247\n",
      "Iteration 228, loss = 0.48749273\n",
      "Iteration 229, loss = 0.48741707\n",
      "Iteration 230, loss = 0.48722347\n",
      "Iteration 231, loss = 0.48713673\n",
      "Iteration 232, loss = 0.48714592\n",
      "Iteration 233, loss = 0.48722690\n",
      "Iteration 234, loss = 0.48667723\n",
      "Iteration 235, loss = 0.48657872\n",
      "Iteration 236, loss = 0.48673574\n",
      "Iteration 237, loss = 0.48629589\n",
      "Iteration 238, loss = 0.48659857\n",
      "Iteration 239, loss = 0.48599725\n",
      "Iteration 240, loss = 0.48639706\n",
      "Iteration 241, loss = 0.48581953\n",
      "Iteration 242, loss = 0.48587949\n",
      "Iteration 243, loss = 0.48582468\n",
      "Iteration 244, loss = 0.48564729\n",
      "Iteration 245, loss = 0.48570591\n",
      "Iteration 246, loss = 0.48530211\n",
      "Iteration 247, loss = 0.48518128\n",
      "Iteration 248, loss = 0.48514218\n",
      "Iteration 249, loss = 0.48500382\n",
      "Iteration 250, loss = 0.48535695\n",
      "Iteration 251, loss = 0.48509750\n",
      "Iteration 252, loss = 0.48466488\n",
      "Iteration 253, loss = 0.48455318\n",
      "Iteration 254, loss = 0.48469861\n",
      "Iteration 255, loss = 0.48498038\n",
      "Iteration 256, loss = 0.48409552\n",
      "Iteration 257, loss = 0.48414170\n",
      "Iteration 258, loss = 0.48413678\n",
      "Iteration 259, loss = 0.48415851\n",
      "Iteration 260, loss = 0.48416791\n",
      "Iteration 261, loss = 0.48410553\n",
      "Iteration 262, loss = 0.48393663\n",
      "Iteration 263, loss = 0.48360490\n",
      "Iteration 264, loss = 0.48363757\n",
      "Iteration 265, loss = 0.48344892\n",
      "Iteration 266, loss = 0.48349201\n",
      "Iteration 267, loss = 0.48358515\n",
      "Iteration 268, loss = 0.48330889\n",
      "Iteration 269, loss = 0.48447841\n",
      "Iteration 270, loss = 0.48335039\n",
      "Iteration 271, loss = 0.48310192\n",
      "Iteration 272, loss = 0.48311540\n",
      "Iteration 273, loss = 0.48334497\n",
      "Iteration 274, loss = 0.48356569\n",
      "Iteration 275, loss = 0.48275928\n",
      "Iteration 276, loss = 0.48277659\n",
      "Iteration 277, loss = 0.48245853\n",
      "Iteration 278, loss = 0.48271631\n",
      "Iteration 279, loss = 0.48278622\n",
      "Iteration 280, loss = 0.48205820\n",
      "Iteration 281, loss = 0.48214562\n",
      "Iteration 282, loss = 0.48238570\n",
      "Iteration 283, loss = 0.48226427\n",
      "Iteration 284, loss = 0.48228574\n",
      "Iteration 285, loss = 0.48204461\n",
      "Iteration 286, loss = 0.48174624\n",
      "Iteration 287, loss = 0.48204784\n",
      "Iteration 288, loss = 0.48241193\n",
      "Iteration 289, loss = 0.48163360\n",
      "Iteration 290, loss = 0.48144401\n",
      "Iteration 291, loss = 0.48194698\n",
      "Iteration 292, loss = 0.48202423\n",
      "Iteration 293, loss = 0.48198609\n",
      "Iteration 294, loss = 0.48151976\n",
      "Iteration 295, loss = 0.48138146\n",
      "Iteration 296, loss = 0.48135578\n",
      "Iteration 297, loss = 0.48135767\n",
      "Iteration 298, loss = 0.48129972\n",
      "Iteration 299, loss = 0.48210664\n",
      "Iteration 300, loss = 0.48132556\n",
      "Iteration 301, loss = 0.48130212\n",
      "Iteration 302, loss = 0.48118750\n",
      "Iteration 303, loss = 0.48083849\n",
      "Iteration 304, loss = 0.48120887\n",
      "Iteration 305, loss = 0.48114963\n",
      "Iteration 306, loss = 0.48089619\n",
      "Iteration 307, loss = 0.48076465\n",
      "Iteration 308, loss = 0.48100308\n",
      "Iteration 309, loss = 0.48074526\n",
      "Iteration 310, loss = 0.48123829\n",
      "Iteration 311, loss = 0.48098229\n",
      "Iteration 312, loss = 0.48070116\n",
      "Iteration 313, loss = 0.48112098\n",
      "Iteration 314, loss = 0.48059777\n",
      "Iteration 315, loss = 0.48056465\n",
      "Iteration 316, loss = 0.48093303\n",
      "Iteration 317, loss = 0.48062830\n",
      "Iteration 318, loss = 0.48029946\n",
      "Iteration 319, loss = 0.48070703\n",
      "Iteration 320, loss = 0.48049735\n",
      "Iteration 321, loss = 0.48011214\n",
      "Iteration 322, loss = 0.48019262\n",
      "Iteration 323, loss = 0.48002439\n",
      "Iteration 324, loss = 0.48002975\n",
      "Iteration 325, loss = 0.48053529\n",
      "Iteration 326, loss = 0.48057597\n",
      "Iteration 327, loss = 0.47992902\n",
      "Iteration 328, loss = 0.48038924\n",
      "Iteration 329, loss = 0.47996999\n",
      "Iteration 330, loss = 0.48077178\n",
      "Iteration 331, loss = 0.47995368\n",
      "Iteration 332, loss = 0.47967256\n",
      "Iteration 333, loss = 0.47976893\n",
      "Iteration 334, loss = 0.47959193\n",
      "Iteration 335, loss = 0.47996248\n",
      "Iteration 336, loss = 0.47940981\n",
      "Iteration 337, loss = 0.47957982\n",
      "Iteration 338, loss = 0.47964595\n",
      "Iteration 339, loss = 0.47928496\n",
      "Iteration 340, loss = 0.47943813\n",
      "Iteration 341, loss = 0.47936844\n",
      "Iteration 342, loss = 0.47987259\n",
      "Iteration 343, loss = 0.48027774\n",
      "Iteration 344, loss = 0.47947446\n",
      "Iteration 345, loss = 0.47906105\n",
      "Iteration 346, loss = 0.47945124\n",
      "Iteration 347, loss = 0.47953839\n",
      "Iteration 348, loss = 0.47928908\n",
      "Iteration 349, loss = 0.47898274\n",
      "Iteration 350, loss = 0.47913583\n",
      "Iteration 351, loss = 0.47896106\n",
      "Iteration 352, loss = 0.47945604\n",
      "Iteration 353, loss = 0.47901107\n",
      "Iteration 354, loss = 0.47887050\n",
      "Iteration 355, loss = 0.47954818\n",
      "Iteration 356, loss = 0.47995914\n",
      "Iteration 357, loss = 0.47975530\n",
      "Iteration 358, loss = 0.47881926\n",
      "Iteration 359, loss = 0.47894866\n",
      "Iteration 360, loss = 0.47915098\n",
      "Iteration 361, loss = 0.47910639\n",
      "Iteration 362, loss = 0.47877446\n",
      "Iteration 363, loss = 0.47851358\n",
      "Iteration 364, loss = 0.47844603\n",
      "Iteration 365, loss = 0.47840321\n",
      "Iteration 366, loss = 0.47879290\n",
      "Iteration 367, loss = 0.47879260\n",
      "Iteration 368, loss = 0.47812293\n",
      "Iteration 369, loss = 0.47853049\n",
      "Iteration 370, loss = 0.47832918\n",
      "Iteration 371, loss = 0.47862240\n",
      "Iteration 372, loss = 0.47820568\n",
      "Iteration 373, loss = 0.47841039\n",
      "Iteration 374, loss = 0.47842197\n",
      "Iteration 375, loss = 0.47823681\n",
      "Iteration 376, loss = 0.47793856\n",
      "Iteration 377, loss = 0.47834847\n",
      "Iteration 378, loss = 0.47784947\n",
      "Iteration 379, loss = 0.47855065\n",
      "Iteration 380, loss = 0.47783093\n",
      "Iteration 381, loss = 0.47823701\n",
      "Iteration 382, loss = 0.47785523\n",
      "Iteration 383, loss = 0.47818800\n",
      "Iteration 384, loss = 0.47795028\n",
      "Iteration 385, loss = 0.47891598\n",
      "Iteration 386, loss = 0.47795638\n",
      "Iteration 387, loss = 0.47780384\n",
      "Iteration 388, loss = 0.47806226\n",
      "Iteration 389, loss = 0.47808935\n",
      "Iteration 390, loss = 0.47769948\n",
      "Iteration 391, loss = 0.47761894\n",
      "Iteration 392, loss = 0.47795048\n",
      "Iteration 393, loss = 0.47774456\n",
      "Iteration 394, loss = 0.47778931\n",
      "Iteration 395, loss = 0.47776274\n",
      "Iteration 396, loss = 0.47764282\n",
      "Iteration 397, loss = 0.47756648\n",
      "Iteration 398, loss = 0.47775800\n",
      "Iteration 399, loss = 0.47750452\n",
      "Iteration 400, loss = 0.47758041\n",
      "Iteration 401, loss = 0.47741182\n",
      "Iteration 402, loss = 0.47732806\n",
      "Iteration 403, loss = 0.47749268\n",
      "Iteration 404, loss = 0.47768564\n",
      "Iteration 405, loss = 0.47723365\n",
      "Iteration 406, loss = 0.47793343\n",
      "Iteration 407, loss = 0.47738550\n",
      "Iteration 408, loss = 0.47754628\n",
      "Iteration 409, loss = 0.47755259\n",
      "Iteration 410, loss = 0.47811019\n",
      "Iteration 411, loss = 0.47751580\n",
      "Iteration 412, loss = 0.47714456\n",
      "Iteration 413, loss = 0.47709139\n",
      "Iteration 414, loss = 0.47713874\n",
      "Iteration 415, loss = 0.47684988\n",
      "Iteration 416, loss = 0.47756583\n",
      "Iteration 417, loss = 0.47752762\n",
      "Iteration 418, loss = 0.47692990\n",
      "Iteration 419, loss = 0.47723828\n",
      "Iteration 420, loss = 0.47719572\n",
      "Iteration 421, loss = 0.47711291\n",
      "Iteration 422, loss = 0.47731405\n",
      "Iteration 423, loss = 0.47689769\n",
      "Iteration 424, loss = 0.47730447\n",
      "Iteration 425, loss = 0.47702407\n",
      "Iteration 426, loss = 0.47681439\n",
      "Iteration 427, loss = 0.47687469\n",
      "Iteration 428, loss = 0.47699091\n",
      "Iteration 429, loss = 0.47691130\n",
      "Iteration 430, loss = 0.47698792\n",
      "Iteration 431, loss = 0.47662825\n",
      "Iteration 432, loss = 0.47700271\n",
      "Iteration 433, loss = 0.47701793\n",
      "Iteration 434, loss = 0.47643370\n",
      "Iteration 435, loss = 0.47730041\n",
      "Iteration 436, loss = 0.47695062\n",
      "Iteration 437, loss = 0.47679703\n",
      "Iteration 438, loss = 0.47634282\n",
      "Iteration 439, loss = 0.47643441\n",
      "Iteration 440, loss = 0.47651548\n",
      "Iteration 441, loss = 0.47636209\n",
      "Iteration 442, loss = 0.47643940\n",
      "Iteration 443, loss = 0.47647330\n",
      "Iteration 444, loss = 0.47642618\n",
      "Iteration 445, loss = 0.47628518\n",
      "Iteration 446, loss = 0.47619331\n",
      "Iteration 447, loss = 0.47655446\n",
      "Iteration 448, loss = 0.47612388\n",
      "Iteration 449, loss = 0.47604826\n",
      "Iteration 450, loss = 0.47623637\n",
      "Iteration 451, loss = 0.47589372\n",
      "Iteration 452, loss = 0.47603649\n",
      "Iteration 453, loss = 0.47645454\n",
      "Iteration 454, loss = 0.47606643\n",
      "Iteration 455, loss = 0.47610452\n",
      "Iteration 456, loss = 0.47569718\n",
      "Iteration 457, loss = 0.47622021\n",
      "Iteration 458, loss = 0.47624116\n",
      "Iteration 459, loss = 0.47586928\n",
      "Iteration 460, loss = 0.47581538\n",
      "Iteration 461, loss = 0.47579776\n",
      "Iteration 462, loss = 0.47552162\n",
      "Iteration 463, loss = 0.47630718\n",
      "Iteration 464, loss = 0.47593065\n",
      "Iteration 465, loss = 0.47548666\n",
      "Iteration 466, loss = 0.47567052\n",
      "Iteration 467, loss = 0.47536268\n",
      "Iteration 468, loss = 0.47554114\n",
      "Iteration 469, loss = 0.47561982\n",
      "Iteration 470, loss = 0.47556599\n",
      "Iteration 471, loss = 0.47552704\n",
      "Iteration 472, loss = 0.47583380\n",
      "Iteration 473, loss = 0.47543192\n",
      "Iteration 474, loss = 0.47565292\n",
      "Iteration 475, loss = 0.47506236\n",
      "Iteration 476, loss = 0.47555115\n",
      "Iteration 477, loss = 0.47569784\n",
      "Iteration 478, loss = 0.47534942\n",
      "Iteration 479, loss = 0.47513211\n",
      "Iteration 480, loss = 0.47538614\n",
      "Iteration 481, loss = 0.47523367\n",
      "Iteration 482, loss = 0.47521172\n",
      "Iteration 483, loss = 0.47563509\n",
      "Iteration 484, loss = 0.47558923\n",
      "Iteration 485, loss = 0.47548480\n",
      "Iteration 486, loss = 0.47514827\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81499480\n",
      "Iteration 2, loss = 0.76005189\n",
      "Iteration 3, loss = 0.72487968\n",
      "Iteration 4, loss = 0.70055484\n",
      "Iteration 5, loss = 0.68116003\n",
      "Iteration 6, loss = 0.66477843\n",
      "Iteration 7, loss = 0.65006926\n",
      "Iteration 8, loss = 0.63675274\n",
      "Iteration 9, loss = 0.62488043\n",
      "Iteration 10, loss = 0.61414301\n",
      "Iteration 11, loss = 0.60505663\n",
      "Iteration 12, loss = 0.59690566\n",
      "Iteration 13, loss = 0.58971780\n",
      "Iteration 14, loss = 0.58363198\n",
      "Iteration 15, loss = 0.57846801\n",
      "Iteration 16, loss = 0.57428892\n",
      "Iteration 17, loss = 0.57075210\n",
      "Iteration 18, loss = 0.56772533\n",
      "Iteration 19, loss = 0.56521085\n",
      "Iteration 20, loss = 0.56293868\n",
      "Iteration 21, loss = 0.56122976\n",
      "Iteration 22, loss = 0.55958583\n",
      "Iteration 23, loss = 0.55791361\n",
      "Iteration 24, loss = 0.55670569\n",
      "Iteration 25, loss = 0.55520497\n",
      "Iteration 26, loss = 0.55396489\n",
      "Iteration 27, loss = 0.55280913\n",
      "Iteration 28, loss = 0.55167205\n",
      "Iteration 29, loss = 0.55097253\n",
      "Iteration 30, loss = 0.54972371\n",
      "Iteration 31, loss = 0.54865975\n",
      "Iteration 32, loss = 0.54797820\n",
      "Iteration 33, loss = 0.54693311\n",
      "Iteration 34, loss = 0.54629222\n",
      "Iteration 35, loss = 0.54601209\n",
      "Iteration 36, loss = 0.54479225\n",
      "Iteration 37, loss = 0.54408772\n",
      "Iteration 38, loss = 0.54333710\n",
      "Iteration 39, loss = 0.54265316\n",
      "Iteration 40, loss = 0.54190215\n",
      "Iteration 41, loss = 0.54122120\n",
      "Iteration 42, loss = 0.54064291\n",
      "Iteration 43, loss = 0.53987456\n",
      "Iteration 44, loss = 0.53927718\n",
      "Iteration 45, loss = 0.53866625\n",
      "Iteration 46, loss = 0.53781148\n",
      "Iteration 47, loss = 0.53721587\n",
      "Iteration 48, loss = 0.53641534\n",
      "Iteration 49, loss = 0.53569976\n",
      "Iteration 50, loss = 0.53514390\n",
      "Iteration 51, loss = 0.53432376\n",
      "Iteration 52, loss = 0.53379103\n",
      "Iteration 53, loss = 0.53264570\n",
      "Iteration 54, loss = 0.53229251\n",
      "Iteration 55, loss = 0.53163220\n",
      "Iteration 56, loss = 0.53082587\n",
      "Iteration 57, loss = 0.53022887\n",
      "Iteration 58, loss = 0.52928874\n",
      "Iteration 59, loss = 0.52866233\n",
      "Iteration 60, loss = 0.52795153\n",
      "Iteration 61, loss = 0.52727305\n",
      "Iteration 62, loss = 0.52649835\n",
      "Iteration 63, loss = 0.52604716\n",
      "Iteration 64, loss = 0.52503650\n",
      "Iteration 65, loss = 0.52443173\n",
      "Iteration 66, loss = 0.52378182\n",
      "Iteration 67, loss = 0.52334267\n",
      "Iteration 68, loss = 0.52214481\n",
      "Iteration 69, loss = 0.52146325\n",
      "Iteration 70, loss = 0.52066119\n",
      "Iteration 71, loss = 0.52015631\n",
      "Iteration 72, loss = 0.51925088\n",
      "Iteration 73, loss = 0.51900918\n",
      "Iteration 74, loss = 0.51801734\n",
      "Iteration 75, loss = 0.51731969\n",
      "Iteration 76, loss = 0.51673985\n",
      "Iteration 77, loss = 0.51604335\n",
      "Iteration 78, loss = 0.51548418\n",
      "Iteration 79, loss = 0.51476767\n",
      "Iteration 80, loss = 0.51412761\n",
      "Iteration 81, loss = 0.51374027\n",
      "Iteration 82, loss = 0.51345088\n",
      "Iteration 83, loss = 0.51241297\n",
      "Iteration 84, loss = 0.51170287\n",
      "Iteration 85, loss = 0.51084479\n",
      "Iteration 86, loss = 0.51013824\n",
      "Iteration 87, loss = 0.50958505\n",
      "Iteration 88, loss = 0.50918317\n",
      "Iteration 89, loss = 0.50864424\n",
      "Iteration 90, loss = 0.50787008\n",
      "Iteration 91, loss = 0.50770295\n",
      "Iteration 92, loss = 0.50726398\n",
      "Iteration 93, loss = 0.50675780\n",
      "Iteration 94, loss = 0.50613267\n",
      "Iteration 95, loss = 0.50544356\n",
      "Iteration 96, loss = 0.50499133\n",
      "Iteration 97, loss = 0.50456219\n",
      "Iteration 98, loss = 0.50414542\n",
      "Iteration 99, loss = 0.50346699\n",
      "Iteration 100, loss = 0.50323242\n",
      "Iteration 101, loss = 0.50278687\n",
      "Iteration 102, loss = 0.50236166\n",
      "Iteration 103, loss = 0.50187763\n",
      "Iteration 104, loss = 0.50135508\n",
      "Iteration 105, loss = 0.50118957\n",
      "Iteration 106, loss = 0.50045588\n",
      "Iteration 107, loss = 0.50027958\n",
      "Iteration 108, loss = 0.49959898\n",
      "Iteration 109, loss = 0.49927482\n",
      "Iteration 110, loss = 0.49886141\n",
      "Iteration 111, loss = 0.49853554\n",
      "Iteration 112, loss = 0.49846757\n",
      "Iteration 113, loss = 0.49794542\n",
      "Iteration 114, loss = 0.49761326\n",
      "Iteration 115, loss = 0.49754425\n",
      "Iteration 116, loss = 0.49759433\n",
      "Iteration 117, loss = 0.49641015\n",
      "Iteration 118, loss = 0.49669867\n",
      "Iteration 119, loss = 0.49614139\n",
      "Iteration 120, loss = 0.49602596\n",
      "Iteration 121, loss = 0.49535189\n",
      "Iteration 122, loss = 0.49557779\n",
      "Iteration 123, loss = 0.49523248\n",
      "Iteration 124, loss = 0.49489518\n",
      "Iteration 125, loss = 0.49443190\n",
      "Iteration 126, loss = 0.49443833\n",
      "Iteration 127, loss = 0.49430430\n",
      "Iteration 128, loss = 0.49363893\n",
      "Iteration 129, loss = 0.49358896\n",
      "Iteration 130, loss = 0.49346180\n",
      "Iteration 131, loss = 0.49287879\n",
      "Iteration 132, loss = 0.49290891\n",
      "Iteration 133, loss = 0.49301986\n",
      "Iteration 134, loss = 0.49236656\n",
      "Iteration 135, loss = 0.49247506\n",
      "Iteration 136, loss = 0.49204433\n",
      "Iteration 137, loss = 0.49150020\n",
      "Iteration 138, loss = 0.49140367\n",
      "Iteration 139, loss = 0.49109816\n",
      "Iteration 140, loss = 0.49101455\n",
      "Iteration 141, loss = 0.49084614\n",
      "Iteration 142, loss = 0.49058295\n",
      "Iteration 143, loss = 0.49019621\n",
      "Iteration 144, loss = 0.49001886\n",
      "Iteration 145, loss = 0.48969801\n",
      "Iteration 146, loss = 0.48948710\n",
      "Iteration 147, loss = 0.48942003\n",
      "Iteration 148, loss = 0.48936092\n",
      "Iteration 149, loss = 0.48888859\n",
      "Iteration 150, loss = 0.48871383\n",
      "Iteration 151, loss = 0.48875085\n",
      "Iteration 152, loss = 0.48919899\n",
      "Iteration 153, loss = 0.48839817\n",
      "Iteration 154, loss = 0.48825427\n",
      "Iteration 155, loss = 0.48780345\n",
      "Iteration 156, loss = 0.48780165\n",
      "Iteration 157, loss = 0.48754126\n",
      "Iteration 158, loss = 0.48747503\n",
      "Iteration 159, loss = 0.48743054\n",
      "Iteration 160, loss = 0.48704061\n",
      "Iteration 161, loss = 0.48716056\n",
      "Iteration 162, loss = 0.48688156\n",
      "Iteration 163, loss = 0.48696566\n",
      "Iteration 164, loss = 0.48650358\n",
      "Iteration 165, loss = 0.48623408\n",
      "Iteration 166, loss = 0.48644574\n",
      "Iteration 167, loss = 0.48618636\n",
      "Iteration 168, loss = 0.48593504\n",
      "Iteration 169, loss = 0.48597129\n",
      "Iteration 170, loss = 0.48584215\n",
      "Iteration 171, loss = 0.48550939\n",
      "Iteration 172, loss = 0.48543384\n",
      "Iteration 173, loss = 0.48526487\n",
      "Iteration 174, loss = 0.48551237\n",
      "Iteration 175, loss = 0.48510856\n",
      "Iteration 176, loss = 0.48491590\n",
      "Iteration 177, loss = 0.48483056\n",
      "Iteration 178, loss = 0.48486859\n",
      "Iteration 179, loss = 0.48466382\n",
      "Iteration 180, loss = 0.48457274\n",
      "Iteration 181, loss = 0.48443509\n",
      "Iteration 182, loss = 0.48444447\n",
      "Iteration 183, loss = 0.48401315\n",
      "Iteration 184, loss = 0.48413740\n",
      "Iteration 185, loss = 0.48393132\n",
      "Iteration 186, loss = 0.48386154\n",
      "Iteration 187, loss = 0.48391156\n",
      "Iteration 188, loss = 0.48357367\n",
      "Iteration 189, loss = 0.48347657\n",
      "Iteration 190, loss = 0.48356274\n",
      "Iteration 191, loss = 0.48338846\n",
      "Iteration 192, loss = 0.48351269\n",
      "Iteration 193, loss = 0.48332648\n",
      "Iteration 194, loss = 0.48342580\n",
      "Iteration 195, loss = 0.48311003\n",
      "Iteration 196, loss = 0.48324100\n",
      "Iteration 197, loss = 0.48314998\n",
      "Iteration 198, loss = 0.48294438\n",
      "Iteration 199, loss = 0.48284040\n",
      "Iteration 200, loss = 0.48256829\n",
      "Iteration 201, loss = 0.48237478\n",
      "Iteration 202, loss = 0.48269553\n",
      "Iteration 203, loss = 0.48246594\n",
      "Iteration 204, loss = 0.48250462\n",
      "Iteration 205, loss = 0.48242767\n",
      "Iteration 206, loss = 0.48236066\n",
      "Iteration 207, loss = 0.48264678\n",
      "Iteration 208, loss = 0.48203869\n",
      "Iteration 209, loss = 0.48203560\n",
      "Iteration 210, loss = 0.48168891\n",
      "Iteration 211, loss = 0.48193316\n",
      "Iteration 212, loss = 0.48204578\n",
      "Iteration 213, loss = 0.48155363\n",
      "Iteration 214, loss = 0.48188055\n",
      "Iteration 215, loss = 0.48147523\n",
      "Iteration 216, loss = 0.48149443\n",
      "Iteration 217, loss = 0.48159714\n",
      "Iteration 218, loss = 0.48132547\n",
      "Iteration 219, loss = 0.48143628\n",
      "Iteration 220, loss = 0.48104370\n",
      "Iteration 221, loss = 0.48121919\n",
      "Iteration 222, loss = 0.48132559\n",
      "Iteration 223, loss = 0.48089193\n",
      "Iteration 224, loss = 0.48091635\n",
      "Iteration 225, loss = 0.48064916\n",
      "Iteration 226, loss = 0.48143286\n",
      "Iteration 227, loss = 0.48137739\n",
      "Iteration 228, loss = 0.48081082\n",
      "Iteration 229, loss = 0.48101036\n",
      "Iteration 230, loss = 0.48064222\n",
      "Iteration 231, loss = 0.48066965\n",
      "Iteration 232, loss = 0.48037197\n",
      "Iteration 233, loss = 0.48047286\n",
      "Iteration 234, loss = 0.48031525\n",
      "Iteration 235, loss = 0.48023567\n",
      "Iteration 236, loss = 0.48015909\n",
      "Iteration 237, loss = 0.47993838\n",
      "Iteration 238, loss = 0.47992093\n",
      "Iteration 239, loss = 0.48007902\n",
      "Iteration 240, loss = 0.47999719\n",
      "Iteration 241, loss = 0.47984620\n",
      "Iteration 242, loss = 0.47992758\n",
      "Iteration 243, loss = 0.47969535\n",
      "Iteration 244, loss = 0.47964986\n",
      "Iteration 245, loss = 0.47954884\n",
      "Iteration 246, loss = 0.47950449\n",
      "Iteration 247, loss = 0.47956869\n",
      "Iteration 248, loss = 0.47952523\n",
      "Iteration 249, loss = 0.47920977\n",
      "Iteration 250, loss = 0.47930093\n",
      "Iteration 251, loss = 0.47921099\n",
      "Iteration 252, loss = 0.47918978\n",
      "Iteration 253, loss = 0.47948997\n",
      "Iteration 254, loss = 0.47899985\n",
      "Iteration 255, loss = 0.47915859\n",
      "Iteration 256, loss = 0.47903684\n",
      "Iteration 257, loss = 0.47878784\n",
      "Iteration 258, loss = 0.47886316\n",
      "Iteration 259, loss = 0.47856628\n",
      "Iteration 260, loss = 0.47857914\n",
      "Iteration 261, loss = 0.47841576\n",
      "Iteration 262, loss = 0.47849414\n",
      "Iteration 263, loss = 0.47858016\n",
      "Iteration 264, loss = 0.47853079\n",
      "Iteration 265, loss = 0.47808365\n",
      "Iteration 266, loss = 0.47870568\n",
      "Iteration 267, loss = 0.47857839\n",
      "Iteration 268, loss = 0.47802491\n",
      "Iteration 269, loss = 0.47816556\n",
      "Iteration 270, loss = 0.47805121\n",
      "Iteration 271, loss = 0.47888235\n",
      "Iteration 272, loss = 0.47962801\n",
      "Iteration 273, loss = 0.47959932\n",
      "Iteration 274, loss = 0.47854972\n",
      "Iteration 275, loss = 0.47843394\n",
      "Iteration 276, loss = 0.47813121\n",
      "Iteration 277, loss = 0.47908253\n",
      "Iteration 278, loss = 0.47754763\n",
      "Iteration 279, loss = 0.47791688\n",
      "Iteration 280, loss = 0.47793050\n",
      "Iteration 281, loss = 0.47754927\n",
      "Iteration 282, loss = 0.47773037\n",
      "Iteration 283, loss = 0.47759346\n",
      "Iteration 284, loss = 0.47805577\n",
      "Iteration 285, loss = 0.47783330\n",
      "Iteration 286, loss = 0.47712138\n",
      "Iteration 287, loss = 0.47757109\n",
      "Iteration 288, loss = 0.47754867\n",
      "Iteration 289, loss = 0.47760455\n",
      "Iteration 290, loss = 0.47724197\n",
      "Iteration 291, loss = 0.47710624\n",
      "Iteration 292, loss = 0.47722597\n",
      "Iteration 293, loss = 0.47694082\n",
      "Iteration 294, loss = 0.47719799\n",
      "Iteration 295, loss = 0.47747715\n",
      "Iteration 296, loss = 0.47689234\n",
      "Iteration 297, loss = 0.47730486\n",
      "Iteration 298, loss = 0.47669497\n",
      "Iteration 299, loss = 0.47725712\n",
      "Iteration 300, loss = 0.47731117\n",
      "Iteration 301, loss = 0.47689100\n",
      "Iteration 302, loss = 0.47773624\n",
      "Iteration 303, loss = 0.47688649\n",
      "Iteration 304, loss = 0.47664335\n",
      "Iteration 305, loss = 0.47665882\n",
      "Iteration 306, loss = 0.47655473\n",
      "Iteration 307, loss = 0.47642496\n",
      "Iteration 308, loss = 0.47659077\n",
      "Iteration 309, loss = 0.47632553\n",
      "Iteration 310, loss = 0.47654826\n",
      "Iteration 311, loss = 0.47636793\n",
      "Iteration 312, loss = 0.47642298\n",
      "Iteration 313, loss = 0.47629865\n",
      "Iteration 314, loss = 0.47652575\n",
      "Iteration 315, loss = 0.47663418\n",
      "Iteration 316, loss = 0.47639269\n",
      "Iteration 317, loss = 0.47620528\n",
      "Iteration 318, loss = 0.47641206\n",
      "Iteration 319, loss = 0.47625526\n",
      "Iteration 320, loss = 0.47631727\n",
      "Iteration 321, loss = 0.47638768\n",
      "Iteration 322, loss = 0.47626933\n",
      "Iteration 323, loss = 0.47602499\n",
      "Iteration 324, loss = 0.47611591\n",
      "Iteration 325, loss = 0.47587076\n",
      "Iteration 326, loss = 0.47596602\n",
      "Iteration 327, loss = 0.47605063\n",
      "Iteration 328, loss = 0.47654017\n",
      "Iteration 329, loss = 0.47578304\n",
      "Iteration 330, loss = 0.47585859\n",
      "Iteration 331, loss = 0.47568104\n",
      "Iteration 332, loss = 0.47595762\n",
      "Iteration 333, loss = 0.47570253\n",
      "Iteration 334, loss = 0.47562684\n",
      "Iteration 335, loss = 0.47587004\n",
      "Iteration 336, loss = 0.47569736\n",
      "Iteration 337, loss = 0.47631757\n",
      "Iteration 338, loss = 0.47575620\n",
      "Iteration 339, loss = 0.47543645\n",
      "Iteration 340, loss = 0.47566559\n",
      "Iteration 341, loss = 0.47594562\n",
      "Iteration 342, loss = 0.47534509\n",
      "Iteration 343, loss = 0.47529819\n",
      "Iteration 344, loss = 0.47541931\n",
      "Iteration 345, loss = 0.47514630\n",
      "Iteration 346, loss = 0.47543064\n",
      "Iteration 347, loss = 0.47499978\n",
      "Iteration 348, loss = 0.47517445\n",
      "Iteration 349, loss = 0.47498123\n",
      "Iteration 350, loss = 0.47519026\n",
      "Iteration 351, loss = 0.47499076\n",
      "Iteration 352, loss = 0.47510945\n",
      "Iteration 353, loss = 0.47545521\n",
      "Iteration 354, loss = 0.47512588\n",
      "Iteration 355, loss = 0.47487370\n",
      "Iteration 356, loss = 0.47474980\n",
      "Iteration 357, loss = 0.47498840\n",
      "Iteration 358, loss = 0.47508956\n",
      "Iteration 359, loss = 0.47514305\n",
      "Iteration 360, loss = 0.47539501\n",
      "Iteration 361, loss = 0.47439917\n",
      "Iteration 362, loss = 0.47473698\n",
      "Iteration 363, loss = 0.47484555\n",
      "Iteration 364, loss = 0.47441024\n",
      "Iteration 365, loss = 0.47468947\n",
      "Iteration 366, loss = 0.47473845\n",
      "Iteration 367, loss = 0.47459295\n",
      "Iteration 368, loss = 0.47478103\n",
      "Iteration 369, loss = 0.47443549\n",
      "Iteration 370, loss = 0.47441566\n",
      "Iteration 371, loss = 0.47466734\n",
      "Iteration 372, loss = 0.47475980\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77688504\n",
      "Iteration 2, loss = 0.74059761\n",
      "Iteration 3, loss = 0.71800226\n",
      "Iteration 4, loss = 0.70395446\n",
      "Iteration 5, loss = 0.69311410\n",
      "Iteration 6, loss = 0.68401760\n",
      "Iteration 7, loss = 0.67490267\n",
      "Iteration 8, loss = 0.66581286\n",
      "Iteration 9, loss = 0.65567570\n",
      "Iteration 10, loss = 0.64464388\n",
      "Iteration 11, loss = 0.63244068\n",
      "Iteration 12, loss = 0.61947243\n",
      "Iteration 13, loss = 0.60592497\n",
      "Iteration 14, loss = 0.59226753\n",
      "Iteration 15, loss = 0.57994574\n",
      "Iteration 16, loss = 0.56966308\n",
      "Iteration 17, loss = 0.56154982\n",
      "Iteration 18, loss = 0.55593168\n",
      "Iteration 19, loss = 0.55187929\n",
      "Iteration 20, loss = 0.54840914\n",
      "Iteration 21, loss = 0.54598156\n",
      "Iteration 22, loss = 0.54383374\n",
      "Iteration 23, loss = 0.54179997\n",
      "Iteration 24, loss = 0.54014363\n",
      "Iteration 25, loss = 0.53835945\n",
      "Iteration 26, loss = 0.53699671\n",
      "Iteration 27, loss = 0.53545428\n",
      "Iteration 28, loss = 0.53400447\n",
      "Iteration 29, loss = 0.53272036\n",
      "Iteration 30, loss = 0.53136215\n",
      "Iteration 31, loss = 0.53031602\n",
      "Iteration 32, loss = 0.52901380\n",
      "Iteration 33, loss = 0.52773439\n",
      "Iteration 34, loss = 0.52662711\n",
      "Iteration 35, loss = 0.52557026\n",
      "Iteration 36, loss = 0.52430600\n",
      "Iteration 37, loss = 0.52321985\n",
      "Iteration 38, loss = 0.52230305\n",
      "Iteration 39, loss = 0.52100187\n",
      "Iteration 40, loss = 0.51974803\n",
      "Iteration 41, loss = 0.51851867\n",
      "Iteration 42, loss = 0.51718277\n",
      "Iteration 43, loss = 0.51594051\n",
      "Iteration 44, loss = 0.51496524\n",
      "Iteration 45, loss = 0.51362561\n",
      "Iteration 46, loss = 0.51218763\n",
      "Iteration 47, loss = 0.51100679\n",
      "Iteration 48, loss = 0.50948544\n",
      "Iteration 49, loss = 0.50859251\n",
      "Iteration 50, loss = 0.50711328\n",
      "Iteration 51, loss = 0.50613239\n",
      "Iteration 52, loss = 0.50532127\n",
      "Iteration 53, loss = 0.50394710\n",
      "Iteration 54, loss = 0.50322258\n",
      "Iteration 55, loss = 0.50194064\n",
      "Iteration 56, loss = 0.50109930\n",
      "Iteration 57, loss = 0.50028358\n",
      "Iteration 58, loss = 0.49912874\n",
      "Iteration 59, loss = 0.49878644\n",
      "Iteration 60, loss = 0.49783962\n",
      "Iteration 61, loss = 0.49713784\n",
      "Iteration 62, loss = 0.49646315\n",
      "Iteration 63, loss = 0.49616576\n",
      "Iteration 64, loss = 0.49529516\n",
      "Iteration 65, loss = 0.49452613\n",
      "Iteration 66, loss = 0.49405610\n",
      "Iteration 67, loss = 0.49336296\n",
      "Iteration 68, loss = 0.49293589\n",
      "Iteration 69, loss = 0.49271254\n",
      "Iteration 70, loss = 0.49167040\n",
      "Iteration 71, loss = 0.49130144\n",
      "Iteration 72, loss = 0.49080915\n",
      "Iteration 73, loss = 0.49036499\n",
      "Iteration 74, loss = 0.49017120\n",
      "Iteration 75, loss = 0.48980858\n",
      "Iteration 76, loss = 0.48935745\n",
      "Iteration 77, loss = 0.48910590\n",
      "Iteration 78, loss = 0.48890046\n",
      "Iteration 79, loss = 0.48870854\n",
      "Iteration 80, loss = 0.48896632\n",
      "Iteration 81, loss = 0.48776644\n",
      "Iteration 82, loss = 0.48776700\n",
      "Iteration 83, loss = 0.48730309\n",
      "Iteration 84, loss = 0.48721853\n",
      "Iteration 85, loss = 0.48701164\n",
      "Iteration 86, loss = 0.48682231\n",
      "Iteration 87, loss = 0.48672072\n",
      "Iteration 88, loss = 0.48673554\n",
      "Iteration 89, loss = 0.48622001\n",
      "Iteration 90, loss = 0.48588702\n",
      "Iteration 91, loss = 0.48589891\n",
      "Iteration 92, loss = 0.48595715\n",
      "Iteration 93, loss = 0.48544953\n",
      "Iteration 94, loss = 0.48536772\n",
      "Iteration 95, loss = 0.48491725\n",
      "Iteration 96, loss = 0.48485206\n",
      "Iteration 97, loss = 0.48459802\n",
      "Iteration 98, loss = 0.48445442\n",
      "Iteration 99, loss = 0.48427137\n",
      "Iteration 100, loss = 0.48413094\n",
      "Iteration 101, loss = 0.48398547\n",
      "Iteration 102, loss = 0.48391464\n",
      "Iteration 103, loss = 0.48373140\n",
      "Iteration 104, loss = 0.48350588\n",
      "Iteration 105, loss = 0.48334700\n",
      "Iteration 106, loss = 0.48309318\n",
      "Iteration 107, loss = 0.48300417\n",
      "Iteration 108, loss = 0.48277736\n",
      "Iteration 109, loss = 0.48273243\n",
      "Iteration 110, loss = 0.48252676\n",
      "Iteration 111, loss = 0.48246900\n",
      "Iteration 112, loss = 0.48232475\n",
      "Iteration 113, loss = 0.48205053\n",
      "Iteration 114, loss = 0.48210870\n",
      "Iteration 115, loss = 0.48182869\n",
      "Iteration 116, loss = 0.48207485\n",
      "Iteration 117, loss = 0.48157699\n",
      "Iteration 118, loss = 0.48165079\n",
      "Iteration 119, loss = 0.48151353\n",
      "Iteration 120, loss = 0.48124177\n",
      "Iteration 121, loss = 0.48124528\n",
      "Iteration 122, loss = 0.48129974\n",
      "Iteration 123, loss = 0.48106493\n",
      "Iteration 124, loss = 0.48116488\n",
      "Iteration 125, loss = 0.48062760\n",
      "Iteration 126, loss = 0.48062598\n",
      "Iteration 127, loss = 0.48052614\n",
      "Iteration 128, loss = 0.48045905\n",
      "Iteration 129, loss = 0.48031572\n",
      "Iteration 130, loss = 0.48015252\n",
      "Iteration 131, loss = 0.48018625\n",
      "Iteration 132, loss = 0.48039564\n",
      "Iteration 133, loss = 0.48024893\n",
      "Iteration 134, loss = 0.48022722\n",
      "Iteration 135, loss = 0.47986787\n",
      "Iteration 136, loss = 0.47975807\n",
      "Iteration 137, loss = 0.47983913\n",
      "Iteration 138, loss = 0.47978193\n",
      "Iteration 139, loss = 0.47954964\n",
      "Iteration 140, loss = 0.47954252\n",
      "Iteration 141, loss = 0.47963355\n",
      "Iteration 142, loss = 0.47963694\n",
      "Iteration 143, loss = 0.47951486\n",
      "Iteration 144, loss = 0.47942908\n",
      "Iteration 145, loss = 0.47928492\n",
      "Iteration 146, loss = 0.47904260\n",
      "Iteration 147, loss = 0.47905703\n",
      "Iteration 148, loss = 0.47886798\n",
      "Iteration 149, loss = 0.47882952\n",
      "Iteration 150, loss = 0.47876832\n",
      "Iteration 151, loss = 0.47872108\n",
      "Iteration 152, loss = 0.47877508\n",
      "Iteration 153, loss = 0.47860040\n",
      "Iteration 154, loss = 0.47891925\n",
      "Iteration 155, loss = 0.47846721\n",
      "Iteration 156, loss = 0.47844366\n",
      "Iteration 157, loss = 0.47832982\n",
      "Iteration 158, loss = 0.47824768\n",
      "Iteration 159, loss = 0.47825705\n",
      "Iteration 160, loss = 0.47818292\n",
      "Iteration 161, loss = 0.47857746\n",
      "Iteration 162, loss = 0.47807832\n",
      "Iteration 163, loss = 0.47795895\n",
      "Iteration 164, loss = 0.47764288\n",
      "Iteration 165, loss = 0.47759764\n",
      "Iteration 166, loss = 0.47742642\n",
      "Iteration 167, loss = 0.47749531\n",
      "Iteration 168, loss = 0.47740145\n",
      "Iteration 169, loss = 0.47741036\n",
      "Iteration 170, loss = 0.47740899\n",
      "Iteration 171, loss = 0.47727548\n",
      "Iteration 172, loss = 0.47732046\n",
      "Iteration 173, loss = 0.47700641\n",
      "Iteration 174, loss = 0.47689761\n",
      "Iteration 175, loss = 0.47716421\n",
      "Iteration 176, loss = 0.47737599\n",
      "Iteration 177, loss = 0.47673936\n",
      "Iteration 178, loss = 0.47675658\n",
      "Iteration 179, loss = 0.47680910\n",
      "Iteration 180, loss = 0.47683024\n",
      "Iteration 181, loss = 0.47647154\n",
      "Iteration 182, loss = 0.47651135\n",
      "Iteration 183, loss = 0.47654261\n",
      "Iteration 184, loss = 0.47647554\n",
      "Iteration 185, loss = 0.47636822\n",
      "Iteration 186, loss = 0.47638413\n",
      "Iteration 187, loss = 0.47632350\n",
      "Iteration 188, loss = 0.47610896\n",
      "Iteration 189, loss = 0.47625954\n",
      "Iteration 190, loss = 0.47606421\n",
      "Iteration 191, loss = 0.47596199\n",
      "Iteration 192, loss = 0.47598494\n",
      "Iteration 193, loss = 0.47606135\n",
      "Iteration 194, loss = 0.47607607\n",
      "Iteration 195, loss = 0.47616926\n",
      "Iteration 196, loss = 0.47592559\n",
      "Iteration 197, loss = 0.47580436\n",
      "Iteration 198, loss = 0.47577633\n",
      "Iteration 199, loss = 0.47575355\n",
      "Iteration 200, loss = 0.47570226\n",
      "Iteration 201, loss = 0.47570749\n",
      "Iteration 202, loss = 0.47576416\n",
      "Iteration 203, loss = 0.47565790\n",
      "Iteration 204, loss = 0.47532308\n",
      "Iteration 205, loss = 0.47550358\n",
      "Iteration 206, loss = 0.47525000\n",
      "Iteration 207, loss = 0.47549271\n",
      "Iteration 208, loss = 0.47542183\n",
      "Iteration 209, loss = 0.47516569\n",
      "Iteration 210, loss = 0.47547838\n",
      "Iteration 211, loss = 0.47524781\n",
      "Iteration 212, loss = 0.47560725\n",
      "Iteration 213, loss = 0.47552345\n",
      "Iteration 214, loss = 0.47525336\n",
      "Iteration 215, loss = 0.47519095\n",
      "Iteration 216, loss = 0.47500015\n",
      "Iteration 217, loss = 0.47519900\n",
      "Iteration 218, loss = 0.47510908\n",
      "Iteration 219, loss = 0.47475755\n",
      "Iteration 220, loss = 0.47491527\n",
      "Iteration 221, loss = 0.47515806\n",
      "Iteration 222, loss = 0.47509001\n",
      "Iteration 223, loss = 0.47476886\n",
      "Iteration 224, loss = 0.47470719\n",
      "Iteration 225, loss = 0.47455846\n",
      "Iteration 226, loss = 0.47464055\n",
      "Iteration 227, loss = 0.47470410\n",
      "Iteration 228, loss = 0.47431244\n",
      "Iteration 229, loss = 0.47457782\n",
      "Iteration 230, loss = 0.47474828\n",
      "Iteration 231, loss = 0.47453867\n",
      "Iteration 232, loss = 0.47443295\n",
      "Iteration 233, loss = 0.47419102\n",
      "Iteration 234, loss = 0.47419415\n",
      "Iteration 235, loss = 0.47416301\n",
      "Iteration 236, loss = 0.47412741\n",
      "Iteration 237, loss = 0.47407466\n",
      "Iteration 238, loss = 0.47418820\n",
      "Iteration 239, loss = 0.47411919\n",
      "Iteration 240, loss = 0.47406766\n",
      "Iteration 241, loss = 0.47411427\n",
      "Iteration 242, loss = 0.47388879\n",
      "Iteration 243, loss = 0.47396461\n",
      "Iteration 244, loss = 0.47396273\n",
      "Iteration 245, loss = 0.47393734\n",
      "Iteration 246, loss = 0.47384780\n",
      "Iteration 247, loss = 0.47402463\n",
      "Iteration 248, loss = 0.47374309\n",
      "Iteration 249, loss = 0.47396696\n",
      "Iteration 250, loss = 0.47401618\n",
      "Iteration 251, loss = 0.47409660\n",
      "Iteration 252, loss = 0.47370851\n",
      "Iteration 253, loss = 0.47368392\n",
      "Iteration 254, loss = 0.47379398\n",
      "Iteration 255, loss = 0.47376571\n",
      "Iteration 256, loss = 0.47347845\n",
      "Iteration 257, loss = 0.47357143\n",
      "Iteration 258, loss = 0.47359422\n",
      "Iteration 259, loss = 0.47352302\n",
      "Iteration 260, loss = 0.47357636\n",
      "Iteration 261, loss = 0.47365784\n",
      "Iteration 262, loss = 0.47337906\n",
      "Iteration 263, loss = 0.47336913\n",
      "Iteration 264, loss = 0.47336891\n",
      "Iteration 265, loss = 0.47346877\n",
      "Iteration 266, loss = 0.47364907\n",
      "Iteration 267, loss = 0.47297489\n",
      "Iteration 268, loss = 0.47335441\n",
      "Iteration 269, loss = 0.47306852\n",
      "Iteration 270, loss = 0.47306135\n",
      "Iteration 271, loss = 0.47299575\n",
      "Iteration 272, loss = 0.47324552\n",
      "Iteration 273, loss = 0.47297558\n",
      "Iteration 274, loss = 0.47288972\n",
      "Iteration 275, loss = 0.47291836\n",
      "Iteration 276, loss = 0.47267832\n",
      "Iteration 277, loss = 0.47280093\n",
      "Iteration 278, loss = 0.47270587\n",
      "Iteration 279, loss = 0.47268430\n",
      "Iteration 280, loss = 0.47279003\n",
      "Iteration 281, loss = 0.47261168\n",
      "Iteration 282, loss = 0.47292498\n",
      "Iteration 283, loss = 0.47253106\n",
      "Iteration 284, loss = 0.47254900\n",
      "Iteration 285, loss = 0.47240689\n",
      "Iteration 286, loss = 0.47255375\n",
      "Iteration 287, loss = 0.47317628\n",
      "Iteration 288, loss = 0.47225435\n",
      "Iteration 289, loss = 0.47234526\n",
      "Iteration 290, loss = 0.47226431\n",
      "Iteration 291, loss = 0.47250329\n",
      "Iteration 292, loss = 0.47221797\n",
      "Iteration 293, loss = 0.47205586\n",
      "Iteration 294, loss = 0.47255352\n",
      "Iteration 295, loss = 0.47214215\n",
      "Iteration 296, loss = 0.47209842\n",
      "Iteration 297, loss = 0.47184596\n",
      "Iteration 298, loss = 0.47207814\n",
      "Iteration 299, loss = 0.47187202\n",
      "Iteration 300, loss = 0.47192521\n",
      "Iteration 301, loss = 0.47212078\n",
      "Iteration 302, loss = 0.47190366\n",
      "Iteration 303, loss = 0.47200053\n",
      "Iteration 304, loss = 0.47192700\n",
      "Iteration 305, loss = 0.47202850\n",
      "Iteration 306, loss = 0.47264393\n",
      "Iteration 307, loss = 0.47145277\n",
      "Iteration 308, loss = 0.47185888\n",
      "Iteration 309, loss = 0.47163420\n",
      "Iteration 310, loss = 0.47164815\n",
      "Iteration 311, loss = 0.47154386\n",
      "Iteration 312, loss = 0.47169991\n",
      "Iteration 313, loss = 0.47155034\n",
      "Iteration 314, loss = 0.47146766\n",
      "Iteration 315, loss = 0.47147466\n",
      "Iteration 316, loss = 0.47148261\n",
      "Iteration 317, loss = 0.47147446\n",
      "Iteration 318, loss = 0.47138922\n",
      "Iteration 319, loss = 0.47166503\n",
      "Iteration 320, loss = 0.47166999\n",
      "Iteration 321, loss = 0.47122478\n",
      "Iteration 322, loss = 0.47137071\n",
      "Iteration 323, loss = 0.47157312\n",
      "Iteration 324, loss = 0.47129045\n",
      "Iteration 325, loss = 0.47119154\n",
      "Iteration 326, loss = 0.47114196\n",
      "Iteration 327, loss = 0.47108880\n",
      "Iteration 328, loss = 0.47103277\n",
      "Iteration 329, loss = 0.47115181\n",
      "Iteration 330, loss = 0.47099148\n",
      "Iteration 331, loss = 0.47138314\n",
      "Iteration 332, loss = 0.47106435\n",
      "Iteration 333, loss = 0.47097512\n",
      "Iteration 334, loss = 0.47106781\n",
      "Iteration 335, loss = 0.47088899\n",
      "Iteration 336, loss = 0.47090166\n",
      "Iteration 337, loss = 0.47090899\n",
      "Iteration 338, loss = 0.47063661\n",
      "Iteration 339, loss = 0.47073625\n",
      "Iteration 340, loss = 0.47096041\n",
      "Iteration 341, loss = 0.47089378\n",
      "Iteration 342, loss = 0.47053088\n",
      "Iteration 343, loss = 0.47059439\n",
      "Iteration 344, loss = 0.47056973\n",
      "Iteration 345, loss = 0.47051907\n",
      "Iteration 346, loss = 0.47053154\n",
      "Iteration 347, loss = 0.47051381\n",
      "Iteration 348, loss = 0.47053489\n",
      "Iteration 349, loss = 0.47041400\n",
      "Iteration 350, loss = 0.47051614\n",
      "Iteration 351, loss = 0.47047856\n",
      "Iteration 352, loss = 0.47037314\n",
      "Iteration 353, loss = 0.47065432\n",
      "Iteration 354, loss = 0.47024704\n",
      "Iteration 355, loss = 0.47024971\n",
      "Iteration 356, loss = 0.47030096\n",
      "Iteration 357, loss = 0.47005646\n",
      "Iteration 358, loss = 0.47016706\n",
      "Iteration 359, loss = 0.47031719\n",
      "Iteration 360, loss = 0.47053089\n",
      "Iteration 361, loss = 0.47027083\n",
      "Iteration 362, loss = 0.47012144\n",
      "Iteration 363, loss = 0.47003181\n",
      "Iteration 364, loss = 0.47024140\n",
      "Iteration 365, loss = 0.47007207\n",
      "Iteration 366, loss = 0.47002462\n",
      "Iteration 367, loss = 0.47003837\n",
      "Iteration 368, loss = 0.46994525\n",
      "Iteration 369, loss = 0.46989222\n",
      "Iteration 370, loss = 0.46977726\n",
      "Iteration 371, loss = 0.46972976\n",
      "Iteration 372, loss = 0.46982547\n",
      "Iteration 373, loss = 0.46993381\n",
      "Iteration 374, loss = 0.46973710\n",
      "Iteration 375, loss = 0.46986033\n",
      "Iteration 376, loss = 0.46992620\n",
      "Iteration 377, loss = 0.46972047\n",
      "Iteration 378, loss = 0.46973076\n",
      "Iteration 379, loss = 0.46978091\n",
      "Iteration 380, loss = 0.46967551\n",
      "Iteration 381, loss = 0.47008193\n",
      "Iteration 382, loss = 0.46956779\n",
      "Iteration 383, loss = 0.46964687\n",
      "Iteration 384, loss = 0.46953497\n",
      "Iteration 385, loss = 0.46936334\n",
      "Iteration 386, loss = 0.46963768\n",
      "Iteration 387, loss = 0.46934046\n",
      "Iteration 388, loss = 0.46929342\n",
      "Iteration 389, loss = 0.46950400\n",
      "Iteration 390, loss = 0.46922705\n",
      "Iteration 391, loss = 0.46957625\n",
      "Iteration 392, loss = 0.46957961\n",
      "Iteration 393, loss = 0.46949887\n",
      "Iteration 394, loss = 0.46933426\n",
      "Iteration 395, loss = 0.46951699\n",
      "Iteration 396, loss = 0.46948486\n",
      "Iteration 397, loss = 0.46904780\n",
      "Iteration 398, loss = 0.46912214\n",
      "Iteration 399, loss = 0.46912324\n",
      "Iteration 400, loss = 0.46907555\n",
      "Iteration 401, loss = 0.46911153\n",
      "Iteration 402, loss = 0.46927184\n",
      "Iteration 403, loss = 0.46902041\n",
      "Iteration 404, loss = 0.46917053\n",
      "Iteration 405, loss = 0.46892773\n",
      "Iteration 406, loss = 0.46912627\n",
      "Iteration 407, loss = 0.46893828\n",
      "Iteration 408, loss = 0.46887715\n",
      "Iteration 409, loss = 0.46902652\n",
      "Iteration 410, loss = 0.46905289\n",
      "Iteration 411, loss = 0.46916446\n",
      "Iteration 412, loss = 0.46886400\n",
      "Iteration 413, loss = 0.46868795\n",
      "Iteration 414, loss = 0.46909125\n",
      "Iteration 415, loss = 0.46889674\n",
      "Iteration 416, loss = 0.46894906\n",
      "Iteration 417, loss = 0.46899570\n",
      "Iteration 418, loss = 0.46869761\n",
      "Iteration 419, loss = 0.46899723\n",
      "Iteration 420, loss = 0.46877987\n",
      "Iteration 421, loss = 0.46851560\n",
      "Iteration 422, loss = 0.46856116\n",
      "Iteration 423, loss = 0.46870256\n",
      "Iteration 424, loss = 0.46849153\n",
      "Iteration 425, loss = 0.46861915\n",
      "Iteration 426, loss = 0.46853419\n",
      "Iteration 427, loss = 0.46852808\n",
      "Iteration 428, loss = 0.46857943\n",
      "Iteration 429, loss = 0.46854814\n",
      "Iteration 430, loss = 0.46833433\n",
      "Iteration 431, loss = 0.46836166\n",
      "Iteration 432, loss = 0.46824337\n",
      "Iteration 433, loss = 0.46823173\n",
      "Iteration 434, loss = 0.46862789\n",
      "Iteration 435, loss = 0.46850357\n",
      "Iteration 436, loss = 0.46858175\n",
      "Iteration 437, loss = 0.46835270\n",
      "Iteration 438, loss = 0.46827527\n",
      "Iteration 439, loss = 0.46837493\n",
      "Iteration 440, loss = 0.46830154\n",
      "Iteration 441, loss = 0.46817012\n",
      "Iteration 442, loss = 0.46825529\n",
      "Iteration 443, loss = 0.46835585\n",
      "Iteration 444, loss = 0.46831888\n",
      "Iteration 445, loss = 0.46818997\n",
      "Iteration 446, loss = 0.46818836\n",
      "Iteration 447, loss = 0.46803950\n",
      "Iteration 448, loss = 0.46823803\n",
      "Iteration 449, loss = 0.46792223\n",
      "Iteration 450, loss = 0.46789256\n",
      "Iteration 451, loss = 0.46791901\n",
      "Iteration 452, loss = 0.46799842\n",
      "Iteration 453, loss = 0.46799034\n",
      "Iteration 454, loss = 0.46813551\n",
      "Iteration 455, loss = 0.46774617\n",
      "Iteration 456, loss = 0.46811870\n",
      "Iteration 457, loss = 0.46787300\n",
      "Iteration 458, loss = 0.46794488\n",
      "Iteration 459, loss = 0.46775822\n",
      "Iteration 460, loss = 0.46776032\n",
      "Iteration 461, loss = 0.46799779\n",
      "Iteration 462, loss = 0.46804971\n",
      "Iteration 463, loss = 0.46778836\n",
      "Iteration 464, loss = 0.46752054\n",
      "Iteration 465, loss = 0.46762236\n",
      "Iteration 466, loss = 0.46752125\n",
      "Iteration 467, loss = 0.46774699\n",
      "Iteration 468, loss = 0.46775224\n",
      "Iteration 469, loss = 0.46788660\n",
      "Iteration 470, loss = 0.46756100\n",
      "Iteration 471, loss = 0.46782319\n",
      "Iteration 472, loss = 0.46727786\n",
      "Iteration 473, loss = 0.46739523\n",
      "Iteration 474, loss = 0.46742035\n",
      "Iteration 475, loss = 0.46757768\n",
      "Iteration 476, loss = 0.46785326\n",
      "Iteration 477, loss = 0.46716918\n",
      "Iteration 478, loss = 0.46729722\n",
      "Iteration 479, loss = 0.46740434\n",
      "Iteration 480, loss = 0.46765203\n",
      "Iteration 481, loss = 0.46741858\n",
      "Iteration 482, loss = 0.46721992\n",
      "Iteration 483, loss = 0.46764881\n",
      "Iteration 484, loss = 0.46731849\n",
      "Iteration 485, loss = 0.46727370\n",
      "Iteration 486, loss = 0.46713031\n",
      "Iteration 487, loss = 0.46722362\n",
      "Iteration 488, loss = 0.46714878\n",
      "Iteration 489, loss = 0.46699757\n",
      "Iteration 490, loss = 0.46755814\n",
      "Iteration 491, loss = 0.46704817\n",
      "Iteration 492, loss = 0.46690161\n",
      "Iteration 493, loss = 0.46695071\n",
      "Iteration 494, loss = 0.46703677\n",
      "Iteration 495, loss = 0.46690468\n",
      "Iteration 496, loss = 0.46693690\n",
      "Iteration 497, loss = 0.46690012\n",
      "Iteration 498, loss = 0.46669815\n",
      "Iteration 499, loss = 0.46704939\n",
      "Iteration 500, loss = 0.46727193\n",
      "Iteration 501, loss = 0.46669260\n",
      "Iteration 502, loss = 0.46697703\n",
      "Iteration 503, loss = 0.46682304\n",
      "Iteration 504, loss = 0.46701165\n",
      "Iteration 505, loss = 0.46673544\n",
      "Iteration 506, loss = 0.46681355\n",
      "Iteration 507, loss = 0.46650878\n",
      "Iteration 508, loss = 0.46675279\n",
      "Iteration 509, loss = 0.46684809\n",
      "Iteration 510, loss = 0.46675762\n",
      "Iteration 511, loss = 0.46704026\n",
      "Iteration 512, loss = 0.46719274\n",
      "Iteration 513, loss = 0.46673948\n",
      "Iteration 514, loss = 0.46666540\n",
      "Iteration 515, loss = 0.46641125\n",
      "Iteration 516, loss = 0.46637276\n",
      "Iteration 517, loss = 0.46644848\n",
      "Iteration 518, loss = 0.46617564\n",
      "Iteration 519, loss = 0.46622084\n",
      "Iteration 520, loss = 0.46652938\n",
      "Iteration 521, loss = 0.46634350\n",
      "Iteration 522, loss = 0.46627539\n",
      "Iteration 523, loss = 0.46635451\n",
      "Iteration 524, loss = 0.46629992\n",
      "Iteration 525, loss = 0.46625893\n",
      "Iteration 526, loss = 0.46623428\n",
      "Iteration 527, loss = 0.46635445\n",
      "Iteration 528, loss = 0.46639287\n",
      "Iteration 529, loss = 0.46614716\n",
      "Iteration 530, loss = 0.46601925\n",
      "Iteration 531, loss = 0.46603440\n",
      "Iteration 532, loss = 0.46632495\n",
      "Iteration 533, loss = 0.46604664\n",
      "Iteration 534, loss = 0.46642419\n",
      "Iteration 535, loss = 0.46591262\n",
      "Iteration 536, loss = 0.46586417\n",
      "Iteration 537, loss = 0.46598776\n",
      "Iteration 538, loss = 0.46585752\n",
      "Iteration 539, loss = 0.46590342\n",
      "Iteration 540, loss = 0.46598359\n",
      "Iteration 541, loss = 0.46603230\n",
      "Iteration 542, loss = 0.46600552\n",
      "Iteration 543, loss = 0.46607314\n",
      "Iteration 544, loss = 0.46584708\n",
      "Iteration 545, loss = 0.46579930\n",
      "Iteration 546, loss = 0.46579568\n",
      "Iteration 547, loss = 0.46594957\n",
      "Iteration 548, loss = 0.46593368\n",
      "Iteration 549, loss = 0.46581891\n",
      "Iteration 550, loss = 0.46560420\n",
      "Iteration 551, loss = 0.46560359\n",
      "Iteration 552, loss = 0.46551680\n",
      "Iteration 553, loss = 0.46554867\n",
      "Iteration 554, loss = 0.46555210\n",
      "Iteration 555, loss = 0.46537615\n",
      "Iteration 556, loss = 0.46536965\n",
      "Iteration 557, loss = 0.46587429\n",
      "Iteration 558, loss = 0.46521488\n",
      "Iteration 559, loss = 0.46552559\n",
      "Iteration 560, loss = 0.46540237\n",
      "Iteration 561, loss = 0.46533592\n",
      "Iteration 562, loss = 0.46523546\n",
      "Iteration 563, loss = 0.46528991\n",
      "Iteration 564, loss = 0.46516891\n",
      "Iteration 565, loss = 0.46535463\n",
      "Iteration 566, loss = 0.46531974\n",
      "Iteration 567, loss = 0.46519201\n",
      "Iteration 568, loss = 0.46504034\n",
      "Iteration 569, loss = 0.46513081\n",
      "Iteration 570, loss = 0.46511849\n",
      "Iteration 571, loss = 0.46517921\n",
      "Iteration 572, loss = 0.46530353\n",
      "Iteration 573, loss = 0.46501536\n",
      "Iteration 574, loss = 0.46505886\n",
      "Iteration 575, loss = 0.46508009\n",
      "Iteration 576, loss = 0.46500270\n",
      "Iteration 577, loss = 0.46497394\n",
      "Iteration 578, loss = 0.46504217\n",
      "Iteration 579, loss = 0.46512626\n",
      "Iteration 580, loss = 0.46487597\n",
      "Iteration 581, loss = 0.46482242\n",
      "Iteration 582, loss = 0.46498610\n",
      "Iteration 583, loss = 0.46464291\n",
      "Iteration 584, loss = 0.46485832\n",
      "Iteration 585, loss = 0.46470142\n",
      "Iteration 586, loss = 0.46474108\n",
      "Iteration 587, loss = 0.46465960\n",
      "Iteration 588, loss = 0.46458115\n",
      "Iteration 589, loss = 0.46447467\n",
      "Iteration 590, loss = 0.46440874\n",
      "Iteration 591, loss = 0.46498440\n",
      "Iteration 592, loss = 0.46453047\n",
      "Iteration 593, loss = 0.46480885\n",
      "Iteration 594, loss = 0.46477899\n",
      "Iteration 595, loss = 0.46444182\n",
      "Iteration 596, loss = 0.46453375\n",
      "Iteration 597, loss = 0.46480902\n",
      "Iteration 598, loss = 0.46438090\n",
      "Iteration 599, loss = 0.46453057\n",
      "Iteration 600, loss = 0.46443162\n",
      "Iteration 601, loss = 0.46427387\n",
      "Iteration 602, loss = 0.46426319\n",
      "Iteration 603, loss = 0.46432828\n",
      "Iteration 604, loss = 0.46430132\n",
      "Iteration 605, loss = 0.46428393\n",
      "Iteration 606, loss = 0.46451912\n",
      "Iteration 607, loss = 0.46431244\n",
      "Iteration 608, loss = 0.46430158\n",
      "Iteration 609, loss = 0.46404264\n",
      "Iteration 610, loss = 0.46422020\n",
      "Iteration 611, loss = 0.46428538\n",
      "Iteration 612, loss = 0.46409175\n",
      "Iteration 613, loss = 0.46427722\n",
      "Iteration 614, loss = 0.46429799\n",
      "Iteration 615, loss = 0.46398099\n",
      "Iteration 616, loss = 0.46405365\n",
      "Iteration 617, loss = 0.46376792\n",
      "Iteration 618, loss = 0.46392384\n",
      "Iteration 619, loss = 0.46388542\n",
      "Iteration 620, loss = 0.46403546\n",
      "Iteration 621, loss = 0.46364140\n",
      "Iteration 622, loss = 0.46359416\n",
      "Iteration 623, loss = 0.46368896\n",
      "Iteration 624, loss = 0.46355052\n",
      "Iteration 625, loss = 0.46359419\n",
      "Iteration 626, loss = 0.46358846\n",
      "Iteration 627, loss = 0.46366804\n",
      "Iteration 628, loss = 0.46360715\n",
      "Iteration 629, loss = 0.46348388\n",
      "Iteration 630, loss = 0.46356529\n",
      "Iteration 631, loss = 0.46330584\n",
      "Iteration 632, loss = 0.46349219\n",
      "Iteration 633, loss = 0.46302345\n",
      "Iteration 634, loss = 0.46335917\n",
      "Iteration 635, loss = 0.46334810\n",
      "Iteration 636, loss = 0.46319455\n",
      "Iteration 637, loss = 0.46326828\n",
      "Iteration 638, loss = 0.46327304\n",
      "Iteration 639, loss = 0.46318709\n",
      "Iteration 640, loss = 0.46318752\n",
      "Iteration 641, loss = 0.46315345\n",
      "Iteration 642, loss = 0.46301706\n",
      "Iteration 643, loss = 0.46343925\n",
      "Iteration 644, loss = 0.46305640\n",
      "Iteration 645, loss = 0.46302316\n",
      "Iteration 646, loss = 0.46291170\n",
      "Iteration 647, loss = 0.46298771\n",
      "Iteration 648, loss = 0.46286410\n",
      "Iteration 649, loss = 0.46289523\n",
      "Iteration 650, loss = 0.46297410\n",
      "Iteration 651, loss = 0.46278867\n",
      "Iteration 652, loss = 0.46292776\n",
      "Iteration 653, loss = 0.46286375\n",
      "Iteration 654, loss = 0.46266938\n",
      "Iteration 655, loss = 0.46278988\n",
      "Iteration 656, loss = 0.46268273\n",
      "Iteration 657, loss = 0.46284594\n",
      "Iteration 658, loss = 0.46276353\n",
      "Iteration 659, loss = 0.46246777\n",
      "Iteration 660, loss = 0.46276170\n",
      "Iteration 661, loss = 0.46281757\n",
      "Iteration 662, loss = 0.46288275\n",
      "Iteration 663, loss = 0.46305700\n",
      "Iteration 664, loss = 0.46244293\n",
      "Iteration 665, loss = 0.46273377\n",
      "Iteration 666, loss = 0.46290924\n",
      "Iteration 667, loss = 0.46269473\n",
      "Iteration 668, loss = 0.46256936\n",
      "Iteration 669, loss = 0.46243401\n",
      "Iteration 670, loss = 0.46245946\n",
      "Iteration 671, loss = 0.46234614\n",
      "Iteration 672, loss = 0.46239507\n",
      "Iteration 673, loss = 0.46277629\n",
      "Iteration 674, loss = 0.46240167\n",
      "Iteration 675, loss = 0.46280222\n",
      "Iteration 676, loss = 0.46233102\n",
      "Iteration 677, loss = 0.46219976\n",
      "Iteration 678, loss = 0.46217454\n",
      "Iteration 679, loss = 0.46257726\n",
      "Iteration 680, loss = 0.46266481\n",
      "Iteration 681, loss = 0.46246989\n",
      "Iteration 682, loss = 0.46259348\n",
      "Iteration 683, loss = 0.46201510\n",
      "Iteration 684, loss = 0.46257168\n",
      "Iteration 685, loss = 0.46239759\n",
      "Iteration 686, loss = 0.46194106\n",
      "Iteration 687, loss = 0.46212420\n",
      "Iteration 688, loss = 0.46227598\n",
      "Iteration 689, loss = 0.46213330\n",
      "Iteration 690, loss = 0.46215291\n",
      "Iteration 691, loss = 0.46214174\n",
      "Iteration 692, loss = 0.46188563\n",
      "Iteration 693, loss = 0.46201959\n",
      "Iteration 694, loss = 0.46199916\n",
      "Iteration 695, loss = 0.46193196\n",
      "Iteration 696, loss = 0.46203327\n",
      "Iteration 697, loss = 0.46195971\n",
      "Iteration 698, loss = 0.46186976\n",
      "Iteration 699, loss = 0.46165813\n",
      "Iteration 700, loss = 0.46193087\n",
      "Iteration 701, loss = 0.46163546\n",
      "Iteration 702, loss = 0.46164329\n",
      "Iteration 703, loss = 0.46189744\n",
      "Iteration 704, loss = 0.46159670\n",
      "Iteration 705, loss = 0.46170673\n",
      "Iteration 706, loss = 0.46149109\n",
      "Iteration 707, loss = 0.46192585\n",
      "Iteration 708, loss = 0.46166596\n",
      "Iteration 709, loss = 0.46159193\n",
      "Iteration 710, loss = 0.46146440\n",
      "Iteration 711, loss = 0.46152385\n",
      "Iteration 712, loss = 0.46158189\n",
      "Iteration 713, loss = 0.46161109\n",
      "Iteration 714, loss = 0.46145930\n",
      "Iteration 715, loss = 0.46157984\n",
      "Iteration 716, loss = 0.46141428\n",
      "Iteration 717, loss = 0.46147491\n",
      "Iteration 718, loss = 0.46130642\n",
      "Iteration 719, loss = 0.46151852\n",
      "Iteration 720, loss = 0.46161220\n",
      "Iteration 721, loss = 0.46153765\n",
      "Iteration 722, loss = 0.46128143\n",
      "Iteration 723, loss = 0.46133396\n",
      "Iteration 724, loss = 0.46130473\n",
      "Iteration 725, loss = 0.46149206\n",
      "Iteration 726, loss = 0.46117105\n",
      "Iteration 727, loss = 0.46113286\n",
      "Iteration 728, loss = 0.46112646\n",
      "Iteration 729, loss = 0.46119931\n",
      "Iteration 730, loss = 0.46137884\n",
      "Iteration 731, loss = 0.46093434\n",
      "Iteration 732, loss = 0.46145638\n",
      "Iteration 733, loss = 0.46089288\n",
      "Iteration 734, loss = 0.46092057\n",
      "Iteration 735, loss = 0.46100926\n",
      "Iteration 736, loss = 0.46105508\n",
      "Iteration 737, loss = 0.46106499\n",
      "Iteration 738, loss = 0.46086615\n",
      "Iteration 739, loss = 0.46069480\n",
      "Iteration 740, loss = 0.46124112\n",
      "Iteration 741, loss = 0.46132320\n",
      "Iteration 742, loss = 0.46096342\n",
      "Iteration 743, loss = 0.46084971\n",
      "Iteration 744, loss = 0.46061523\n",
      "Iteration 745, loss = 0.46082165\n",
      "Iteration 746, loss = 0.46068533\n",
      "Iteration 747, loss = 0.46069069\n",
      "Iteration 748, loss = 0.46069069\n",
      "Iteration 749, loss = 0.46062234\n",
      "Iteration 750, loss = 0.46077783\n",
      "Iteration 751, loss = 0.46053625\n",
      "Iteration 752, loss = 0.46099624\n",
      "Iteration 753, loss = 0.46062408\n",
      "Iteration 754, loss = 0.46123984\n",
      "Iteration 755, loss = 0.46141183\n",
      "Iteration 756, loss = 0.46086492\n",
      "Iteration 757, loss = 0.46108556\n",
      "Iteration 758, loss = 0.46092914\n",
      "Iteration 759, loss = 0.46047224\n",
      "Iteration 760, loss = 0.46037946\n",
      "Iteration 761, loss = 0.46048276\n",
      "Iteration 762, loss = 0.46010852\n",
      "Iteration 763, loss = 0.46039098\n",
      "Iteration 764, loss = 0.46033379\n",
      "Iteration 765, loss = 0.46030135\n",
      "Iteration 766, loss = 0.46080630\n",
      "Iteration 767, loss = 0.46039355\n",
      "Iteration 768, loss = 0.46023697\n",
      "Iteration 769, loss = 0.46011186\n",
      "Iteration 770, loss = 0.46031192\n",
      "Iteration 771, loss = 0.46026595\n",
      "Iteration 772, loss = 0.46004524\n",
      "Iteration 773, loss = 0.46006783\n",
      "Iteration 774, loss = 0.46008314\n",
      "Iteration 775, loss = 0.45992608\n",
      "Iteration 776, loss = 0.45997772\n",
      "Iteration 777, loss = 0.46013663\n",
      "Iteration 778, loss = 0.46009292\n",
      "Iteration 779, loss = 0.45998145\n",
      "Iteration 780, loss = 0.46049172\n",
      "Iteration 781, loss = 0.45993194\n",
      "Iteration 782, loss = 0.46072761\n",
      "Iteration 783, loss = 0.45980962\n",
      "Iteration 784, loss = 0.45995793\n",
      "Iteration 785, loss = 0.45988903\n",
      "Iteration 786, loss = 0.45974749\n",
      "Iteration 787, loss = 0.46005341\n",
      "Iteration 788, loss = 0.46013907\n",
      "Iteration 789, loss = 0.46062237\n",
      "Iteration 790, loss = 0.45997905\n",
      "Iteration 791, loss = 0.45973523\n",
      "Iteration 792, loss = 0.45967926\n",
      "Iteration 793, loss = 0.45997979\n",
      "Iteration 794, loss = 0.45986031\n",
      "Iteration 795, loss = 0.45978832\n",
      "Iteration 796, loss = 0.45991506\n",
      "Iteration 797, loss = 0.45980728\n",
      "Iteration 798, loss = 0.45980342\n",
      "Iteration 799, loss = 0.45989129\n",
      "Iteration 800, loss = 0.45998959\n",
      "Iteration 801, loss = 0.45988145\n",
      "Iteration 802, loss = 0.45953739\n",
      "Iteration 803, loss = 0.45963252\n",
      "Iteration 804, loss = 0.45970291\n",
      "Iteration 805, loss = 0.46001811\n",
      "Iteration 806, loss = 0.45961498\n",
      "Iteration 807, loss = 0.46001071\n",
      "Iteration 808, loss = 0.45980771\n",
      "Iteration 809, loss = 0.45960074\n",
      "Iteration 810, loss = 0.45938130\n",
      "Iteration 811, loss = 0.45947830\n",
      "Iteration 812, loss = 0.45943210\n",
      "Iteration 813, loss = 0.45964715\n",
      "Iteration 814, loss = 0.45942277\n",
      "Iteration 815, loss = 0.45986273\n",
      "Iteration 816, loss = 0.45950785\n",
      "Iteration 817, loss = 0.45968735\n",
      "Iteration 818, loss = 0.45992026\n",
      "Iteration 819, loss = 0.45947834\n",
      "Iteration 820, loss = 0.45921035\n",
      "Iteration 821, loss = 0.45971888\n",
      "Iteration 822, loss = 0.45948637\n",
      "Iteration 823, loss = 0.45972901\n",
      "Iteration 824, loss = 0.45907439\n",
      "Iteration 825, loss = 0.45925781\n",
      "Iteration 826, loss = 0.45976683\n",
      "Iteration 827, loss = 0.45974201\n",
      "Iteration 828, loss = 0.45916528\n",
      "Iteration 829, loss = 0.45974161\n",
      "Iteration 830, loss = 0.45942533\n",
      "Iteration 831, loss = 0.45955696\n",
      "Iteration 832, loss = 0.45930675\n",
      "Iteration 833, loss = 0.45950240\n",
      "Iteration 834, loss = 0.45942507\n",
      "Iteration 835, loss = 0.45901668\n",
      "Iteration 836, loss = 0.45910948\n",
      "Iteration 837, loss = 0.45924464\n",
      "Iteration 838, loss = 0.45896685\n",
      "Iteration 839, loss = 0.45954385\n",
      "Iteration 840, loss = 0.45919475\n",
      "Iteration 841, loss = 0.45898585\n",
      "Iteration 842, loss = 0.45907699\n",
      "Iteration 843, loss = 0.45898558\n",
      "Iteration 844, loss = 0.45897485\n",
      "Iteration 845, loss = 0.45932388\n",
      "Iteration 846, loss = 0.45919759\n",
      "Iteration 847, loss = 0.45893601\n",
      "Iteration 848, loss = 0.45895410\n",
      "Iteration 849, loss = 0.45920085\n",
      "Iteration 850, loss = 0.45914609\n",
      "Iteration 851, loss = 0.45871134\n",
      "Iteration 852, loss = 0.45900478\n",
      "Iteration 853, loss = 0.45870120\n",
      "Iteration 854, loss = 0.45887816\n",
      "Iteration 855, loss = 0.45868688\n",
      "Iteration 856, loss = 0.45911406\n",
      "Iteration 857, loss = 0.45867970\n",
      "Iteration 858, loss = 0.45858370\n",
      "Iteration 859, loss = 0.45871055\n",
      "Iteration 860, loss = 0.45863697\n",
      "Iteration 861, loss = 0.45884411\n",
      "Iteration 862, loss = 0.45851116\n",
      "Iteration 863, loss = 0.45886432\n",
      "Iteration 864, loss = 0.45883037\n",
      "Iteration 865, loss = 0.45861355\n",
      "Iteration 866, loss = 0.45835801\n",
      "Iteration 867, loss = 0.45881123\n",
      "Iteration 868, loss = 0.45849492\n",
      "Iteration 869, loss = 0.45848518\n",
      "Iteration 870, loss = 0.45865159\n",
      "Iteration 871, loss = 0.45861868\n",
      "Iteration 872, loss = 0.45845723\n",
      "Iteration 873, loss = 0.45835058\n",
      "Iteration 874, loss = 0.45874632\n",
      "Iteration 875, loss = 0.45843369\n",
      "Iteration 876, loss = 0.45842920\n",
      "Iteration 877, loss = 0.45829376\n",
      "Iteration 878, loss = 0.45838918\n",
      "Iteration 879, loss = 0.45793711\n",
      "Iteration 880, loss = 0.45829081\n",
      "Iteration 881, loss = 0.45863677\n",
      "Iteration 882, loss = 0.45798447\n",
      "Iteration 883, loss = 0.45830283\n",
      "Iteration 884, loss = 0.45821555\n",
      "Iteration 885, loss = 0.45828979\n",
      "Iteration 886, loss = 0.45804985\n",
      "Iteration 887, loss = 0.45816726\n",
      "Iteration 888, loss = 0.45809092\n",
      "Iteration 889, loss = 0.45764715\n",
      "Iteration 890, loss = 0.45839012\n",
      "Iteration 891, loss = 0.45804740\n",
      "Iteration 892, loss = 0.45824465\n",
      "Iteration 893, loss = 0.45797214\n",
      "Iteration 894, loss = 0.45771933\n",
      "Iteration 895, loss = 0.45808743\n",
      "Iteration 896, loss = 0.45805417\n",
      "Iteration 897, loss = 0.45835162\n",
      "Iteration 898, loss = 0.45820895\n",
      "Iteration 899, loss = 0.45812198\n",
      "Iteration 900, loss = 0.45835395\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81807866\n",
      "Iteration 2, loss = 0.72196009\n",
      "Iteration 3, loss = 0.67081343\n",
      "Iteration 4, loss = 0.64348685\n",
      "Iteration 5, loss = 0.62594834\n",
      "Iteration 6, loss = 0.61279344\n",
      "Iteration 7, loss = 0.60231750\n",
      "Iteration 8, loss = 0.59358004\n",
      "Iteration 9, loss = 0.58676469\n",
      "Iteration 10, loss = 0.58109310\n",
      "Iteration 11, loss = 0.57659886\n",
      "Iteration 12, loss = 0.57306801\n",
      "Iteration 13, loss = 0.57024170\n",
      "Iteration 14, loss = 0.56785273\n",
      "Iteration 15, loss = 0.56577981\n",
      "Iteration 16, loss = 0.56403405\n",
      "Iteration 17, loss = 0.56239253\n",
      "Iteration 18, loss = 0.56069626\n",
      "Iteration 19, loss = 0.55923752\n",
      "Iteration 20, loss = 0.55779249\n",
      "Iteration 21, loss = 0.55654263\n",
      "Iteration 22, loss = 0.55532023\n",
      "Iteration 23, loss = 0.55404300\n",
      "Iteration 24, loss = 0.55290607\n",
      "Iteration 25, loss = 0.55181329\n",
      "Iteration 26, loss = 0.55061542\n",
      "Iteration 27, loss = 0.54960402\n",
      "Iteration 28, loss = 0.54864307\n",
      "Iteration 29, loss = 0.54774495\n",
      "Iteration 30, loss = 0.54672671\n",
      "Iteration 31, loss = 0.54583484\n",
      "Iteration 32, loss = 0.54513034\n",
      "Iteration 33, loss = 0.54424096\n",
      "Iteration 34, loss = 0.54347118\n",
      "Iteration 35, loss = 0.54276907\n",
      "Iteration 36, loss = 0.54214354\n",
      "Iteration 37, loss = 0.54151637\n",
      "Iteration 38, loss = 0.54084506\n",
      "Iteration 39, loss = 0.54023835\n",
      "Iteration 40, loss = 0.53950992\n",
      "Iteration 41, loss = 0.53893184\n",
      "Iteration 42, loss = 0.53847348\n",
      "Iteration 43, loss = 0.53788412\n",
      "Iteration 44, loss = 0.53740809\n",
      "Iteration 45, loss = 0.53677389\n",
      "Iteration 46, loss = 0.53628427\n",
      "Iteration 47, loss = 0.53616001\n",
      "Iteration 48, loss = 0.53536954\n",
      "Iteration 49, loss = 0.53557236\n",
      "Iteration 50, loss = 0.53455799\n",
      "Iteration 51, loss = 0.53413027\n",
      "Iteration 52, loss = 0.53399037\n",
      "Iteration 53, loss = 0.53318040\n",
      "Iteration 54, loss = 0.53264549\n",
      "Iteration 55, loss = 0.53232424\n",
      "Iteration 56, loss = 0.53178561\n",
      "Iteration 57, loss = 0.53139237\n",
      "Iteration 58, loss = 0.53077223\n",
      "Iteration 59, loss = 0.53067695\n",
      "Iteration 60, loss = 0.53001228\n",
      "Iteration 61, loss = 0.52956894\n",
      "Iteration 62, loss = 0.52923707\n",
      "Iteration 63, loss = 0.52874242\n",
      "Iteration 64, loss = 0.52844525\n",
      "Iteration 65, loss = 0.52814534\n",
      "Iteration 66, loss = 0.52753583\n",
      "Iteration 67, loss = 0.52695083\n",
      "Iteration 68, loss = 0.52636772\n",
      "Iteration 69, loss = 0.52589736\n",
      "Iteration 70, loss = 0.52532536\n",
      "Iteration 71, loss = 0.52465362\n",
      "Iteration 72, loss = 0.52408953\n",
      "Iteration 73, loss = 0.52358121\n",
      "Iteration 74, loss = 0.52282203\n",
      "Iteration 75, loss = 0.52201449\n",
      "Iteration 76, loss = 0.52103734\n",
      "Iteration 77, loss = 0.52012842\n",
      "Iteration 78, loss = 0.51923329\n",
      "Iteration 79, loss = 0.51802239\n",
      "Iteration 80, loss = 0.51720101\n",
      "Iteration 81, loss = 0.51600998\n",
      "Iteration 82, loss = 0.51496225\n",
      "Iteration 83, loss = 0.51395972\n",
      "Iteration 84, loss = 0.51318507\n",
      "Iteration 85, loss = 0.51206773\n",
      "Iteration 86, loss = 0.51130563\n",
      "Iteration 87, loss = 0.51052580\n",
      "Iteration 88, loss = 0.50968332\n",
      "Iteration 89, loss = 0.50863166\n",
      "Iteration 90, loss = 0.50804634\n",
      "Iteration 91, loss = 0.50722537\n",
      "Iteration 92, loss = 0.50656759\n",
      "Iteration 93, loss = 0.50603154\n",
      "Iteration 94, loss = 0.50532648\n",
      "Iteration 95, loss = 0.50475344\n",
      "Iteration 96, loss = 0.50395030\n",
      "Iteration 97, loss = 0.50365404\n",
      "Iteration 98, loss = 0.50329789\n",
      "Iteration 99, loss = 0.50274056\n",
      "Iteration 100, loss = 0.50278680\n",
      "Iteration 101, loss = 0.50187676\n",
      "Iteration 102, loss = 0.50172674\n",
      "Iteration 103, loss = 0.50101223\n",
      "Iteration 104, loss = 0.50071025\n",
      "Iteration 105, loss = 0.50056210\n",
      "Iteration 106, loss = 0.50003901\n",
      "Iteration 107, loss = 0.49981081\n",
      "Iteration 108, loss = 0.49958728\n",
      "Iteration 109, loss = 0.49931436\n",
      "Iteration 110, loss = 0.49862768\n",
      "Iteration 111, loss = 0.49884378\n",
      "Iteration 112, loss = 0.49850724\n",
      "Iteration 113, loss = 0.49803405\n",
      "Iteration 114, loss = 0.49772514\n",
      "Iteration 115, loss = 0.49753667\n",
      "Iteration 116, loss = 0.49712572\n",
      "Iteration 117, loss = 0.49707785\n",
      "Iteration 118, loss = 0.49686792\n",
      "Iteration 119, loss = 0.49655146\n",
      "Iteration 120, loss = 0.49637308\n",
      "Iteration 121, loss = 0.49633987\n",
      "Iteration 122, loss = 0.49579033\n",
      "Iteration 123, loss = 0.49580270\n",
      "Iteration 124, loss = 0.49562928\n",
      "Iteration 125, loss = 0.49517507\n",
      "Iteration 126, loss = 0.49495268\n",
      "Iteration 127, loss = 0.49490587\n",
      "Iteration 128, loss = 0.49450947\n",
      "Iteration 129, loss = 0.49450205\n",
      "Iteration 130, loss = 0.49429613\n",
      "Iteration 131, loss = 0.49421752\n",
      "Iteration 132, loss = 0.49350380\n",
      "Iteration 133, loss = 0.49328929\n",
      "Iteration 134, loss = 0.49400044\n",
      "Iteration 135, loss = 0.49284526\n",
      "Iteration 136, loss = 0.49274721\n",
      "Iteration 137, loss = 0.49243449\n",
      "Iteration 138, loss = 0.49257425\n",
      "Iteration 139, loss = 0.49263143\n",
      "Iteration 140, loss = 0.49184507\n",
      "Iteration 141, loss = 0.49193944\n",
      "Iteration 142, loss = 0.49186021\n",
      "Iteration 143, loss = 0.49165063\n",
      "Iteration 144, loss = 0.49145862\n",
      "Iteration 145, loss = 0.49121524\n",
      "Iteration 146, loss = 0.49097470\n",
      "Iteration 147, loss = 0.49097321\n",
      "Iteration 148, loss = 0.49069067\n",
      "Iteration 149, loss = 0.49075930\n",
      "Iteration 150, loss = 0.49043894\n",
      "Iteration 151, loss = 0.49006324\n",
      "Iteration 152, loss = 0.49026956\n",
      "Iteration 153, loss = 0.49007472\n",
      "Iteration 154, loss = 0.48978710\n",
      "Iteration 155, loss = 0.48978842\n",
      "Iteration 156, loss = 0.48949245\n",
      "Iteration 157, loss = 0.48963153\n",
      "Iteration 158, loss = 0.48907368\n",
      "Iteration 159, loss = 0.48909015\n",
      "Iteration 160, loss = 0.48941006\n",
      "Iteration 161, loss = 0.48951290\n",
      "Iteration 162, loss = 0.48866583\n",
      "Iteration 163, loss = 0.48871075\n",
      "Iteration 164, loss = 0.48832728\n",
      "Iteration 165, loss = 0.48814675\n",
      "Iteration 166, loss = 0.48819486\n",
      "Iteration 167, loss = 0.48843319\n",
      "Iteration 168, loss = 0.48801066\n",
      "Iteration 169, loss = 0.48796359\n",
      "Iteration 170, loss = 0.48800044\n",
      "Iteration 171, loss = 0.48757568\n",
      "Iteration 172, loss = 0.48751649\n",
      "Iteration 173, loss = 0.48736130\n",
      "Iteration 174, loss = 0.48727361\n",
      "Iteration 175, loss = 0.48720388\n",
      "Iteration 176, loss = 0.48710437\n",
      "Iteration 177, loss = 0.48691181\n",
      "Iteration 178, loss = 0.48700644\n",
      "Iteration 179, loss = 0.48674788\n",
      "Iteration 180, loss = 0.48658765\n",
      "Iteration 181, loss = 0.48652538\n",
      "Iteration 182, loss = 0.48669443\n",
      "Iteration 183, loss = 0.48630006\n",
      "Iteration 184, loss = 0.48631556\n",
      "Iteration 185, loss = 0.48646113\n",
      "Iteration 186, loss = 0.48609185\n",
      "Iteration 187, loss = 0.48619729\n",
      "Iteration 188, loss = 0.48602843\n",
      "Iteration 189, loss = 0.48602548\n",
      "Iteration 190, loss = 0.48619248\n",
      "Iteration 191, loss = 0.48623085\n",
      "Iteration 192, loss = 0.48562709\n",
      "Iteration 193, loss = 0.48543248\n",
      "Iteration 194, loss = 0.48555331\n",
      "Iteration 195, loss = 0.48593372\n",
      "Iteration 196, loss = 0.48508855\n",
      "Iteration 197, loss = 0.48525239\n",
      "Iteration 198, loss = 0.48475097\n",
      "Iteration 199, loss = 0.48506768\n",
      "Iteration 200, loss = 0.48550822\n",
      "Iteration 201, loss = 0.48492378\n",
      "Iteration 202, loss = 0.48463489\n",
      "Iteration 203, loss = 0.48462236\n",
      "Iteration 204, loss = 0.48447615\n",
      "Iteration 205, loss = 0.48467098\n",
      "Iteration 206, loss = 0.48437421\n",
      "Iteration 207, loss = 0.48411369\n",
      "Iteration 208, loss = 0.48437133\n",
      "Iteration 209, loss = 0.48439357\n",
      "Iteration 210, loss = 0.48415679\n",
      "Iteration 211, loss = 0.48409528\n",
      "Iteration 212, loss = 0.48359459\n",
      "Iteration 213, loss = 0.48398633\n",
      "Iteration 214, loss = 0.48354994\n",
      "Iteration 215, loss = 0.48394926\n",
      "Iteration 216, loss = 0.48395566\n",
      "Iteration 217, loss = 0.48331285\n",
      "Iteration 218, loss = 0.48346521\n",
      "Iteration 219, loss = 0.48347521\n",
      "Iteration 220, loss = 0.48369965\n",
      "Iteration 221, loss = 0.48285248\n",
      "Iteration 222, loss = 0.48300362\n",
      "Iteration 223, loss = 0.48295687\n",
      "Iteration 224, loss = 0.48303346\n",
      "Iteration 225, loss = 0.48270697\n",
      "Iteration 226, loss = 0.48232840\n",
      "Iteration 227, loss = 0.48256443\n",
      "Iteration 228, loss = 0.48242664\n",
      "Iteration 229, loss = 0.48238567\n",
      "Iteration 230, loss = 0.48217518\n",
      "Iteration 231, loss = 0.48215569\n",
      "Iteration 232, loss = 0.48174162\n",
      "Iteration 233, loss = 0.48183550\n",
      "Iteration 234, loss = 0.48210706\n",
      "Iteration 235, loss = 0.48173636\n",
      "Iteration 236, loss = 0.48167558\n",
      "Iteration 237, loss = 0.48145202\n",
      "Iteration 238, loss = 0.48147423\n",
      "Iteration 239, loss = 0.48128535\n",
      "Iteration 240, loss = 0.48131794\n",
      "Iteration 241, loss = 0.48121085\n",
      "Iteration 242, loss = 0.48090744\n",
      "Iteration 243, loss = 0.48094208\n",
      "Iteration 244, loss = 0.48143884\n",
      "Iteration 245, loss = 0.48091084\n",
      "Iteration 246, loss = 0.48102885\n",
      "Iteration 247, loss = 0.48102319\n",
      "Iteration 248, loss = 0.48051246\n",
      "Iteration 249, loss = 0.48126782\n",
      "Iteration 250, loss = 0.48086324\n",
      "Iteration 251, loss = 0.48042338\n",
      "Iteration 252, loss = 0.48053381\n",
      "Iteration 253, loss = 0.48000876\n",
      "Iteration 254, loss = 0.47996410\n",
      "Iteration 255, loss = 0.48002728\n",
      "Iteration 256, loss = 0.48032062\n",
      "Iteration 257, loss = 0.48031000\n",
      "Iteration 258, loss = 0.47979582\n",
      "Iteration 259, loss = 0.47993881\n",
      "Iteration 260, loss = 0.47963469\n",
      "Iteration 261, loss = 0.47982378\n",
      "Iteration 262, loss = 0.47940802\n",
      "Iteration 263, loss = 0.47951400\n",
      "Iteration 264, loss = 0.47929646\n",
      "Iteration 265, loss = 0.47945695\n",
      "Iteration 266, loss = 0.47919095\n",
      "Iteration 267, loss = 0.47927470\n",
      "Iteration 268, loss = 0.47906297\n",
      "Iteration 269, loss = 0.47919340\n",
      "Iteration 270, loss = 0.47924060\n",
      "Iteration 271, loss = 0.47906403\n",
      "Iteration 272, loss = 0.47881575\n",
      "Iteration 273, loss = 0.47862509\n",
      "Iteration 274, loss = 0.47869889\n",
      "Iteration 275, loss = 0.47881186\n",
      "Iteration 276, loss = 0.47826136\n",
      "Iteration 277, loss = 0.47853348\n",
      "Iteration 278, loss = 0.47849233\n",
      "Iteration 279, loss = 0.47846098\n",
      "Iteration 280, loss = 0.47800678\n",
      "Iteration 281, loss = 0.47813981\n",
      "Iteration 282, loss = 0.47800959\n",
      "Iteration 283, loss = 0.47844990\n",
      "Iteration 284, loss = 0.47800142\n",
      "Iteration 285, loss = 0.47796241\n",
      "Iteration 286, loss = 0.47804822\n",
      "Iteration 287, loss = 0.47840013\n",
      "Iteration 288, loss = 0.47741516\n",
      "Iteration 289, loss = 0.47769142\n",
      "Iteration 290, loss = 0.47736798\n",
      "Iteration 291, loss = 0.47732458\n",
      "Iteration 292, loss = 0.47779813\n",
      "Iteration 293, loss = 0.47729789\n",
      "Iteration 294, loss = 0.47726251\n",
      "Iteration 295, loss = 0.47750219\n",
      "Iteration 296, loss = 0.47760178\n",
      "Iteration 297, loss = 0.47710321\n",
      "Iteration 298, loss = 0.47716317\n",
      "Iteration 299, loss = 0.47726443\n",
      "Iteration 300, loss = 0.47697147\n",
      "Iteration 301, loss = 0.47708937\n",
      "Iteration 302, loss = 0.47680909\n",
      "Iteration 303, loss = 0.47676725\n",
      "Iteration 304, loss = 0.47668551\n",
      "Iteration 305, loss = 0.47688939\n",
      "Iteration 306, loss = 0.47681311\n",
      "Iteration 307, loss = 0.47671474\n",
      "Iteration 308, loss = 0.47704810\n",
      "Iteration 309, loss = 0.47638107\n",
      "Iteration 310, loss = 0.47645462\n",
      "Iteration 311, loss = 0.47673377\n",
      "Iteration 312, loss = 0.47683886\n",
      "Iteration 313, loss = 0.47650725\n",
      "Iteration 314, loss = 0.47638433\n",
      "Iteration 315, loss = 0.47642657\n",
      "Iteration 316, loss = 0.47616564\n",
      "Iteration 317, loss = 0.47628108\n",
      "Iteration 318, loss = 0.47655434\n",
      "Iteration 319, loss = 0.47635129\n",
      "Iteration 320, loss = 0.47584510\n",
      "Iteration 321, loss = 0.47603540\n",
      "Iteration 322, loss = 0.47595377\n",
      "Iteration 323, loss = 0.47580789\n",
      "Iteration 324, loss = 0.47594655\n",
      "Iteration 325, loss = 0.47593850\n",
      "Iteration 326, loss = 0.47576984\n",
      "Iteration 327, loss = 0.47581750\n",
      "Iteration 328, loss = 0.47622910\n",
      "Iteration 329, loss = 0.47629828\n",
      "Iteration 330, loss = 0.47601836\n",
      "Iteration 331, loss = 0.47618934\n",
      "Iteration 332, loss = 0.47531736\n",
      "Iteration 333, loss = 0.47570922\n",
      "Iteration 334, loss = 0.47542959\n",
      "Iteration 335, loss = 0.47582898\n",
      "Iteration 336, loss = 0.47581390\n",
      "Iteration 337, loss = 0.47542301\n",
      "Iteration 338, loss = 0.47562410\n",
      "Iteration 339, loss = 0.47563333\n",
      "Iteration 340, loss = 0.47553835\n",
      "Iteration 341, loss = 0.47512802\n",
      "Iteration 342, loss = 0.47509269\n",
      "Iteration 343, loss = 0.47505893\n",
      "Iteration 344, loss = 0.47514133\n",
      "Iteration 345, loss = 0.47484763\n",
      "Iteration 346, loss = 0.47477232\n",
      "Iteration 347, loss = 0.47526523\n",
      "Iteration 348, loss = 0.47472288\n",
      "Iteration 349, loss = 0.47476921\n",
      "Iteration 350, loss = 0.47491683\n",
      "Iteration 351, loss = 0.47466042\n",
      "Iteration 352, loss = 0.47456128\n",
      "Iteration 353, loss = 0.47489565\n",
      "Iteration 354, loss = 0.47461084\n",
      "Iteration 355, loss = 0.47461374\n",
      "Iteration 356, loss = 0.47487682\n",
      "Iteration 357, loss = 0.47455388\n",
      "Iteration 358, loss = 0.47428644\n",
      "Iteration 359, loss = 0.47448113\n",
      "Iteration 360, loss = 0.47435200\n",
      "Iteration 361, loss = 0.47420807\n",
      "Iteration 362, loss = 0.47436349\n",
      "Iteration 363, loss = 0.47413572\n",
      "Iteration 364, loss = 0.47430842\n",
      "Iteration 365, loss = 0.47398728\n",
      "Iteration 366, loss = 0.47423533\n",
      "Iteration 367, loss = 0.47484109\n",
      "Iteration 368, loss = 0.47383072\n",
      "Iteration 369, loss = 0.47423776\n",
      "Iteration 370, loss = 0.47431623\n",
      "Iteration 371, loss = 0.47428355\n",
      "Iteration 372, loss = 0.47386257\n",
      "Iteration 373, loss = 0.47377088\n",
      "Iteration 374, loss = 0.47488074\n",
      "Iteration 375, loss = 0.47389898\n",
      "Iteration 376, loss = 0.47390362\n",
      "Iteration 377, loss = 0.47383471\n",
      "Iteration 378, loss = 0.47371415\n",
      "Iteration 379, loss = 0.47362043\n",
      "Iteration 380, loss = 0.47337781\n",
      "Iteration 381, loss = 0.47375982\n",
      "Iteration 382, loss = 0.47373234\n",
      "Iteration 383, loss = 0.47345200\n",
      "Iteration 384, loss = 0.47403847\n",
      "Iteration 385, loss = 0.47342069\n",
      "Iteration 386, loss = 0.47343490\n",
      "Iteration 387, loss = 0.47314004\n",
      "Iteration 388, loss = 0.47301215\n",
      "Iteration 389, loss = 0.47333844\n",
      "Iteration 390, loss = 0.47338726\n",
      "Iteration 391, loss = 0.47311605\n",
      "Iteration 392, loss = 0.47335950\n",
      "Iteration 393, loss = 0.47299414\n",
      "Iteration 394, loss = 0.47297039\n",
      "Iteration 395, loss = 0.47305521\n",
      "Iteration 396, loss = 0.47283578\n",
      "Iteration 397, loss = 0.47324559\n",
      "Iteration 398, loss = 0.47297666\n",
      "Iteration 399, loss = 0.47289605\n",
      "Iteration 400, loss = 0.47284158\n",
      "Iteration 401, loss = 0.47312371\n",
      "Iteration 402, loss = 0.47264072\n",
      "Iteration 403, loss = 0.47291328\n",
      "Iteration 404, loss = 0.47340571\n",
      "Iteration 405, loss = 0.47325310\n",
      "Iteration 406, loss = 0.47328687\n",
      "Iteration 407, loss = 0.47286166\n",
      "Iteration 408, loss = 0.47249721\n",
      "Iteration 409, loss = 0.47268229\n",
      "Iteration 410, loss = 0.47295167\n",
      "Iteration 411, loss = 0.47255137\n",
      "Iteration 412, loss = 0.47281009\n",
      "Iteration 413, loss = 0.47260132\n",
      "Iteration 414, loss = 0.47269285\n",
      "Iteration 415, loss = 0.47243658\n",
      "Iteration 416, loss = 0.47224427\n",
      "Iteration 417, loss = 0.47252501\n",
      "Iteration 418, loss = 0.47212664\n",
      "Iteration 419, loss = 0.47226503\n",
      "Iteration 420, loss = 0.47204720\n",
      "Iteration 421, loss = 0.47255337\n",
      "Iteration 422, loss = 0.47227619\n",
      "Iteration 423, loss = 0.47210976\n",
      "Iteration 424, loss = 0.47202941\n",
      "Iteration 425, loss = 0.47200293\n",
      "Iteration 426, loss = 0.47191747\n",
      "Iteration 427, loss = 0.47204440\n",
      "Iteration 428, loss = 0.47175982\n",
      "Iteration 429, loss = 0.47207133\n",
      "Iteration 430, loss = 0.47204706\n",
      "Iteration 431, loss = 0.47169710\n",
      "Iteration 432, loss = 0.47214703\n",
      "Iteration 433, loss = 0.47232236\n",
      "Iteration 434, loss = 0.47202827\n",
      "Iteration 435, loss = 0.47168144\n",
      "Iteration 436, loss = 0.47193044\n",
      "Iteration 437, loss = 0.47196918\n",
      "Iteration 438, loss = 0.47196418\n",
      "Iteration 439, loss = 0.47177488\n",
      "Iteration 440, loss = 0.47234862\n",
      "Iteration 441, loss = 0.47154364\n",
      "Iteration 442, loss = 0.47150283\n",
      "Iteration 443, loss = 0.47176518\n",
      "Iteration 444, loss = 0.47177447\n",
      "Iteration 445, loss = 0.47227590\n",
      "Iteration 446, loss = 0.47160893\n",
      "Iteration 447, loss = 0.47151188\n",
      "Iteration 448, loss = 0.47141845\n",
      "Iteration 449, loss = 0.47141500\n",
      "Iteration 450, loss = 0.47128625\n",
      "Iteration 451, loss = 0.47150246\n",
      "Iteration 452, loss = 0.47185835\n",
      "Iteration 453, loss = 0.47128468\n",
      "Iteration 454, loss = 0.47132728\n",
      "Iteration 455, loss = 0.47133037\n",
      "Iteration 456, loss = 0.47110699\n",
      "Iteration 457, loss = 0.47125875\n",
      "Iteration 458, loss = 0.47130340\n",
      "Iteration 459, loss = 0.47168332\n",
      "Iteration 460, loss = 0.47107798\n",
      "Iteration 461, loss = 0.47101737\n",
      "Iteration 462, loss = 0.47121782\n",
      "Iteration 463, loss = 0.47133853\n",
      "Iteration 464, loss = 0.47126528\n",
      "Iteration 465, loss = 0.47164149\n",
      "Iteration 466, loss = 0.47114736\n",
      "Iteration 467, loss = 0.47117146\n",
      "Iteration 468, loss = 0.47077304\n",
      "Iteration 469, loss = 0.47089039\n",
      "Iteration 470, loss = 0.47098853\n",
      "Iteration 471, loss = 0.47090775\n",
      "Iteration 472, loss = 0.47067094\n",
      "Iteration 473, loss = 0.47049914\n",
      "Iteration 474, loss = 0.47039724\n",
      "Iteration 475, loss = 0.47073581\n",
      "Iteration 476, loss = 0.47079454\n",
      "Iteration 477, loss = 0.47051664\n",
      "Iteration 478, loss = 0.47087047\n",
      "Iteration 479, loss = 0.47034725\n",
      "Iteration 480, loss = 0.47032884\n",
      "Iteration 481, loss = 0.47036430\n",
      "Iteration 482, loss = 0.47028095\n",
      "Iteration 483, loss = 0.47071642\n",
      "Iteration 484, loss = 0.47089586\n",
      "Iteration 485, loss = 0.47056383\n",
      "Iteration 486, loss = 0.47030458\n",
      "Iteration 487, loss = 0.47070137\n",
      "Iteration 488, loss = 0.47036990\n",
      "Iteration 489, loss = 0.47024215\n",
      "Iteration 490, loss = 0.47073154\n",
      "Iteration 491, loss = 0.47025442\n",
      "Iteration 492, loss = 0.47007044\n",
      "Iteration 493, loss = 0.47116034\n",
      "Iteration 494, loss = 0.47097564\n",
      "Iteration 495, loss = 0.47005264\n",
      "Iteration 496, loss = 0.47044747\n",
      "Iteration 497, loss = 0.47021580\n",
      "Iteration 498, loss = 0.47007232\n",
      "Iteration 499, loss = 0.47000805\n",
      "Iteration 500, loss = 0.47038177\n",
      "Iteration 501, loss = 0.46989009\n",
      "Iteration 502, loss = 0.46996068\n",
      "Iteration 503, loss = 0.47049996\n",
      "Iteration 504, loss = 0.47025118\n",
      "Iteration 505, loss = 0.47042238\n",
      "Iteration 506, loss = 0.47018316\n",
      "Iteration 507, loss = 0.47058988\n",
      "Iteration 508, loss = 0.46992829\n",
      "Iteration 509, loss = 0.47025529\n",
      "Iteration 510, loss = 0.47013331\n",
      "Iteration 511, loss = 0.46979527\n",
      "Iteration 512, loss = 0.46973677\n",
      "Iteration 513, loss = 0.46982796\n",
      "Iteration 514, loss = 0.46992298\n",
      "Iteration 515, loss = 0.46991412\n",
      "Iteration 516, loss = 0.46975824\n",
      "Iteration 517, loss = 0.46968031\n",
      "Iteration 518, loss = 0.47006803\n",
      "Iteration 519, loss = 0.47001736\n",
      "Iteration 520, loss = 0.46955643\n",
      "Iteration 521, loss = 0.46980860\n",
      "Iteration 522, loss = 0.46974493\n",
      "Iteration 523, loss = 0.46968662\n",
      "Iteration 524, loss = 0.46975015\n",
      "Iteration 525, loss = 0.46953192\n",
      "Iteration 526, loss = 0.46971782\n",
      "Iteration 527, loss = 0.46969121\n",
      "Iteration 528, loss = 0.47036084\n",
      "Iteration 529, loss = 0.46959721\n",
      "Iteration 530, loss = 0.46993125\n",
      "Iteration 531, loss = 0.46967454\n",
      "Iteration 532, loss = 0.46963758\n",
      "Iteration 533, loss = 0.46943739\n",
      "Iteration 534, loss = 0.46941883\n",
      "Iteration 535, loss = 0.46958134\n",
      "Iteration 536, loss = 0.46950842\n",
      "Iteration 537, loss = 0.46963035\n",
      "Iteration 538, loss = 0.46952223\n",
      "Iteration 539, loss = 0.46975539\n",
      "Iteration 540, loss = 0.46969951\n",
      "Iteration 541, loss = 0.46935943\n",
      "Iteration 542, loss = 0.46956149\n",
      "Iteration 543, loss = 0.46930484\n",
      "Iteration 544, loss = 0.46937475\n",
      "Iteration 545, loss = 0.46932702\n",
      "Iteration 546, loss = 0.46946987\n",
      "Iteration 547, loss = 0.46991489\n",
      "Iteration 548, loss = 0.46960542\n",
      "Iteration 549, loss = 0.47000919\n",
      "Iteration 550, loss = 0.46971900\n",
      "Iteration 551, loss = 0.46952779\n",
      "Iteration 552, loss = 0.46951882\n",
      "Iteration 553, loss = 0.46905008\n",
      "Iteration 554, loss = 0.46958254\n",
      "Iteration 555, loss = 0.47005014\n",
      "Iteration 556, loss = 0.47031303\n",
      "Iteration 557, loss = 0.46950931\n",
      "Iteration 558, loss = 0.46946496\n",
      "Iteration 559, loss = 0.46931330\n",
      "Iteration 560, loss = 0.46927018\n",
      "Iteration 561, loss = 0.46946682\n",
      "Iteration 562, loss = 0.46937996\n",
      "Iteration 563, loss = 0.46951044\n",
      "Iteration 564, loss = 0.46918602\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71719471\n",
      "Iteration 2, loss = 0.70614161\n",
      "Iteration 3, loss = 0.69579198\n",
      "Iteration 4, loss = 0.68361760\n",
      "Iteration 5, loss = 0.66967770\n",
      "Iteration 6, loss = 0.65435276\n",
      "Iteration 7, loss = 0.63898622\n",
      "Iteration 8, loss = 0.62382671\n",
      "Iteration 9, loss = 0.60907982\n",
      "Iteration 10, loss = 0.59559157\n",
      "Iteration 11, loss = 0.58413923\n",
      "Iteration 12, loss = 0.57488726\n",
      "Iteration 13, loss = 0.56709539\n",
      "Iteration 14, loss = 0.56187743\n",
      "Iteration 15, loss = 0.55779705\n",
      "Iteration 16, loss = 0.55459890\n",
      "Iteration 17, loss = 0.55202687\n",
      "Iteration 18, loss = 0.54991924\n",
      "Iteration 19, loss = 0.54824573\n",
      "Iteration 20, loss = 0.54626726\n",
      "Iteration 21, loss = 0.54455504\n",
      "Iteration 22, loss = 0.54322710\n",
      "Iteration 23, loss = 0.54161652\n",
      "Iteration 24, loss = 0.54034650\n",
      "Iteration 25, loss = 0.53905209\n",
      "Iteration 26, loss = 0.53813338\n",
      "Iteration 27, loss = 0.53687495\n",
      "Iteration 28, loss = 0.53570428\n",
      "Iteration 29, loss = 0.53468157\n",
      "Iteration 30, loss = 0.53370148\n",
      "Iteration 31, loss = 0.53282719\n",
      "Iteration 32, loss = 0.53206253\n",
      "Iteration 33, loss = 0.53097010\n",
      "Iteration 34, loss = 0.53016008\n",
      "Iteration 35, loss = 0.52943903\n",
      "Iteration 36, loss = 0.52824672\n",
      "Iteration 37, loss = 0.52756009\n",
      "Iteration 38, loss = 0.52678121\n",
      "Iteration 39, loss = 0.52578157\n",
      "Iteration 40, loss = 0.52492351\n",
      "Iteration 41, loss = 0.52411050\n",
      "Iteration 42, loss = 0.52336013\n",
      "Iteration 43, loss = 0.52244306\n",
      "Iteration 44, loss = 0.52175370\n",
      "Iteration 45, loss = 0.52081961\n",
      "Iteration 46, loss = 0.51994614\n",
      "Iteration 47, loss = 0.51962632\n",
      "Iteration 48, loss = 0.51848184\n",
      "Iteration 49, loss = 0.51748319\n",
      "Iteration 50, loss = 0.51668361\n",
      "Iteration 51, loss = 0.51623945\n",
      "Iteration 52, loss = 0.51517219\n",
      "Iteration 53, loss = 0.51435970\n",
      "Iteration 54, loss = 0.51380864\n",
      "Iteration 55, loss = 0.51386960\n",
      "Iteration 56, loss = 0.51256465\n",
      "Iteration 57, loss = 0.51198589\n",
      "Iteration 58, loss = 0.51119189\n",
      "Iteration 59, loss = 0.51045253\n",
      "Iteration 60, loss = 0.50988789\n",
      "Iteration 61, loss = 0.50928884\n",
      "Iteration 62, loss = 0.50893027\n",
      "Iteration 63, loss = 0.50821628\n",
      "Iteration 64, loss = 0.50782611\n",
      "Iteration 65, loss = 0.50770574\n",
      "Iteration 66, loss = 0.50670502\n",
      "Iteration 67, loss = 0.50605847\n",
      "Iteration 68, loss = 0.50591772\n",
      "Iteration 69, loss = 0.50506559\n",
      "Iteration 70, loss = 0.50461425\n",
      "Iteration 71, loss = 0.50414270\n",
      "Iteration 72, loss = 0.50382194\n",
      "Iteration 73, loss = 0.50329835\n",
      "Iteration 74, loss = 0.50273857\n",
      "Iteration 75, loss = 0.50271466\n",
      "Iteration 76, loss = 0.50210220\n",
      "Iteration 77, loss = 0.50158661\n",
      "Iteration 78, loss = 0.50138619\n",
      "Iteration 79, loss = 0.50084505\n",
      "Iteration 80, loss = 0.50052529\n",
      "Iteration 81, loss = 0.50023264\n",
      "Iteration 82, loss = 0.50035597\n",
      "Iteration 83, loss = 0.49988900\n",
      "Iteration 84, loss = 0.49951479\n",
      "Iteration 85, loss = 0.49902348\n",
      "Iteration 86, loss = 0.49923439\n",
      "Iteration 87, loss = 0.49848612\n",
      "Iteration 88, loss = 0.49844629\n",
      "Iteration 89, loss = 0.49804039\n",
      "Iteration 90, loss = 0.49771910\n",
      "Iteration 91, loss = 0.49743207\n",
      "Iteration 92, loss = 0.49729628\n",
      "Iteration 93, loss = 0.49701806\n",
      "Iteration 94, loss = 0.49695538\n",
      "Iteration 95, loss = 0.49674265\n",
      "Iteration 96, loss = 0.49658519\n",
      "Iteration 97, loss = 0.49633461\n",
      "Iteration 98, loss = 0.49599956\n",
      "Iteration 99, loss = 0.49595507\n",
      "Iteration 100, loss = 0.49587481\n",
      "Iteration 101, loss = 0.49551001\n",
      "Iteration 102, loss = 0.49546865\n",
      "Iteration 103, loss = 0.49502154\n",
      "Iteration 104, loss = 0.49486409\n",
      "Iteration 105, loss = 0.49497132\n",
      "Iteration 106, loss = 0.49490779\n",
      "Iteration 107, loss = 0.49468841\n",
      "Iteration 108, loss = 0.49414157\n",
      "Iteration 109, loss = 0.49442347\n",
      "Iteration 110, loss = 0.49379728\n",
      "Iteration 111, loss = 0.49379276\n",
      "Iteration 112, loss = 0.49392322\n",
      "Iteration 113, loss = 0.49342458\n",
      "Iteration 114, loss = 0.49359518\n",
      "Iteration 115, loss = 0.49316209\n",
      "Iteration 116, loss = 0.49313898\n",
      "Iteration 117, loss = 0.49314197\n",
      "Iteration 118, loss = 0.49280903\n",
      "Iteration 119, loss = 0.49282739\n",
      "Iteration 120, loss = 0.49296526\n",
      "Iteration 121, loss = 0.49259312\n",
      "Iteration 122, loss = 0.49216432\n",
      "Iteration 123, loss = 0.49242654\n",
      "Iteration 124, loss = 0.49201549\n",
      "Iteration 125, loss = 0.49200067\n",
      "Iteration 126, loss = 0.49206794\n",
      "Iteration 127, loss = 0.49158221\n",
      "Iteration 128, loss = 0.49193166\n",
      "Iteration 129, loss = 0.49151810\n",
      "Iteration 130, loss = 0.49140561\n",
      "Iteration 131, loss = 0.49109517\n",
      "Iteration 132, loss = 0.49116271\n",
      "Iteration 133, loss = 0.49093732\n",
      "Iteration 134, loss = 0.49088860\n",
      "Iteration 135, loss = 0.49067016\n",
      "Iteration 136, loss = 0.49060216\n",
      "Iteration 137, loss = 0.49089337\n",
      "Iteration 138, loss = 0.49028259\n",
      "Iteration 139, loss = 0.49032077\n",
      "Iteration 140, loss = 0.49005148\n",
      "Iteration 141, loss = 0.49005991\n",
      "Iteration 142, loss = 0.49016679\n",
      "Iteration 143, loss = 0.48948160\n",
      "Iteration 144, loss = 0.48965263\n",
      "Iteration 145, loss = 0.48956813\n",
      "Iteration 146, loss = 0.48927397\n",
      "Iteration 147, loss = 0.48958309\n",
      "Iteration 148, loss = 0.48894089\n",
      "Iteration 149, loss = 0.48899295\n",
      "Iteration 150, loss = 0.48892300\n",
      "Iteration 151, loss = 0.48907429\n",
      "Iteration 152, loss = 0.48852103\n",
      "Iteration 153, loss = 0.48891651\n",
      "Iteration 154, loss = 0.48894728\n",
      "Iteration 155, loss = 0.48843347\n",
      "Iteration 156, loss = 0.48860504\n",
      "Iteration 157, loss = 0.48829488\n",
      "Iteration 158, loss = 0.48831783\n",
      "Iteration 159, loss = 0.48803837\n",
      "Iteration 160, loss = 0.48791412\n",
      "Iteration 161, loss = 0.48784896\n",
      "Iteration 162, loss = 0.48774557\n",
      "Iteration 163, loss = 0.48760526\n",
      "Iteration 164, loss = 0.48768808\n",
      "Iteration 165, loss = 0.48752084\n",
      "Iteration 166, loss = 0.48742072\n",
      "Iteration 167, loss = 0.48743656\n",
      "Iteration 168, loss = 0.48771704\n",
      "Iteration 169, loss = 0.48738949\n",
      "Iteration 170, loss = 0.48742500\n",
      "Iteration 171, loss = 0.48709804\n",
      "Iteration 172, loss = 0.48715236\n",
      "Iteration 173, loss = 0.48706757\n",
      "Iteration 174, loss = 0.48659278\n",
      "Iteration 175, loss = 0.48686916\n",
      "Iteration 176, loss = 0.48656051\n",
      "Iteration 177, loss = 0.48690538\n",
      "Iteration 178, loss = 0.48640778\n",
      "Iteration 179, loss = 0.48661141\n",
      "Iteration 180, loss = 0.48643144\n",
      "Iteration 181, loss = 0.48669089\n",
      "Iteration 182, loss = 0.48627765\n",
      "Iteration 183, loss = 0.48615258\n",
      "Iteration 184, loss = 0.48707288\n",
      "Iteration 185, loss = 0.48683363\n",
      "Iteration 186, loss = 0.48618813\n",
      "Iteration 187, loss = 0.48593327\n",
      "Iteration 188, loss = 0.48580456\n",
      "Iteration 189, loss = 0.48594172\n",
      "Iteration 190, loss = 0.48554307\n",
      "Iteration 191, loss = 0.48543701\n",
      "Iteration 192, loss = 0.48542428\n",
      "Iteration 193, loss = 0.48525454\n",
      "Iteration 194, loss = 0.48513748\n",
      "Iteration 195, loss = 0.48533185\n",
      "Iteration 196, loss = 0.48487006\n",
      "Iteration 197, loss = 0.48498686\n",
      "Iteration 198, loss = 0.48513210\n",
      "Iteration 199, loss = 0.48476650\n",
      "Iteration 200, loss = 0.48485950\n",
      "Iteration 201, loss = 0.48466700\n",
      "Iteration 202, loss = 0.48473687\n",
      "Iteration 203, loss = 0.48457516\n",
      "Iteration 204, loss = 0.48436993\n",
      "Iteration 205, loss = 0.48430929\n",
      "Iteration 206, loss = 0.48422270\n",
      "Iteration 207, loss = 0.48397267\n",
      "Iteration 208, loss = 0.48402327\n",
      "Iteration 209, loss = 0.48405439\n",
      "Iteration 210, loss = 0.48389459\n",
      "Iteration 211, loss = 0.48416546\n",
      "Iteration 212, loss = 0.48422367\n",
      "Iteration 213, loss = 0.48418433\n",
      "Iteration 214, loss = 0.48357393\n",
      "Iteration 215, loss = 0.48370006\n",
      "Iteration 216, loss = 0.48357142\n",
      "Iteration 217, loss = 0.48348823\n",
      "Iteration 218, loss = 0.48320239\n",
      "Iteration 219, loss = 0.48309222\n",
      "Iteration 220, loss = 0.48315933\n",
      "Iteration 221, loss = 0.48300005\n",
      "Iteration 222, loss = 0.48323771\n",
      "Iteration 223, loss = 0.48303222\n",
      "Iteration 224, loss = 0.48326484\n",
      "Iteration 225, loss = 0.48263806\n",
      "Iteration 226, loss = 0.48268272\n",
      "Iteration 227, loss = 0.48246460\n",
      "Iteration 228, loss = 0.48247262\n",
      "Iteration 229, loss = 0.48213762\n",
      "Iteration 230, loss = 0.48226390\n",
      "Iteration 231, loss = 0.48198741\n",
      "Iteration 232, loss = 0.48212442\n",
      "Iteration 233, loss = 0.48205672\n",
      "Iteration 234, loss = 0.48207097\n",
      "Iteration 235, loss = 0.48178238\n",
      "Iteration 236, loss = 0.48176650\n",
      "Iteration 237, loss = 0.48163183\n",
      "Iteration 238, loss = 0.48168125\n",
      "Iteration 239, loss = 0.48155416\n",
      "Iteration 240, loss = 0.48158577\n",
      "Iteration 241, loss = 0.48142191\n",
      "Iteration 242, loss = 0.48130169\n",
      "Iteration 243, loss = 0.48141821\n",
      "Iteration 244, loss = 0.48148385\n",
      "Iteration 245, loss = 0.48159899\n",
      "Iteration 246, loss = 0.48106298\n",
      "Iteration 247, loss = 0.48107734\n",
      "Iteration 248, loss = 0.48086337\n",
      "Iteration 249, loss = 0.48114551\n",
      "Iteration 250, loss = 0.48074555\n",
      "Iteration 251, loss = 0.48068225\n",
      "Iteration 252, loss = 0.48061940\n",
      "Iteration 253, loss = 0.48083471\n",
      "Iteration 254, loss = 0.48046959\n",
      "Iteration 255, loss = 0.48072089\n",
      "Iteration 256, loss = 0.48040275\n",
      "Iteration 257, loss = 0.48033992\n",
      "Iteration 258, loss = 0.48005134\n",
      "Iteration 259, loss = 0.48007589\n",
      "Iteration 260, loss = 0.48048365\n",
      "Iteration 261, loss = 0.48024928\n",
      "Iteration 262, loss = 0.47981916\n",
      "Iteration 263, loss = 0.47982583\n",
      "Iteration 264, loss = 0.48004351\n",
      "Iteration 265, loss = 0.47961978\n",
      "Iteration 266, loss = 0.47961630\n",
      "Iteration 267, loss = 0.47972377\n",
      "Iteration 268, loss = 0.47931819\n",
      "Iteration 269, loss = 0.47956635\n",
      "Iteration 270, loss = 0.47923621\n",
      "Iteration 271, loss = 0.47940070\n",
      "Iteration 272, loss = 0.47956709\n",
      "Iteration 273, loss = 0.47952485\n",
      "Iteration 274, loss = 0.47893005\n",
      "Iteration 275, loss = 0.47881484\n",
      "Iteration 276, loss = 0.47892405\n",
      "Iteration 277, loss = 0.47884157\n",
      "Iteration 278, loss = 0.47881507\n",
      "Iteration 279, loss = 0.47895659\n",
      "Iteration 280, loss = 0.47884277\n",
      "Iteration 281, loss = 0.47848772\n",
      "Iteration 282, loss = 0.47858139\n",
      "Iteration 283, loss = 0.47845696\n",
      "Iteration 284, loss = 0.47833767\n",
      "Iteration 285, loss = 0.47840208\n",
      "Iteration 286, loss = 0.47812447\n",
      "Iteration 287, loss = 0.47805823\n",
      "Iteration 288, loss = 0.47809396\n",
      "Iteration 289, loss = 0.47815256\n",
      "Iteration 290, loss = 0.47800344\n",
      "Iteration 291, loss = 0.47757559\n",
      "Iteration 292, loss = 0.47812468\n",
      "Iteration 293, loss = 0.47751353\n",
      "Iteration 294, loss = 0.47780444\n",
      "Iteration 295, loss = 0.47846004\n",
      "Iteration 296, loss = 0.47797894\n",
      "Iteration 297, loss = 0.47751135\n",
      "Iteration 298, loss = 0.47744391\n",
      "Iteration 299, loss = 0.47741698\n",
      "Iteration 300, loss = 0.47738815\n",
      "Iteration 301, loss = 0.47711163\n",
      "Iteration 302, loss = 0.47709854\n",
      "Iteration 303, loss = 0.47683451\n",
      "Iteration 304, loss = 0.47680044\n",
      "Iteration 305, loss = 0.47702272\n",
      "Iteration 306, loss = 0.47691665\n",
      "Iteration 307, loss = 0.47672095\n",
      "Iteration 308, loss = 0.47668603\n",
      "Iteration 309, loss = 0.47663588\n",
      "Iteration 310, loss = 0.47664235\n",
      "Iteration 311, loss = 0.47640919\n",
      "Iteration 312, loss = 0.47618233\n",
      "Iteration 313, loss = 0.47623835\n",
      "Iteration 314, loss = 0.47621958\n",
      "Iteration 315, loss = 0.47611297\n",
      "Iteration 316, loss = 0.47617172\n",
      "Iteration 317, loss = 0.47597424\n",
      "Iteration 318, loss = 0.47585250\n",
      "Iteration 319, loss = 0.47601987\n",
      "Iteration 320, loss = 0.47613787\n",
      "Iteration 321, loss = 0.47568118\n",
      "Iteration 322, loss = 0.47572795\n",
      "Iteration 323, loss = 0.47552466\n",
      "Iteration 324, loss = 0.47555359\n",
      "Iteration 325, loss = 0.47563146\n",
      "Iteration 326, loss = 0.47541958\n",
      "Iteration 327, loss = 0.47554265\n",
      "Iteration 328, loss = 0.47609938\n",
      "Iteration 329, loss = 0.47575848\n",
      "Iteration 330, loss = 0.47524849\n",
      "Iteration 331, loss = 0.47542285\n",
      "Iteration 332, loss = 0.47536932\n",
      "Iteration 333, loss = 0.47514990\n",
      "Iteration 334, loss = 0.47525592\n",
      "Iteration 335, loss = 0.47498198\n",
      "Iteration 336, loss = 0.47487561\n",
      "Iteration 337, loss = 0.47550433\n",
      "Iteration 338, loss = 0.47503392\n",
      "Iteration 339, loss = 0.47482776\n",
      "Iteration 340, loss = 0.47475226\n",
      "Iteration 341, loss = 0.47474429\n",
      "Iteration 342, loss = 0.47448578\n",
      "Iteration 343, loss = 0.47484947\n",
      "Iteration 344, loss = 0.47449175\n",
      "Iteration 345, loss = 0.47461304\n",
      "Iteration 346, loss = 0.47440278\n",
      "Iteration 347, loss = 0.47420967\n",
      "Iteration 348, loss = 0.47455448\n",
      "Iteration 349, loss = 0.47421777\n",
      "Iteration 350, loss = 0.47421724\n",
      "Iteration 351, loss = 0.47427788\n",
      "Iteration 352, loss = 0.47449702\n",
      "Iteration 353, loss = 0.47396580\n",
      "Iteration 354, loss = 0.47400101\n",
      "Iteration 355, loss = 0.47397798\n",
      "Iteration 356, loss = 0.47389647\n",
      "Iteration 357, loss = 0.47388122\n",
      "Iteration 358, loss = 0.47389825\n",
      "Iteration 359, loss = 0.47390557\n",
      "Iteration 360, loss = 0.47385365\n",
      "Iteration 361, loss = 0.47396204\n",
      "Iteration 362, loss = 0.47381323\n",
      "Iteration 363, loss = 0.47370377\n",
      "Iteration 364, loss = 0.47341665\n",
      "Iteration 365, loss = 0.47362971\n",
      "Iteration 366, loss = 0.47340886\n",
      "Iteration 367, loss = 0.47343350\n",
      "Iteration 368, loss = 0.47332633\n",
      "Iteration 369, loss = 0.47335277\n",
      "Iteration 370, loss = 0.47344973\n",
      "Iteration 371, loss = 0.47355310\n",
      "Iteration 372, loss = 0.47305697\n",
      "Iteration 373, loss = 0.47315951\n",
      "Iteration 374, loss = 0.47305909\n",
      "Iteration 375, loss = 0.47313095\n",
      "Iteration 376, loss = 0.47305624\n",
      "Iteration 377, loss = 0.47288707\n",
      "Iteration 378, loss = 0.47277648\n",
      "Iteration 379, loss = 0.47298322\n",
      "Iteration 380, loss = 0.47291004\n",
      "Iteration 381, loss = 0.47267731\n",
      "Iteration 382, loss = 0.47260840\n",
      "Iteration 383, loss = 0.47240984\n",
      "Iteration 384, loss = 0.47260343\n",
      "Iteration 385, loss = 0.47234224\n",
      "Iteration 386, loss = 0.47334826\n",
      "Iteration 387, loss = 0.47290238\n",
      "Iteration 388, loss = 0.47211178\n",
      "Iteration 389, loss = 0.47217882\n",
      "Iteration 390, loss = 0.47209908\n",
      "Iteration 391, loss = 0.47225704\n",
      "Iteration 392, loss = 0.47216945\n",
      "Iteration 393, loss = 0.47216278\n",
      "Iteration 394, loss = 0.47225112\n",
      "Iteration 395, loss = 0.47220566\n",
      "Iteration 396, loss = 0.47203135\n",
      "Iteration 397, loss = 0.47191194\n",
      "Iteration 398, loss = 0.47254006\n",
      "Iteration 399, loss = 0.47225098\n",
      "Iteration 400, loss = 0.47172900\n",
      "Iteration 401, loss = 0.47187030\n",
      "Iteration 402, loss = 0.47190469\n",
      "Iteration 403, loss = 0.47138435\n",
      "Iteration 404, loss = 0.47149161\n",
      "Iteration 405, loss = 0.47168347\n",
      "Iteration 406, loss = 0.47151714\n",
      "Iteration 407, loss = 0.47140934\n",
      "Iteration 408, loss = 0.47149438\n",
      "Iteration 409, loss = 0.47127995\n",
      "Iteration 410, loss = 0.47121844\n",
      "Iteration 411, loss = 0.47183555\n",
      "Iteration 412, loss = 0.47190479\n",
      "Iteration 413, loss = 0.47101491\n",
      "Iteration 414, loss = 0.47133885\n",
      "Iteration 415, loss = 0.47120788\n",
      "Iteration 416, loss = 0.47137850\n",
      "Iteration 417, loss = 0.47149191\n",
      "Iteration 418, loss = 0.47096614\n",
      "Iteration 419, loss = 0.47099341\n",
      "Iteration 420, loss = 0.47123704\n",
      "Iteration 421, loss = 0.47122608\n",
      "Iteration 422, loss = 0.47078923\n",
      "Iteration 423, loss = 0.47149972\n",
      "Iteration 424, loss = 0.47102818\n",
      "Iteration 425, loss = 0.47106236\n",
      "Iteration 426, loss = 0.47041371\n",
      "Iteration 427, loss = 0.47062785\n",
      "Iteration 428, loss = 0.47082425\n",
      "Iteration 429, loss = 0.47118955\n",
      "Iteration 430, loss = 0.47037300\n",
      "Iteration 431, loss = 0.47020704\n",
      "Iteration 432, loss = 0.47056140\n",
      "Iteration 433, loss = 0.47031321\n",
      "Iteration 434, loss = 0.47026068\n",
      "Iteration 435, loss = 0.47016185\n",
      "Iteration 436, loss = 0.47018496\n",
      "Iteration 437, loss = 0.47029533\n",
      "Iteration 438, loss = 0.47003725\n",
      "Iteration 439, loss = 0.46997636\n",
      "Iteration 440, loss = 0.46992605\n",
      "Iteration 441, loss = 0.46999359\n",
      "Iteration 442, loss = 0.46975433\n",
      "Iteration 443, loss = 0.46962791\n",
      "Iteration 444, loss = 0.47026960\n",
      "Iteration 445, loss = 0.46958737\n",
      "Iteration 446, loss = 0.46962578\n",
      "Iteration 447, loss = 0.46958135\n",
      "Iteration 448, loss = 0.46956836\n",
      "Iteration 449, loss = 0.46938190\n",
      "Iteration 450, loss = 0.46962609\n",
      "Iteration 451, loss = 0.46969117\n",
      "Iteration 452, loss = 0.46997110\n",
      "Iteration 453, loss = 0.46912165\n",
      "Iteration 454, loss = 0.46948140\n",
      "Iteration 455, loss = 0.46939541\n",
      "Iteration 456, loss = 0.46926831\n",
      "Iteration 457, loss = 0.46921324\n",
      "Iteration 458, loss = 0.46948475\n",
      "Iteration 459, loss = 0.46921505\n",
      "Iteration 460, loss = 0.46907635\n",
      "Iteration 461, loss = 0.46934725\n",
      "Iteration 462, loss = 0.47002189\n",
      "Iteration 463, loss = 0.46914980\n",
      "Iteration 464, loss = 0.46882119\n",
      "Iteration 465, loss = 0.46911735\n",
      "Iteration 466, loss = 0.46868380\n",
      "Iteration 467, loss = 0.46872072\n",
      "Iteration 468, loss = 0.46857635\n",
      "Iteration 469, loss = 0.46847527\n",
      "Iteration 470, loss = 0.46863675\n",
      "Iteration 471, loss = 0.46875244\n",
      "Iteration 472, loss = 0.46827082\n",
      "Iteration 473, loss = 0.46851569\n",
      "Iteration 474, loss = 0.46871943\n",
      "Iteration 475, loss = 0.46834322\n",
      "Iteration 476, loss = 0.46830381\n",
      "Iteration 477, loss = 0.46816315\n",
      "Iteration 478, loss = 0.46820182\n",
      "Iteration 479, loss = 0.46820489\n",
      "Iteration 480, loss = 0.46802215\n",
      "Iteration 481, loss = 0.46829422\n",
      "Iteration 482, loss = 0.46821640\n",
      "Iteration 483, loss = 0.46795662\n",
      "Iteration 484, loss = 0.46820210\n",
      "Iteration 485, loss = 0.46785357\n",
      "Iteration 486, loss = 0.46812738\n",
      "Iteration 487, loss = 0.46787271\n",
      "Iteration 488, loss = 0.46774858\n",
      "Iteration 489, loss = 0.46774533\n",
      "Iteration 490, loss = 0.46773622\n",
      "Iteration 491, loss = 0.46767468\n",
      "Iteration 492, loss = 0.46823449\n",
      "Iteration 493, loss = 0.46761013\n",
      "Iteration 494, loss = 0.46751035\n",
      "Iteration 495, loss = 0.46739014\n",
      "Iteration 496, loss = 0.46744059\n",
      "Iteration 497, loss = 0.46821822\n",
      "Iteration 498, loss = 0.46710877\n",
      "Iteration 499, loss = 0.46732053\n",
      "Iteration 500, loss = 0.46743397\n",
      "Iteration 501, loss = 0.46721981\n",
      "Iteration 502, loss = 0.46726499\n",
      "Iteration 503, loss = 0.46713288\n",
      "Iteration 504, loss = 0.46712878\n",
      "Iteration 505, loss = 0.46713273\n",
      "Iteration 506, loss = 0.46734654\n",
      "Iteration 507, loss = 0.46691375\n",
      "Iteration 508, loss = 0.46712039\n",
      "Iteration 509, loss = 0.46721698\n",
      "Iteration 510, loss = 0.46691689\n",
      "Iteration 511, loss = 0.46662691\n",
      "Iteration 512, loss = 0.46688728\n",
      "Iteration 513, loss = 0.46687885\n",
      "Iteration 514, loss = 0.46674723\n",
      "Iteration 515, loss = 0.46654207\n",
      "Iteration 516, loss = 0.46655499\n",
      "Iteration 517, loss = 0.46655072\n",
      "Iteration 518, loss = 0.46665900\n",
      "Iteration 519, loss = 0.46680172\n",
      "Iteration 520, loss = 0.46658228\n",
      "Iteration 521, loss = 0.46673354\n",
      "Iteration 522, loss = 0.46643590\n",
      "Iteration 523, loss = 0.46624938\n",
      "Iteration 524, loss = 0.46614777\n",
      "Iteration 525, loss = 0.46636770\n",
      "Iteration 526, loss = 0.46634150\n",
      "Iteration 527, loss = 0.46674381\n",
      "Iteration 528, loss = 0.46630022\n",
      "Iteration 529, loss = 0.46654937\n",
      "Iteration 530, loss = 0.46620944\n",
      "Iteration 531, loss = 0.46602734\n",
      "Iteration 532, loss = 0.46631503\n",
      "Iteration 533, loss = 0.46606869\n",
      "Iteration 534, loss = 0.46580243\n",
      "Iteration 535, loss = 0.46575131\n",
      "Iteration 536, loss = 0.46591648\n",
      "Iteration 537, loss = 0.46575290\n",
      "Iteration 538, loss = 0.46575949\n",
      "Iteration 539, loss = 0.46596659\n",
      "Iteration 540, loss = 0.46591448\n",
      "Iteration 541, loss = 0.46580684\n",
      "Iteration 542, loss = 0.46567131\n",
      "Iteration 543, loss = 0.46575785\n",
      "Iteration 544, loss = 0.46584671\n",
      "Iteration 545, loss = 0.46578470\n",
      "Iteration 546, loss = 0.46552059\n",
      "Iteration 547, loss = 0.46549438\n",
      "Iteration 548, loss = 0.46522850\n",
      "Iteration 549, loss = 0.46575674\n",
      "Iteration 550, loss = 0.46557160\n",
      "Iteration 551, loss = 0.46552592\n",
      "Iteration 552, loss = 0.46559731\n",
      "Iteration 553, loss = 0.46617653\n",
      "Iteration 554, loss = 0.46520549\n",
      "Iteration 555, loss = 0.46515138\n",
      "Iteration 556, loss = 0.46503838\n",
      "Iteration 557, loss = 0.46501183\n",
      "Iteration 558, loss = 0.46514986\n",
      "Iteration 559, loss = 0.46532055\n",
      "Iteration 560, loss = 0.46520161\n",
      "Iteration 561, loss = 0.46498412\n",
      "Iteration 562, loss = 0.46522564\n",
      "Iteration 563, loss = 0.46489739\n",
      "Iteration 564, loss = 0.46489489\n",
      "Iteration 565, loss = 0.46495997\n",
      "Iteration 566, loss = 0.46471675\n",
      "Iteration 567, loss = 0.46456302\n",
      "Iteration 568, loss = 0.46476646\n",
      "Iteration 569, loss = 0.46458478\n",
      "Iteration 570, loss = 0.46497948\n",
      "Iteration 571, loss = 0.46468664\n",
      "Iteration 572, loss = 0.46490949\n",
      "Iteration 573, loss = 0.46465689\n",
      "Iteration 574, loss = 0.46453876\n",
      "Iteration 575, loss = 0.46525817\n",
      "Iteration 576, loss = 0.46470762\n",
      "Iteration 577, loss = 0.46484656\n",
      "Iteration 578, loss = 0.46486003\n",
      "Iteration 579, loss = 0.46475273\n",
      "Iteration 580, loss = 0.46433472\n",
      "Iteration 581, loss = 0.46508265\n",
      "Iteration 582, loss = 0.46453933\n",
      "Iteration 583, loss = 0.46468275\n",
      "Iteration 584, loss = 0.46431117\n",
      "Iteration 585, loss = 0.46428153\n",
      "Iteration 586, loss = 0.46404735\n",
      "Iteration 587, loss = 0.46431482\n",
      "Iteration 588, loss = 0.46416271\n",
      "Iteration 589, loss = 0.46422786\n",
      "Iteration 590, loss = 0.46472276\n",
      "Iteration 591, loss = 0.46430703\n",
      "Iteration 592, loss = 0.46391669\n",
      "Iteration 593, loss = 0.46411346\n",
      "Iteration 594, loss = 0.46382851\n",
      "Iteration 595, loss = 0.46396503\n",
      "Iteration 596, loss = 0.46411106\n",
      "Iteration 597, loss = 0.46461929\n",
      "Iteration 598, loss = 0.46380276\n",
      "Iteration 599, loss = 0.46374025\n",
      "Iteration 600, loss = 0.46398217\n",
      "Iteration 601, loss = 0.46349430\n",
      "Iteration 602, loss = 0.46388996\n",
      "Iteration 603, loss = 0.46383444\n",
      "Iteration 604, loss = 0.46356237\n",
      "Iteration 605, loss = 0.46329384\n",
      "Iteration 606, loss = 0.46359804\n",
      "Iteration 607, loss = 0.46362728\n",
      "Iteration 608, loss = 0.46340524\n",
      "Iteration 609, loss = 0.46369230\n",
      "Iteration 610, loss = 0.46332324\n",
      "Iteration 611, loss = 0.46349535\n",
      "Iteration 612, loss = 0.46329212\n",
      "Iteration 613, loss = 0.46355731\n",
      "Iteration 614, loss = 0.46379876\n",
      "Iteration 615, loss = 0.46314638\n",
      "Iteration 616, loss = 0.46355347\n",
      "Iteration 617, loss = 0.46284665\n",
      "Iteration 618, loss = 0.46312459\n",
      "Iteration 619, loss = 0.46324804\n",
      "Iteration 620, loss = 0.46304437\n",
      "Iteration 621, loss = 0.46371219\n",
      "Iteration 622, loss = 0.46379954\n",
      "Iteration 623, loss = 0.46271786\n",
      "Iteration 624, loss = 0.46260095\n",
      "Iteration 625, loss = 0.46271572\n",
      "Iteration 626, loss = 0.46251997\n",
      "Iteration 627, loss = 0.46308959\n",
      "Iteration 628, loss = 0.46252719\n",
      "Iteration 629, loss = 0.46255951\n",
      "Iteration 630, loss = 0.46259426\n",
      "Iteration 631, loss = 0.46319880\n",
      "Iteration 632, loss = 0.46260268\n",
      "Iteration 633, loss = 0.46246655\n",
      "Iteration 634, loss = 0.46259008\n",
      "Iteration 635, loss = 0.46250774\n",
      "Iteration 636, loss = 0.46228264\n",
      "Iteration 637, loss = 0.46229681\n",
      "Iteration 638, loss = 0.46212741\n",
      "Iteration 639, loss = 0.46237845\n",
      "Iteration 640, loss = 0.46216855\n",
      "Iteration 641, loss = 0.46225043\n",
      "Iteration 642, loss = 0.46191674\n",
      "Iteration 643, loss = 0.46187400\n",
      "Iteration 644, loss = 0.46189886\n",
      "Iteration 645, loss = 0.46256177\n",
      "Iteration 646, loss = 0.46198349\n",
      "Iteration 647, loss = 0.46195335\n",
      "Iteration 648, loss = 0.46185348\n",
      "Iteration 649, loss = 0.46189965\n",
      "Iteration 650, loss = 0.46224421\n",
      "Iteration 651, loss = 0.46189335\n",
      "Iteration 652, loss = 0.46232970\n",
      "Iteration 653, loss = 0.46164257\n",
      "Iteration 654, loss = 0.46148739\n",
      "Iteration 655, loss = 0.46148212\n",
      "Iteration 656, loss = 0.46151466\n",
      "Iteration 657, loss = 0.46140275\n",
      "Iteration 658, loss = 0.46126514\n",
      "Iteration 659, loss = 0.46159428\n",
      "Iteration 660, loss = 0.46120540\n",
      "Iteration 661, loss = 0.46119174\n",
      "Iteration 662, loss = 0.46091425\n",
      "Iteration 663, loss = 0.46109430\n",
      "Iteration 664, loss = 0.46106907\n",
      "Iteration 665, loss = 0.46107164\n",
      "Iteration 666, loss = 0.46112900\n",
      "Iteration 667, loss = 0.46114350\n",
      "Iteration 668, loss = 0.46133768\n",
      "Iteration 669, loss = 0.46106086\n",
      "Iteration 670, loss = 0.46108544\n",
      "Iteration 671, loss = 0.46086053\n",
      "Iteration 672, loss = 0.46069446\n",
      "Iteration 673, loss = 0.46103259\n",
      "Iteration 674, loss = 0.46112183\n",
      "Iteration 675, loss = 0.46065882\n",
      "Iteration 676, loss = 0.46101678\n",
      "Iteration 677, loss = 0.46137132\n",
      "Iteration 678, loss = 0.46066030\n",
      "Iteration 679, loss = 0.46072132\n",
      "Iteration 680, loss = 0.46103365\n",
      "Iteration 681, loss = 0.46084825\n",
      "Iteration 682, loss = 0.46080218\n",
      "Iteration 683, loss = 0.46081657\n",
      "Iteration 684, loss = 0.46045783\n",
      "Iteration 685, loss = 0.46061080\n",
      "Iteration 686, loss = 0.46030058\n",
      "Iteration 687, loss = 0.46051453\n",
      "Iteration 688, loss = 0.46061482\n",
      "Iteration 689, loss = 0.46040871\n",
      "Iteration 690, loss = 0.46064695\n",
      "Iteration 691, loss = 0.46055168\n",
      "Iteration 692, loss = 0.46061856\n",
      "Iteration 693, loss = 0.46052456\n",
      "Iteration 694, loss = 0.46018689\n",
      "Iteration 695, loss = 0.46029406\n",
      "Iteration 696, loss = 0.46021722\n",
      "Iteration 697, loss = 0.46055517\n",
      "Iteration 698, loss = 0.46034957\n",
      "Iteration 699, loss = 0.45985949\n",
      "Iteration 700, loss = 0.45991330\n",
      "Iteration 701, loss = 0.45993339\n",
      "Iteration 702, loss = 0.45991134\n",
      "Iteration 703, loss = 0.45986624\n",
      "Iteration 704, loss = 0.45988444\n",
      "Iteration 705, loss = 0.45982304\n",
      "Iteration 706, loss = 0.45971393\n",
      "Iteration 707, loss = 0.46004526\n",
      "Iteration 708, loss = 0.46000165\n",
      "Iteration 709, loss = 0.46017106\n",
      "Iteration 710, loss = 0.46010107\n",
      "Iteration 711, loss = 0.45976165\n",
      "Iteration 712, loss = 0.46006409\n",
      "Iteration 713, loss = 0.46007789\n",
      "Iteration 714, loss = 0.45982578\n",
      "Iteration 715, loss = 0.45967232\n",
      "Iteration 716, loss = 0.46027562\n",
      "Iteration 717, loss = 0.45973618\n",
      "Iteration 718, loss = 0.45935304\n",
      "Iteration 719, loss = 0.45932642\n",
      "Iteration 720, loss = 0.45942130\n",
      "Iteration 721, loss = 0.45933967\n",
      "Iteration 722, loss = 0.45950479\n",
      "Iteration 723, loss = 0.45918370\n",
      "Iteration 724, loss = 0.45942252\n",
      "Iteration 725, loss = 0.45937031\n",
      "Iteration 726, loss = 0.45911386\n",
      "Iteration 727, loss = 0.45919475\n",
      "Iteration 728, loss = 0.45904109\n",
      "Iteration 729, loss = 0.45925901\n",
      "Iteration 730, loss = 0.45931462\n",
      "Iteration 731, loss = 0.45916597\n",
      "Iteration 732, loss = 0.45899376\n",
      "Iteration 733, loss = 0.45911285\n",
      "Iteration 734, loss = 0.45888442\n",
      "Iteration 735, loss = 0.45890839\n",
      "Iteration 736, loss = 0.45885429\n",
      "Iteration 737, loss = 0.45903012\n",
      "Iteration 738, loss = 0.45911180\n",
      "Iteration 739, loss = 0.45874384\n",
      "Iteration 740, loss = 0.45872022\n",
      "Iteration 741, loss = 0.45879925\n",
      "Iteration 742, loss = 0.45887838\n",
      "Iteration 743, loss = 0.45878748\n",
      "Iteration 744, loss = 0.45874188\n",
      "Iteration 745, loss = 0.45886141\n",
      "Iteration 746, loss = 0.45858039\n",
      "Iteration 747, loss = 0.45866685\n",
      "Iteration 748, loss = 0.45867682\n",
      "Iteration 749, loss = 0.45839448\n",
      "Iteration 750, loss = 0.45880660\n",
      "Iteration 751, loss = 0.45837201\n",
      "Iteration 752, loss = 0.45850574\n",
      "Iteration 753, loss = 0.45882761\n",
      "Iteration 754, loss = 0.45892506\n",
      "Iteration 755, loss = 0.45823204\n",
      "Iteration 756, loss = 0.45833809\n",
      "Iteration 757, loss = 0.45832313\n",
      "Iteration 758, loss = 0.45904813\n",
      "Iteration 759, loss = 0.45831304\n",
      "Iteration 760, loss = 0.45796964\n",
      "Iteration 761, loss = 0.45842958\n",
      "Iteration 762, loss = 0.45859774\n",
      "Iteration 763, loss = 0.45862312\n",
      "Iteration 764, loss = 0.45850090\n",
      "Iteration 765, loss = 0.45787484\n",
      "Iteration 766, loss = 0.45799252\n",
      "Iteration 767, loss = 0.45829065\n",
      "Iteration 768, loss = 0.45817218\n",
      "Iteration 769, loss = 0.45834916\n",
      "Iteration 770, loss = 0.45821501\n",
      "Iteration 771, loss = 0.45771804\n",
      "Iteration 772, loss = 0.45778344\n",
      "Iteration 773, loss = 0.45796920\n",
      "Iteration 774, loss = 0.45758377\n",
      "Iteration 775, loss = 0.45805005\n",
      "Iteration 776, loss = 0.45794400\n",
      "Iteration 777, loss = 0.45772902\n",
      "Iteration 778, loss = 0.45749338\n",
      "Iteration 779, loss = 0.45752225\n",
      "Iteration 780, loss = 0.45746704\n",
      "Iteration 781, loss = 0.45756365\n",
      "Iteration 782, loss = 0.45724166\n",
      "Iteration 783, loss = 0.45755916\n",
      "Iteration 784, loss = 0.45758971\n",
      "Iteration 785, loss = 0.45788287\n",
      "Iteration 786, loss = 0.45716728\n",
      "Iteration 787, loss = 0.45722423\n",
      "Iteration 788, loss = 0.45732700\n",
      "Iteration 789, loss = 0.45769109\n",
      "Iteration 790, loss = 0.45722468\n",
      "Iteration 791, loss = 0.45739901\n",
      "Iteration 792, loss = 0.45707291\n",
      "Iteration 793, loss = 0.45735375\n",
      "Iteration 794, loss = 0.45725595\n",
      "Iteration 795, loss = 0.45704257\n",
      "Iteration 796, loss = 0.45792744\n",
      "Iteration 797, loss = 0.45722630\n",
      "Iteration 798, loss = 0.45698052\n",
      "Iteration 799, loss = 0.45706535\n",
      "Iteration 800, loss = 0.45692256\n",
      "Iteration 801, loss = 0.45686044\n",
      "Iteration 802, loss = 0.45698958\n",
      "Iteration 803, loss = 0.45668524\n",
      "Iteration 804, loss = 0.45701545\n",
      "Iteration 805, loss = 0.45689722\n",
      "Iteration 806, loss = 0.45733524\n",
      "Iteration 807, loss = 0.45666589\n",
      "Iteration 808, loss = 0.45640858\n",
      "Iteration 809, loss = 0.45660039\n",
      "Iteration 810, loss = 0.45655219\n",
      "Iteration 811, loss = 0.45669851\n",
      "Iteration 812, loss = 0.45667245\n",
      "Iteration 813, loss = 0.45650179\n",
      "Iteration 814, loss = 0.45654175\n",
      "Iteration 815, loss = 0.45644780\n",
      "Iteration 816, loss = 0.45625415\n",
      "Iteration 817, loss = 0.45635432\n",
      "Iteration 818, loss = 0.45612538\n",
      "Iteration 819, loss = 0.45647113\n",
      "Iteration 820, loss = 0.45593820\n",
      "Iteration 821, loss = 0.45627879\n",
      "Iteration 822, loss = 0.45619756\n",
      "Iteration 823, loss = 0.45588164\n",
      "Iteration 824, loss = 0.45580104\n",
      "Iteration 825, loss = 0.45576960\n",
      "Iteration 826, loss = 0.45608554\n",
      "Iteration 827, loss = 0.45558599\n",
      "Iteration 828, loss = 0.45572585\n",
      "Iteration 829, loss = 0.45660661\n",
      "Iteration 830, loss = 0.45588161\n",
      "Iteration 831, loss = 0.45577477\n",
      "Iteration 832, loss = 0.45568242\n",
      "Iteration 833, loss = 0.45567787\n",
      "Iteration 834, loss = 0.45615936\n",
      "Iteration 835, loss = 0.45587419\n",
      "Iteration 836, loss = 0.45602924\n",
      "Iteration 837, loss = 0.45571545\n",
      "Iteration 838, loss = 0.45592581\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79077244\n",
      "Iteration 2, loss = 0.75370746\n",
      "Iteration 3, loss = 0.72454932\n",
      "Iteration 4, loss = 0.70024175\n",
      "Iteration 5, loss = 0.67878797\n",
      "Iteration 6, loss = 0.65921435\n",
      "Iteration 7, loss = 0.64152789\n",
      "Iteration 8, loss = 0.62582756\n",
      "Iteration 9, loss = 0.61145455\n",
      "Iteration 10, loss = 0.60020084\n",
      "Iteration 11, loss = 0.59140411\n",
      "Iteration 12, loss = 0.58469724\n",
      "Iteration 13, loss = 0.57921603\n",
      "Iteration 14, loss = 0.57549648\n",
      "Iteration 15, loss = 0.57243157\n",
      "Iteration 16, loss = 0.56968372\n",
      "Iteration 17, loss = 0.56732167\n",
      "Iteration 18, loss = 0.56511971\n",
      "Iteration 19, loss = 0.56329766\n",
      "Iteration 20, loss = 0.56154889\n",
      "Iteration 21, loss = 0.55983414\n",
      "Iteration 22, loss = 0.55829556\n",
      "Iteration 23, loss = 0.55675059\n",
      "Iteration 24, loss = 0.55521786\n",
      "Iteration 25, loss = 0.55399394\n",
      "Iteration 26, loss = 0.55265978\n",
      "Iteration 27, loss = 0.55092333\n",
      "Iteration 28, loss = 0.54950203\n",
      "Iteration 29, loss = 0.54796348\n",
      "Iteration 30, loss = 0.54636125\n",
      "Iteration 31, loss = 0.54476651\n",
      "Iteration 32, loss = 0.54325029\n",
      "Iteration 33, loss = 0.54173332\n",
      "Iteration 34, loss = 0.54012018\n",
      "Iteration 35, loss = 0.53870369\n",
      "Iteration 36, loss = 0.53736998\n",
      "Iteration 37, loss = 0.53551839\n",
      "Iteration 38, loss = 0.53436411\n",
      "Iteration 39, loss = 0.53249889\n",
      "Iteration 40, loss = 0.53122288\n",
      "Iteration 41, loss = 0.52981767\n",
      "Iteration 42, loss = 0.52844318\n",
      "Iteration 43, loss = 0.52690619\n",
      "Iteration 44, loss = 0.52556115\n",
      "Iteration 45, loss = 0.52413474\n",
      "Iteration 46, loss = 0.52280640\n",
      "Iteration 47, loss = 0.52150344\n",
      "Iteration 48, loss = 0.52019602\n",
      "Iteration 49, loss = 0.51911763\n",
      "Iteration 50, loss = 0.51797905\n",
      "Iteration 51, loss = 0.51697219\n",
      "Iteration 52, loss = 0.51581673\n",
      "Iteration 53, loss = 0.51474662\n",
      "Iteration 54, loss = 0.51380709\n",
      "Iteration 55, loss = 0.51289835\n",
      "Iteration 56, loss = 0.51200875\n",
      "Iteration 57, loss = 0.51077279\n",
      "Iteration 58, loss = 0.50992578\n",
      "Iteration 59, loss = 0.50911202\n",
      "Iteration 60, loss = 0.50839456\n",
      "Iteration 61, loss = 0.50777182\n",
      "Iteration 62, loss = 0.50675022\n",
      "Iteration 63, loss = 0.50606188\n",
      "Iteration 64, loss = 0.50510981\n",
      "Iteration 65, loss = 0.50485189\n",
      "Iteration 66, loss = 0.50415744\n",
      "Iteration 67, loss = 0.50328400\n",
      "Iteration 68, loss = 0.50273469\n",
      "Iteration 69, loss = 0.50217268\n",
      "Iteration 70, loss = 0.50167878\n",
      "Iteration 71, loss = 0.50130075\n",
      "Iteration 72, loss = 0.50068010\n",
      "Iteration 73, loss = 0.50038196\n",
      "Iteration 74, loss = 0.49982154\n",
      "Iteration 75, loss = 0.49922537\n",
      "Iteration 76, loss = 0.49935835\n",
      "Iteration 77, loss = 0.49836128\n",
      "Iteration 78, loss = 0.49801917\n",
      "Iteration 79, loss = 0.49789630\n",
      "Iteration 80, loss = 0.49709571\n",
      "Iteration 81, loss = 0.49674621\n",
      "Iteration 82, loss = 0.49630626\n",
      "Iteration 83, loss = 0.49642324\n",
      "Iteration 84, loss = 0.49575724\n",
      "Iteration 85, loss = 0.49546852\n",
      "Iteration 86, loss = 0.49479558\n",
      "Iteration 87, loss = 0.49469579\n",
      "Iteration 88, loss = 0.49439507\n",
      "Iteration 89, loss = 0.49448340\n",
      "Iteration 90, loss = 0.49416004\n",
      "Iteration 91, loss = 0.49381964\n",
      "Iteration 92, loss = 0.49317392\n",
      "Iteration 93, loss = 0.49312134\n",
      "Iteration 94, loss = 0.49273781\n",
      "Iteration 95, loss = 0.49253395\n",
      "Iteration 96, loss = 0.49245758\n",
      "Iteration 97, loss = 0.49255029\n",
      "Iteration 98, loss = 0.49195226\n",
      "Iteration 99, loss = 0.49216098\n",
      "Iteration 100, loss = 0.49134524\n",
      "Iteration 101, loss = 0.49144850\n",
      "Iteration 102, loss = 0.49091851\n",
      "Iteration 103, loss = 0.49055459\n",
      "Iteration 104, loss = 0.49081281\n",
      "Iteration 105, loss = 0.49036095\n",
      "Iteration 106, loss = 0.49013601\n",
      "Iteration 107, loss = 0.48983290\n",
      "Iteration 108, loss = 0.48961661\n",
      "Iteration 109, loss = 0.48946022\n",
      "Iteration 110, loss = 0.48907270\n",
      "Iteration 111, loss = 0.48911054\n",
      "Iteration 112, loss = 0.48880822\n",
      "Iteration 113, loss = 0.48928640\n",
      "Iteration 114, loss = 0.48878801\n",
      "Iteration 115, loss = 0.48859687\n",
      "Iteration 116, loss = 0.48786784\n",
      "Iteration 117, loss = 0.48783236\n",
      "Iteration 118, loss = 0.48759147\n",
      "Iteration 119, loss = 0.48747893\n",
      "Iteration 120, loss = 0.48730845\n",
      "Iteration 121, loss = 0.48711650\n",
      "Iteration 122, loss = 0.48699550\n",
      "Iteration 123, loss = 0.48674948\n",
      "Iteration 124, loss = 0.48656644\n",
      "Iteration 125, loss = 0.48661329\n",
      "Iteration 126, loss = 0.48635741\n",
      "Iteration 127, loss = 0.48600876\n",
      "Iteration 128, loss = 0.48588275\n",
      "Iteration 129, loss = 0.48583983\n",
      "Iteration 130, loss = 0.48578492\n",
      "Iteration 131, loss = 0.48539602\n",
      "Iteration 132, loss = 0.48538422\n",
      "Iteration 133, loss = 0.48519306\n",
      "Iteration 134, loss = 0.48501711\n",
      "Iteration 135, loss = 0.48493350\n",
      "Iteration 136, loss = 0.48463830\n",
      "Iteration 137, loss = 0.48536747\n",
      "Iteration 138, loss = 0.48466608\n",
      "Iteration 139, loss = 0.48412577\n",
      "Iteration 140, loss = 0.48449438\n",
      "Iteration 141, loss = 0.48403446\n",
      "Iteration 142, loss = 0.48394428\n",
      "Iteration 143, loss = 0.48384327\n",
      "Iteration 144, loss = 0.48363973\n",
      "Iteration 145, loss = 0.48414766\n",
      "Iteration 146, loss = 0.48341926\n",
      "Iteration 147, loss = 0.48310218\n",
      "Iteration 148, loss = 0.48327383\n",
      "Iteration 149, loss = 0.48307071\n",
      "Iteration 150, loss = 0.48286975\n",
      "Iteration 151, loss = 0.48279134\n",
      "Iteration 152, loss = 0.48289310\n",
      "Iteration 153, loss = 0.48255963\n",
      "Iteration 154, loss = 0.48238315\n",
      "Iteration 155, loss = 0.48235935\n",
      "Iteration 156, loss = 0.48255624\n",
      "Iteration 157, loss = 0.48216886\n",
      "Iteration 158, loss = 0.48190128\n",
      "Iteration 159, loss = 0.48182380\n",
      "Iteration 160, loss = 0.48176934\n",
      "Iteration 161, loss = 0.48170082\n",
      "Iteration 162, loss = 0.48154252\n",
      "Iteration 163, loss = 0.48154120\n",
      "Iteration 164, loss = 0.48101206\n",
      "Iteration 165, loss = 0.48120120\n",
      "Iteration 166, loss = 0.48104354\n",
      "Iteration 167, loss = 0.48062662\n",
      "Iteration 168, loss = 0.48056746\n",
      "Iteration 169, loss = 0.48030740\n",
      "Iteration 170, loss = 0.48032271\n",
      "Iteration 171, loss = 0.47997416\n",
      "Iteration 172, loss = 0.47975660\n",
      "Iteration 173, loss = 0.47980407\n",
      "Iteration 174, loss = 0.48002853\n",
      "Iteration 175, loss = 0.47962411\n",
      "Iteration 176, loss = 0.47917874\n",
      "Iteration 177, loss = 0.47924909\n",
      "Iteration 178, loss = 0.47906401\n",
      "Iteration 179, loss = 0.47890899\n",
      "Iteration 180, loss = 0.47890397\n",
      "Iteration 181, loss = 0.47888416\n",
      "Iteration 182, loss = 0.47852635\n",
      "Iteration 183, loss = 0.47845647\n",
      "Iteration 184, loss = 0.47852754\n",
      "Iteration 185, loss = 0.47852030\n",
      "Iteration 186, loss = 0.47805192\n",
      "Iteration 187, loss = 0.47836662\n",
      "Iteration 188, loss = 0.47790084\n",
      "Iteration 189, loss = 0.47805687\n",
      "Iteration 190, loss = 0.47799566\n",
      "Iteration 191, loss = 0.47815829\n",
      "Iteration 192, loss = 0.47744973\n",
      "Iteration 193, loss = 0.47754190\n",
      "Iteration 194, loss = 0.47755942\n",
      "Iteration 195, loss = 0.47733274\n",
      "Iteration 196, loss = 0.47728734\n",
      "Iteration 197, loss = 0.47748929\n",
      "Iteration 198, loss = 0.47716583\n",
      "Iteration 199, loss = 0.47701718\n",
      "Iteration 200, loss = 0.47768487\n",
      "Iteration 201, loss = 0.47742586\n",
      "Iteration 202, loss = 0.47672812\n",
      "Iteration 203, loss = 0.47663151\n",
      "Iteration 204, loss = 0.47640044\n",
      "Iteration 205, loss = 0.47618753\n",
      "Iteration 206, loss = 0.47619662\n",
      "Iteration 207, loss = 0.47604738\n",
      "Iteration 208, loss = 0.47614635\n",
      "Iteration 209, loss = 0.47621805\n",
      "Iteration 210, loss = 0.47593418\n",
      "Iteration 211, loss = 0.47597126\n",
      "Iteration 212, loss = 0.47582683\n",
      "Iteration 213, loss = 0.47583088\n",
      "Iteration 214, loss = 0.47556632\n",
      "Iteration 215, loss = 0.47560923\n",
      "Iteration 216, loss = 0.47536963\n",
      "Iteration 217, loss = 0.47553587\n",
      "Iteration 218, loss = 0.47519284\n",
      "Iteration 219, loss = 0.47527396\n",
      "Iteration 220, loss = 0.47529711\n",
      "Iteration 221, loss = 0.47524490\n",
      "Iteration 222, loss = 0.47498434\n",
      "Iteration 223, loss = 0.47479625\n",
      "Iteration 224, loss = 0.47489514\n",
      "Iteration 225, loss = 0.47483086\n",
      "Iteration 226, loss = 0.47452415\n",
      "Iteration 227, loss = 0.47470338\n",
      "Iteration 228, loss = 0.47459111\n",
      "Iteration 229, loss = 0.47430157\n",
      "Iteration 230, loss = 0.47474499\n",
      "Iteration 231, loss = 0.47450596\n",
      "Iteration 232, loss = 0.47432023\n",
      "Iteration 233, loss = 0.47425366\n",
      "Iteration 234, loss = 0.47423460\n",
      "Iteration 235, loss = 0.47417675\n",
      "Iteration 236, loss = 0.47425902\n",
      "Iteration 237, loss = 0.47418722\n",
      "Iteration 238, loss = 0.47366952\n",
      "Iteration 239, loss = 0.47409959\n",
      "Iteration 240, loss = 0.47373629\n",
      "Iteration 241, loss = 0.47389193\n",
      "Iteration 242, loss = 0.47363113\n",
      "Iteration 243, loss = 0.47346409\n",
      "Iteration 244, loss = 0.47360527\n",
      "Iteration 245, loss = 0.47341244\n",
      "Iteration 246, loss = 0.47320136\n",
      "Iteration 247, loss = 0.47335831\n",
      "Iteration 248, loss = 0.47337833\n",
      "Iteration 249, loss = 0.47316755\n",
      "Iteration 250, loss = 0.47317801\n",
      "Iteration 251, loss = 0.47327405\n",
      "Iteration 252, loss = 0.47311556\n",
      "Iteration 253, loss = 0.47295688\n",
      "Iteration 254, loss = 0.47305580\n",
      "Iteration 255, loss = 0.47291847\n",
      "Iteration 256, loss = 0.47283511\n",
      "Iteration 257, loss = 0.47283628\n",
      "Iteration 258, loss = 0.47290815\n",
      "Iteration 259, loss = 0.47266845\n",
      "Iteration 260, loss = 0.47283680\n",
      "Iteration 261, loss = 0.47289387\n",
      "Iteration 262, loss = 0.47290347\n",
      "Iteration 263, loss = 0.47253785\n",
      "Iteration 264, loss = 0.47225939\n",
      "Iteration 265, loss = 0.47249492\n",
      "Iteration 266, loss = 0.47240201\n",
      "Iteration 267, loss = 0.47250391\n",
      "Iteration 268, loss = 0.47248309\n",
      "Iteration 269, loss = 0.47184159\n",
      "Iteration 270, loss = 0.47273372\n",
      "Iteration 271, loss = 0.47198538\n",
      "Iteration 272, loss = 0.47213997\n",
      "Iteration 273, loss = 0.47208182\n",
      "Iteration 274, loss = 0.47173475\n",
      "Iteration 275, loss = 0.47189393\n",
      "Iteration 276, loss = 0.47165120\n",
      "Iteration 277, loss = 0.47194249\n",
      "Iteration 278, loss = 0.47196231\n",
      "Iteration 279, loss = 0.47189795\n",
      "Iteration 280, loss = 0.47163927\n",
      "Iteration 281, loss = 0.47134511\n",
      "Iteration 282, loss = 0.47171635\n",
      "Iteration 283, loss = 0.47145956\n",
      "Iteration 284, loss = 0.47105032\n",
      "Iteration 285, loss = 0.47158375\n",
      "Iteration 286, loss = 0.47119940\n",
      "Iteration 287, loss = 0.47143446\n",
      "Iteration 288, loss = 0.47105260\n",
      "Iteration 289, loss = 0.47156767\n",
      "Iteration 290, loss = 0.47113606\n",
      "Iteration 291, loss = 0.47135474\n",
      "Iteration 292, loss = 0.47083247\n",
      "Iteration 293, loss = 0.47085374\n",
      "Iteration 294, loss = 0.47078651\n",
      "Iteration 295, loss = 0.47087499\n",
      "Iteration 296, loss = 0.47067034\n",
      "Iteration 297, loss = 0.47066710\n",
      "Iteration 298, loss = 0.47057356\n",
      "Iteration 299, loss = 0.47204798\n",
      "Iteration 300, loss = 0.47080901\n",
      "Iteration 301, loss = 0.47030447\n",
      "Iteration 302, loss = 0.47064690\n",
      "Iteration 303, loss = 0.47042026\n",
      "Iteration 304, loss = 0.47066940\n",
      "Iteration 305, loss = 0.47058195\n",
      "Iteration 306, loss = 0.47044716\n",
      "Iteration 307, loss = 0.47017408\n",
      "Iteration 308, loss = 0.47030319\n",
      "Iteration 309, loss = 0.47000891\n",
      "Iteration 310, loss = 0.47011426\n",
      "Iteration 311, loss = 0.46983177\n",
      "Iteration 312, loss = 0.47026983\n",
      "Iteration 313, loss = 0.46998510\n",
      "Iteration 314, loss = 0.47014630\n",
      "Iteration 315, loss = 0.46977158\n",
      "Iteration 316, loss = 0.46972199\n",
      "Iteration 317, loss = 0.46953802\n",
      "Iteration 318, loss = 0.46968067\n",
      "Iteration 319, loss = 0.46962901\n",
      "Iteration 320, loss = 0.46954358\n",
      "Iteration 321, loss = 0.46943961\n",
      "Iteration 322, loss = 0.46926216\n",
      "Iteration 323, loss = 0.46933617\n",
      "Iteration 324, loss = 0.46953576\n",
      "Iteration 325, loss = 0.46959980\n",
      "Iteration 326, loss = 0.46968719\n",
      "Iteration 327, loss = 0.46941146\n",
      "Iteration 328, loss = 0.46919137\n",
      "Iteration 329, loss = 0.46962809\n",
      "Iteration 330, loss = 0.46937057\n",
      "Iteration 331, loss = 0.46916586\n",
      "Iteration 332, loss = 0.46912595\n",
      "Iteration 333, loss = 0.46884582\n",
      "Iteration 334, loss = 0.46883152\n",
      "Iteration 335, loss = 0.46890952\n",
      "Iteration 336, loss = 0.46899209\n",
      "Iteration 337, loss = 0.46885696\n",
      "Iteration 338, loss = 0.46912887\n",
      "Iteration 339, loss = 0.46949277\n",
      "Iteration 340, loss = 0.46927428\n",
      "Iteration 341, loss = 0.46870459\n",
      "Iteration 342, loss = 0.46863433\n",
      "Iteration 343, loss = 0.46867699\n",
      "Iteration 344, loss = 0.46841449\n",
      "Iteration 345, loss = 0.46861535\n",
      "Iteration 346, loss = 0.46874689\n",
      "Iteration 347, loss = 0.46859531\n",
      "Iteration 348, loss = 0.46855119\n",
      "Iteration 349, loss = 0.46855816\n",
      "Iteration 350, loss = 0.46831878\n",
      "Iteration 351, loss = 0.46836379\n",
      "Iteration 352, loss = 0.46828373\n",
      "Iteration 353, loss = 0.46852143\n",
      "Iteration 354, loss = 0.46859175\n",
      "Iteration 355, loss = 0.46836836\n",
      "Iteration 356, loss = 0.46818354\n",
      "Iteration 357, loss = 0.46812600\n",
      "Iteration 358, loss = 0.46808296\n",
      "Iteration 359, loss = 0.46815452\n",
      "Iteration 360, loss = 0.46806633\n",
      "Iteration 361, loss = 0.46815432\n",
      "Iteration 362, loss = 0.46800786\n",
      "Iteration 363, loss = 0.46820343\n",
      "Iteration 364, loss = 0.46792448\n",
      "Iteration 365, loss = 0.46811655\n",
      "Iteration 366, loss = 0.46802884\n",
      "Iteration 367, loss = 0.46787494\n",
      "Iteration 368, loss = 0.46802394\n",
      "Iteration 369, loss = 0.46794343\n",
      "Iteration 370, loss = 0.46846738\n",
      "Iteration 371, loss = 0.46793627\n",
      "Iteration 372, loss = 0.46777557\n",
      "Iteration 373, loss = 0.46774901\n",
      "Iteration 374, loss = 0.46758900\n",
      "Iteration 375, loss = 0.46769926\n",
      "Iteration 376, loss = 0.46759455\n",
      "Iteration 377, loss = 0.46760669\n",
      "Iteration 378, loss = 0.46770338\n",
      "Iteration 379, loss = 0.46767600\n",
      "Iteration 380, loss = 0.46761402\n",
      "Iteration 381, loss = 0.46761399\n",
      "Iteration 382, loss = 0.46773734\n",
      "Iteration 383, loss = 0.46761176\n",
      "Iteration 384, loss = 0.46743138\n",
      "Iteration 385, loss = 0.46750738\n",
      "Iteration 386, loss = 0.46735618\n",
      "Iteration 387, loss = 0.46736396\n",
      "Iteration 388, loss = 0.46727077\n",
      "Iteration 389, loss = 0.46752487\n",
      "Iteration 390, loss = 0.46728834\n",
      "Iteration 391, loss = 0.46718853\n",
      "Iteration 392, loss = 0.46752639\n",
      "Iteration 393, loss = 0.46721385\n",
      "Iteration 394, loss = 0.46704982\n",
      "Iteration 395, loss = 0.46721781\n",
      "Iteration 396, loss = 0.46702248\n",
      "Iteration 397, loss = 0.46694825\n",
      "Iteration 398, loss = 0.46690572\n",
      "Iteration 399, loss = 0.46708468\n",
      "Iteration 400, loss = 0.46715659\n",
      "Iteration 401, loss = 0.46663042\n",
      "Iteration 402, loss = 0.46693721\n",
      "Iteration 403, loss = 0.46684769\n",
      "Iteration 404, loss = 0.46675995\n",
      "Iteration 405, loss = 0.46662980\n",
      "Iteration 406, loss = 0.46679881\n",
      "Iteration 407, loss = 0.46704728\n",
      "Iteration 408, loss = 0.46681445\n",
      "Iteration 409, loss = 0.46681099\n",
      "Iteration 410, loss = 0.46646617\n",
      "Iteration 411, loss = 0.46670547\n",
      "Iteration 412, loss = 0.46669762\n",
      "Iteration 413, loss = 0.46686194\n",
      "Iteration 414, loss = 0.46657685\n",
      "Iteration 415, loss = 0.46657491\n",
      "Iteration 416, loss = 0.46636526\n",
      "Iteration 417, loss = 0.46651249\n",
      "Iteration 418, loss = 0.46631649\n",
      "Iteration 419, loss = 0.46634943\n",
      "Iteration 420, loss = 0.46613792\n",
      "Iteration 421, loss = 0.46631442\n",
      "Iteration 422, loss = 0.46630392\n",
      "Iteration 423, loss = 0.46663148\n",
      "Iteration 424, loss = 0.46600602\n",
      "Iteration 425, loss = 0.46622720\n",
      "Iteration 426, loss = 0.46608913\n",
      "Iteration 427, loss = 0.46628880\n",
      "Iteration 428, loss = 0.46624281\n",
      "Iteration 429, loss = 0.46616534\n",
      "Iteration 430, loss = 0.46612219\n",
      "Iteration 431, loss = 0.46571438\n",
      "Iteration 432, loss = 0.46592354\n",
      "Iteration 433, loss = 0.46597092\n",
      "Iteration 434, loss = 0.46604648\n",
      "Iteration 435, loss = 0.46557618\n",
      "Iteration 436, loss = 0.46585387\n",
      "Iteration 437, loss = 0.46568521\n",
      "Iteration 438, loss = 0.46582309\n",
      "Iteration 439, loss = 0.46564782\n",
      "Iteration 440, loss = 0.46570956\n",
      "Iteration 441, loss = 0.46611522\n",
      "Iteration 442, loss = 0.46574085\n",
      "Iteration 443, loss = 0.46575348\n",
      "Iteration 444, loss = 0.46553261\n",
      "Iteration 445, loss = 0.46551889\n",
      "Iteration 446, loss = 0.46554005\n",
      "Iteration 447, loss = 0.46578050\n",
      "Iteration 448, loss = 0.46556724\n",
      "Iteration 449, loss = 0.46572539\n",
      "Iteration 450, loss = 0.46549756\n",
      "Iteration 451, loss = 0.46558620\n",
      "Iteration 452, loss = 0.46522730\n",
      "Iteration 453, loss = 0.46521257\n",
      "Iteration 454, loss = 0.46517735\n",
      "Iteration 455, loss = 0.46552144\n",
      "Iteration 456, loss = 0.46544145\n",
      "Iteration 457, loss = 0.46532611\n",
      "Iteration 458, loss = 0.46519143\n",
      "Iteration 459, loss = 0.46499947\n",
      "Iteration 460, loss = 0.46516187\n",
      "Iteration 461, loss = 0.46499567\n",
      "Iteration 462, loss = 0.46526470\n",
      "Iteration 463, loss = 0.46518729\n",
      "Iteration 464, loss = 0.46502640\n",
      "Iteration 465, loss = 0.46537383\n",
      "Iteration 466, loss = 0.46542093\n",
      "Iteration 467, loss = 0.46551919\n",
      "Iteration 468, loss = 0.46503813\n",
      "Iteration 469, loss = 0.46482683\n",
      "Iteration 470, loss = 0.46537390\n",
      "Iteration 471, loss = 0.46513095\n",
      "Iteration 472, loss = 0.46517056\n",
      "Iteration 473, loss = 0.46478470\n",
      "Iteration 474, loss = 0.46523853\n",
      "Iteration 475, loss = 0.46504854\n",
      "Iteration 476, loss = 0.46473971\n",
      "Iteration 477, loss = 0.46498227\n",
      "Iteration 478, loss = 0.46467487\n",
      "Iteration 479, loss = 0.46472750\n",
      "Iteration 480, loss = 0.46480655\n",
      "Iteration 481, loss = 0.46488886\n",
      "Iteration 482, loss = 0.46458698\n",
      "Iteration 483, loss = 0.46459869\n",
      "Iteration 484, loss = 0.46476441\n",
      "Iteration 485, loss = 0.46457138\n",
      "Iteration 486, loss = 0.46484612\n",
      "Iteration 487, loss = 0.46461339\n",
      "Iteration 488, loss = 0.46449488\n",
      "Iteration 489, loss = 0.46457321\n",
      "Iteration 490, loss = 0.46474074\n",
      "Iteration 491, loss = 0.46440696\n",
      "Iteration 492, loss = 0.46451773\n",
      "Iteration 493, loss = 0.46488936\n",
      "Iteration 494, loss = 0.46459880\n",
      "Iteration 495, loss = 0.46433952\n",
      "Iteration 496, loss = 0.46434541\n",
      "Iteration 497, loss = 0.46423724\n",
      "Iteration 498, loss = 0.46435686\n",
      "Iteration 499, loss = 0.46447156\n",
      "Iteration 500, loss = 0.46468218\n",
      "Iteration 501, loss = 0.46453305\n",
      "Iteration 502, loss = 0.46442043\n",
      "Iteration 503, loss = 0.46431418\n",
      "Iteration 504, loss = 0.46456081\n",
      "Iteration 505, loss = 0.46430506\n",
      "Iteration 506, loss = 0.46435458\n",
      "Iteration 507, loss = 0.46454618\n",
      "Iteration 508, loss = 0.46427876\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83123794\n",
      "Iteration 2, loss = 0.77812459\n",
      "Iteration 3, loss = 0.74297364\n",
      "Iteration 4, loss = 0.71868966\n",
      "Iteration 5, loss = 0.70118154\n",
      "Iteration 6, loss = 0.68568277\n",
      "Iteration 7, loss = 0.67119095\n",
      "Iteration 8, loss = 0.65667660\n",
      "Iteration 9, loss = 0.64252606\n",
      "Iteration 10, loss = 0.62852787\n",
      "Iteration 11, loss = 0.61528167\n",
      "Iteration 12, loss = 0.60418644\n",
      "Iteration 13, loss = 0.59501559\n",
      "Iteration 14, loss = 0.58822932\n",
      "Iteration 15, loss = 0.58292957\n",
      "Iteration 16, loss = 0.57895739\n",
      "Iteration 17, loss = 0.57600053\n",
      "Iteration 18, loss = 0.57337225\n",
      "Iteration 19, loss = 0.57134375\n",
      "Iteration 20, loss = 0.56964430\n",
      "Iteration 21, loss = 0.56791070\n",
      "Iteration 22, loss = 0.56634176\n",
      "Iteration 23, loss = 0.56492076\n",
      "Iteration 24, loss = 0.56370847\n",
      "Iteration 25, loss = 0.56225431\n",
      "Iteration 26, loss = 0.56106885\n",
      "Iteration 27, loss = 0.55962890\n",
      "Iteration 28, loss = 0.55845515\n",
      "Iteration 29, loss = 0.55726172\n",
      "Iteration 30, loss = 0.55631113\n",
      "Iteration 31, loss = 0.55518599\n",
      "Iteration 32, loss = 0.55410223\n",
      "Iteration 33, loss = 0.55305709\n",
      "Iteration 34, loss = 0.55187021\n",
      "Iteration 35, loss = 0.55081016\n",
      "Iteration 36, loss = 0.54970086\n",
      "Iteration 37, loss = 0.54858201\n",
      "Iteration 38, loss = 0.54752502\n",
      "Iteration 39, loss = 0.54635001\n",
      "Iteration 40, loss = 0.54538562\n",
      "Iteration 41, loss = 0.54387417\n",
      "Iteration 42, loss = 0.54283076\n",
      "Iteration 43, loss = 0.54141445\n",
      "Iteration 44, loss = 0.54033235\n",
      "Iteration 45, loss = 0.53897911\n",
      "Iteration 46, loss = 0.53806027\n",
      "Iteration 47, loss = 0.53667474\n",
      "Iteration 48, loss = 0.53573918\n",
      "Iteration 49, loss = 0.53468510\n",
      "Iteration 50, loss = 0.53338997\n",
      "Iteration 51, loss = 0.53228076\n",
      "Iteration 52, loss = 0.53112537\n",
      "Iteration 53, loss = 0.53029826\n",
      "Iteration 54, loss = 0.52902530\n",
      "Iteration 55, loss = 0.52802222\n",
      "Iteration 56, loss = 0.52697774\n",
      "Iteration 57, loss = 0.52606150\n",
      "Iteration 58, loss = 0.52513224\n",
      "Iteration 59, loss = 0.52415104\n",
      "Iteration 60, loss = 0.52323981\n",
      "Iteration 61, loss = 0.52257750\n",
      "Iteration 62, loss = 0.52195484\n",
      "Iteration 63, loss = 0.52098585\n",
      "Iteration 64, loss = 0.52048865\n",
      "Iteration 65, loss = 0.51991417\n",
      "Iteration 66, loss = 0.51934971\n",
      "Iteration 67, loss = 0.51863955\n",
      "Iteration 68, loss = 0.51818983\n",
      "Iteration 69, loss = 0.51768572\n",
      "Iteration 70, loss = 0.51737612\n",
      "Iteration 71, loss = 0.51711561\n",
      "Iteration 72, loss = 0.51641244\n",
      "Iteration 73, loss = 0.51604672\n",
      "Iteration 74, loss = 0.51558239\n",
      "Iteration 75, loss = 0.51503626\n",
      "Iteration 76, loss = 0.51473457\n",
      "Iteration 77, loss = 0.51418326\n",
      "Iteration 78, loss = 0.51372133\n",
      "Iteration 79, loss = 0.51354159\n",
      "Iteration 80, loss = 0.51287810\n",
      "Iteration 81, loss = 0.51256272\n",
      "Iteration 82, loss = 0.51237669\n",
      "Iteration 83, loss = 0.51169915\n",
      "Iteration 84, loss = 0.51176923\n",
      "Iteration 85, loss = 0.51098297\n",
      "Iteration 86, loss = 0.51083136\n",
      "Iteration 87, loss = 0.51029823\n",
      "Iteration 88, loss = 0.50980924\n",
      "Iteration 89, loss = 0.50926773\n",
      "Iteration 90, loss = 0.50903868\n",
      "Iteration 91, loss = 0.50876669\n",
      "Iteration 92, loss = 0.50817686\n",
      "Iteration 93, loss = 0.50759064\n",
      "Iteration 94, loss = 0.50733782\n",
      "Iteration 95, loss = 0.50667630\n",
      "Iteration 96, loss = 0.50617350\n",
      "Iteration 97, loss = 0.50591107\n",
      "Iteration 98, loss = 0.50537154\n",
      "Iteration 99, loss = 0.50500830\n",
      "Iteration 100, loss = 0.50475559\n",
      "Iteration 101, loss = 0.50426697\n",
      "Iteration 102, loss = 0.50381324\n",
      "Iteration 103, loss = 0.50343437\n",
      "Iteration 104, loss = 0.50302002\n",
      "Iteration 105, loss = 0.50253536\n",
      "Iteration 106, loss = 0.50231850\n",
      "Iteration 107, loss = 0.50185548\n",
      "Iteration 108, loss = 0.50148547\n",
      "Iteration 109, loss = 0.50139862\n",
      "Iteration 110, loss = 0.50081540\n",
      "Iteration 111, loss = 0.50076917\n",
      "Iteration 112, loss = 0.50031952\n",
      "Iteration 113, loss = 0.50024613\n",
      "Iteration 114, loss = 0.49981459\n",
      "Iteration 115, loss = 0.49949173\n",
      "Iteration 116, loss = 0.49941029\n",
      "Iteration 117, loss = 0.49902975\n",
      "Iteration 118, loss = 0.49883747\n",
      "Iteration 119, loss = 0.49842178\n",
      "Iteration 120, loss = 0.49844210\n",
      "Iteration 121, loss = 0.49793552\n",
      "Iteration 122, loss = 0.49792736\n",
      "Iteration 123, loss = 0.49776378\n",
      "Iteration 124, loss = 0.49759567\n",
      "Iteration 125, loss = 0.49764188\n",
      "Iteration 126, loss = 0.49728241\n",
      "Iteration 127, loss = 0.49709610\n",
      "Iteration 128, loss = 0.49687718\n",
      "Iteration 129, loss = 0.49653193\n",
      "Iteration 130, loss = 0.49641245\n",
      "Iteration 131, loss = 0.49619181\n",
      "Iteration 132, loss = 0.49601889\n",
      "Iteration 133, loss = 0.49594370\n",
      "Iteration 134, loss = 0.49544038\n",
      "Iteration 135, loss = 0.49570057\n",
      "Iteration 136, loss = 0.49539656\n",
      "Iteration 137, loss = 0.49524748\n",
      "Iteration 138, loss = 0.49551783\n",
      "Iteration 139, loss = 0.49500375\n",
      "Iteration 140, loss = 0.49501119\n",
      "Iteration 141, loss = 0.49499724\n",
      "Iteration 142, loss = 0.49465109\n",
      "Iteration 143, loss = 0.49444746\n",
      "Iteration 144, loss = 0.49449296\n",
      "Iteration 145, loss = 0.49403964\n",
      "Iteration 146, loss = 0.49401177\n",
      "Iteration 147, loss = 0.49408103\n",
      "Iteration 148, loss = 0.49389807\n",
      "Iteration 149, loss = 0.49355880\n",
      "Iteration 150, loss = 0.49366901\n",
      "Iteration 151, loss = 0.49344738\n",
      "Iteration 152, loss = 0.49343892\n",
      "Iteration 153, loss = 0.49320285\n",
      "Iteration 154, loss = 0.49290797\n",
      "Iteration 155, loss = 0.49301831\n",
      "Iteration 156, loss = 0.49267820\n",
      "Iteration 157, loss = 0.49258250\n",
      "Iteration 158, loss = 0.49258695\n",
      "Iteration 159, loss = 0.49250927\n",
      "Iteration 160, loss = 0.49248974\n",
      "Iteration 161, loss = 0.49200304\n",
      "Iteration 162, loss = 0.49212648\n",
      "Iteration 163, loss = 0.49180796\n",
      "Iteration 164, loss = 0.49190117\n",
      "Iteration 165, loss = 0.49146130\n",
      "Iteration 166, loss = 0.49153844\n",
      "Iteration 167, loss = 0.49177104\n",
      "Iteration 168, loss = 0.49126987\n",
      "Iteration 169, loss = 0.49123527\n",
      "Iteration 170, loss = 0.49129670\n",
      "Iteration 171, loss = 0.49093927\n",
      "Iteration 172, loss = 0.49088937\n",
      "Iteration 173, loss = 0.49081079\n",
      "Iteration 174, loss = 0.49176190\n",
      "Iteration 175, loss = 0.49049699\n",
      "Iteration 176, loss = 0.49038064\n",
      "Iteration 177, loss = 0.49052910\n",
      "Iteration 178, loss = 0.49047873\n",
      "Iteration 179, loss = 0.48995800\n",
      "Iteration 180, loss = 0.49007761\n",
      "Iteration 181, loss = 0.49017228\n",
      "Iteration 182, loss = 0.49008581\n",
      "Iteration 183, loss = 0.48993996\n",
      "Iteration 184, loss = 0.49018793\n",
      "Iteration 185, loss = 0.48969827\n",
      "Iteration 186, loss = 0.48976556\n",
      "Iteration 187, loss = 0.48981053\n",
      "Iteration 188, loss = 0.48965342\n",
      "Iteration 189, loss = 0.48922324\n",
      "Iteration 190, loss = 0.48946666\n",
      "Iteration 191, loss = 0.48940059\n",
      "Iteration 192, loss = 0.48924147\n",
      "Iteration 193, loss = 0.48908816\n",
      "Iteration 194, loss = 0.48914514\n",
      "Iteration 195, loss = 0.48940428\n",
      "Iteration 196, loss = 0.48907924\n",
      "Iteration 197, loss = 0.48881708\n",
      "Iteration 198, loss = 0.48910723\n",
      "Iteration 199, loss = 0.48905171\n",
      "Iteration 200, loss = 0.48873366\n",
      "Iteration 201, loss = 0.48855827\n",
      "Iteration 202, loss = 0.48863738\n",
      "Iteration 203, loss = 0.48878054\n",
      "Iteration 204, loss = 0.48854469\n",
      "Iteration 205, loss = 0.48867426\n",
      "Iteration 206, loss = 0.48854393\n",
      "Iteration 207, loss = 0.48849432\n",
      "Iteration 208, loss = 0.48831329\n",
      "Iteration 209, loss = 0.48841458\n",
      "Iteration 210, loss = 0.48854073\n",
      "Iteration 211, loss = 0.48830253\n",
      "Iteration 212, loss = 0.48793337\n",
      "Iteration 213, loss = 0.48818921\n",
      "Iteration 214, loss = 0.48798345\n",
      "Iteration 215, loss = 0.48806333\n",
      "Iteration 216, loss = 0.48817417\n",
      "Iteration 217, loss = 0.48830587\n",
      "Iteration 218, loss = 0.48782239\n",
      "Iteration 219, loss = 0.48795935\n",
      "Iteration 220, loss = 0.48768442\n",
      "Iteration 221, loss = 0.48780835\n",
      "Iteration 222, loss = 0.48773819\n",
      "Iteration 223, loss = 0.48767206\n",
      "Iteration 224, loss = 0.48751865\n",
      "Iteration 225, loss = 0.48745959\n",
      "Iteration 226, loss = 0.48737930\n",
      "Iteration 227, loss = 0.48762764\n",
      "Iteration 228, loss = 0.48742790\n",
      "Iteration 229, loss = 0.48711617\n",
      "Iteration 230, loss = 0.48754741\n",
      "Iteration 231, loss = 0.48724981\n",
      "Iteration 232, loss = 0.48768608\n",
      "Iteration 233, loss = 0.48726146\n",
      "Iteration 234, loss = 0.48714084\n",
      "Iteration 235, loss = 0.48706373\n",
      "Iteration 236, loss = 0.48760616\n",
      "Iteration 237, loss = 0.48688683\n",
      "Iteration 238, loss = 0.48704985\n",
      "Iteration 239, loss = 0.48674129\n",
      "Iteration 240, loss = 0.48675594\n",
      "Iteration 241, loss = 0.48674817\n",
      "Iteration 242, loss = 0.48651661\n",
      "Iteration 243, loss = 0.48647760\n",
      "Iteration 244, loss = 0.48645540\n",
      "Iteration 245, loss = 0.48647333\n",
      "Iteration 246, loss = 0.48680302\n",
      "Iteration 247, loss = 0.48641274\n",
      "Iteration 248, loss = 0.48661218\n",
      "Iteration 249, loss = 0.48615915\n",
      "Iteration 250, loss = 0.48630188\n",
      "Iteration 251, loss = 0.48612241\n",
      "Iteration 252, loss = 0.48600140\n",
      "Iteration 253, loss = 0.48616051\n",
      "Iteration 254, loss = 0.48603038\n",
      "Iteration 255, loss = 0.48599624\n",
      "Iteration 256, loss = 0.48576961\n",
      "Iteration 257, loss = 0.48581476\n",
      "Iteration 258, loss = 0.48577604\n",
      "Iteration 259, loss = 0.48617307\n",
      "Iteration 260, loss = 0.48552842\n",
      "Iteration 261, loss = 0.48651940\n",
      "Iteration 262, loss = 0.48575441\n",
      "Iteration 263, loss = 0.48561852\n",
      "Iteration 264, loss = 0.48530525\n",
      "Iteration 265, loss = 0.48562501\n",
      "Iteration 266, loss = 0.48564673\n",
      "Iteration 267, loss = 0.48523384\n",
      "Iteration 268, loss = 0.48523081\n",
      "Iteration 269, loss = 0.48522027\n",
      "Iteration 270, loss = 0.48501277\n",
      "Iteration 271, loss = 0.48526220\n",
      "Iteration 272, loss = 0.48522029\n",
      "Iteration 273, loss = 0.48502557\n",
      "Iteration 274, loss = 0.48480073\n",
      "Iteration 275, loss = 0.48472049\n",
      "Iteration 276, loss = 0.48476277\n",
      "Iteration 277, loss = 0.48459935\n",
      "Iteration 278, loss = 0.48469135\n",
      "Iteration 279, loss = 0.48456450\n",
      "Iteration 280, loss = 0.48455374\n",
      "Iteration 281, loss = 0.48448985\n",
      "Iteration 282, loss = 0.48456940\n",
      "Iteration 283, loss = 0.48438218\n",
      "Iteration 284, loss = 0.48432318\n",
      "Iteration 285, loss = 0.48428534\n",
      "Iteration 286, loss = 0.48416383\n",
      "Iteration 287, loss = 0.48454385\n",
      "Iteration 288, loss = 0.48413621\n",
      "Iteration 289, loss = 0.48404166\n",
      "Iteration 290, loss = 0.48424791\n",
      "Iteration 291, loss = 0.48404257\n",
      "Iteration 292, loss = 0.48412759\n",
      "Iteration 293, loss = 0.48397012\n",
      "Iteration 294, loss = 0.48397965\n",
      "Iteration 295, loss = 0.48375906\n",
      "Iteration 296, loss = 0.48396661\n",
      "Iteration 297, loss = 0.48384903\n",
      "Iteration 298, loss = 0.48377232\n",
      "Iteration 299, loss = 0.48367370\n",
      "Iteration 300, loss = 0.48372074\n",
      "Iteration 301, loss = 0.48377864\n",
      "Iteration 302, loss = 0.48364686\n",
      "Iteration 303, loss = 0.48396608\n",
      "Iteration 304, loss = 0.48365270\n",
      "Iteration 305, loss = 0.48351079\n",
      "Iteration 306, loss = 0.48332309\n",
      "Iteration 307, loss = 0.48312957\n",
      "Iteration 308, loss = 0.48365015\n",
      "Iteration 309, loss = 0.48328045\n",
      "Iteration 310, loss = 0.48325670\n",
      "Iteration 311, loss = 0.48308647\n",
      "Iteration 312, loss = 0.48323656\n",
      "Iteration 313, loss = 0.48335750\n",
      "Iteration 314, loss = 0.48297681\n",
      "Iteration 315, loss = 0.48317131\n",
      "Iteration 316, loss = 0.48327028\n",
      "Iteration 317, loss = 0.48301712\n",
      "Iteration 318, loss = 0.48331567\n",
      "Iteration 319, loss = 0.48283234\n",
      "Iteration 320, loss = 0.48265787\n",
      "Iteration 321, loss = 0.48277551\n",
      "Iteration 322, loss = 0.48313018\n",
      "Iteration 323, loss = 0.48267872\n",
      "Iteration 324, loss = 0.48252505\n",
      "Iteration 325, loss = 0.48262209\n",
      "Iteration 326, loss = 0.48241107\n",
      "Iteration 327, loss = 0.48259054\n",
      "Iteration 328, loss = 0.48239669\n",
      "Iteration 329, loss = 0.48249423\n",
      "Iteration 330, loss = 0.48220747\n",
      "Iteration 331, loss = 0.48262002\n",
      "Iteration 332, loss = 0.48225103\n",
      "Iteration 333, loss = 0.48213324\n",
      "Iteration 334, loss = 0.48233470\n",
      "Iteration 335, loss = 0.48212796\n",
      "Iteration 336, loss = 0.48194782\n",
      "Iteration 337, loss = 0.48209992\n",
      "Iteration 338, loss = 0.48208546\n",
      "Iteration 339, loss = 0.48199330\n",
      "Iteration 340, loss = 0.48177542\n",
      "Iteration 341, loss = 0.48173684\n",
      "Iteration 342, loss = 0.48188983\n",
      "Iteration 343, loss = 0.48199772\n",
      "Iteration 344, loss = 0.48171492\n",
      "Iteration 345, loss = 0.48230102\n",
      "Iteration 346, loss = 0.48146216\n",
      "Iteration 347, loss = 0.48146531\n",
      "Iteration 348, loss = 0.48182638\n",
      "Iteration 349, loss = 0.48166269\n",
      "Iteration 350, loss = 0.48130581\n",
      "Iteration 351, loss = 0.48134531\n",
      "Iteration 352, loss = 0.48124715\n",
      "Iteration 353, loss = 0.48144224\n",
      "Iteration 354, loss = 0.48103545\n",
      "Iteration 355, loss = 0.48100060\n",
      "Iteration 356, loss = 0.48115392\n",
      "Iteration 357, loss = 0.48117555\n",
      "Iteration 358, loss = 0.48112466\n",
      "Iteration 359, loss = 0.48072099\n",
      "Iteration 360, loss = 0.48114170\n",
      "Iteration 361, loss = 0.48091229\n",
      "Iteration 362, loss = 0.48079717\n",
      "Iteration 363, loss = 0.48054711\n",
      "Iteration 364, loss = 0.48075155\n",
      "Iteration 365, loss = 0.48072079\n",
      "Iteration 366, loss = 0.48051480\n",
      "Iteration 367, loss = 0.48037555\n",
      "Iteration 368, loss = 0.48033789\n",
      "Iteration 369, loss = 0.48041070\n",
      "Iteration 370, loss = 0.48069331\n",
      "Iteration 371, loss = 0.48051853\n",
      "Iteration 372, loss = 0.48006744\n",
      "Iteration 373, loss = 0.48021814\n",
      "Iteration 374, loss = 0.48032847\n",
      "Iteration 375, loss = 0.47999467\n",
      "Iteration 376, loss = 0.47994367\n",
      "Iteration 377, loss = 0.48008477\n",
      "Iteration 378, loss = 0.48062589\n",
      "Iteration 379, loss = 0.48017258\n",
      "Iteration 380, loss = 0.47947655\n",
      "Iteration 381, loss = 0.47995664\n",
      "Iteration 382, loss = 0.47961238\n",
      "Iteration 383, loss = 0.47962396\n",
      "Iteration 384, loss = 0.47944829\n",
      "Iteration 385, loss = 0.47950773\n",
      "Iteration 386, loss = 0.47939653\n",
      "Iteration 387, loss = 0.47941923\n",
      "Iteration 388, loss = 0.47953651\n",
      "Iteration 389, loss = 0.47921038\n",
      "Iteration 390, loss = 0.47928598\n",
      "Iteration 391, loss = 0.47981078\n",
      "Iteration 392, loss = 0.48010540\n",
      "Iteration 393, loss = 0.47940437\n",
      "Iteration 394, loss = 0.47930210\n",
      "Iteration 395, loss = 0.47911342\n",
      "Iteration 396, loss = 0.47959080\n",
      "Iteration 397, loss = 0.47903833\n",
      "Iteration 398, loss = 0.47922423\n",
      "Iteration 399, loss = 0.47916010\n",
      "Iteration 400, loss = 0.47876091\n",
      "Iteration 401, loss = 0.47906848\n",
      "Iteration 402, loss = 0.47883573\n",
      "Iteration 403, loss = 0.47855380\n",
      "Iteration 404, loss = 0.47860901\n",
      "Iteration 405, loss = 0.47885140\n",
      "Iteration 406, loss = 0.47875607\n",
      "Iteration 407, loss = 0.47860588\n",
      "Iteration 408, loss = 0.47898982\n",
      "Iteration 409, loss = 0.47844855\n",
      "Iteration 410, loss = 0.47813080\n",
      "Iteration 411, loss = 0.47836164\n",
      "Iteration 412, loss = 0.47831611\n",
      "Iteration 413, loss = 0.47821523\n",
      "Iteration 414, loss = 0.47803185\n",
      "Iteration 415, loss = 0.47810879\n",
      "Iteration 416, loss = 0.47818951\n",
      "Iteration 417, loss = 0.47813953\n",
      "Iteration 418, loss = 0.47801796\n",
      "Iteration 419, loss = 0.47789836\n",
      "Iteration 420, loss = 0.47777319\n",
      "Iteration 421, loss = 0.47778324\n",
      "Iteration 422, loss = 0.47878793\n",
      "Iteration 423, loss = 0.47805555\n",
      "Iteration 424, loss = 0.47768322\n",
      "Iteration 425, loss = 0.47771001\n",
      "Iteration 426, loss = 0.47772393\n",
      "Iteration 427, loss = 0.47749752\n",
      "Iteration 428, loss = 0.47766283\n",
      "Iteration 429, loss = 0.47772948\n",
      "Iteration 430, loss = 0.47758590\n",
      "Iteration 431, loss = 0.47778235\n",
      "Iteration 432, loss = 0.47818141\n",
      "Iteration 433, loss = 0.47744509\n",
      "Iteration 434, loss = 0.47726138\n",
      "Iteration 435, loss = 0.47738547\n",
      "Iteration 436, loss = 0.47744935\n",
      "Iteration 437, loss = 0.47733874\n",
      "Iteration 438, loss = 0.47721496\n",
      "Iteration 439, loss = 0.47745251\n",
      "Iteration 440, loss = 0.47721152\n",
      "Iteration 441, loss = 0.47694006\n",
      "Iteration 442, loss = 0.47697056\n",
      "Iteration 443, loss = 0.47677562\n",
      "Iteration 444, loss = 0.47683022\n",
      "Iteration 445, loss = 0.47694696\n",
      "Iteration 446, loss = 0.47673313\n",
      "Iteration 447, loss = 0.47691060\n",
      "Iteration 448, loss = 0.47660541\n",
      "Iteration 449, loss = 0.47667692\n",
      "Iteration 450, loss = 0.47664051\n",
      "Iteration 451, loss = 0.47647484\n",
      "Iteration 452, loss = 0.47683855\n",
      "Iteration 453, loss = 0.47649972\n",
      "Iteration 454, loss = 0.47658362\n",
      "Iteration 455, loss = 0.47645121\n",
      "Iteration 456, loss = 0.47646411\n",
      "Iteration 457, loss = 0.47674548\n",
      "Iteration 458, loss = 0.47646224\n",
      "Iteration 459, loss = 0.47649019\n",
      "Iteration 460, loss = 0.47652997\n",
      "Iteration 461, loss = 0.47647525\n",
      "Iteration 462, loss = 0.47639397\n",
      "Iteration 463, loss = 0.47603318\n",
      "Iteration 464, loss = 0.47680300\n",
      "Iteration 465, loss = 0.47643056\n",
      "Iteration 466, loss = 0.47620407\n",
      "Iteration 467, loss = 0.47612074\n",
      "Iteration 468, loss = 0.47606735\n",
      "Iteration 469, loss = 0.47614587\n",
      "Iteration 470, loss = 0.47610862\n",
      "Iteration 471, loss = 0.47586617\n",
      "Iteration 472, loss = 0.47598551\n",
      "Iteration 473, loss = 0.47620532\n",
      "Iteration 474, loss = 0.47611367\n",
      "Iteration 475, loss = 0.47583083\n",
      "Iteration 476, loss = 0.47635807\n",
      "Iteration 477, loss = 0.47625896\n",
      "Iteration 478, loss = 0.47576351\n",
      "Iteration 479, loss = 0.47580921\n",
      "Iteration 480, loss = 0.47562394\n",
      "Iteration 481, loss = 0.47620089\n",
      "Iteration 482, loss = 0.47603014\n",
      "Iteration 483, loss = 0.47553172\n",
      "Iteration 484, loss = 0.47553808\n",
      "Iteration 485, loss = 0.47558342\n",
      "Iteration 486, loss = 0.47556634\n",
      "Iteration 487, loss = 0.47551961\n",
      "Iteration 488, loss = 0.47549514\n",
      "Iteration 489, loss = 0.47547575\n",
      "Iteration 490, loss = 0.47533476\n",
      "Iteration 491, loss = 0.47581154\n",
      "Iteration 492, loss = 0.47549798\n",
      "Iteration 493, loss = 0.47561304\n",
      "Iteration 494, loss = 0.47520492\n",
      "Iteration 495, loss = 0.47532727\n",
      "Iteration 496, loss = 0.47541977\n",
      "Iteration 497, loss = 0.47579128\n",
      "Iteration 498, loss = 0.47533199\n",
      "Iteration 499, loss = 0.47505875\n",
      "Iteration 500, loss = 0.47494303\n",
      "Iteration 501, loss = 0.47517421\n",
      "Iteration 502, loss = 0.47520257\n",
      "Iteration 503, loss = 0.47515271\n",
      "Iteration 504, loss = 0.47534752\n",
      "Iteration 505, loss = 0.47530291\n",
      "Iteration 506, loss = 0.47485524\n",
      "Iteration 507, loss = 0.47507863\n",
      "Iteration 508, loss = 0.47493565\n",
      "Iteration 509, loss = 0.47514208\n",
      "Iteration 510, loss = 0.47470820\n",
      "Iteration 511, loss = 0.47492342\n",
      "Iteration 512, loss = 0.47476260\n",
      "Iteration 513, loss = 0.47455840\n",
      "Iteration 514, loss = 0.47467648\n",
      "Iteration 515, loss = 0.47459321\n",
      "Iteration 516, loss = 0.47526597\n",
      "Iteration 517, loss = 0.47533903\n",
      "Iteration 518, loss = 0.47475894\n",
      "Iteration 519, loss = 0.47463399\n",
      "Iteration 520, loss = 0.47450998\n",
      "Iteration 521, loss = 0.47458916\n",
      "Iteration 522, loss = 0.47448932\n",
      "Iteration 523, loss = 0.47461709\n",
      "Iteration 524, loss = 0.47394151\n",
      "Iteration 525, loss = 0.47427627\n",
      "Iteration 526, loss = 0.47393339\n",
      "Iteration 527, loss = 0.47439319\n",
      "Iteration 528, loss = 0.47384859\n",
      "Iteration 529, loss = 0.47458156\n",
      "Iteration 530, loss = 0.47455628\n",
      "Iteration 531, loss = 0.47416222\n",
      "Iteration 532, loss = 0.47374129\n",
      "Iteration 533, loss = 0.47372483\n",
      "Iteration 534, loss = 0.47392648\n",
      "Iteration 535, loss = 0.47370341\n",
      "Iteration 536, loss = 0.47380448\n",
      "Iteration 537, loss = 0.47369720\n",
      "Iteration 538, loss = 0.47379007\n",
      "Iteration 539, loss = 0.47363930\n",
      "Iteration 540, loss = 0.47373728\n",
      "Iteration 541, loss = 0.47378665\n",
      "Iteration 542, loss = 0.47345272\n",
      "Iteration 543, loss = 0.47363155\n",
      "Iteration 544, loss = 0.47356561\n",
      "Iteration 545, loss = 0.47372867\n",
      "Iteration 546, loss = 0.47360946\n",
      "Iteration 547, loss = 0.47396479\n",
      "Iteration 548, loss = 0.47328992\n",
      "Iteration 549, loss = 0.47324911\n",
      "Iteration 550, loss = 0.47321151\n",
      "Iteration 551, loss = 0.47349593\n",
      "Iteration 552, loss = 0.47344109\n",
      "Iteration 553, loss = 0.47343683\n",
      "Iteration 554, loss = 0.47325012\n",
      "Iteration 555, loss = 0.47362310\n",
      "Iteration 556, loss = 0.47316964\n",
      "Iteration 557, loss = 0.47341318\n",
      "Iteration 558, loss = 0.47332383\n",
      "Iteration 559, loss = 0.47320960\n",
      "Iteration 560, loss = 0.47304987\n",
      "Iteration 561, loss = 0.47322998\n",
      "Iteration 562, loss = 0.47309446\n",
      "Iteration 563, loss = 0.47310035\n",
      "Iteration 564, loss = 0.47297818\n",
      "Iteration 565, loss = 0.47288689\n",
      "Iteration 566, loss = 0.47294175\n",
      "Iteration 567, loss = 0.47273898\n",
      "Iteration 568, loss = 0.47307383\n",
      "Iteration 569, loss = 0.47284173\n",
      "Iteration 570, loss = 0.47276829\n",
      "Iteration 571, loss = 0.47273857\n",
      "Iteration 572, loss = 0.47271109\n",
      "Iteration 573, loss = 0.47282966\n",
      "Iteration 574, loss = 0.47261144\n",
      "Iteration 575, loss = 0.47270686\n",
      "Iteration 576, loss = 0.47289256\n",
      "Iteration 577, loss = 0.47281166\n",
      "Iteration 578, loss = 0.47254928\n",
      "Iteration 579, loss = 0.47270346\n",
      "Iteration 580, loss = 0.47288895\n",
      "Iteration 581, loss = 0.47250932\n",
      "Iteration 582, loss = 0.47240333\n",
      "Iteration 583, loss = 0.47284081\n",
      "Iteration 584, loss = 0.47310198\n",
      "Iteration 585, loss = 0.47241452\n",
      "Iteration 586, loss = 0.47242172\n",
      "Iteration 587, loss = 0.47258123\n",
      "Iteration 588, loss = 0.47239558\n",
      "Iteration 589, loss = 0.47273226\n",
      "Iteration 590, loss = 0.47224834\n",
      "Iteration 591, loss = 0.47256623\n",
      "Iteration 592, loss = 0.47234817\n",
      "Iteration 593, loss = 0.47272221\n",
      "Iteration 594, loss = 0.47235984\n",
      "Iteration 595, loss = 0.47238133\n",
      "Iteration 596, loss = 0.47218290\n",
      "Iteration 597, loss = 0.47262447\n",
      "Iteration 598, loss = 0.47209608\n",
      "Iteration 599, loss = 0.47217151\n",
      "Iteration 600, loss = 0.47220152\n",
      "Iteration 601, loss = 0.47206756\n",
      "Iteration 602, loss = 0.47216679\n",
      "Iteration 603, loss = 0.47199072\n",
      "Iteration 604, loss = 0.47195297\n",
      "Iteration 605, loss = 0.47219578\n",
      "Iteration 606, loss = 0.47238402\n",
      "Iteration 607, loss = 0.47174509\n",
      "Iteration 608, loss = 0.47185544\n",
      "Iteration 609, loss = 0.47182227\n",
      "Iteration 610, loss = 0.47238943\n",
      "Iteration 611, loss = 0.47210561\n",
      "Iteration 612, loss = 0.47226215\n",
      "Iteration 613, loss = 0.47188931\n",
      "Iteration 614, loss = 0.47227903\n",
      "Iteration 615, loss = 0.47193049\n",
      "Iteration 616, loss = 0.47171517\n",
      "Iteration 617, loss = 0.47170289\n",
      "Iteration 618, loss = 0.47157495\n",
      "Iteration 619, loss = 0.47187581\n",
      "Iteration 620, loss = 0.47210641\n",
      "Iteration 621, loss = 0.47247360\n",
      "Iteration 622, loss = 0.47178389\n",
      "Iteration 623, loss = 0.47175434\n",
      "Iteration 624, loss = 0.47162627\n",
      "Iteration 625, loss = 0.47174328\n",
      "Iteration 626, loss = 0.47173278\n",
      "Iteration 627, loss = 0.47152168\n",
      "Iteration 628, loss = 0.47155560\n",
      "Iteration 629, loss = 0.47158506\n",
      "Iteration 630, loss = 0.47190520\n",
      "Iteration 631, loss = 0.47197115\n",
      "Iteration 632, loss = 0.47157776\n",
      "Iteration 633, loss = 0.47156198\n",
      "Iteration 634, loss = 0.47167186\n",
      "Iteration 635, loss = 0.47142338\n",
      "Iteration 636, loss = 0.47160283\n",
      "Iteration 637, loss = 0.47147844\n",
      "Iteration 638, loss = 0.47121666\n",
      "Iteration 639, loss = 0.47146636\n",
      "Iteration 640, loss = 0.47158006\n",
      "Iteration 641, loss = 0.47157574\n",
      "Iteration 642, loss = 0.47182544\n",
      "Iteration 643, loss = 0.47151037\n",
      "Iteration 644, loss = 0.47133786\n",
      "Iteration 645, loss = 0.47152838\n",
      "Iteration 646, loss = 0.47138059\n",
      "Iteration 647, loss = 0.47126045\n",
      "Iteration 648, loss = 0.47121630\n",
      "Iteration 649, loss = 0.47171042\n",
      "Iteration 650, loss = 0.47094372\n",
      "Iteration 651, loss = 0.47120045\n",
      "Iteration 652, loss = 0.47143313\n",
      "Iteration 653, loss = 0.47164047\n",
      "Iteration 654, loss = 0.47105563\n",
      "Iteration 655, loss = 0.47091231\n",
      "Iteration 656, loss = 0.47108592\n",
      "Iteration 657, loss = 0.47087502\n",
      "Iteration 658, loss = 0.47080724\n",
      "Iteration 659, loss = 0.47081748\n",
      "Iteration 660, loss = 0.47137720\n",
      "Iteration 661, loss = 0.47149998\n",
      "Iteration 662, loss = 0.47074226\n",
      "Iteration 663, loss = 0.47072968\n",
      "Iteration 664, loss = 0.47082635\n",
      "Iteration 665, loss = 0.47078709\n",
      "Iteration 666, loss = 0.47075270\n",
      "Iteration 667, loss = 0.47087885\n",
      "Iteration 668, loss = 0.47068147\n",
      "Iteration 669, loss = 0.47051191\n",
      "Iteration 670, loss = 0.47093072\n",
      "Iteration 671, loss = 0.47061154\n",
      "Iteration 672, loss = 0.47099019\n",
      "Iteration 673, loss = 0.47065507\n",
      "Iteration 674, loss = 0.47064917\n",
      "Iteration 675, loss = 0.47040194\n",
      "Iteration 676, loss = 0.47041466\n",
      "Iteration 677, loss = 0.47065093\n",
      "Iteration 678, loss = 0.47039017\n",
      "Iteration 679, loss = 0.47051889\n",
      "Iteration 680, loss = 0.47032461\n",
      "Iteration 681, loss = 0.47053907\n",
      "Iteration 682, loss = 0.47050735\n",
      "Iteration 683, loss = 0.47043073\n",
      "Iteration 684, loss = 0.47035598\n",
      "Iteration 685, loss = 0.47015723\n",
      "Iteration 686, loss = 0.47055700\n",
      "Iteration 687, loss = 0.47037246\n",
      "Iteration 688, loss = 0.47020420\n",
      "Iteration 689, loss = 0.47018318\n",
      "Iteration 690, loss = 0.46988914\n",
      "Iteration 691, loss = 0.47011446\n",
      "Iteration 692, loss = 0.47028303\n",
      "Iteration 693, loss = 0.47025721\n",
      "Iteration 694, loss = 0.46997753\n",
      "Iteration 695, loss = 0.47016363\n",
      "Iteration 696, loss = 0.47030524\n",
      "Iteration 697, loss = 0.47035796\n",
      "Iteration 698, loss = 0.47016026\n",
      "Iteration 699, loss = 0.47013103\n",
      "Iteration 700, loss = 0.46995306\n",
      "Iteration 701, loss = 0.46968252\n",
      "Iteration 702, loss = 0.47036963\n",
      "Iteration 703, loss = 0.47001364\n",
      "Iteration 704, loss = 0.47005208\n",
      "Iteration 705, loss = 0.46973101\n",
      "Iteration 706, loss = 0.46996150\n",
      "Iteration 707, loss = 0.47021194\n",
      "Iteration 708, loss = 0.46987202\n",
      "Iteration 709, loss = 0.46982524\n",
      "Iteration 710, loss = 0.46985165\n",
      "Iteration 711, loss = 0.46951762\n",
      "Iteration 712, loss = 0.46998787\n",
      "Iteration 713, loss = 0.46971128\n",
      "Iteration 714, loss = 0.46981516\n",
      "Iteration 715, loss = 0.46992638\n",
      "Iteration 716, loss = 0.46961970\n",
      "Iteration 717, loss = 0.46989048\n",
      "Iteration 718, loss = 0.47009069\n",
      "Iteration 719, loss = 0.46965424\n",
      "Iteration 720, loss = 0.46954695\n",
      "Iteration 721, loss = 0.46964239\n",
      "Iteration 722, loss = 0.46966923\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "scores_ann = cross_val_score(model_ann, X_customer_balanced, Y_customer_balanced, cv=kf_ann, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.73469388 0.74744898 0.76020408 0.72704082 0.70663265 0.71428571\n",
      " 0.75765306 0.77295918 0.74168798 0.74936061]\n",
      "Score médio: 0.7411966960697323\n",
      "Desvio padrão: 0.019769875644670528\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_ann}\")\n",
    "print(f\"Score médio: {np.mean(scores_ann)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_ann)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - 66%(Normal) 72%(Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "model_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted\n",
    "model_tree_boosted = DecisionTreeClassifier(criterion='gini', splitter='random', min_samples_leaf=10, min_samples_split=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_tree = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree = cross_val_score(model_tree, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.6505102  0.65306122 0.66071429 0.67091837 0.62755102 0.63010204\n",
      " 0.68367347 0.69642857 0.69309463 0.68030691]\n",
      "Score médio: 0.6646360718200324\n",
      "Desvio padrão: 0.023205420177937815\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree_boosted = cross_val_score(model_tree_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70918367 0.70153061 0.73979592 0.73469388 0.72193878 0.69897959\n",
      " 0.70918367 0.7627551  0.73401535 0.73657289]\n",
      "Score médio: 0.7248649459783915\n",
      "Desvio padrão: 0.019211770606043753\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "parametros = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'min_samples_leaf': 10, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.7309914770505903\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculando desvio padrão\n",
    "# std_dev = np.std(scores_tree_boosted)\n",
    "\n",
    "# # Plotando o desvio padrão\n",
    "# plt.bar('Desvio Padrão', std_dev)\n",
    "# plt.ylabel('Valor')\n",
    "# plt.title('Desvio Padrão dos Scores')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p = 2)\n",
    "kf_knn = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn = cross_val_score(model_knn, X_customer_balanced, Y_customer_balanced, cv=kf_knn, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70153061 0.70408163 0.72704082 0.69132653 0.68877551 0.66071429\n",
      " 0.68367347 0.75510204 0.70588235 0.71611253]\n",
      "Score médio: 0.703423978286967\n",
      "Desvio padrão: 0.02444292091659416\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_knn}\")\n",
    "print(f\"Score médio: {np.mean(scores_knn)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(random_state=42, max_iter=150)\n",
    "kf_logistic = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic = cross_val_score(model_logistic, X_customer_balanced, Y_customer_balanced, cv=kf_logistic, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.67602041 0.70663265 0.71938776 0.71683673 0.66326531 0.67091837\n",
      " 0.72959184 0.75255102 0.71355499 0.74680307]\n",
      "Score médio: 0.7095562137898638\n",
      "Desvio padrão: 0.029277793572685756\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_logistic}\")\n",
    "print(f\"Score médio: {np.mean(scores_logistic)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_logistic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = GaussianNB()\n",
    "kf_naive = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = cross_val_score(model_naive, X_customer_balanced, Y_customer_balanced, cv=kf_naive, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.68877551 0.7244898  0.70918367 0.73214286 0.68877551 0.68112245\n",
      " 0.72193878 0.77295918 0.72634271 0.75959079]\n",
      "Score médio: 0.7205321258938358\n",
      "Desvio padrão: 0.028564427059807447\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_naive}\")\n",
    "print(f\"Score médio: {np.mean(scores_naive)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_naive)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(n_estimators=80, criterion='entropy', random_state=42)\n",
    "kf_forest = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_forest = cross_val_score(model_forest, X_customer_balanced, Y_customer_balanced, cv=kf_forest, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.75       0.75765306 0.7627551  0.75       0.7372449  0.72193878\n",
      " 0.73469388 0.78316327 0.76470588 0.76982097]\n",
      "Score médio: 0.7531975833811785\n",
      "Desvio padrão: 0.01735613718267566\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_forest}\")\n",
    "print(f\"Score médio: {np.mean(scores_forest)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_forest)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
