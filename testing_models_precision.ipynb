{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('customer.pkl', 'rb') as f:\n",
    "    X_customer_balanced, Y_customer_balanced = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *SVM - 74,17%(Normal) 74,96%(Boosted)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3918, 10), (3918,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_customer_balanced.shape, Y_customer_balanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf', random_state=42, C=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boosted = SVC(kernel='rbf', random_state=42, C=10.0, tol=0.001, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_customer_balanced, Y_customer_balanced, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_boosted = cross_val_score(model_boosted, X_customer_balanced, Y_customer_balanced, cv=kf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.7372449  0.75       0.75255102 0.73469388 0.72704082 0.69897959\n",
      " 0.73469388 0.78316327 0.72890026 0.76982097]\n",
      "Score médio: 0.7417088574560259\n",
      "Desvio padrão: 0.022401792602462955\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores}\")\n",
    "print(f\"Score médio: {np.mean(scores)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.73979592 0.75       0.76785714 0.75255102 0.71938776 0.70918367\n",
      " 0.75       0.78316327 0.75191816 0.77237852]\n",
      "Score médio: 0.749623545070202\n",
      "Desvio padrão: 0.02153010197017537\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    'C': [0.1, 1.0, 10.0],  # Regularização\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],  # Kernels mais comuns\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.001],  # Para 'rbf' kernel\n",
    "    'tol': [0.001, 0.0001]  # Tolerância de otimização\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf', 'tol': 0.001}\n",
      "0.7468110615893867\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Artificial Neural Network - 74,5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ann_boosted = MLPClassifier(max_iter=1500, verbose=True, solver='adam', activation='relu', hidden_layer_sizes=(50, 50), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = MLPClassifier(max_iter=1500, verbose=True, tol=0.000000, solver='adam', activation='relu', hidden_layer_sizes=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_ann = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76138989\n",
      "Iteration 2, loss = 0.72384907\n",
      "Iteration 3, loss = 0.69953451\n",
      "Iteration 4, loss = 0.68094347\n",
      "Iteration 5, loss = 0.66722715\n",
      "Iteration 6, loss = 0.65546919\n",
      "Iteration 7, loss = 0.64430400\n",
      "Iteration 8, loss = 0.63371888\n",
      "Iteration 9, loss = 0.62314497\n",
      "Iteration 10, loss = 0.61348529\n",
      "Iteration 11, loss = 0.60379113\n",
      "Iteration 12, loss = 0.59475241\n",
      "Iteration 13, loss = 0.58646577\n",
      "Iteration 14, loss = 0.57899789\n",
      "Iteration 15, loss = 0.57291362\n",
      "Iteration 16, loss = 0.56742829\n",
      "Iteration 17, loss = 0.56301630\n",
      "Iteration 18, loss = 0.55968415\n",
      "Iteration 19, loss = 0.55656447\n",
      "Iteration 20, loss = 0.55409431\n",
      "Iteration 21, loss = 0.55190973\n",
      "Iteration 22, loss = 0.54995516\n",
      "Iteration 23, loss = 0.54800247\n",
      "Iteration 24, loss = 0.54624929\n",
      "Iteration 25, loss = 0.54432915\n",
      "Iteration 26, loss = 0.54255074\n",
      "Iteration 27, loss = 0.54070145\n",
      "Iteration 28, loss = 0.53922931\n",
      "Iteration 29, loss = 0.53776417\n",
      "Iteration 30, loss = 0.53611454\n",
      "Iteration 31, loss = 0.53437457\n",
      "Iteration 32, loss = 0.53268326\n",
      "Iteration 33, loss = 0.53137826\n",
      "Iteration 34, loss = 0.52967663\n",
      "Iteration 35, loss = 0.52812246\n",
      "Iteration 36, loss = 0.52671500\n",
      "Iteration 37, loss = 0.52529585\n",
      "Iteration 38, loss = 0.52395221\n",
      "Iteration 39, loss = 0.52268719\n",
      "Iteration 40, loss = 0.52133527\n",
      "Iteration 41, loss = 0.52009180\n",
      "Iteration 42, loss = 0.51894529\n",
      "Iteration 43, loss = 0.51767357\n",
      "Iteration 44, loss = 0.51654539\n",
      "Iteration 45, loss = 0.51537317\n",
      "Iteration 46, loss = 0.51464837\n",
      "Iteration 47, loss = 0.51342667\n",
      "Iteration 48, loss = 0.51237804\n",
      "Iteration 49, loss = 0.51139644\n",
      "Iteration 50, loss = 0.51057718\n",
      "Iteration 51, loss = 0.51003962\n",
      "Iteration 52, loss = 0.50913477\n",
      "Iteration 53, loss = 0.50835228\n",
      "Iteration 54, loss = 0.50755801\n",
      "Iteration 55, loss = 0.50704804\n",
      "Iteration 56, loss = 0.50670719\n",
      "Iteration 57, loss = 0.50620659\n",
      "Iteration 58, loss = 0.50534794\n",
      "Iteration 59, loss = 0.50513243\n",
      "Iteration 60, loss = 0.50438106\n",
      "Iteration 61, loss = 0.50381854\n",
      "Iteration 62, loss = 0.50337994\n",
      "Iteration 63, loss = 0.50294813\n",
      "Iteration 64, loss = 0.50241329\n",
      "Iteration 65, loss = 0.50224605\n",
      "Iteration 66, loss = 0.50165690\n",
      "Iteration 67, loss = 0.50125116\n",
      "Iteration 68, loss = 0.50083678\n",
      "Iteration 69, loss = 0.50045918\n",
      "Iteration 70, loss = 0.49991722\n",
      "Iteration 71, loss = 0.49981443\n",
      "Iteration 72, loss = 0.49939943\n",
      "Iteration 73, loss = 0.49915693\n",
      "Iteration 74, loss = 0.49882666\n",
      "Iteration 75, loss = 0.49850602\n",
      "Iteration 76, loss = 0.49839403\n",
      "Iteration 77, loss = 0.49797668\n",
      "Iteration 78, loss = 0.49760788\n",
      "Iteration 79, loss = 0.49727882\n",
      "Iteration 80, loss = 0.49735718\n",
      "Iteration 81, loss = 0.49727285\n",
      "Iteration 82, loss = 0.49660929\n",
      "Iteration 83, loss = 0.49619892\n",
      "Iteration 84, loss = 0.49634103\n",
      "Iteration 85, loss = 0.49602345\n",
      "Iteration 86, loss = 0.49568175\n",
      "Iteration 87, loss = 0.49544600\n",
      "Iteration 88, loss = 0.49551936\n",
      "Iteration 89, loss = 0.49508217\n",
      "Iteration 90, loss = 0.49500804\n",
      "Iteration 91, loss = 0.49481797\n",
      "Iteration 92, loss = 0.49466980\n",
      "Iteration 93, loss = 0.49453712\n",
      "Iteration 94, loss = 0.49419401\n",
      "Iteration 95, loss = 0.49410823\n",
      "Iteration 96, loss = 0.49398182\n",
      "Iteration 97, loss = 0.49389937\n",
      "Iteration 98, loss = 0.49370952\n",
      "Iteration 99, loss = 0.49353806\n",
      "Iteration 100, loss = 0.49329075\n",
      "Iteration 101, loss = 0.49330389\n",
      "Iteration 102, loss = 0.49305570\n",
      "Iteration 103, loss = 0.49293494\n",
      "Iteration 104, loss = 0.49292031\n",
      "Iteration 105, loss = 0.49289112\n",
      "Iteration 106, loss = 0.49258574\n",
      "Iteration 107, loss = 0.49261543\n",
      "Iteration 108, loss = 0.49250860\n",
      "Iteration 109, loss = 0.49227970\n",
      "Iteration 110, loss = 0.49206424\n",
      "Iteration 111, loss = 0.49208735\n",
      "Iteration 112, loss = 0.49186198\n",
      "Iteration 113, loss = 0.49169237\n",
      "Iteration 114, loss = 0.49161658\n",
      "Iteration 115, loss = 0.49145904\n",
      "Iteration 116, loss = 0.49132081\n",
      "Iteration 117, loss = 0.49152966\n",
      "Iteration 118, loss = 0.49129171\n",
      "Iteration 119, loss = 0.49119547\n",
      "Iteration 120, loss = 0.49091101\n",
      "Iteration 121, loss = 0.49097972\n",
      "Iteration 122, loss = 0.49080561\n",
      "Iteration 123, loss = 0.49072731\n",
      "Iteration 124, loss = 0.49055167\n",
      "Iteration 125, loss = 0.49039298\n",
      "Iteration 126, loss = 0.49026118\n",
      "Iteration 127, loss = 0.49109827\n",
      "Iteration 128, loss = 0.49008641\n",
      "Iteration 129, loss = 0.49004621\n",
      "Iteration 130, loss = 0.49006200\n",
      "Iteration 131, loss = 0.49028937\n",
      "Iteration 132, loss = 0.48979958\n",
      "Iteration 133, loss = 0.48963094\n",
      "Iteration 134, loss = 0.48967232\n",
      "Iteration 135, loss = 0.48993843\n",
      "Iteration 136, loss = 0.48930274\n",
      "Iteration 137, loss = 0.48943807\n",
      "Iteration 138, loss = 0.48921165\n",
      "Iteration 139, loss = 0.48906384\n",
      "Iteration 140, loss = 0.48909365\n",
      "Iteration 141, loss = 0.48891158\n",
      "Iteration 142, loss = 0.48909308\n",
      "Iteration 143, loss = 0.48878616\n",
      "Iteration 144, loss = 0.48887932\n",
      "Iteration 145, loss = 0.48885144\n",
      "Iteration 146, loss = 0.48893524\n",
      "Iteration 147, loss = 0.48853055\n",
      "Iteration 148, loss = 0.48861731\n",
      "Iteration 149, loss = 0.48835239\n",
      "Iteration 150, loss = 0.48835728\n",
      "Iteration 151, loss = 0.48825519\n",
      "Iteration 152, loss = 0.48817668\n",
      "Iteration 153, loss = 0.48799413\n",
      "Iteration 154, loss = 0.48802371\n",
      "Iteration 155, loss = 0.48776524\n",
      "Iteration 156, loss = 0.48780573\n",
      "Iteration 157, loss = 0.48795753\n",
      "Iteration 158, loss = 0.48748453\n",
      "Iteration 159, loss = 0.48752721\n",
      "Iteration 160, loss = 0.48771787\n",
      "Iteration 161, loss = 0.48738034\n",
      "Iteration 162, loss = 0.48749773\n",
      "Iteration 163, loss = 0.48731848\n",
      "Iteration 164, loss = 0.48733216\n",
      "Iteration 165, loss = 0.48713646\n",
      "Iteration 166, loss = 0.48684995\n",
      "Iteration 167, loss = 0.48693762\n",
      "Iteration 168, loss = 0.48678555\n",
      "Iteration 169, loss = 0.48689180\n",
      "Iteration 170, loss = 0.48672450\n",
      "Iteration 171, loss = 0.48670230\n",
      "Iteration 172, loss = 0.48664242\n",
      "Iteration 173, loss = 0.48676399\n",
      "Iteration 174, loss = 0.48647492\n",
      "Iteration 175, loss = 0.48652124\n",
      "Iteration 176, loss = 0.48654904\n",
      "Iteration 177, loss = 0.48676843\n",
      "Iteration 178, loss = 0.48651802\n",
      "Iteration 179, loss = 0.48618936\n",
      "Iteration 180, loss = 0.48626798\n",
      "Iteration 181, loss = 0.48624523\n",
      "Iteration 182, loss = 0.48615197\n",
      "Iteration 183, loss = 0.48604783\n",
      "Iteration 184, loss = 0.48612875\n",
      "Iteration 185, loss = 0.48591836\n",
      "Iteration 186, loss = 0.48604654\n",
      "Iteration 187, loss = 0.48576698\n",
      "Iteration 188, loss = 0.48580061\n",
      "Iteration 189, loss = 0.48581448\n",
      "Iteration 190, loss = 0.48586237\n",
      "Iteration 191, loss = 0.48598478\n",
      "Iteration 192, loss = 0.48566478\n",
      "Iteration 193, loss = 0.48604785\n",
      "Iteration 194, loss = 0.48553429\n",
      "Iteration 195, loss = 0.48548311\n",
      "Iteration 196, loss = 0.48540239\n",
      "Iteration 197, loss = 0.48529944\n",
      "Iteration 198, loss = 0.48552064\n",
      "Iteration 199, loss = 0.48518389\n",
      "Iteration 200, loss = 0.48545999\n",
      "Iteration 201, loss = 0.48517971\n",
      "Iteration 202, loss = 0.48534450\n",
      "Iteration 203, loss = 0.48524917\n",
      "Iteration 204, loss = 0.48509038\n",
      "Iteration 205, loss = 0.48532778\n",
      "Iteration 206, loss = 0.48523660\n",
      "Iteration 207, loss = 0.48554960\n",
      "Iteration 208, loss = 0.48508442\n",
      "Iteration 209, loss = 0.48505401\n",
      "Iteration 210, loss = 0.48491711\n",
      "Iteration 211, loss = 0.48502916\n",
      "Iteration 212, loss = 0.48501742\n",
      "Iteration 213, loss = 0.48484213\n",
      "Iteration 214, loss = 0.48478236\n",
      "Iteration 215, loss = 0.48486881\n",
      "Iteration 216, loss = 0.48467849\n",
      "Iteration 217, loss = 0.48487756\n",
      "Iteration 218, loss = 0.48462967\n",
      "Iteration 219, loss = 0.48462270\n",
      "Iteration 220, loss = 0.48457723\n",
      "Iteration 221, loss = 0.48482866\n",
      "Iteration 222, loss = 0.48450929\n",
      "Iteration 223, loss = 0.48446105\n",
      "Iteration 224, loss = 0.48460586\n",
      "Iteration 225, loss = 0.48453271\n",
      "Iteration 226, loss = 0.48447632\n",
      "Iteration 227, loss = 0.48432804\n",
      "Iteration 228, loss = 0.48442011\n",
      "Iteration 229, loss = 0.48429159\n",
      "Iteration 230, loss = 0.48434357\n",
      "Iteration 231, loss = 0.48422761\n",
      "Iteration 232, loss = 0.48411461\n",
      "Iteration 233, loss = 0.48419780\n",
      "Iteration 234, loss = 0.48436069\n",
      "Iteration 235, loss = 0.48405866\n",
      "Iteration 236, loss = 0.48425161\n",
      "Iteration 237, loss = 0.48428974\n",
      "Iteration 238, loss = 0.48409847\n",
      "Iteration 239, loss = 0.48388554\n",
      "Iteration 240, loss = 0.48405029\n",
      "Iteration 241, loss = 0.48388070\n",
      "Iteration 242, loss = 0.48369046\n",
      "Iteration 243, loss = 0.48368576\n",
      "Iteration 244, loss = 0.48359592\n",
      "Iteration 245, loss = 0.48382575\n",
      "Iteration 246, loss = 0.48364436\n",
      "Iteration 247, loss = 0.48382181\n",
      "Iteration 248, loss = 0.48375330\n",
      "Iteration 249, loss = 0.48366921\n",
      "Iteration 250, loss = 0.48358371\n",
      "Iteration 251, loss = 0.48353536\n",
      "Iteration 252, loss = 0.48329492\n",
      "Iteration 253, loss = 0.48337349\n",
      "Iteration 254, loss = 0.48325558\n",
      "Iteration 255, loss = 0.48332251\n",
      "Iteration 256, loss = 0.48324812\n",
      "Iteration 257, loss = 0.48324815\n",
      "Iteration 258, loss = 0.48355840\n",
      "Iteration 259, loss = 0.48303727\n",
      "Iteration 260, loss = 0.48324314\n",
      "Iteration 261, loss = 0.48339039\n",
      "Iteration 262, loss = 0.48298929\n",
      "Iteration 263, loss = 0.48297910\n",
      "Iteration 264, loss = 0.48298401\n",
      "Iteration 265, loss = 0.48300740\n",
      "Iteration 266, loss = 0.48286590\n",
      "Iteration 267, loss = 0.48271851\n",
      "Iteration 268, loss = 0.48295665\n",
      "Iteration 269, loss = 0.48292571\n",
      "Iteration 270, loss = 0.48283528\n",
      "Iteration 271, loss = 0.48272806\n",
      "Iteration 272, loss = 0.48265215\n",
      "Iteration 273, loss = 0.48249633\n",
      "Iteration 274, loss = 0.48264751\n",
      "Iteration 275, loss = 0.48243205\n",
      "Iteration 276, loss = 0.48245467\n",
      "Iteration 277, loss = 0.48261246\n",
      "Iteration 278, loss = 0.48251991\n",
      "Iteration 279, loss = 0.48222575\n",
      "Iteration 280, loss = 0.48211091\n",
      "Iteration 281, loss = 0.48213294\n",
      "Iteration 282, loss = 0.48206238\n",
      "Iteration 283, loss = 0.48240383\n",
      "Iteration 284, loss = 0.48214276\n",
      "Iteration 285, loss = 0.48233405\n",
      "Iteration 286, loss = 0.48227627\n",
      "Iteration 287, loss = 0.48174773\n",
      "Iteration 288, loss = 0.48206787\n",
      "Iteration 289, loss = 0.48191826\n",
      "Iteration 290, loss = 0.48180436\n",
      "Iteration 291, loss = 0.48201595\n",
      "Iteration 292, loss = 0.48196236\n",
      "Iteration 293, loss = 0.48212069\n",
      "Iteration 294, loss = 0.48168767\n",
      "Iteration 295, loss = 0.48142675\n",
      "Iteration 296, loss = 0.48150195\n",
      "Iteration 297, loss = 0.48179632\n",
      "Iteration 298, loss = 0.48168496\n",
      "Iteration 299, loss = 0.48117058\n",
      "Iteration 300, loss = 0.48117796\n",
      "Iteration 301, loss = 0.48115566\n",
      "Iteration 302, loss = 0.48108987\n",
      "Iteration 303, loss = 0.48138903\n",
      "Iteration 304, loss = 0.48086372\n",
      "Iteration 305, loss = 0.48147521\n",
      "Iteration 306, loss = 0.48093072\n",
      "Iteration 307, loss = 0.48087876\n",
      "Iteration 308, loss = 0.48080122\n",
      "Iteration 309, loss = 0.48109024\n",
      "Iteration 310, loss = 0.48064309\n",
      "Iteration 311, loss = 0.48059769\n",
      "Iteration 312, loss = 0.48041672\n",
      "Iteration 313, loss = 0.48069289\n",
      "Iteration 314, loss = 0.48039458\n",
      "Iteration 315, loss = 0.48055080\n",
      "Iteration 316, loss = 0.48072470\n",
      "Iteration 317, loss = 0.48014717\n",
      "Iteration 318, loss = 0.48031922\n",
      "Iteration 319, loss = 0.48070174\n",
      "Iteration 320, loss = 0.48003417\n",
      "Iteration 321, loss = 0.48053276\n",
      "Iteration 322, loss = 0.48033016\n",
      "Iteration 323, loss = 0.47984792\n",
      "Iteration 324, loss = 0.47985727\n",
      "Iteration 325, loss = 0.47988706\n",
      "Iteration 326, loss = 0.47971333\n",
      "Iteration 327, loss = 0.47982247\n",
      "Iteration 328, loss = 0.47978159\n",
      "Iteration 329, loss = 0.47960048\n",
      "Iteration 330, loss = 0.47954799\n",
      "Iteration 331, loss = 0.47966072\n",
      "Iteration 332, loss = 0.47983512\n",
      "Iteration 333, loss = 0.47945932\n",
      "Iteration 334, loss = 0.47983212\n",
      "Iteration 335, loss = 0.47957106\n",
      "Iteration 336, loss = 0.47932505\n",
      "Iteration 337, loss = 0.47932622\n",
      "Iteration 338, loss = 0.47916575\n",
      "Iteration 339, loss = 0.47916812\n",
      "Iteration 340, loss = 0.47927498\n",
      "Iteration 341, loss = 0.47913771\n",
      "Iteration 342, loss = 0.47908419\n",
      "Iteration 343, loss = 0.47944351\n",
      "Iteration 344, loss = 0.47908841\n",
      "Iteration 345, loss = 0.47896873\n",
      "Iteration 346, loss = 0.47905485\n",
      "Iteration 347, loss = 0.47903594\n",
      "Iteration 348, loss = 0.47881058\n",
      "Iteration 349, loss = 0.47887252\n",
      "Iteration 350, loss = 0.47906083\n",
      "Iteration 351, loss = 0.47906528\n",
      "Iteration 352, loss = 0.47865685\n",
      "Iteration 353, loss = 0.47896073\n",
      "Iteration 354, loss = 0.47867902\n",
      "Iteration 355, loss = 0.47851134\n",
      "Iteration 356, loss = 0.47866886\n",
      "Iteration 357, loss = 0.47846390\n",
      "Iteration 358, loss = 0.47869965\n",
      "Iteration 359, loss = 0.47862759\n",
      "Iteration 360, loss = 0.47855402\n",
      "Iteration 361, loss = 0.47857776\n",
      "Iteration 362, loss = 0.47836307\n",
      "Iteration 363, loss = 0.47842420\n",
      "Iteration 364, loss = 0.47837527\n",
      "Iteration 365, loss = 0.47851163\n",
      "Iteration 366, loss = 0.47853776\n",
      "Iteration 367, loss = 0.47832524\n",
      "Iteration 368, loss = 0.47821223\n",
      "Iteration 369, loss = 0.47815415\n",
      "Iteration 370, loss = 0.47840500\n",
      "Iteration 371, loss = 0.47812573\n",
      "Iteration 372, loss = 0.47844338\n",
      "Iteration 373, loss = 0.47809626\n",
      "Iteration 374, loss = 0.47814808\n",
      "Iteration 375, loss = 0.47828405\n",
      "Iteration 376, loss = 0.47778301\n",
      "Iteration 377, loss = 0.47795368\n",
      "Iteration 378, loss = 0.47794857\n",
      "Iteration 379, loss = 0.47847473\n",
      "Iteration 380, loss = 0.47831895\n",
      "Iteration 381, loss = 0.47787249\n",
      "Iteration 382, loss = 0.47790927\n",
      "Iteration 383, loss = 0.47791985\n",
      "Iteration 384, loss = 0.47789266\n",
      "Iteration 385, loss = 0.47788105\n",
      "Iteration 386, loss = 0.47776934\n",
      "Iteration 387, loss = 0.47799443\n",
      "Iteration 388, loss = 0.47819923\n",
      "Iteration 389, loss = 0.47780063\n",
      "Iteration 390, loss = 0.47780987\n",
      "Iteration 391, loss = 0.47784446\n",
      "Iteration 392, loss = 0.47784401\n",
      "Iteration 393, loss = 0.47789092\n",
      "Iteration 394, loss = 0.47749080\n",
      "Iteration 395, loss = 0.47814467\n",
      "Iteration 396, loss = 0.47779308\n",
      "Iteration 397, loss = 0.47749579\n",
      "Iteration 398, loss = 0.47769062\n",
      "Iteration 399, loss = 0.47756403\n",
      "Iteration 400, loss = 0.47769969\n",
      "Iteration 401, loss = 0.47816684\n",
      "Iteration 402, loss = 0.47734387\n",
      "Iteration 403, loss = 0.47765905\n",
      "Iteration 404, loss = 0.47725505\n",
      "Iteration 405, loss = 0.47757098\n",
      "Iteration 406, loss = 0.47752664\n",
      "Iteration 407, loss = 0.47733749\n",
      "Iteration 408, loss = 0.47734629\n",
      "Iteration 409, loss = 0.47747014\n",
      "Iteration 410, loss = 0.47724884\n",
      "Iteration 411, loss = 0.47702306\n",
      "Iteration 412, loss = 0.47722026\n",
      "Iteration 413, loss = 0.47716854\n",
      "Iteration 414, loss = 0.47732401\n",
      "Iteration 415, loss = 0.47710139\n",
      "Iteration 416, loss = 0.47708530\n",
      "Iteration 417, loss = 0.47713505\n",
      "Iteration 418, loss = 0.47698854\n",
      "Iteration 419, loss = 0.47747539\n",
      "Iteration 420, loss = 0.47754748\n",
      "Iteration 421, loss = 0.47696783\n",
      "Iteration 422, loss = 0.47705080\n",
      "Iteration 423, loss = 0.47683989\n",
      "Iteration 424, loss = 0.47720234\n",
      "Iteration 425, loss = 0.47684705\n",
      "Iteration 426, loss = 0.47694265\n",
      "Iteration 427, loss = 0.47688466\n",
      "Iteration 428, loss = 0.47667106\n",
      "Iteration 429, loss = 0.47670990\n",
      "Iteration 430, loss = 0.47681726\n",
      "Iteration 431, loss = 0.47685748\n",
      "Iteration 432, loss = 0.47737772\n",
      "Iteration 433, loss = 0.47659958\n",
      "Iteration 434, loss = 0.47671273\n",
      "Iteration 435, loss = 0.47645658\n",
      "Iteration 436, loss = 0.47678999\n",
      "Iteration 437, loss = 0.47656086\n",
      "Iteration 438, loss = 0.47650551\n",
      "Iteration 439, loss = 0.47626944\n",
      "Iteration 440, loss = 0.47642163\n",
      "Iteration 441, loss = 0.47624894\n",
      "Iteration 442, loss = 0.47671402\n",
      "Iteration 443, loss = 0.47653810\n",
      "Iteration 444, loss = 0.47617420\n",
      "Iteration 445, loss = 0.47610663\n",
      "Iteration 446, loss = 0.47625591\n",
      "Iteration 447, loss = 0.47633025\n",
      "Iteration 448, loss = 0.47619569\n",
      "Iteration 449, loss = 0.47620502\n",
      "Iteration 450, loss = 0.47635611\n",
      "Iteration 451, loss = 0.47589524\n",
      "Iteration 452, loss = 0.47613147\n",
      "Iteration 453, loss = 0.47608645\n",
      "Iteration 454, loss = 0.47599624\n",
      "Iteration 455, loss = 0.47631452\n",
      "Iteration 456, loss = 0.47607943\n",
      "Iteration 457, loss = 0.47637507\n",
      "Iteration 458, loss = 0.47663565\n",
      "Iteration 459, loss = 0.47635286\n",
      "Iteration 460, loss = 0.47591946\n",
      "Iteration 461, loss = 0.47575700\n",
      "Iteration 462, loss = 0.47600731\n",
      "Iteration 463, loss = 0.47569795\n",
      "Iteration 464, loss = 0.47583316\n",
      "Iteration 465, loss = 0.47598180\n",
      "Iteration 466, loss = 0.47592225\n",
      "Iteration 467, loss = 0.47581579\n",
      "Iteration 468, loss = 0.47567108\n",
      "Iteration 469, loss = 0.47584007\n",
      "Iteration 470, loss = 0.47552322\n",
      "Iteration 471, loss = 0.47553470\n",
      "Iteration 472, loss = 0.47594692\n",
      "Iteration 473, loss = 0.47578708\n",
      "Iteration 474, loss = 0.47592755\n",
      "Iteration 475, loss = 0.47577120\n",
      "Iteration 476, loss = 0.47603664\n",
      "Iteration 477, loss = 0.47577826\n",
      "Iteration 478, loss = 0.47565824\n",
      "Iteration 479, loss = 0.47551308\n",
      "Iteration 480, loss = 0.47575764\n",
      "Iteration 481, loss = 0.47586322\n",
      "Iteration 482, loss = 0.47530940\n",
      "Iteration 483, loss = 0.47539515\n",
      "Iteration 484, loss = 0.47536361\n",
      "Iteration 485, loss = 0.47532380\n",
      "Iteration 486, loss = 0.47545358\n",
      "Iteration 487, loss = 0.47514719\n",
      "Iteration 488, loss = 0.47549932\n",
      "Iteration 489, loss = 0.47514187\n",
      "Iteration 490, loss = 0.47519536\n",
      "Iteration 491, loss = 0.47532438\n",
      "Iteration 492, loss = 0.47540931\n",
      "Iteration 493, loss = 0.47515963\n",
      "Iteration 494, loss = 0.47491098\n",
      "Iteration 495, loss = 0.47528683\n",
      "Iteration 496, loss = 0.47525114\n",
      "Iteration 497, loss = 0.47521814\n",
      "Iteration 498, loss = 0.47501043\n",
      "Iteration 499, loss = 0.47518163\n",
      "Iteration 500, loss = 0.47490603\n",
      "Iteration 501, loss = 0.47533317\n",
      "Iteration 502, loss = 0.47522811\n",
      "Iteration 503, loss = 0.47499609\n",
      "Iteration 504, loss = 0.47507467\n",
      "Iteration 505, loss = 0.47497367\n",
      "Iteration 506, loss = 0.47490932\n",
      "Iteration 507, loss = 0.47478475\n",
      "Iteration 508, loss = 0.47491459\n",
      "Iteration 509, loss = 0.47463766\n",
      "Iteration 510, loss = 0.47473283\n",
      "Iteration 511, loss = 0.47464329\n",
      "Iteration 512, loss = 0.47457279\n",
      "Iteration 513, loss = 0.47473743\n",
      "Iteration 514, loss = 0.47459318\n",
      "Iteration 515, loss = 0.47466487\n",
      "Iteration 516, loss = 0.47473483\n",
      "Iteration 517, loss = 0.47463397\n",
      "Iteration 518, loss = 0.47467049\n",
      "Iteration 519, loss = 0.47519256\n",
      "Iteration 520, loss = 0.47441831\n",
      "Iteration 521, loss = 0.47477000\n",
      "Iteration 522, loss = 0.47441870\n",
      "Iteration 523, loss = 0.47482825\n",
      "Iteration 524, loss = 0.47462546\n",
      "Iteration 525, loss = 0.47432687\n",
      "Iteration 526, loss = 0.47440365\n",
      "Iteration 527, loss = 0.47433526\n",
      "Iteration 528, loss = 0.47462925\n",
      "Iteration 529, loss = 0.47426025\n",
      "Iteration 530, loss = 0.47418423\n",
      "Iteration 531, loss = 0.47426821\n",
      "Iteration 532, loss = 0.47434164\n",
      "Iteration 533, loss = 0.47401686\n",
      "Iteration 534, loss = 0.47414894\n",
      "Iteration 535, loss = 0.47397647\n",
      "Iteration 536, loss = 0.47407310\n",
      "Iteration 537, loss = 0.47412011\n",
      "Iteration 538, loss = 0.47446853\n",
      "Iteration 539, loss = 0.47424275\n",
      "Iteration 540, loss = 0.47398576\n",
      "Iteration 541, loss = 0.47397776\n",
      "Iteration 542, loss = 0.47401428\n",
      "Iteration 543, loss = 0.47388010\n",
      "Iteration 544, loss = 0.47368823\n",
      "Iteration 545, loss = 0.47388068\n",
      "Iteration 546, loss = 0.47366516\n",
      "Iteration 547, loss = 0.47390492\n",
      "Iteration 548, loss = 0.47384139\n",
      "Iteration 549, loss = 0.47366806\n",
      "Iteration 550, loss = 0.47389187\n",
      "Iteration 551, loss = 0.47378912\n",
      "Iteration 552, loss = 0.47380020\n",
      "Iteration 553, loss = 0.47331265\n",
      "Iteration 554, loss = 0.47386707\n",
      "Iteration 555, loss = 0.47352783\n",
      "Iteration 556, loss = 0.47364118\n",
      "Iteration 557, loss = 0.47365631\n",
      "Iteration 558, loss = 0.47368543\n",
      "Iteration 559, loss = 0.47366257\n",
      "Iteration 560, loss = 0.47363635\n",
      "Iteration 561, loss = 0.47331827\n",
      "Iteration 562, loss = 0.47346862\n",
      "Iteration 563, loss = 0.47328842\n",
      "Iteration 564, loss = 0.47352246\n",
      "Iteration 565, loss = 0.47348807\n",
      "Iteration 566, loss = 0.47343741\n",
      "Iteration 567, loss = 0.47322377\n",
      "Iteration 568, loss = 0.47347144\n",
      "Iteration 569, loss = 0.47319602\n",
      "Iteration 570, loss = 0.47328843\n",
      "Iteration 571, loss = 0.47337124\n",
      "Iteration 572, loss = 0.47328028\n",
      "Iteration 573, loss = 0.47343448\n",
      "Iteration 574, loss = 0.47339428\n",
      "Iteration 575, loss = 0.47303565\n",
      "Iteration 576, loss = 0.47312851\n",
      "Iteration 577, loss = 0.47346150\n",
      "Iteration 578, loss = 0.47307697\n",
      "Iteration 579, loss = 0.47315583\n",
      "Iteration 580, loss = 0.47309472\n",
      "Iteration 581, loss = 0.47311124\n",
      "Iteration 582, loss = 0.47300654\n",
      "Iteration 583, loss = 0.47317547\n",
      "Iteration 584, loss = 0.47297427\n",
      "Iteration 585, loss = 0.47354071\n",
      "Iteration 586, loss = 0.47372693\n",
      "Iteration 587, loss = 0.47280897\n",
      "Iteration 588, loss = 0.47306290\n",
      "Iteration 589, loss = 0.47300517\n",
      "Iteration 590, loss = 0.47300953\n",
      "Iteration 591, loss = 0.47290865\n",
      "Iteration 592, loss = 0.47299942\n",
      "Iteration 593, loss = 0.47296976\n",
      "Iteration 594, loss = 0.47309693\n",
      "Iteration 595, loss = 0.47385369\n",
      "Iteration 596, loss = 0.47301910\n",
      "Iteration 597, loss = 0.47285203\n",
      "Iteration 598, loss = 0.47301493\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74204098\n",
      "Iteration 2, loss = 0.68686323\n",
      "Iteration 3, loss = 0.65646997\n",
      "Iteration 4, loss = 0.63782378\n",
      "Iteration 5, loss = 0.62294887\n",
      "Iteration 6, loss = 0.60931928\n",
      "Iteration 7, loss = 0.59772239\n",
      "Iteration 8, loss = 0.58835378\n",
      "Iteration 9, loss = 0.58041215\n",
      "Iteration 10, loss = 0.57459726\n",
      "Iteration 11, loss = 0.56972045\n",
      "Iteration 12, loss = 0.56579148\n",
      "Iteration 13, loss = 0.56268236\n",
      "Iteration 14, loss = 0.56023405\n",
      "Iteration 15, loss = 0.55803674\n",
      "Iteration 16, loss = 0.55596606\n",
      "Iteration 17, loss = 0.55438152\n",
      "Iteration 18, loss = 0.55269831\n",
      "Iteration 19, loss = 0.55102578\n",
      "Iteration 20, loss = 0.54956497\n",
      "Iteration 21, loss = 0.54815934\n",
      "Iteration 22, loss = 0.54696722\n",
      "Iteration 23, loss = 0.54550990\n",
      "Iteration 24, loss = 0.54429056\n",
      "Iteration 25, loss = 0.54300621\n",
      "Iteration 26, loss = 0.54147888\n",
      "Iteration 27, loss = 0.54020819\n",
      "Iteration 28, loss = 0.53896093\n",
      "Iteration 29, loss = 0.53786915\n",
      "Iteration 30, loss = 0.53678020\n",
      "Iteration 31, loss = 0.53538102\n",
      "Iteration 32, loss = 0.53452876\n",
      "Iteration 33, loss = 0.53325725\n",
      "Iteration 34, loss = 0.53222507\n",
      "Iteration 35, loss = 0.53104251\n",
      "Iteration 36, loss = 0.53000668\n",
      "Iteration 37, loss = 0.52912917\n",
      "Iteration 38, loss = 0.52787723\n",
      "Iteration 39, loss = 0.52689904\n",
      "Iteration 40, loss = 0.52585301\n",
      "Iteration 41, loss = 0.52500252\n",
      "Iteration 42, loss = 0.52397193\n",
      "Iteration 43, loss = 0.52290644\n",
      "Iteration 44, loss = 0.52168730\n",
      "Iteration 45, loss = 0.52097626\n",
      "Iteration 46, loss = 0.52005503\n",
      "Iteration 47, loss = 0.51910685\n",
      "Iteration 48, loss = 0.51828854\n",
      "Iteration 49, loss = 0.51763574\n",
      "Iteration 50, loss = 0.51654950\n",
      "Iteration 51, loss = 0.51590192\n",
      "Iteration 52, loss = 0.51513050\n",
      "Iteration 53, loss = 0.51424259\n",
      "Iteration 54, loss = 0.51367513\n",
      "Iteration 55, loss = 0.51302303\n",
      "Iteration 56, loss = 0.51245317\n",
      "Iteration 57, loss = 0.51189509\n",
      "Iteration 58, loss = 0.51110575\n",
      "Iteration 59, loss = 0.51055707\n",
      "Iteration 60, loss = 0.51025810\n",
      "Iteration 61, loss = 0.50957183\n",
      "Iteration 62, loss = 0.50916320\n",
      "Iteration 63, loss = 0.50852758\n",
      "Iteration 64, loss = 0.50823889\n",
      "Iteration 65, loss = 0.50773923\n",
      "Iteration 66, loss = 0.50718634\n",
      "Iteration 67, loss = 0.50664822\n",
      "Iteration 68, loss = 0.50611947\n",
      "Iteration 69, loss = 0.50544675\n",
      "Iteration 70, loss = 0.50510033\n",
      "Iteration 71, loss = 0.50529274\n",
      "Iteration 72, loss = 0.50421101\n",
      "Iteration 73, loss = 0.50407836\n",
      "Iteration 74, loss = 0.50381225\n",
      "Iteration 75, loss = 0.50316204\n",
      "Iteration 76, loss = 0.50324212\n",
      "Iteration 77, loss = 0.50270299\n",
      "Iteration 78, loss = 0.50242463\n",
      "Iteration 79, loss = 0.50233292\n",
      "Iteration 80, loss = 0.50185698\n",
      "Iteration 81, loss = 0.50127029\n",
      "Iteration 82, loss = 0.50126146\n",
      "Iteration 83, loss = 0.50103484\n",
      "Iteration 84, loss = 0.50055526\n",
      "Iteration 85, loss = 0.50012780\n",
      "Iteration 86, loss = 0.49998871\n",
      "Iteration 87, loss = 0.49979573\n",
      "Iteration 88, loss = 0.49951384\n",
      "Iteration 89, loss = 0.49937797\n",
      "Iteration 90, loss = 0.49901313\n",
      "Iteration 91, loss = 0.49876952\n",
      "Iteration 92, loss = 0.49852055\n",
      "Iteration 93, loss = 0.49820698\n",
      "Iteration 94, loss = 0.49803936\n",
      "Iteration 95, loss = 0.49776738\n",
      "Iteration 96, loss = 0.49753566\n",
      "Iteration 97, loss = 0.49734462\n",
      "Iteration 98, loss = 0.49694206\n",
      "Iteration 99, loss = 0.49683058\n",
      "Iteration 100, loss = 0.49649178\n",
      "Iteration 101, loss = 0.49631556\n",
      "Iteration 102, loss = 0.49577867\n",
      "Iteration 103, loss = 0.49578059\n",
      "Iteration 104, loss = 0.49563793\n",
      "Iteration 105, loss = 0.49512744\n",
      "Iteration 106, loss = 0.49477804\n",
      "Iteration 107, loss = 0.49470335\n",
      "Iteration 108, loss = 0.49415146\n",
      "Iteration 109, loss = 0.49390733\n",
      "Iteration 110, loss = 0.49356373\n",
      "Iteration 111, loss = 0.49340945\n",
      "Iteration 112, loss = 0.49325244\n",
      "Iteration 113, loss = 0.49318553\n",
      "Iteration 114, loss = 0.49287846\n",
      "Iteration 115, loss = 0.49279169\n",
      "Iteration 116, loss = 0.49219272\n",
      "Iteration 117, loss = 0.49200394\n",
      "Iteration 118, loss = 0.49202414\n",
      "Iteration 119, loss = 0.49168284\n",
      "Iteration 120, loss = 0.49150762\n",
      "Iteration 121, loss = 0.49117045\n",
      "Iteration 122, loss = 0.49121326\n",
      "Iteration 123, loss = 0.49098986\n",
      "Iteration 124, loss = 0.49079726\n",
      "Iteration 125, loss = 0.49070712\n",
      "Iteration 126, loss = 0.49065725\n",
      "Iteration 127, loss = 0.49062207\n",
      "Iteration 128, loss = 0.49015291\n",
      "Iteration 129, loss = 0.48993785\n",
      "Iteration 130, loss = 0.48976365\n",
      "Iteration 131, loss = 0.48976682\n",
      "Iteration 132, loss = 0.48967101\n",
      "Iteration 133, loss = 0.48931432\n",
      "Iteration 134, loss = 0.48953922\n",
      "Iteration 135, loss = 0.48884481\n",
      "Iteration 136, loss = 0.48890894\n",
      "Iteration 137, loss = 0.48885921\n",
      "Iteration 138, loss = 0.48856849\n",
      "Iteration 139, loss = 0.48870274\n",
      "Iteration 140, loss = 0.48818444\n",
      "Iteration 141, loss = 0.48846302\n",
      "Iteration 142, loss = 0.48829277\n",
      "Iteration 143, loss = 0.48790711\n",
      "Iteration 144, loss = 0.48808880\n",
      "Iteration 145, loss = 0.48753711\n",
      "Iteration 146, loss = 0.48758243\n",
      "Iteration 147, loss = 0.48749528\n",
      "Iteration 148, loss = 0.48716770\n",
      "Iteration 149, loss = 0.48721300\n",
      "Iteration 150, loss = 0.48747819\n",
      "Iteration 151, loss = 0.48694814\n",
      "Iteration 152, loss = 0.48676611\n",
      "Iteration 153, loss = 0.48681747\n",
      "Iteration 154, loss = 0.48654303\n",
      "Iteration 155, loss = 0.48665557\n",
      "Iteration 156, loss = 0.48657207\n",
      "Iteration 157, loss = 0.48616847\n",
      "Iteration 158, loss = 0.48646455\n",
      "Iteration 159, loss = 0.48624050\n",
      "Iteration 160, loss = 0.48621968\n",
      "Iteration 161, loss = 0.48625322\n",
      "Iteration 162, loss = 0.48597906\n",
      "Iteration 163, loss = 0.48568660\n",
      "Iteration 164, loss = 0.48574374\n",
      "Iteration 165, loss = 0.48558018\n",
      "Iteration 166, loss = 0.48532490\n",
      "Iteration 167, loss = 0.48526617\n",
      "Iteration 168, loss = 0.48537583\n",
      "Iteration 169, loss = 0.48517964\n",
      "Iteration 170, loss = 0.48511010\n",
      "Iteration 171, loss = 0.48503067\n",
      "Iteration 172, loss = 0.48496813\n",
      "Iteration 173, loss = 0.48479661\n",
      "Iteration 174, loss = 0.48464618\n",
      "Iteration 175, loss = 0.48462988\n",
      "Iteration 176, loss = 0.48460565\n",
      "Iteration 177, loss = 0.48437683\n",
      "Iteration 178, loss = 0.48474486\n",
      "Iteration 179, loss = 0.48446399\n",
      "Iteration 180, loss = 0.48397948\n",
      "Iteration 181, loss = 0.48416841\n",
      "Iteration 182, loss = 0.48391109\n",
      "Iteration 183, loss = 0.48385294\n",
      "Iteration 184, loss = 0.48395765\n",
      "Iteration 185, loss = 0.48369777\n",
      "Iteration 186, loss = 0.48376721\n",
      "Iteration 187, loss = 0.48358614\n",
      "Iteration 188, loss = 0.48349065\n",
      "Iteration 189, loss = 0.48399528\n",
      "Iteration 190, loss = 0.48364900\n",
      "Iteration 191, loss = 0.48351879\n",
      "Iteration 192, loss = 0.48333006\n",
      "Iteration 193, loss = 0.48338345\n",
      "Iteration 194, loss = 0.48327004\n",
      "Iteration 195, loss = 0.48337017\n",
      "Iteration 196, loss = 0.48285991\n",
      "Iteration 197, loss = 0.48358487\n",
      "Iteration 198, loss = 0.48295879\n",
      "Iteration 199, loss = 0.48276384\n",
      "Iteration 200, loss = 0.48289571\n",
      "Iteration 201, loss = 0.48278724\n",
      "Iteration 202, loss = 0.48274642\n",
      "Iteration 203, loss = 0.48299761\n",
      "Iteration 204, loss = 0.48272970\n",
      "Iteration 205, loss = 0.48247212\n",
      "Iteration 206, loss = 0.48245767\n",
      "Iteration 207, loss = 0.48220156\n",
      "Iteration 208, loss = 0.48267691\n",
      "Iteration 209, loss = 0.48228866\n",
      "Iteration 210, loss = 0.48251593\n",
      "Iteration 211, loss = 0.48228353\n",
      "Iteration 212, loss = 0.48210833\n",
      "Iteration 213, loss = 0.48211628\n",
      "Iteration 214, loss = 0.48193084\n",
      "Iteration 215, loss = 0.48223695\n",
      "Iteration 216, loss = 0.48193652\n",
      "Iteration 217, loss = 0.48193138\n",
      "Iteration 218, loss = 0.48168910\n",
      "Iteration 219, loss = 0.48207039\n",
      "Iteration 220, loss = 0.48173663\n",
      "Iteration 221, loss = 0.48175600\n",
      "Iteration 222, loss = 0.48174959\n",
      "Iteration 223, loss = 0.48160808\n",
      "Iteration 224, loss = 0.48165044\n",
      "Iteration 225, loss = 0.48186854\n",
      "Iteration 226, loss = 0.48154673\n",
      "Iteration 227, loss = 0.48167956\n",
      "Iteration 228, loss = 0.48150863\n",
      "Iteration 229, loss = 0.48136528\n",
      "Iteration 230, loss = 0.48150397\n",
      "Iteration 231, loss = 0.48138720\n",
      "Iteration 232, loss = 0.48113130\n",
      "Iteration 233, loss = 0.48148949\n",
      "Iteration 234, loss = 0.48125805\n",
      "Iteration 235, loss = 0.48142754\n",
      "Iteration 236, loss = 0.48187328\n",
      "Iteration 237, loss = 0.48122679\n",
      "Iteration 238, loss = 0.48100543\n",
      "Iteration 239, loss = 0.48089320\n",
      "Iteration 240, loss = 0.48098341\n",
      "Iteration 241, loss = 0.48093643\n",
      "Iteration 242, loss = 0.48089016\n",
      "Iteration 243, loss = 0.48091150\n",
      "Iteration 244, loss = 0.48083595\n",
      "Iteration 245, loss = 0.48086944\n",
      "Iteration 246, loss = 0.48081089\n",
      "Iteration 247, loss = 0.48095749\n",
      "Iteration 248, loss = 0.48093968\n",
      "Iteration 249, loss = 0.48048171\n",
      "Iteration 250, loss = 0.48065209\n",
      "Iteration 251, loss = 0.48068478\n",
      "Iteration 252, loss = 0.48075046\n",
      "Iteration 253, loss = 0.48089366\n",
      "Iteration 254, loss = 0.48045139\n",
      "Iteration 255, loss = 0.48042767\n",
      "Iteration 256, loss = 0.48028254\n",
      "Iteration 257, loss = 0.48047728\n",
      "Iteration 258, loss = 0.48026584\n",
      "Iteration 259, loss = 0.48024384\n",
      "Iteration 260, loss = 0.48026549\n",
      "Iteration 261, loss = 0.48028664\n",
      "Iteration 262, loss = 0.48025592\n",
      "Iteration 263, loss = 0.48036548\n",
      "Iteration 264, loss = 0.47993696\n",
      "Iteration 265, loss = 0.48037888\n",
      "Iteration 266, loss = 0.48000758\n",
      "Iteration 267, loss = 0.48043212\n",
      "Iteration 268, loss = 0.47999242\n",
      "Iteration 269, loss = 0.48016027\n",
      "Iteration 270, loss = 0.47996116\n",
      "Iteration 271, loss = 0.48061786\n",
      "Iteration 272, loss = 0.47991699\n",
      "Iteration 273, loss = 0.48004557\n",
      "Iteration 274, loss = 0.47997712\n",
      "Iteration 275, loss = 0.47988782\n",
      "Iteration 276, loss = 0.47984633\n",
      "Iteration 277, loss = 0.47986369\n",
      "Iteration 278, loss = 0.47993629\n",
      "Iteration 279, loss = 0.47989639\n",
      "Iteration 280, loss = 0.47980936\n",
      "Iteration 281, loss = 0.47989141\n",
      "Iteration 282, loss = 0.47989766\n",
      "Iteration 283, loss = 0.47976019\n",
      "Iteration 284, loss = 0.47991935\n",
      "Iteration 285, loss = 0.47964718\n",
      "Iteration 286, loss = 0.47971584\n",
      "Iteration 287, loss = 0.47963342\n",
      "Iteration 288, loss = 0.47955762\n",
      "Iteration 289, loss = 0.47950202\n",
      "Iteration 290, loss = 0.47953221\n",
      "Iteration 291, loss = 0.47960489\n",
      "Iteration 292, loss = 0.47942633\n",
      "Iteration 293, loss = 0.47921782\n",
      "Iteration 294, loss = 0.47943844\n",
      "Iteration 295, loss = 0.47933651\n",
      "Iteration 296, loss = 0.47939111\n",
      "Iteration 297, loss = 0.47932591\n",
      "Iteration 298, loss = 0.47948852\n",
      "Iteration 299, loss = 0.47933199\n",
      "Iteration 300, loss = 0.47984282\n",
      "Iteration 301, loss = 0.47925250\n",
      "Iteration 302, loss = 0.47939342\n",
      "Iteration 303, loss = 0.47920732\n",
      "Iteration 304, loss = 0.47922671\n",
      "Iteration 305, loss = 0.47920564\n",
      "Iteration 306, loss = 0.47920050\n",
      "Iteration 307, loss = 0.47940262\n",
      "Iteration 308, loss = 0.47988859\n",
      "Iteration 309, loss = 0.47905900\n",
      "Iteration 310, loss = 0.47918266\n",
      "Iteration 311, loss = 0.47904456\n",
      "Iteration 312, loss = 0.47946151\n",
      "Iteration 313, loss = 0.47910571\n",
      "Iteration 314, loss = 0.47895741\n",
      "Iteration 315, loss = 0.47895421\n",
      "Iteration 316, loss = 0.47914638\n",
      "Iteration 317, loss = 0.47944625\n",
      "Iteration 318, loss = 0.47924390\n",
      "Iteration 319, loss = 0.47879529\n",
      "Iteration 320, loss = 0.47880689\n",
      "Iteration 321, loss = 0.47890249\n",
      "Iteration 322, loss = 0.47879164\n",
      "Iteration 323, loss = 0.47878160\n",
      "Iteration 324, loss = 0.47909844\n",
      "Iteration 325, loss = 0.47871488\n",
      "Iteration 326, loss = 0.47883138\n",
      "Iteration 327, loss = 0.47872430\n",
      "Iteration 328, loss = 0.47883697\n",
      "Iteration 329, loss = 0.47949140\n",
      "Iteration 330, loss = 0.47864486\n",
      "Iteration 331, loss = 0.47854750\n",
      "Iteration 332, loss = 0.47852178\n",
      "Iteration 333, loss = 0.47867030\n",
      "Iteration 334, loss = 0.47865738\n",
      "Iteration 335, loss = 0.47846019\n",
      "Iteration 336, loss = 0.47865368\n",
      "Iteration 337, loss = 0.47851655\n",
      "Iteration 338, loss = 0.47857082\n",
      "Iteration 339, loss = 0.47833442\n",
      "Iteration 340, loss = 0.47876121\n",
      "Iteration 341, loss = 0.47860690\n",
      "Iteration 342, loss = 0.47813033\n",
      "Iteration 343, loss = 0.47833066\n",
      "Iteration 344, loss = 0.47829297\n",
      "Iteration 345, loss = 0.47829885\n",
      "Iteration 346, loss = 0.47819779\n",
      "Iteration 347, loss = 0.47837287\n",
      "Iteration 348, loss = 0.47808912\n",
      "Iteration 349, loss = 0.47809217\n",
      "Iteration 350, loss = 0.47802523\n",
      "Iteration 351, loss = 0.47818271\n",
      "Iteration 352, loss = 0.47780897\n",
      "Iteration 353, loss = 0.47784074\n",
      "Iteration 354, loss = 0.47802632\n",
      "Iteration 355, loss = 0.47785913\n",
      "Iteration 356, loss = 0.47788158\n",
      "Iteration 357, loss = 0.47770770\n",
      "Iteration 358, loss = 0.47760035\n",
      "Iteration 359, loss = 0.47755119\n",
      "Iteration 360, loss = 0.47769197\n",
      "Iteration 361, loss = 0.47761803\n",
      "Iteration 362, loss = 0.47739740\n",
      "Iteration 363, loss = 0.47740990\n",
      "Iteration 364, loss = 0.47730650\n",
      "Iteration 365, loss = 0.47743505\n",
      "Iteration 366, loss = 0.47744093\n",
      "Iteration 367, loss = 0.47726860\n",
      "Iteration 368, loss = 0.47735361\n",
      "Iteration 369, loss = 0.47739737\n",
      "Iteration 370, loss = 0.47728473\n",
      "Iteration 371, loss = 0.47728810\n",
      "Iteration 372, loss = 0.47771721\n",
      "Iteration 373, loss = 0.47733096\n",
      "Iteration 374, loss = 0.47703252\n",
      "Iteration 375, loss = 0.47711058\n",
      "Iteration 376, loss = 0.47705462\n",
      "Iteration 377, loss = 0.47704133\n",
      "Iteration 378, loss = 0.47690906\n",
      "Iteration 379, loss = 0.47700094\n",
      "Iteration 380, loss = 0.47728318\n",
      "Iteration 381, loss = 0.47676401\n",
      "Iteration 382, loss = 0.47707895\n",
      "Iteration 383, loss = 0.47671669\n",
      "Iteration 384, loss = 0.47677022\n",
      "Iteration 385, loss = 0.47694868\n",
      "Iteration 386, loss = 0.47671963\n",
      "Iteration 387, loss = 0.47652891\n",
      "Iteration 388, loss = 0.47664341\n",
      "Iteration 389, loss = 0.47675852\n",
      "Iteration 390, loss = 0.47650063\n",
      "Iteration 391, loss = 0.47635860\n",
      "Iteration 392, loss = 0.47679754\n",
      "Iteration 393, loss = 0.47634312\n",
      "Iteration 394, loss = 0.47644706\n",
      "Iteration 395, loss = 0.47649422\n",
      "Iteration 396, loss = 0.47651192\n",
      "Iteration 397, loss = 0.47621384\n",
      "Iteration 398, loss = 0.47629889\n",
      "Iteration 399, loss = 0.47623526\n",
      "Iteration 400, loss = 0.47593186\n",
      "Iteration 401, loss = 0.47623181\n",
      "Iteration 402, loss = 0.47595528\n",
      "Iteration 403, loss = 0.47613890\n",
      "Iteration 404, loss = 0.47596366\n",
      "Iteration 405, loss = 0.47576251\n",
      "Iteration 406, loss = 0.47605671\n",
      "Iteration 407, loss = 0.47606315\n",
      "Iteration 408, loss = 0.47615752\n",
      "Iteration 409, loss = 0.47570023\n",
      "Iteration 410, loss = 0.47571867\n",
      "Iteration 411, loss = 0.47573196\n",
      "Iteration 412, loss = 0.47546980\n",
      "Iteration 413, loss = 0.47535084\n",
      "Iteration 414, loss = 0.47570595\n",
      "Iteration 415, loss = 0.47575987\n",
      "Iteration 416, loss = 0.47537454\n",
      "Iteration 417, loss = 0.47565589\n",
      "Iteration 418, loss = 0.47530141\n",
      "Iteration 419, loss = 0.47585032\n",
      "Iteration 420, loss = 0.47507387\n",
      "Iteration 421, loss = 0.47550021\n",
      "Iteration 422, loss = 0.47501082\n",
      "Iteration 423, loss = 0.47518429\n",
      "Iteration 424, loss = 0.47550573\n",
      "Iteration 425, loss = 0.47503880\n",
      "Iteration 426, loss = 0.47506287\n",
      "Iteration 427, loss = 0.47503660\n",
      "Iteration 428, loss = 0.47511546\n",
      "Iteration 429, loss = 0.47481859\n",
      "Iteration 430, loss = 0.47494838\n",
      "Iteration 431, loss = 0.47510487\n",
      "Iteration 432, loss = 0.47551243\n",
      "Iteration 433, loss = 0.47495070\n",
      "Iteration 434, loss = 0.47464596\n",
      "Iteration 435, loss = 0.47473071\n",
      "Iteration 436, loss = 0.47467631\n",
      "Iteration 437, loss = 0.47458051\n",
      "Iteration 438, loss = 0.47492239\n",
      "Iteration 439, loss = 0.47510055\n",
      "Iteration 440, loss = 0.47454817\n",
      "Iteration 441, loss = 0.47470198\n",
      "Iteration 442, loss = 0.47453826\n",
      "Iteration 443, loss = 0.47467252\n",
      "Iteration 444, loss = 0.47441402\n",
      "Iteration 445, loss = 0.47440748\n",
      "Iteration 446, loss = 0.47441630\n",
      "Iteration 447, loss = 0.47420999\n",
      "Iteration 448, loss = 0.47417489\n",
      "Iteration 449, loss = 0.47429044\n",
      "Iteration 450, loss = 0.47411184\n",
      "Iteration 451, loss = 0.47411161\n",
      "Iteration 452, loss = 0.47433655\n",
      "Iteration 453, loss = 0.47400513\n",
      "Iteration 454, loss = 0.47390861\n",
      "Iteration 455, loss = 0.47413443\n",
      "Iteration 456, loss = 0.47388427\n",
      "Iteration 457, loss = 0.47389260\n",
      "Iteration 458, loss = 0.47405270\n",
      "Iteration 459, loss = 0.47489393\n",
      "Iteration 460, loss = 0.47382702\n",
      "Iteration 461, loss = 0.47373956\n",
      "Iteration 462, loss = 0.47370573\n",
      "Iteration 463, loss = 0.47393674\n",
      "Iteration 464, loss = 0.47359662\n",
      "Iteration 465, loss = 0.47438891\n",
      "Iteration 466, loss = 0.47390514\n",
      "Iteration 467, loss = 0.47357924\n",
      "Iteration 468, loss = 0.47326941\n",
      "Iteration 469, loss = 0.47344558\n",
      "Iteration 470, loss = 0.47357081\n",
      "Iteration 471, loss = 0.47331268\n",
      "Iteration 472, loss = 0.47332676\n",
      "Iteration 473, loss = 0.47343856\n",
      "Iteration 474, loss = 0.47329081\n",
      "Iteration 475, loss = 0.47305432\n",
      "Iteration 476, loss = 0.47307815\n",
      "Iteration 477, loss = 0.47297746\n",
      "Iteration 478, loss = 0.47305066\n",
      "Iteration 479, loss = 0.47325481\n",
      "Iteration 480, loss = 0.47298824\n",
      "Iteration 481, loss = 0.47301717\n",
      "Iteration 482, loss = 0.47333732\n",
      "Iteration 483, loss = 0.47336158\n",
      "Iteration 484, loss = 0.47305264\n",
      "Iteration 485, loss = 0.47304096\n",
      "Iteration 486, loss = 0.47291014\n",
      "Iteration 487, loss = 0.47272763\n",
      "Iteration 488, loss = 0.47265563\n",
      "Iteration 489, loss = 0.47252080\n",
      "Iteration 490, loss = 0.47232199\n",
      "Iteration 491, loss = 0.47279623\n",
      "Iteration 492, loss = 0.47327964\n",
      "Iteration 493, loss = 0.47279525\n",
      "Iteration 494, loss = 0.47234344\n",
      "Iteration 495, loss = 0.47236271\n",
      "Iteration 496, loss = 0.47241711\n",
      "Iteration 497, loss = 0.47252941\n",
      "Iteration 498, loss = 0.47228376\n",
      "Iteration 499, loss = 0.47243460\n",
      "Iteration 500, loss = 0.47225379\n",
      "Iteration 501, loss = 0.47222669\n",
      "Iteration 502, loss = 0.47217882\n",
      "Iteration 503, loss = 0.47266889\n",
      "Iteration 504, loss = 0.47219018\n",
      "Iteration 505, loss = 0.47244705\n",
      "Iteration 506, loss = 0.47254325\n",
      "Iteration 507, loss = 0.47215671\n",
      "Iteration 508, loss = 0.47175650\n",
      "Iteration 509, loss = 0.47221992\n",
      "Iteration 510, loss = 0.47203611\n",
      "Iteration 511, loss = 0.47179396\n",
      "Iteration 512, loss = 0.47189692\n",
      "Iteration 513, loss = 0.47174464\n",
      "Iteration 514, loss = 0.47186533\n",
      "Iteration 515, loss = 0.47190443\n",
      "Iteration 516, loss = 0.47170530\n",
      "Iteration 517, loss = 0.47166120\n",
      "Iteration 518, loss = 0.47186255\n",
      "Iteration 519, loss = 0.47160937\n",
      "Iteration 520, loss = 0.47185349\n",
      "Iteration 521, loss = 0.47149173\n",
      "Iteration 522, loss = 0.47152073\n",
      "Iteration 523, loss = 0.47161741\n",
      "Iteration 524, loss = 0.47142355\n",
      "Iteration 525, loss = 0.47159678\n",
      "Iteration 526, loss = 0.47179142\n",
      "Iteration 527, loss = 0.47167933\n",
      "Iteration 528, loss = 0.47155140\n",
      "Iteration 529, loss = 0.47151427\n",
      "Iteration 530, loss = 0.47128384\n",
      "Iteration 531, loss = 0.47124100\n",
      "Iteration 532, loss = 0.47137835\n",
      "Iteration 533, loss = 0.47113038\n",
      "Iteration 534, loss = 0.47119073\n",
      "Iteration 535, loss = 0.47112540\n",
      "Iteration 536, loss = 0.47122491\n",
      "Iteration 537, loss = 0.47115425\n",
      "Iteration 538, loss = 0.47175789\n",
      "Iteration 539, loss = 0.47110238\n",
      "Iteration 540, loss = 0.47109783\n",
      "Iteration 541, loss = 0.47123122\n",
      "Iteration 542, loss = 0.47105945\n",
      "Iteration 543, loss = 0.47098408\n",
      "Iteration 544, loss = 0.47110249\n",
      "Iteration 545, loss = 0.47116796\n",
      "Iteration 546, loss = 0.47063104\n",
      "Iteration 547, loss = 0.47116775\n",
      "Iteration 548, loss = 0.47121217\n",
      "Iteration 549, loss = 0.47157321\n",
      "Iteration 550, loss = 0.47098323\n",
      "Iteration 551, loss = 0.47069552\n",
      "Iteration 552, loss = 0.47074525\n",
      "Iteration 553, loss = 0.47076114\n",
      "Iteration 554, loss = 0.47062907\n",
      "Iteration 555, loss = 0.47053801\n",
      "Iteration 556, loss = 0.47101219\n",
      "Iteration 557, loss = 0.47087607\n",
      "Iteration 558, loss = 0.47059060\n",
      "Iteration 559, loss = 0.47064750\n",
      "Iteration 560, loss = 0.47057733\n",
      "Iteration 561, loss = 0.47034124\n",
      "Iteration 562, loss = 0.47063862\n",
      "Iteration 563, loss = 0.47058068\n",
      "Iteration 564, loss = 0.47069933\n",
      "Iteration 565, loss = 0.47055302\n",
      "Iteration 566, loss = 0.47038292\n",
      "Iteration 567, loss = 0.47008822\n",
      "Iteration 568, loss = 0.47006483\n",
      "Iteration 569, loss = 0.47050948\n",
      "Iteration 570, loss = 0.47033889\n",
      "Iteration 571, loss = 0.47008018\n",
      "Iteration 572, loss = 0.47025536\n",
      "Iteration 573, loss = 0.46988460\n",
      "Iteration 574, loss = 0.47013181\n",
      "Iteration 575, loss = 0.46995551\n",
      "Iteration 576, loss = 0.46977380\n",
      "Iteration 577, loss = 0.46980185\n",
      "Iteration 578, loss = 0.47000415\n",
      "Iteration 579, loss = 0.46992540\n",
      "Iteration 580, loss = 0.46976884\n",
      "Iteration 581, loss = 0.46976117\n",
      "Iteration 582, loss = 0.46970191\n",
      "Iteration 583, loss = 0.46959912\n",
      "Iteration 584, loss = 0.46987386\n",
      "Iteration 585, loss = 0.46966180\n",
      "Iteration 586, loss = 0.46967364\n",
      "Iteration 587, loss = 0.46963489\n",
      "Iteration 588, loss = 0.46966887\n",
      "Iteration 589, loss = 0.46982160\n",
      "Iteration 590, loss = 0.46949417\n",
      "Iteration 591, loss = 0.46945650\n",
      "Iteration 592, loss = 0.46979415\n",
      "Iteration 593, loss = 0.46973578\n",
      "Iteration 594, loss = 0.46939179\n",
      "Iteration 595, loss = 0.46942928\n",
      "Iteration 596, loss = 0.46956091\n",
      "Iteration 597, loss = 0.46934166\n",
      "Iteration 598, loss = 0.46959481\n",
      "Iteration 599, loss = 0.46931225\n",
      "Iteration 600, loss = 0.46942988\n",
      "Iteration 601, loss = 0.46927790\n",
      "Iteration 602, loss = 0.46932846\n",
      "Iteration 603, loss = 0.46909212\n",
      "Iteration 604, loss = 0.46927987\n",
      "Iteration 605, loss = 0.46905852\n",
      "Iteration 606, loss = 0.46914519\n",
      "Iteration 607, loss = 0.46904072\n",
      "Iteration 608, loss = 0.46932955\n",
      "Iteration 609, loss = 0.46895002\n",
      "Iteration 610, loss = 0.46981937\n",
      "Iteration 611, loss = 0.46889277\n",
      "Iteration 612, loss = 0.46918651\n",
      "Iteration 613, loss = 0.46888047\n",
      "Iteration 614, loss = 0.46895259\n",
      "Iteration 615, loss = 0.46894778\n",
      "Iteration 616, loss = 0.46904749\n",
      "Iteration 617, loss = 0.46891352\n",
      "Iteration 618, loss = 0.46895493\n",
      "Iteration 619, loss = 0.46888197\n",
      "Iteration 620, loss = 0.46888048\n",
      "Iteration 621, loss = 0.46888157\n",
      "Iteration 622, loss = 0.46888776\n",
      "Iteration 623, loss = 0.46886562\n",
      "Iteration 624, loss = 0.46941010\n",
      "Iteration 625, loss = 0.46881712\n",
      "Iteration 626, loss = 0.46882646\n",
      "Iteration 627, loss = 0.46877620\n",
      "Iteration 628, loss = 0.46882767\n",
      "Iteration 629, loss = 0.46882309\n",
      "Iteration 630, loss = 0.46868971\n",
      "Iteration 631, loss = 0.46864512\n",
      "Iteration 632, loss = 0.46857757\n",
      "Iteration 633, loss = 0.46874026\n",
      "Iteration 634, loss = 0.46890257\n",
      "Iteration 635, loss = 0.46879710\n",
      "Iteration 636, loss = 0.46924713\n",
      "Iteration 637, loss = 0.46892044\n",
      "Iteration 638, loss = 0.46839235\n",
      "Iteration 639, loss = 0.46842964\n",
      "Iteration 640, loss = 0.46875570\n",
      "Iteration 641, loss = 0.46849829\n",
      "Iteration 642, loss = 0.46833804\n",
      "Iteration 643, loss = 0.46840775\n",
      "Iteration 644, loss = 0.46868367\n",
      "Iteration 645, loss = 0.46866026\n",
      "Iteration 646, loss = 0.46842616\n",
      "Iteration 647, loss = 0.46845467\n",
      "Iteration 648, loss = 0.46824635\n",
      "Iteration 649, loss = 0.46827633\n",
      "Iteration 650, loss = 0.46848081\n",
      "Iteration 651, loss = 0.46828494\n",
      "Iteration 652, loss = 0.46850739\n",
      "Iteration 653, loss = 0.46825118\n",
      "Iteration 654, loss = 0.46837249\n",
      "Iteration 655, loss = 0.46843774\n",
      "Iteration 656, loss = 0.46821417\n",
      "Iteration 657, loss = 0.46818197\n",
      "Iteration 658, loss = 0.46785467\n",
      "Iteration 659, loss = 0.46804272\n",
      "Iteration 660, loss = 0.46805418\n",
      "Iteration 661, loss = 0.46842144\n",
      "Iteration 662, loss = 0.46801805\n",
      "Iteration 663, loss = 0.46802073\n",
      "Iteration 664, loss = 0.46792610\n",
      "Iteration 665, loss = 0.46799353\n",
      "Iteration 666, loss = 0.46784025\n",
      "Iteration 667, loss = 0.46781682\n",
      "Iteration 668, loss = 0.46786692\n",
      "Iteration 669, loss = 0.46827017\n",
      "Iteration 670, loss = 0.46789113\n",
      "Iteration 671, loss = 0.46797595\n",
      "Iteration 672, loss = 0.46779919\n",
      "Iteration 673, loss = 0.46785433\n",
      "Iteration 674, loss = 0.46796990\n",
      "Iteration 675, loss = 0.46764437\n",
      "Iteration 676, loss = 0.46752629\n",
      "Iteration 677, loss = 0.46748653\n",
      "Iteration 678, loss = 0.46772049\n",
      "Iteration 679, loss = 0.46808985\n",
      "Iteration 680, loss = 0.46761590\n",
      "Iteration 681, loss = 0.46753276\n",
      "Iteration 682, loss = 0.46779930\n",
      "Iteration 683, loss = 0.46753332\n",
      "Iteration 684, loss = 0.46754681\n",
      "Iteration 685, loss = 0.46734381\n",
      "Iteration 686, loss = 0.46750758\n",
      "Iteration 687, loss = 0.46784294\n",
      "Iteration 688, loss = 0.46759173\n",
      "Iteration 689, loss = 0.46752307\n",
      "Iteration 690, loss = 0.46737959\n",
      "Iteration 691, loss = 0.46735219\n",
      "Iteration 692, loss = 0.46734228\n",
      "Iteration 693, loss = 0.46750581\n",
      "Iteration 694, loss = 0.46740528\n",
      "Iteration 695, loss = 0.46739960\n",
      "Iteration 696, loss = 0.46734411\n",
      "Iteration 697, loss = 0.46724142\n",
      "Iteration 698, loss = 0.46719810\n",
      "Iteration 699, loss = 0.46757723\n",
      "Iteration 700, loss = 0.46733763\n",
      "Iteration 701, loss = 0.46735192\n",
      "Iteration 702, loss = 0.46720976\n",
      "Iteration 703, loss = 0.46745962\n",
      "Iteration 704, loss = 0.46730986\n",
      "Iteration 705, loss = 0.46739510\n",
      "Iteration 706, loss = 0.46714115\n",
      "Iteration 707, loss = 0.46739181\n",
      "Iteration 708, loss = 0.46727543\n",
      "Iteration 709, loss = 0.46702443\n",
      "Iteration 710, loss = 0.46710360\n",
      "Iteration 711, loss = 0.46708351\n",
      "Iteration 712, loss = 0.46710744\n",
      "Iteration 713, loss = 0.46695184\n",
      "Iteration 714, loss = 0.46699573\n",
      "Iteration 715, loss = 0.46688597\n",
      "Iteration 716, loss = 0.46683349\n",
      "Iteration 717, loss = 0.46680581\n",
      "Iteration 718, loss = 0.46686564\n",
      "Iteration 719, loss = 0.46725257\n",
      "Iteration 720, loss = 0.46700930\n",
      "Iteration 721, loss = 0.46676008\n",
      "Iteration 722, loss = 0.46694785\n",
      "Iteration 723, loss = 0.46673787\n",
      "Iteration 724, loss = 0.46696214\n",
      "Iteration 725, loss = 0.46653670\n",
      "Iteration 726, loss = 0.46687642\n",
      "Iteration 727, loss = 0.46675673\n",
      "Iteration 728, loss = 0.46693449\n",
      "Iteration 729, loss = 0.46670280\n",
      "Iteration 730, loss = 0.46700813\n",
      "Iteration 731, loss = 0.46677508\n",
      "Iteration 732, loss = 0.46672043\n",
      "Iteration 733, loss = 0.46700449\n",
      "Iteration 734, loss = 0.46670791\n",
      "Iteration 735, loss = 0.46685445\n",
      "Iteration 736, loss = 0.46666961\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82786662\n",
      "Iteration 2, loss = 0.76669876\n",
      "Iteration 3, loss = 0.72654607\n",
      "Iteration 4, loss = 0.69918782\n",
      "Iteration 5, loss = 0.67843741\n",
      "Iteration 6, loss = 0.66113081\n",
      "Iteration 7, loss = 0.64595380\n",
      "Iteration 8, loss = 0.63220542\n",
      "Iteration 9, loss = 0.62003591\n",
      "Iteration 10, loss = 0.60887707\n",
      "Iteration 11, loss = 0.59799678\n",
      "Iteration 12, loss = 0.58852672\n",
      "Iteration 13, loss = 0.58038820\n",
      "Iteration 14, loss = 0.57353971\n",
      "Iteration 15, loss = 0.56802183\n",
      "Iteration 16, loss = 0.56341468\n",
      "Iteration 17, loss = 0.55971843\n",
      "Iteration 18, loss = 0.55661534\n",
      "Iteration 19, loss = 0.55462372\n",
      "Iteration 20, loss = 0.55263876\n",
      "Iteration 21, loss = 0.55101722\n",
      "Iteration 22, loss = 0.54943698\n",
      "Iteration 23, loss = 0.54827552\n",
      "Iteration 24, loss = 0.54701298\n",
      "Iteration 25, loss = 0.54595703\n",
      "Iteration 26, loss = 0.54504740\n",
      "Iteration 27, loss = 0.54419794\n",
      "Iteration 28, loss = 0.54342570\n",
      "Iteration 29, loss = 0.54271264\n",
      "Iteration 30, loss = 0.54191108\n",
      "Iteration 31, loss = 0.54095995\n",
      "Iteration 32, loss = 0.54044804\n",
      "Iteration 33, loss = 0.54012253\n",
      "Iteration 34, loss = 0.53914389\n",
      "Iteration 35, loss = 0.53853975\n",
      "Iteration 36, loss = 0.53832558\n",
      "Iteration 37, loss = 0.53737612\n",
      "Iteration 38, loss = 0.53761549\n",
      "Iteration 39, loss = 0.53676437\n",
      "Iteration 40, loss = 0.53626099\n",
      "Iteration 41, loss = 0.53574202\n",
      "Iteration 42, loss = 0.53542029\n",
      "Iteration 43, loss = 0.53485992\n",
      "Iteration 44, loss = 0.53441177\n",
      "Iteration 45, loss = 0.53403876\n",
      "Iteration 46, loss = 0.53378878\n",
      "Iteration 47, loss = 0.53323927\n",
      "Iteration 48, loss = 0.53308638\n",
      "Iteration 49, loss = 0.53258097\n",
      "Iteration 50, loss = 0.53222364\n",
      "Iteration 51, loss = 0.53174675\n",
      "Iteration 52, loss = 0.53132664\n",
      "Iteration 53, loss = 0.53077804\n",
      "Iteration 54, loss = 0.53075209\n",
      "Iteration 55, loss = 0.53022347\n",
      "Iteration 56, loss = 0.52947876\n",
      "Iteration 57, loss = 0.52900927\n",
      "Iteration 58, loss = 0.52867869\n",
      "Iteration 59, loss = 0.52815587\n",
      "Iteration 60, loss = 0.52791134\n",
      "Iteration 61, loss = 0.52744778\n",
      "Iteration 62, loss = 0.52684047\n",
      "Iteration 63, loss = 0.52640991\n",
      "Iteration 64, loss = 0.52616035\n",
      "Iteration 65, loss = 0.52570454\n",
      "Iteration 66, loss = 0.52507680\n",
      "Iteration 67, loss = 0.52439555\n",
      "Iteration 68, loss = 0.52391444\n",
      "Iteration 69, loss = 0.52374575\n",
      "Iteration 70, loss = 0.52284065\n",
      "Iteration 71, loss = 0.52246886\n",
      "Iteration 72, loss = 0.52207645\n",
      "Iteration 73, loss = 0.52158873\n",
      "Iteration 74, loss = 0.52095519\n",
      "Iteration 75, loss = 0.52055674\n",
      "Iteration 76, loss = 0.52011291\n",
      "Iteration 77, loss = 0.51913424\n",
      "Iteration 78, loss = 0.51868269\n",
      "Iteration 79, loss = 0.51858712\n",
      "Iteration 80, loss = 0.51792615\n",
      "Iteration 81, loss = 0.51731787\n",
      "Iteration 82, loss = 0.51662877\n",
      "Iteration 83, loss = 0.51634906\n",
      "Iteration 84, loss = 0.51617385\n",
      "Iteration 85, loss = 0.51569257\n",
      "Iteration 86, loss = 0.51460405\n",
      "Iteration 87, loss = 0.51383124\n",
      "Iteration 88, loss = 0.51333900\n",
      "Iteration 89, loss = 0.51281364\n",
      "Iteration 90, loss = 0.51202681\n",
      "Iteration 91, loss = 0.51131091\n",
      "Iteration 92, loss = 0.51092892\n",
      "Iteration 93, loss = 0.51009939\n",
      "Iteration 94, loss = 0.50956302\n",
      "Iteration 95, loss = 0.50929644\n",
      "Iteration 96, loss = 0.50867394\n",
      "Iteration 97, loss = 0.50798655\n",
      "Iteration 98, loss = 0.50782829\n",
      "Iteration 99, loss = 0.50700699\n",
      "Iteration 100, loss = 0.50678690\n",
      "Iteration 101, loss = 0.50622726\n",
      "Iteration 102, loss = 0.50592674\n",
      "Iteration 103, loss = 0.50532186\n",
      "Iteration 104, loss = 0.50528898\n",
      "Iteration 105, loss = 0.50433464\n",
      "Iteration 106, loss = 0.50394538\n",
      "Iteration 107, loss = 0.50351741\n",
      "Iteration 108, loss = 0.50322639\n",
      "Iteration 109, loss = 0.50312070\n",
      "Iteration 110, loss = 0.50268310\n",
      "Iteration 111, loss = 0.50243157\n",
      "Iteration 112, loss = 0.50164662\n",
      "Iteration 113, loss = 0.50128371\n",
      "Iteration 114, loss = 0.50128748\n",
      "Iteration 115, loss = 0.50089951\n",
      "Iteration 116, loss = 0.50027511\n",
      "Iteration 117, loss = 0.49980963\n",
      "Iteration 118, loss = 0.49956063\n",
      "Iteration 119, loss = 0.49922420\n",
      "Iteration 120, loss = 0.49892937\n",
      "Iteration 121, loss = 0.49909673\n",
      "Iteration 122, loss = 0.49844954\n",
      "Iteration 123, loss = 0.49807505\n",
      "Iteration 124, loss = 0.49777288\n",
      "Iteration 125, loss = 0.49738286\n",
      "Iteration 126, loss = 0.49740313\n",
      "Iteration 127, loss = 0.49690431\n",
      "Iteration 128, loss = 0.49683848\n",
      "Iteration 129, loss = 0.49652046\n",
      "Iteration 130, loss = 0.49632678\n",
      "Iteration 131, loss = 0.49642719\n",
      "Iteration 132, loss = 0.49574801\n",
      "Iteration 133, loss = 0.49566409\n",
      "Iteration 134, loss = 0.49595631\n",
      "Iteration 135, loss = 0.49536957\n",
      "Iteration 136, loss = 0.49516279\n",
      "Iteration 137, loss = 0.49490974\n",
      "Iteration 138, loss = 0.49488416\n",
      "Iteration 139, loss = 0.49468126\n",
      "Iteration 140, loss = 0.49454409\n",
      "Iteration 141, loss = 0.49442714\n",
      "Iteration 142, loss = 0.49400953\n",
      "Iteration 143, loss = 0.49407290\n",
      "Iteration 144, loss = 0.49367998\n",
      "Iteration 145, loss = 0.49375568\n",
      "Iteration 146, loss = 0.49353770\n",
      "Iteration 147, loss = 0.49343343\n",
      "Iteration 148, loss = 0.49326514\n",
      "Iteration 149, loss = 0.49360480\n",
      "Iteration 150, loss = 0.49298860\n",
      "Iteration 151, loss = 0.49285888\n",
      "Iteration 152, loss = 0.49264446\n",
      "Iteration 153, loss = 0.49265804\n",
      "Iteration 154, loss = 0.49230474\n",
      "Iteration 155, loss = 0.49228806\n",
      "Iteration 156, loss = 0.49237730\n",
      "Iteration 157, loss = 0.49191037\n",
      "Iteration 158, loss = 0.49200174\n",
      "Iteration 159, loss = 0.49169782\n",
      "Iteration 160, loss = 0.49160872\n",
      "Iteration 161, loss = 0.49162819\n",
      "Iteration 162, loss = 0.49184282\n",
      "Iteration 163, loss = 0.49124287\n",
      "Iteration 164, loss = 0.49131309\n",
      "Iteration 165, loss = 0.49115007\n",
      "Iteration 166, loss = 0.49090435\n",
      "Iteration 167, loss = 0.49086763\n",
      "Iteration 168, loss = 0.49091070\n",
      "Iteration 169, loss = 0.49067064\n",
      "Iteration 170, loss = 0.49070055\n",
      "Iteration 171, loss = 0.49062235\n",
      "Iteration 172, loss = 0.49052558\n",
      "Iteration 173, loss = 0.49019078\n",
      "Iteration 174, loss = 0.48997593\n",
      "Iteration 175, loss = 0.48997287\n",
      "Iteration 176, loss = 0.49028656\n",
      "Iteration 177, loss = 0.49006022\n",
      "Iteration 178, loss = 0.48941788\n",
      "Iteration 179, loss = 0.48965068\n",
      "Iteration 180, loss = 0.48977001\n",
      "Iteration 181, loss = 0.48938671\n",
      "Iteration 182, loss = 0.49008158\n",
      "Iteration 183, loss = 0.48927835\n",
      "Iteration 184, loss = 0.48918499\n",
      "Iteration 185, loss = 0.48897328\n",
      "Iteration 186, loss = 0.48930646\n",
      "Iteration 187, loss = 0.48887850\n",
      "Iteration 188, loss = 0.48887896\n",
      "Iteration 189, loss = 0.48858023\n",
      "Iteration 190, loss = 0.48866863\n",
      "Iteration 191, loss = 0.48842532\n",
      "Iteration 192, loss = 0.48830925\n",
      "Iteration 193, loss = 0.48819621\n",
      "Iteration 194, loss = 0.48801013\n",
      "Iteration 195, loss = 0.48886633\n",
      "Iteration 196, loss = 0.48811788\n",
      "Iteration 197, loss = 0.48792563\n",
      "Iteration 198, loss = 0.48779114\n",
      "Iteration 199, loss = 0.48798428\n",
      "Iteration 200, loss = 0.48750884\n",
      "Iteration 201, loss = 0.48770058\n",
      "Iteration 202, loss = 0.48773337\n",
      "Iteration 203, loss = 0.48742224\n",
      "Iteration 204, loss = 0.48792614\n",
      "Iteration 205, loss = 0.48755433\n",
      "Iteration 206, loss = 0.48731182\n",
      "Iteration 207, loss = 0.48723970\n",
      "Iteration 208, loss = 0.48722262\n",
      "Iteration 209, loss = 0.48729285\n",
      "Iteration 210, loss = 0.48701902\n",
      "Iteration 211, loss = 0.48696894\n",
      "Iteration 212, loss = 0.48679653\n",
      "Iteration 213, loss = 0.48707182\n",
      "Iteration 214, loss = 0.48673546\n",
      "Iteration 215, loss = 0.48658437\n",
      "Iteration 216, loss = 0.48676403\n",
      "Iteration 217, loss = 0.48655166\n",
      "Iteration 218, loss = 0.48681409\n",
      "Iteration 219, loss = 0.48639155\n",
      "Iteration 220, loss = 0.48677446\n",
      "Iteration 221, loss = 0.48613634\n",
      "Iteration 222, loss = 0.48621274\n",
      "Iteration 223, loss = 0.48628142\n",
      "Iteration 224, loss = 0.48610776\n",
      "Iteration 225, loss = 0.48596647\n",
      "Iteration 226, loss = 0.48594782\n",
      "Iteration 227, loss = 0.48573393\n",
      "Iteration 228, loss = 0.48576240\n",
      "Iteration 229, loss = 0.48564462\n",
      "Iteration 230, loss = 0.48530221\n",
      "Iteration 231, loss = 0.48542908\n",
      "Iteration 232, loss = 0.48529751\n",
      "Iteration 233, loss = 0.48545309\n",
      "Iteration 234, loss = 0.48540626\n",
      "Iteration 235, loss = 0.48509622\n",
      "Iteration 236, loss = 0.48510182\n",
      "Iteration 237, loss = 0.48536495\n",
      "Iteration 238, loss = 0.48485819\n",
      "Iteration 239, loss = 0.48479848\n",
      "Iteration 240, loss = 0.48486806\n",
      "Iteration 241, loss = 0.48478359\n",
      "Iteration 242, loss = 0.48490548\n",
      "Iteration 243, loss = 0.48466926\n",
      "Iteration 244, loss = 0.48467499\n",
      "Iteration 245, loss = 0.48439814\n",
      "Iteration 246, loss = 0.48451335\n",
      "Iteration 247, loss = 0.48473637\n",
      "Iteration 248, loss = 0.48416501\n",
      "Iteration 249, loss = 0.48451635\n",
      "Iteration 250, loss = 0.48422250\n",
      "Iteration 251, loss = 0.48427435\n",
      "Iteration 252, loss = 0.48424360\n",
      "Iteration 253, loss = 0.48397290\n",
      "Iteration 254, loss = 0.48425905\n",
      "Iteration 255, loss = 0.48382417\n",
      "Iteration 256, loss = 0.48372297\n",
      "Iteration 257, loss = 0.48385071\n",
      "Iteration 258, loss = 0.48373493\n",
      "Iteration 259, loss = 0.48364220\n",
      "Iteration 260, loss = 0.48365930\n",
      "Iteration 261, loss = 0.48362850\n",
      "Iteration 262, loss = 0.48371577\n",
      "Iteration 263, loss = 0.48351922\n",
      "Iteration 264, loss = 0.48320759\n",
      "Iteration 265, loss = 0.48309001\n",
      "Iteration 266, loss = 0.48325570\n",
      "Iteration 267, loss = 0.48289724\n",
      "Iteration 268, loss = 0.48309587\n",
      "Iteration 269, loss = 0.48301904\n",
      "Iteration 270, loss = 0.48315065\n",
      "Iteration 271, loss = 0.48284323\n",
      "Iteration 272, loss = 0.48270822\n",
      "Iteration 273, loss = 0.48264670\n",
      "Iteration 274, loss = 0.48265613\n",
      "Iteration 275, loss = 0.48283762\n",
      "Iteration 276, loss = 0.48252172\n",
      "Iteration 277, loss = 0.48251112\n",
      "Iteration 278, loss = 0.48240550\n",
      "Iteration 279, loss = 0.48245933\n",
      "Iteration 280, loss = 0.48221872\n",
      "Iteration 281, loss = 0.48238301\n",
      "Iteration 282, loss = 0.48205578\n",
      "Iteration 283, loss = 0.48310194\n",
      "Iteration 284, loss = 0.48268008\n",
      "Iteration 285, loss = 0.48189421\n",
      "Iteration 286, loss = 0.48196884\n",
      "Iteration 287, loss = 0.48223549\n",
      "Iteration 288, loss = 0.48223738\n",
      "Iteration 289, loss = 0.48165130\n",
      "Iteration 290, loss = 0.48160458\n",
      "Iteration 291, loss = 0.48264400\n",
      "Iteration 292, loss = 0.48156776\n",
      "Iteration 293, loss = 0.48148293\n",
      "Iteration 294, loss = 0.48145091\n",
      "Iteration 295, loss = 0.48144864\n",
      "Iteration 296, loss = 0.48140741\n",
      "Iteration 297, loss = 0.48120576\n",
      "Iteration 298, loss = 0.48111339\n",
      "Iteration 299, loss = 0.48123560\n",
      "Iteration 300, loss = 0.48096394\n",
      "Iteration 301, loss = 0.48110980\n",
      "Iteration 302, loss = 0.48093725\n",
      "Iteration 303, loss = 0.48102649\n",
      "Iteration 304, loss = 0.48074629\n",
      "Iteration 305, loss = 0.48083630\n",
      "Iteration 306, loss = 0.48069384\n",
      "Iteration 307, loss = 0.48068591\n",
      "Iteration 308, loss = 0.48051628\n",
      "Iteration 309, loss = 0.48071626\n",
      "Iteration 310, loss = 0.48062946\n",
      "Iteration 311, loss = 0.48054997\n",
      "Iteration 312, loss = 0.48091565\n",
      "Iteration 313, loss = 0.48047646\n",
      "Iteration 314, loss = 0.48035986\n",
      "Iteration 315, loss = 0.48060971\n",
      "Iteration 316, loss = 0.48046268\n",
      "Iteration 317, loss = 0.48001104\n",
      "Iteration 318, loss = 0.48031914\n",
      "Iteration 319, loss = 0.47999255\n",
      "Iteration 320, loss = 0.48042664\n",
      "Iteration 321, loss = 0.48004548\n",
      "Iteration 322, loss = 0.47978819\n",
      "Iteration 323, loss = 0.47999647\n",
      "Iteration 324, loss = 0.48004469\n",
      "Iteration 325, loss = 0.47991209\n",
      "Iteration 326, loss = 0.47996519\n",
      "Iteration 327, loss = 0.47986682\n",
      "Iteration 328, loss = 0.47950701\n",
      "Iteration 329, loss = 0.47987749\n",
      "Iteration 330, loss = 0.47968937\n",
      "Iteration 331, loss = 0.47958526\n",
      "Iteration 332, loss = 0.47979874\n",
      "Iteration 333, loss = 0.47963984\n",
      "Iteration 334, loss = 0.47941808\n",
      "Iteration 335, loss = 0.47946783\n",
      "Iteration 336, loss = 0.47971025\n",
      "Iteration 337, loss = 0.48016451\n",
      "Iteration 338, loss = 0.47932734\n",
      "Iteration 339, loss = 0.47910599\n",
      "Iteration 340, loss = 0.47895002\n",
      "Iteration 341, loss = 0.47904936\n",
      "Iteration 342, loss = 0.47896441\n",
      "Iteration 343, loss = 0.47901404\n",
      "Iteration 344, loss = 0.47888436\n",
      "Iteration 345, loss = 0.47883059\n",
      "Iteration 346, loss = 0.47910440\n",
      "Iteration 347, loss = 0.47873517\n",
      "Iteration 348, loss = 0.47896928\n",
      "Iteration 349, loss = 0.47894754\n",
      "Iteration 350, loss = 0.47884123\n",
      "Iteration 351, loss = 0.47897138\n",
      "Iteration 352, loss = 0.47881931\n",
      "Iteration 353, loss = 0.47864869\n",
      "Iteration 354, loss = 0.47895384\n",
      "Iteration 355, loss = 0.47848215\n",
      "Iteration 356, loss = 0.47861935\n",
      "Iteration 357, loss = 0.47845996\n",
      "Iteration 358, loss = 0.47874722\n",
      "Iteration 359, loss = 0.47900537\n",
      "Iteration 360, loss = 0.47853811\n",
      "Iteration 361, loss = 0.47832339\n",
      "Iteration 362, loss = 0.47814698\n",
      "Iteration 363, loss = 0.47826377\n",
      "Iteration 364, loss = 0.47802497\n",
      "Iteration 365, loss = 0.47836415\n",
      "Iteration 366, loss = 0.47843830\n",
      "Iteration 367, loss = 0.47824948\n",
      "Iteration 368, loss = 0.47828628\n",
      "Iteration 369, loss = 0.47812414\n",
      "Iteration 370, loss = 0.47789771\n",
      "Iteration 371, loss = 0.47795140\n",
      "Iteration 372, loss = 0.47840047\n",
      "Iteration 373, loss = 0.47787305\n",
      "Iteration 374, loss = 0.47798253\n",
      "Iteration 375, loss = 0.47795802\n",
      "Iteration 376, loss = 0.47819756\n",
      "Iteration 377, loss = 0.47793861\n",
      "Iteration 378, loss = 0.47790645\n",
      "Iteration 379, loss = 0.47804334\n",
      "Iteration 380, loss = 0.47812375\n",
      "Iteration 381, loss = 0.47778034\n",
      "Iteration 382, loss = 0.47786892\n",
      "Iteration 383, loss = 0.47767631\n",
      "Iteration 384, loss = 0.47782602\n",
      "Iteration 385, loss = 0.47822907\n",
      "Iteration 386, loss = 0.47747861\n",
      "Iteration 387, loss = 0.47755652\n",
      "Iteration 388, loss = 0.47756392\n",
      "Iteration 389, loss = 0.47782022\n",
      "Iteration 390, loss = 0.47733872\n",
      "Iteration 391, loss = 0.47720246\n",
      "Iteration 392, loss = 0.47725265\n",
      "Iteration 393, loss = 0.47732559\n",
      "Iteration 394, loss = 0.47709722\n",
      "Iteration 395, loss = 0.47728717\n",
      "Iteration 396, loss = 0.47722765\n",
      "Iteration 397, loss = 0.47719301\n",
      "Iteration 398, loss = 0.47774557\n",
      "Iteration 399, loss = 0.47693866\n",
      "Iteration 400, loss = 0.47779747\n",
      "Iteration 401, loss = 0.47731014\n",
      "Iteration 402, loss = 0.47739644\n",
      "Iteration 403, loss = 0.47739079\n",
      "Iteration 404, loss = 0.47740399\n",
      "Iteration 405, loss = 0.47712200\n",
      "Iteration 406, loss = 0.47690343\n",
      "Iteration 407, loss = 0.47687103\n",
      "Iteration 408, loss = 0.47701006\n",
      "Iteration 409, loss = 0.47740493\n",
      "Iteration 410, loss = 0.47693985\n",
      "Iteration 411, loss = 0.47680776\n",
      "Iteration 412, loss = 0.47679932\n",
      "Iteration 413, loss = 0.47673458\n",
      "Iteration 414, loss = 0.47684821\n",
      "Iteration 415, loss = 0.47648703\n",
      "Iteration 416, loss = 0.47716718\n",
      "Iteration 417, loss = 0.47652661\n",
      "Iteration 418, loss = 0.47665904\n",
      "Iteration 419, loss = 0.47663610\n",
      "Iteration 420, loss = 0.47636662\n",
      "Iteration 421, loss = 0.47654717\n",
      "Iteration 422, loss = 0.47682464\n",
      "Iteration 423, loss = 0.47668048\n",
      "Iteration 424, loss = 0.47623565\n",
      "Iteration 425, loss = 0.47634110\n",
      "Iteration 426, loss = 0.47617877\n",
      "Iteration 427, loss = 0.47626891\n",
      "Iteration 428, loss = 0.47624258\n",
      "Iteration 429, loss = 0.47630397\n",
      "Iteration 430, loss = 0.47608102\n",
      "Iteration 431, loss = 0.47606762\n",
      "Iteration 432, loss = 0.47670947\n",
      "Iteration 433, loss = 0.47647707\n",
      "Iteration 434, loss = 0.47596406\n",
      "Iteration 435, loss = 0.47614979\n",
      "Iteration 436, loss = 0.47586682\n",
      "Iteration 437, loss = 0.47617302\n",
      "Iteration 438, loss = 0.47596441\n",
      "Iteration 439, loss = 0.47646411\n",
      "Iteration 440, loss = 0.47660510\n",
      "Iteration 441, loss = 0.47566327\n",
      "Iteration 442, loss = 0.47577219\n",
      "Iteration 443, loss = 0.47554787\n",
      "Iteration 444, loss = 0.47586291\n",
      "Iteration 445, loss = 0.47574427\n",
      "Iteration 446, loss = 0.47573530\n",
      "Iteration 447, loss = 0.47568826\n",
      "Iteration 448, loss = 0.47605899\n",
      "Iteration 449, loss = 0.47578053\n",
      "Iteration 450, loss = 0.47545047\n",
      "Iteration 451, loss = 0.47547677\n",
      "Iteration 452, loss = 0.47544735\n",
      "Iteration 453, loss = 0.47570670\n",
      "Iteration 454, loss = 0.47549554\n",
      "Iteration 455, loss = 0.47535303\n",
      "Iteration 456, loss = 0.47530539\n",
      "Iteration 457, loss = 0.47536353\n",
      "Iteration 458, loss = 0.47535781\n",
      "Iteration 459, loss = 0.47498361\n",
      "Iteration 460, loss = 0.47532880\n",
      "Iteration 461, loss = 0.47488302\n",
      "Iteration 462, loss = 0.47508461\n",
      "Iteration 463, loss = 0.47514185\n",
      "Iteration 464, loss = 0.47500948\n",
      "Iteration 465, loss = 0.47496819\n",
      "Iteration 466, loss = 0.47501423\n",
      "Iteration 467, loss = 0.47546836\n",
      "Iteration 468, loss = 0.47485627\n",
      "Iteration 469, loss = 0.47491102\n",
      "Iteration 470, loss = 0.47492231\n",
      "Iteration 471, loss = 0.47448924\n",
      "Iteration 472, loss = 0.47523840\n",
      "Iteration 473, loss = 0.47485379\n",
      "Iteration 474, loss = 0.47454291\n",
      "Iteration 475, loss = 0.47476903\n",
      "Iteration 476, loss = 0.47497168\n",
      "Iteration 477, loss = 0.47480238\n",
      "Iteration 478, loss = 0.47463216\n",
      "Iteration 479, loss = 0.47439034\n",
      "Iteration 480, loss = 0.47465756\n",
      "Iteration 481, loss = 0.47481389\n",
      "Iteration 482, loss = 0.47467476\n",
      "Iteration 483, loss = 0.47533240\n",
      "Iteration 484, loss = 0.47484406\n",
      "Iteration 485, loss = 0.47452345\n",
      "Iteration 486, loss = 0.47436385\n",
      "Iteration 487, loss = 0.47428254\n",
      "Iteration 488, loss = 0.47445533\n",
      "Iteration 489, loss = 0.47477509\n",
      "Iteration 490, loss = 0.47429101\n",
      "Iteration 491, loss = 0.47471561\n",
      "Iteration 492, loss = 0.47427575\n",
      "Iteration 493, loss = 0.47479037\n",
      "Iteration 494, loss = 0.47433419\n",
      "Iteration 495, loss = 0.47434089\n",
      "Iteration 496, loss = 0.47434839\n",
      "Iteration 497, loss = 0.47440442\n",
      "Iteration 498, loss = 0.47402278\n",
      "Iteration 499, loss = 0.47429604\n",
      "Iteration 500, loss = 0.47398807\n",
      "Iteration 501, loss = 0.47445296\n",
      "Iteration 502, loss = 0.47437362\n",
      "Iteration 503, loss = 0.47404755\n",
      "Iteration 504, loss = 0.47411486\n",
      "Iteration 505, loss = 0.47426182\n",
      "Iteration 506, loss = 0.47391028\n",
      "Iteration 507, loss = 0.47394704\n",
      "Iteration 508, loss = 0.47390442\n",
      "Iteration 509, loss = 0.47388652\n",
      "Iteration 510, loss = 0.47404891\n",
      "Iteration 511, loss = 0.47399461\n",
      "Iteration 512, loss = 0.47394091\n",
      "Iteration 513, loss = 0.47377175\n",
      "Iteration 514, loss = 0.47381020\n",
      "Iteration 515, loss = 0.47408251\n",
      "Iteration 516, loss = 0.47385175\n",
      "Iteration 517, loss = 0.47376503\n",
      "Iteration 518, loss = 0.47393369\n",
      "Iteration 519, loss = 0.47392723\n",
      "Iteration 520, loss = 0.47362150\n",
      "Iteration 521, loss = 0.47377848\n",
      "Iteration 522, loss = 0.47359784\n",
      "Iteration 523, loss = 0.47372687\n",
      "Iteration 524, loss = 0.47367758\n",
      "Iteration 525, loss = 0.47347837\n",
      "Iteration 526, loss = 0.47368145\n",
      "Iteration 527, loss = 0.47360371\n",
      "Iteration 528, loss = 0.47445936\n",
      "Iteration 529, loss = 0.47403843\n",
      "Iteration 530, loss = 0.47364132\n",
      "Iteration 531, loss = 0.47361073\n",
      "Iteration 532, loss = 0.47364508\n",
      "Iteration 533, loss = 0.47338569\n",
      "Iteration 534, loss = 0.47367550\n",
      "Iteration 535, loss = 0.47325769\n",
      "Iteration 536, loss = 0.47356091\n",
      "Iteration 537, loss = 0.47334775\n",
      "Iteration 538, loss = 0.47348706\n",
      "Iteration 539, loss = 0.47349493\n",
      "Iteration 540, loss = 0.47362752\n",
      "Iteration 541, loss = 0.47334977\n",
      "Iteration 542, loss = 0.47326197\n",
      "Iteration 543, loss = 0.47326550\n",
      "Iteration 544, loss = 0.47343839\n",
      "Iteration 545, loss = 0.47292190\n",
      "Iteration 546, loss = 0.47371344\n",
      "Iteration 547, loss = 0.47297492\n",
      "Iteration 548, loss = 0.47346008\n",
      "Iteration 549, loss = 0.47348996\n",
      "Iteration 550, loss = 0.47311108\n",
      "Iteration 551, loss = 0.47317603\n",
      "Iteration 552, loss = 0.47325954\n",
      "Iteration 553, loss = 0.47292707\n",
      "Iteration 554, loss = 0.47299620\n",
      "Iteration 555, loss = 0.47329349\n",
      "Iteration 556, loss = 0.47295642\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.87112383\n",
      "Iteration 2, loss = 0.80056590\n",
      "Iteration 3, loss = 0.75416310\n",
      "Iteration 4, loss = 0.72141507\n",
      "Iteration 5, loss = 0.69711191\n",
      "Iteration 6, loss = 0.67764927\n",
      "Iteration 7, loss = 0.66100417\n",
      "Iteration 8, loss = 0.64670484\n",
      "Iteration 9, loss = 0.63407477\n",
      "Iteration 10, loss = 0.62254141\n",
      "Iteration 11, loss = 0.61237494\n",
      "Iteration 12, loss = 0.60378698\n",
      "Iteration 13, loss = 0.59619382\n",
      "Iteration 14, loss = 0.58985418\n",
      "Iteration 15, loss = 0.58440595\n",
      "Iteration 16, loss = 0.58024477\n",
      "Iteration 17, loss = 0.57645481\n",
      "Iteration 18, loss = 0.57332158\n",
      "Iteration 19, loss = 0.57044501\n",
      "Iteration 20, loss = 0.56835402\n",
      "Iteration 21, loss = 0.56599286\n",
      "Iteration 22, loss = 0.56403993\n",
      "Iteration 23, loss = 0.56212244\n",
      "Iteration 24, loss = 0.56061928\n",
      "Iteration 25, loss = 0.55918149\n",
      "Iteration 26, loss = 0.55768130\n",
      "Iteration 27, loss = 0.55625162\n",
      "Iteration 28, loss = 0.55496772\n",
      "Iteration 29, loss = 0.55404953\n",
      "Iteration 30, loss = 0.55263332\n",
      "Iteration 31, loss = 0.55128855\n",
      "Iteration 32, loss = 0.55027351\n",
      "Iteration 33, loss = 0.54916697\n",
      "Iteration 34, loss = 0.54789671\n",
      "Iteration 35, loss = 0.54685555\n",
      "Iteration 36, loss = 0.54590125\n",
      "Iteration 37, loss = 0.54491534\n",
      "Iteration 38, loss = 0.54397803\n",
      "Iteration 39, loss = 0.54290719\n",
      "Iteration 40, loss = 0.54192716\n",
      "Iteration 41, loss = 0.54112980\n",
      "Iteration 42, loss = 0.54017720\n",
      "Iteration 43, loss = 0.53908661\n",
      "Iteration 44, loss = 0.53848254\n",
      "Iteration 45, loss = 0.53757337\n",
      "Iteration 46, loss = 0.53673474\n",
      "Iteration 47, loss = 0.53600366\n",
      "Iteration 48, loss = 0.53523737\n",
      "Iteration 49, loss = 0.53422050\n",
      "Iteration 50, loss = 0.53348227\n",
      "Iteration 51, loss = 0.53254839\n",
      "Iteration 52, loss = 0.53179590\n",
      "Iteration 53, loss = 0.53101641\n",
      "Iteration 54, loss = 0.53020839\n",
      "Iteration 55, loss = 0.52951925\n",
      "Iteration 56, loss = 0.52871044\n",
      "Iteration 57, loss = 0.52803018\n",
      "Iteration 58, loss = 0.52732318\n",
      "Iteration 59, loss = 0.52670100\n",
      "Iteration 60, loss = 0.52576429\n",
      "Iteration 61, loss = 0.52537534\n",
      "Iteration 62, loss = 0.52456660\n",
      "Iteration 63, loss = 0.52408968\n",
      "Iteration 64, loss = 0.52318551\n",
      "Iteration 65, loss = 0.52265745\n",
      "Iteration 66, loss = 0.52225176\n",
      "Iteration 67, loss = 0.52123457\n",
      "Iteration 68, loss = 0.52082138\n",
      "Iteration 69, loss = 0.52018430\n",
      "Iteration 70, loss = 0.51952853\n",
      "Iteration 71, loss = 0.51890707\n",
      "Iteration 72, loss = 0.51827054\n",
      "Iteration 73, loss = 0.51774801\n",
      "Iteration 74, loss = 0.51732009\n",
      "Iteration 75, loss = 0.51654143\n",
      "Iteration 76, loss = 0.51634529\n",
      "Iteration 77, loss = 0.51576390\n",
      "Iteration 78, loss = 0.51492916\n",
      "Iteration 79, loss = 0.51456579\n",
      "Iteration 80, loss = 0.51361829\n",
      "Iteration 81, loss = 0.51330433\n",
      "Iteration 82, loss = 0.51267359\n",
      "Iteration 83, loss = 0.51211831\n",
      "Iteration 84, loss = 0.51178183\n",
      "Iteration 85, loss = 0.51126015\n",
      "Iteration 86, loss = 0.51043449\n",
      "Iteration 87, loss = 0.50989350\n",
      "Iteration 88, loss = 0.50954115\n",
      "Iteration 89, loss = 0.50880812\n",
      "Iteration 90, loss = 0.50855276\n",
      "Iteration 91, loss = 0.50804184\n",
      "Iteration 92, loss = 0.50735559\n",
      "Iteration 93, loss = 0.50692084\n",
      "Iteration 94, loss = 0.50670382\n",
      "Iteration 95, loss = 0.50645782\n",
      "Iteration 96, loss = 0.50553028\n",
      "Iteration 97, loss = 0.50529007\n",
      "Iteration 98, loss = 0.50487677\n",
      "Iteration 99, loss = 0.50450342\n",
      "Iteration 100, loss = 0.50425918\n",
      "Iteration 101, loss = 0.50377959\n",
      "Iteration 102, loss = 0.50378880\n",
      "Iteration 103, loss = 0.50301610\n",
      "Iteration 104, loss = 0.50282816\n",
      "Iteration 105, loss = 0.50250043\n",
      "Iteration 106, loss = 0.50226311\n",
      "Iteration 107, loss = 0.50206335\n",
      "Iteration 108, loss = 0.50179335\n",
      "Iteration 109, loss = 0.50160238\n",
      "Iteration 110, loss = 0.50140091\n",
      "Iteration 111, loss = 0.50116449\n",
      "Iteration 112, loss = 0.50071314\n",
      "Iteration 113, loss = 0.50060344\n",
      "Iteration 114, loss = 0.50043508\n",
      "Iteration 115, loss = 0.50038679\n",
      "Iteration 116, loss = 0.49995132\n",
      "Iteration 117, loss = 0.49994725\n",
      "Iteration 118, loss = 0.49985203\n",
      "Iteration 119, loss = 0.49975318\n",
      "Iteration 120, loss = 0.49892659\n",
      "Iteration 121, loss = 0.49902897\n",
      "Iteration 122, loss = 0.49841991\n",
      "Iteration 123, loss = 0.49843655\n",
      "Iteration 124, loss = 0.49800310\n",
      "Iteration 125, loss = 0.49809409\n",
      "Iteration 126, loss = 0.49810606\n",
      "Iteration 127, loss = 0.49759376\n",
      "Iteration 128, loss = 0.49726889\n",
      "Iteration 129, loss = 0.49733396\n",
      "Iteration 130, loss = 0.49669293\n",
      "Iteration 131, loss = 0.49681461\n",
      "Iteration 132, loss = 0.49652022\n",
      "Iteration 133, loss = 0.49632687\n",
      "Iteration 134, loss = 0.49617783\n",
      "Iteration 135, loss = 0.49615609\n",
      "Iteration 136, loss = 0.49592160\n",
      "Iteration 137, loss = 0.49601322\n",
      "Iteration 138, loss = 0.49549424\n",
      "Iteration 139, loss = 0.49545448\n",
      "Iteration 140, loss = 0.49517146\n",
      "Iteration 141, loss = 0.49545930\n",
      "Iteration 142, loss = 0.49500971\n",
      "Iteration 143, loss = 0.49479067\n",
      "Iteration 144, loss = 0.49471989\n",
      "Iteration 145, loss = 0.49458822\n",
      "Iteration 146, loss = 0.49450160\n",
      "Iteration 147, loss = 0.49417941\n",
      "Iteration 148, loss = 0.49423538\n",
      "Iteration 149, loss = 0.49394064\n",
      "Iteration 150, loss = 0.49417168\n",
      "Iteration 151, loss = 0.49381653\n",
      "Iteration 152, loss = 0.49337775\n",
      "Iteration 153, loss = 0.49337561\n",
      "Iteration 154, loss = 0.49329465\n",
      "Iteration 155, loss = 0.49306839\n",
      "Iteration 156, loss = 0.49304289\n",
      "Iteration 157, loss = 0.49293823\n",
      "Iteration 158, loss = 0.49264678\n",
      "Iteration 159, loss = 0.49257139\n",
      "Iteration 160, loss = 0.49260618\n",
      "Iteration 161, loss = 0.49222314\n",
      "Iteration 162, loss = 0.49241293\n",
      "Iteration 163, loss = 0.49213732\n",
      "Iteration 164, loss = 0.49198810\n",
      "Iteration 165, loss = 0.49183375\n",
      "Iteration 166, loss = 0.49162746\n",
      "Iteration 167, loss = 0.49166340\n",
      "Iteration 168, loss = 0.49144119\n",
      "Iteration 169, loss = 0.49152329\n",
      "Iteration 170, loss = 0.49116858\n",
      "Iteration 171, loss = 0.49118124\n",
      "Iteration 172, loss = 0.49084107\n",
      "Iteration 173, loss = 0.49090708\n",
      "Iteration 174, loss = 0.49085340\n",
      "Iteration 175, loss = 0.49056488\n",
      "Iteration 176, loss = 0.49036101\n",
      "Iteration 177, loss = 0.49016269\n",
      "Iteration 178, loss = 0.49023536\n",
      "Iteration 179, loss = 0.49022275\n",
      "Iteration 180, loss = 0.48974542\n",
      "Iteration 181, loss = 0.49009311\n",
      "Iteration 182, loss = 0.48997876\n",
      "Iteration 183, loss = 0.48948768\n",
      "Iteration 184, loss = 0.48959403\n",
      "Iteration 185, loss = 0.48935524\n",
      "Iteration 186, loss = 0.48937399\n",
      "Iteration 187, loss = 0.48926942\n",
      "Iteration 188, loss = 0.48922862\n",
      "Iteration 189, loss = 0.48923918\n",
      "Iteration 190, loss = 0.48879312\n",
      "Iteration 191, loss = 0.48868824\n",
      "Iteration 192, loss = 0.48837374\n",
      "Iteration 193, loss = 0.48887833\n",
      "Iteration 194, loss = 0.48820654\n",
      "Iteration 195, loss = 0.48834402\n",
      "Iteration 196, loss = 0.48835918\n",
      "Iteration 197, loss = 0.48787063\n",
      "Iteration 198, loss = 0.48762664\n",
      "Iteration 199, loss = 0.48763947\n",
      "Iteration 200, loss = 0.48792439\n",
      "Iteration 201, loss = 0.48734730\n",
      "Iteration 202, loss = 0.48748074\n",
      "Iteration 203, loss = 0.48706535\n",
      "Iteration 204, loss = 0.48746402\n",
      "Iteration 205, loss = 0.48687687\n",
      "Iteration 206, loss = 0.48696287\n",
      "Iteration 207, loss = 0.48659382\n",
      "Iteration 208, loss = 0.48657110\n",
      "Iteration 209, loss = 0.48645045\n",
      "Iteration 210, loss = 0.48639032\n",
      "Iteration 211, loss = 0.48611526\n",
      "Iteration 212, loss = 0.48621144\n",
      "Iteration 213, loss = 0.48683636\n",
      "Iteration 214, loss = 0.48591176\n",
      "Iteration 215, loss = 0.48582381\n",
      "Iteration 216, loss = 0.48572136\n",
      "Iteration 217, loss = 0.48547179\n",
      "Iteration 218, loss = 0.48542906\n",
      "Iteration 219, loss = 0.48503035\n",
      "Iteration 220, loss = 0.48484905\n",
      "Iteration 221, loss = 0.48481943\n",
      "Iteration 222, loss = 0.48512367\n",
      "Iteration 223, loss = 0.48498856\n",
      "Iteration 224, loss = 0.48451910\n",
      "Iteration 225, loss = 0.48447313\n",
      "Iteration 226, loss = 0.48446778\n",
      "Iteration 227, loss = 0.48417529\n",
      "Iteration 228, loss = 0.48435549\n",
      "Iteration 229, loss = 0.48424432\n",
      "Iteration 230, loss = 0.48389638\n",
      "Iteration 231, loss = 0.48385839\n",
      "Iteration 232, loss = 0.48380306\n",
      "Iteration 233, loss = 0.48361215\n",
      "Iteration 234, loss = 0.48372384\n",
      "Iteration 235, loss = 0.48335741\n",
      "Iteration 236, loss = 0.48343311\n",
      "Iteration 237, loss = 0.48314755\n",
      "Iteration 238, loss = 0.48291695\n",
      "Iteration 239, loss = 0.48290290\n",
      "Iteration 240, loss = 0.48261149\n",
      "Iteration 241, loss = 0.48271690\n",
      "Iteration 242, loss = 0.48248619\n",
      "Iteration 243, loss = 0.48251967\n",
      "Iteration 244, loss = 0.48239010\n",
      "Iteration 245, loss = 0.48234424\n",
      "Iteration 246, loss = 0.48233014\n",
      "Iteration 247, loss = 0.48251770\n",
      "Iteration 248, loss = 0.48206977\n",
      "Iteration 249, loss = 0.48186009\n",
      "Iteration 250, loss = 0.48167247\n",
      "Iteration 251, loss = 0.48179782\n",
      "Iteration 252, loss = 0.48159447\n",
      "Iteration 253, loss = 0.48187669\n",
      "Iteration 254, loss = 0.48153245\n",
      "Iteration 255, loss = 0.48197111\n",
      "Iteration 256, loss = 0.48162715\n",
      "Iteration 257, loss = 0.48139822\n",
      "Iteration 258, loss = 0.48124822\n",
      "Iteration 259, loss = 0.48121008\n",
      "Iteration 260, loss = 0.48100039\n",
      "Iteration 261, loss = 0.48114013\n",
      "Iteration 262, loss = 0.48148318\n",
      "Iteration 263, loss = 0.48066851\n",
      "Iteration 264, loss = 0.48100765\n",
      "Iteration 265, loss = 0.48076712\n",
      "Iteration 266, loss = 0.48125810\n",
      "Iteration 267, loss = 0.48069210\n",
      "Iteration 268, loss = 0.48053351\n",
      "Iteration 269, loss = 0.48040500\n",
      "Iteration 270, loss = 0.48031259\n",
      "Iteration 271, loss = 0.48031350\n",
      "Iteration 272, loss = 0.48018498\n",
      "Iteration 273, loss = 0.48016655\n",
      "Iteration 274, loss = 0.47996341\n",
      "Iteration 275, loss = 0.48000962\n",
      "Iteration 276, loss = 0.47982483\n",
      "Iteration 277, loss = 0.48064240\n",
      "Iteration 278, loss = 0.47962598\n",
      "Iteration 279, loss = 0.47969520\n",
      "Iteration 280, loss = 0.47999491\n",
      "Iteration 281, loss = 0.47974628\n",
      "Iteration 282, loss = 0.47935137\n",
      "Iteration 283, loss = 0.47947538\n",
      "Iteration 284, loss = 0.47932225\n",
      "Iteration 285, loss = 0.47929224\n",
      "Iteration 286, loss = 0.47902487\n",
      "Iteration 287, loss = 0.47908357\n",
      "Iteration 288, loss = 0.47891391\n",
      "Iteration 289, loss = 0.47884470\n",
      "Iteration 290, loss = 0.47902215\n",
      "Iteration 291, loss = 0.47883483\n",
      "Iteration 292, loss = 0.47875424\n",
      "Iteration 293, loss = 0.47919300\n",
      "Iteration 294, loss = 0.47869380\n",
      "Iteration 295, loss = 0.47864621\n",
      "Iteration 296, loss = 0.47846488\n",
      "Iteration 297, loss = 0.47844941\n",
      "Iteration 298, loss = 0.47834580\n",
      "Iteration 299, loss = 0.47815594\n",
      "Iteration 300, loss = 0.47809671\n",
      "Iteration 301, loss = 0.47846912\n",
      "Iteration 302, loss = 0.47840985\n",
      "Iteration 303, loss = 0.47839641\n",
      "Iteration 304, loss = 0.47769056\n",
      "Iteration 305, loss = 0.47814896\n",
      "Iteration 306, loss = 0.47777713\n",
      "Iteration 307, loss = 0.47768938\n",
      "Iteration 308, loss = 0.47766376\n",
      "Iteration 309, loss = 0.47773474\n",
      "Iteration 310, loss = 0.47748233\n",
      "Iteration 311, loss = 0.47737500\n",
      "Iteration 312, loss = 0.47735183\n",
      "Iteration 313, loss = 0.47738346\n",
      "Iteration 314, loss = 0.47748128\n",
      "Iteration 315, loss = 0.47724537\n",
      "Iteration 316, loss = 0.47739878\n",
      "Iteration 317, loss = 0.47741860\n",
      "Iteration 318, loss = 0.47732129\n",
      "Iteration 319, loss = 0.47703692\n",
      "Iteration 320, loss = 0.47687351\n",
      "Iteration 321, loss = 0.47690799\n",
      "Iteration 322, loss = 0.47705825\n",
      "Iteration 323, loss = 0.47695812\n",
      "Iteration 324, loss = 0.47750062\n",
      "Iteration 325, loss = 0.47695474\n",
      "Iteration 326, loss = 0.47669085\n",
      "Iteration 327, loss = 0.47664316\n",
      "Iteration 328, loss = 0.47668213\n",
      "Iteration 329, loss = 0.47631364\n",
      "Iteration 330, loss = 0.47631681\n",
      "Iteration 331, loss = 0.47644931\n",
      "Iteration 332, loss = 0.47645640\n",
      "Iteration 333, loss = 0.47671196\n",
      "Iteration 334, loss = 0.47616317\n",
      "Iteration 335, loss = 0.47618767\n",
      "Iteration 336, loss = 0.47623252\n",
      "Iteration 337, loss = 0.47608959\n",
      "Iteration 338, loss = 0.47593271\n",
      "Iteration 339, loss = 0.47593786\n",
      "Iteration 340, loss = 0.47575158\n",
      "Iteration 341, loss = 0.47603431\n",
      "Iteration 342, loss = 0.47562898\n",
      "Iteration 343, loss = 0.47564596\n",
      "Iteration 344, loss = 0.47581501\n",
      "Iteration 345, loss = 0.47562739\n",
      "Iteration 346, loss = 0.47553055\n",
      "Iteration 347, loss = 0.47524372\n",
      "Iteration 348, loss = 0.47537374\n",
      "Iteration 349, loss = 0.47534855\n",
      "Iteration 350, loss = 0.47510551\n",
      "Iteration 351, loss = 0.47550059\n",
      "Iteration 352, loss = 0.47548826\n",
      "Iteration 353, loss = 0.47480751\n",
      "Iteration 354, loss = 0.47535228\n",
      "Iteration 355, loss = 0.47495886\n",
      "Iteration 356, loss = 0.47479675\n",
      "Iteration 357, loss = 0.47515061\n",
      "Iteration 358, loss = 0.47457746\n",
      "Iteration 359, loss = 0.47449011\n",
      "Iteration 360, loss = 0.47447131\n",
      "Iteration 361, loss = 0.47465734\n",
      "Iteration 362, loss = 0.47464447\n",
      "Iteration 363, loss = 0.47494789\n",
      "Iteration 364, loss = 0.47454213\n",
      "Iteration 365, loss = 0.47461638\n",
      "Iteration 366, loss = 0.47395163\n",
      "Iteration 367, loss = 0.47435375\n",
      "Iteration 368, loss = 0.47420468\n",
      "Iteration 369, loss = 0.47414565\n",
      "Iteration 370, loss = 0.47422450\n",
      "Iteration 371, loss = 0.47405806\n",
      "Iteration 372, loss = 0.47370837\n",
      "Iteration 373, loss = 0.47374436\n",
      "Iteration 374, loss = 0.47374404\n",
      "Iteration 375, loss = 0.47389262\n",
      "Iteration 376, loss = 0.47362113\n",
      "Iteration 377, loss = 0.47356366\n",
      "Iteration 378, loss = 0.47356534\n",
      "Iteration 379, loss = 0.47343718\n",
      "Iteration 380, loss = 0.47329893\n",
      "Iteration 381, loss = 0.47324905\n",
      "Iteration 382, loss = 0.47312606\n",
      "Iteration 383, loss = 0.47310930\n",
      "Iteration 384, loss = 0.47277272\n",
      "Iteration 385, loss = 0.47298302\n",
      "Iteration 386, loss = 0.47301792\n",
      "Iteration 387, loss = 0.47312099\n",
      "Iteration 388, loss = 0.47279080\n",
      "Iteration 389, loss = 0.47288574\n",
      "Iteration 390, loss = 0.47275667\n",
      "Iteration 391, loss = 0.47256770\n",
      "Iteration 392, loss = 0.47272730\n",
      "Iteration 393, loss = 0.47299162\n",
      "Iteration 394, loss = 0.47255100\n",
      "Iteration 395, loss = 0.47227177\n",
      "Iteration 396, loss = 0.47240922\n",
      "Iteration 397, loss = 0.47214321\n",
      "Iteration 398, loss = 0.47269032\n",
      "Iteration 399, loss = 0.47215116\n",
      "Iteration 400, loss = 0.47221290\n",
      "Iteration 401, loss = 0.47201256\n",
      "Iteration 402, loss = 0.47221681\n",
      "Iteration 403, loss = 0.47215828\n",
      "Iteration 404, loss = 0.47221408\n",
      "Iteration 405, loss = 0.47253638\n",
      "Iteration 406, loss = 0.47215476\n",
      "Iteration 407, loss = 0.47186021\n",
      "Iteration 408, loss = 0.47172506\n",
      "Iteration 409, loss = 0.47160023\n",
      "Iteration 410, loss = 0.47155602\n",
      "Iteration 411, loss = 0.47143029\n",
      "Iteration 412, loss = 0.47147230\n",
      "Iteration 413, loss = 0.47151350\n",
      "Iteration 414, loss = 0.47211704\n",
      "Iteration 415, loss = 0.47143429\n",
      "Iteration 416, loss = 0.47119099\n",
      "Iteration 417, loss = 0.47112713\n",
      "Iteration 418, loss = 0.47101303\n",
      "Iteration 419, loss = 0.47113579\n",
      "Iteration 420, loss = 0.47145002\n",
      "Iteration 421, loss = 0.47188915\n",
      "Iteration 422, loss = 0.47091288\n",
      "Iteration 423, loss = 0.47095080\n",
      "Iteration 424, loss = 0.47086944\n",
      "Iteration 425, loss = 0.47097647\n",
      "Iteration 426, loss = 0.47064762\n",
      "Iteration 427, loss = 0.47128304\n",
      "Iteration 428, loss = 0.47064124\n",
      "Iteration 429, loss = 0.47080269\n",
      "Iteration 430, loss = 0.47062126\n",
      "Iteration 431, loss = 0.47057877\n",
      "Iteration 432, loss = 0.47041134\n",
      "Iteration 433, loss = 0.47031897\n",
      "Iteration 434, loss = 0.47053147\n",
      "Iteration 435, loss = 0.47058032\n",
      "Iteration 436, loss = 0.47076682\n",
      "Iteration 437, loss = 0.47007044\n",
      "Iteration 438, loss = 0.47034632\n",
      "Iteration 439, loss = 0.47039041\n",
      "Iteration 440, loss = 0.47050845\n",
      "Iteration 441, loss = 0.47041164\n",
      "Iteration 442, loss = 0.47015068\n",
      "Iteration 443, loss = 0.46997455\n",
      "Iteration 444, loss = 0.47000659\n",
      "Iteration 445, loss = 0.46995964\n",
      "Iteration 446, loss = 0.46977796\n",
      "Iteration 447, loss = 0.46986733\n",
      "Iteration 448, loss = 0.46985356\n",
      "Iteration 449, loss = 0.46995058\n",
      "Iteration 450, loss = 0.46955351\n",
      "Iteration 451, loss = 0.46969046\n",
      "Iteration 452, loss = 0.46945258\n",
      "Iteration 453, loss = 0.46960152\n",
      "Iteration 454, loss = 0.46951128\n",
      "Iteration 455, loss = 0.46944446\n",
      "Iteration 456, loss = 0.46951980\n",
      "Iteration 457, loss = 0.46944792\n",
      "Iteration 458, loss = 0.46952167\n",
      "Iteration 459, loss = 0.46918102\n",
      "Iteration 460, loss = 0.46928953\n",
      "Iteration 461, loss = 0.46925312\n",
      "Iteration 462, loss = 0.46941442\n",
      "Iteration 463, loss = 0.46903957\n",
      "Iteration 464, loss = 0.46931500\n",
      "Iteration 465, loss = 0.46923100\n",
      "Iteration 466, loss = 0.46878390\n",
      "Iteration 467, loss = 0.46893340\n",
      "Iteration 468, loss = 0.46909520\n",
      "Iteration 469, loss = 0.46912840\n",
      "Iteration 470, loss = 0.46895058\n",
      "Iteration 471, loss = 0.46879399\n",
      "Iteration 472, loss = 0.46869743\n",
      "Iteration 473, loss = 0.46874559\n",
      "Iteration 474, loss = 0.46889238\n",
      "Iteration 475, loss = 0.46921656\n",
      "Iteration 476, loss = 0.46969177\n",
      "Iteration 477, loss = 0.46873036\n",
      "Iteration 478, loss = 0.46869275\n",
      "Iteration 479, loss = 0.46864454\n",
      "Iteration 480, loss = 0.46830135\n",
      "Iteration 481, loss = 0.46837630\n",
      "Iteration 482, loss = 0.46835980\n",
      "Iteration 483, loss = 0.46819606\n",
      "Iteration 484, loss = 0.46823219\n",
      "Iteration 485, loss = 0.46824479\n",
      "Iteration 486, loss = 0.46860743\n",
      "Iteration 487, loss = 0.46821313\n",
      "Iteration 488, loss = 0.46832184\n",
      "Iteration 489, loss = 0.46823882\n",
      "Iteration 490, loss = 0.46805380\n",
      "Iteration 491, loss = 0.46808905\n",
      "Iteration 492, loss = 0.46790330\n",
      "Iteration 493, loss = 0.46788826\n",
      "Iteration 494, loss = 0.46809045\n",
      "Iteration 495, loss = 0.46798799\n",
      "Iteration 496, loss = 0.46767084\n",
      "Iteration 497, loss = 0.46777601\n",
      "Iteration 498, loss = 0.46764461\n",
      "Iteration 499, loss = 0.46760943\n",
      "Iteration 500, loss = 0.46764680\n",
      "Iteration 501, loss = 0.46744693\n",
      "Iteration 502, loss = 0.46757858\n",
      "Iteration 503, loss = 0.46777368\n",
      "Iteration 504, loss = 0.46751882\n",
      "Iteration 505, loss = 0.46765225\n",
      "Iteration 506, loss = 0.46727798\n",
      "Iteration 507, loss = 0.46733877\n",
      "Iteration 508, loss = 0.46749615\n",
      "Iteration 509, loss = 0.46704703\n",
      "Iteration 510, loss = 0.46733432\n",
      "Iteration 511, loss = 0.46713677\n",
      "Iteration 512, loss = 0.46733923\n",
      "Iteration 513, loss = 0.46768292\n",
      "Iteration 514, loss = 0.46722073\n",
      "Iteration 515, loss = 0.46691803\n",
      "Iteration 516, loss = 0.46704692\n",
      "Iteration 517, loss = 0.46696233\n",
      "Iteration 518, loss = 0.46698012\n",
      "Iteration 519, loss = 0.46691993\n",
      "Iteration 520, loss = 0.46703491\n",
      "Iteration 521, loss = 0.46724103\n",
      "Iteration 522, loss = 0.46736748\n",
      "Iteration 523, loss = 0.46662170\n",
      "Iteration 524, loss = 0.46717623\n",
      "Iteration 525, loss = 0.46686495\n",
      "Iteration 526, loss = 0.46766083\n",
      "Iteration 527, loss = 0.46700229\n",
      "Iteration 528, loss = 0.46660886\n",
      "Iteration 529, loss = 0.46678711\n",
      "Iteration 530, loss = 0.46657627\n",
      "Iteration 531, loss = 0.46658693\n",
      "Iteration 532, loss = 0.46666034\n",
      "Iteration 533, loss = 0.46656521\n",
      "Iteration 534, loss = 0.46632666\n",
      "Iteration 535, loss = 0.46671528\n",
      "Iteration 536, loss = 0.46681617\n",
      "Iteration 537, loss = 0.46681306\n",
      "Iteration 538, loss = 0.46638913\n",
      "Iteration 539, loss = 0.46600170\n",
      "Iteration 540, loss = 0.46626161\n",
      "Iteration 541, loss = 0.46641141\n",
      "Iteration 542, loss = 0.46592477\n",
      "Iteration 543, loss = 0.46628363\n",
      "Iteration 544, loss = 0.46588181\n",
      "Iteration 545, loss = 0.46563364\n",
      "Iteration 546, loss = 0.46591371\n",
      "Iteration 547, loss = 0.46611855\n",
      "Iteration 548, loss = 0.46609758\n",
      "Iteration 549, loss = 0.46606069\n",
      "Iteration 550, loss = 0.46574984\n",
      "Iteration 551, loss = 0.46573149\n",
      "Iteration 552, loss = 0.46572325\n",
      "Iteration 553, loss = 0.46614208\n",
      "Iteration 554, loss = 0.46567692\n",
      "Iteration 555, loss = 0.46542450\n",
      "Iteration 556, loss = 0.46619489\n",
      "Iteration 557, loss = 0.46556269\n",
      "Iteration 558, loss = 0.46558692\n",
      "Iteration 559, loss = 0.46529929\n",
      "Iteration 560, loss = 0.46547062\n",
      "Iteration 561, loss = 0.46573564\n",
      "Iteration 562, loss = 0.46639928\n",
      "Iteration 563, loss = 0.46547858\n",
      "Iteration 564, loss = 0.46518462\n",
      "Iteration 565, loss = 0.46545101\n",
      "Iteration 566, loss = 0.46510306\n",
      "Iteration 567, loss = 0.46501101\n",
      "Iteration 568, loss = 0.46528268\n",
      "Iteration 569, loss = 0.46478167\n",
      "Iteration 570, loss = 0.46480089\n",
      "Iteration 571, loss = 0.46483921\n",
      "Iteration 572, loss = 0.46507505\n",
      "Iteration 573, loss = 0.46504729\n",
      "Iteration 574, loss = 0.46499267\n",
      "Iteration 575, loss = 0.46469653\n",
      "Iteration 576, loss = 0.46505479\n",
      "Iteration 577, loss = 0.46478797\n",
      "Iteration 578, loss = 0.46450155\n",
      "Iteration 579, loss = 0.46437056\n",
      "Iteration 580, loss = 0.46449745\n",
      "Iteration 581, loss = 0.46464216\n",
      "Iteration 582, loss = 0.46448746\n",
      "Iteration 583, loss = 0.46455251\n",
      "Iteration 584, loss = 0.46414089\n",
      "Iteration 585, loss = 0.46460343\n",
      "Iteration 586, loss = 0.46481661\n",
      "Iteration 587, loss = 0.46453694\n",
      "Iteration 588, loss = 0.46428444\n",
      "Iteration 589, loss = 0.46400628\n",
      "Iteration 590, loss = 0.46416683\n",
      "Iteration 591, loss = 0.46410012\n",
      "Iteration 592, loss = 0.46415525\n",
      "Iteration 593, loss = 0.46425859\n",
      "Iteration 594, loss = 0.46393071\n",
      "Iteration 595, loss = 0.46393697\n",
      "Iteration 596, loss = 0.46406599\n",
      "Iteration 597, loss = 0.46376721\n",
      "Iteration 598, loss = 0.46376409\n",
      "Iteration 599, loss = 0.46435981\n",
      "Iteration 600, loss = 0.46354769\n",
      "Iteration 601, loss = 0.46355028\n",
      "Iteration 602, loss = 0.46376671\n",
      "Iteration 603, loss = 0.46370357\n",
      "Iteration 604, loss = 0.46420190\n",
      "Iteration 605, loss = 0.46352809\n",
      "Iteration 606, loss = 0.46379542\n",
      "Iteration 607, loss = 0.46347406\n",
      "Iteration 608, loss = 0.46382772\n",
      "Iteration 609, loss = 0.46370389\n",
      "Iteration 610, loss = 0.46323157\n",
      "Iteration 611, loss = 0.46333700\n",
      "Iteration 612, loss = 0.46324752\n",
      "Iteration 613, loss = 0.46293859\n",
      "Iteration 614, loss = 0.46328239\n",
      "Iteration 615, loss = 0.46326195\n",
      "Iteration 616, loss = 0.46277697\n",
      "Iteration 617, loss = 0.46317741\n",
      "Iteration 618, loss = 0.46303202\n",
      "Iteration 619, loss = 0.46318360\n",
      "Iteration 620, loss = 0.46291653\n",
      "Iteration 621, loss = 0.46322151\n",
      "Iteration 622, loss = 0.46278698\n",
      "Iteration 623, loss = 0.46309546\n",
      "Iteration 624, loss = 0.46309536\n",
      "Iteration 625, loss = 0.46245761\n",
      "Iteration 626, loss = 0.46300382\n",
      "Iteration 627, loss = 0.46265463\n",
      "Iteration 628, loss = 0.46254924\n",
      "Iteration 629, loss = 0.46254583\n",
      "Iteration 630, loss = 0.46243162\n",
      "Iteration 631, loss = 0.46226040\n",
      "Iteration 632, loss = 0.46205275\n",
      "Iteration 633, loss = 0.46224553\n",
      "Iteration 634, loss = 0.46226333\n",
      "Iteration 635, loss = 0.46233337\n",
      "Iteration 636, loss = 0.46224398\n",
      "Iteration 637, loss = 0.46211692\n",
      "Iteration 638, loss = 0.46190387\n",
      "Iteration 639, loss = 0.46207183\n",
      "Iteration 640, loss = 0.46225722\n",
      "Iteration 641, loss = 0.46208038\n",
      "Iteration 642, loss = 0.46196412\n",
      "Iteration 643, loss = 0.46180854\n",
      "Iteration 644, loss = 0.46191950\n",
      "Iteration 645, loss = 0.46175016\n",
      "Iteration 646, loss = 0.46177791\n",
      "Iteration 647, loss = 0.46191035\n",
      "Iteration 648, loss = 0.46144186\n",
      "Iteration 649, loss = 0.46146125\n",
      "Iteration 650, loss = 0.46187776\n",
      "Iteration 651, loss = 0.46134030\n",
      "Iteration 652, loss = 0.46160826\n",
      "Iteration 653, loss = 0.46157433\n",
      "Iteration 654, loss = 0.46203786\n",
      "Iteration 655, loss = 0.46140478\n",
      "Iteration 656, loss = 0.46132914\n",
      "Iteration 657, loss = 0.46113351\n",
      "Iteration 658, loss = 0.46140144\n",
      "Iteration 659, loss = 0.46134246\n",
      "Iteration 660, loss = 0.46125084\n",
      "Iteration 661, loss = 0.46143400\n",
      "Iteration 662, loss = 0.46124761\n",
      "Iteration 663, loss = 0.46143727\n",
      "Iteration 664, loss = 0.46135743\n",
      "Iteration 665, loss = 0.46113306\n",
      "Iteration 666, loss = 0.46085242\n",
      "Iteration 667, loss = 0.46123736\n",
      "Iteration 668, loss = 0.46085765\n",
      "Iteration 669, loss = 0.46083757\n",
      "Iteration 670, loss = 0.46069754\n",
      "Iteration 671, loss = 0.46083239\n",
      "Iteration 672, loss = 0.46092535\n",
      "Iteration 673, loss = 0.46138641\n",
      "Iteration 674, loss = 0.46054358\n",
      "Iteration 675, loss = 0.46071931\n",
      "Iteration 676, loss = 0.46097029\n",
      "Iteration 677, loss = 0.46060642\n",
      "Iteration 678, loss = 0.46059458\n",
      "Iteration 679, loss = 0.46058909\n",
      "Iteration 680, loss = 0.46060943\n",
      "Iteration 681, loss = 0.46043915\n",
      "Iteration 682, loss = 0.46053161\n",
      "Iteration 683, loss = 0.46042007\n",
      "Iteration 684, loss = 0.46053179\n",
      "Iteration 685, loss = 0.46042425\n",
      "Iteration 686, loss = 0.46100596\n",
      "Iteration 687, loss = 0.46037504\n",
      "Iteration 688, loss = 0.46019354\n",
      "Iteration 689, loss = 0.46043291\n",
      "Iteration 690, loss = 0.46022589\n",
      "Iteration 691, loss = 0.46017519\n",
      "Iteration 692, loss = 0.46003025\n",
      "Iteration 693, loss = 0.45983569\n",
      "Iteration 694, loss = 0.45993122\n",
      "Iteration 695, loss = 0.46020157\n",
      "Iteration 696, loss = 0.45998509\n",
      "Iteration 697, loss = 0.45983836\n",
      "Iteration 698, loss = 0.46001947\n",
      "Iteration 699, loss = 0.45976747\n",
      "Iteration 700, loss = 0.46011620\n",
      "Iteration 701, loss = 0.45983954\n",
      "Iteration 702, loss = 0.45972179\n",
      "Iteration 703, loss = 0.45987080\n",
      "Iteration 704, loss = 0.45977153\n",
      "Iteration 705, loss = 0.45965352\n",
      "Iteration 706, loss = 0.46019177\n",
      "Iteration 707, loss = 0.45965706\n",
      "Iteration 708, loss = 0.45954385\n",
      "Iteration 709, loss = 0.45978463\n",
      "Iteration 710, loss = 0.45984483\n",
      "Iteration 711, loss = 0.46003781\n",
      "Iteration 712, loss = 0.46026276\n",
      "Iteration 713, loss = 0.45983637\n",
      "Iteration 714, loss = 0.45992643\n",
      "Iteration 715, loss = 0.45997812\n",
      "Iteration 716, loss = 0.45931680\n",
      "Iteration 717, loss = 0.45964905\n",
      "Iteration 718, loss = 0.45946100\n",
      "Iteration 719, loss = 0.45931086\n",
      "Iteration 720, loss = 0.45926055\n",
      "Iteration 721, loss = 0.45921045\n",
      "Iteration 722, loss = 0.45925345\n",
      "Iteration 723, loss = 0.45912966\n",
      "Iteration 724, loss = 0.45937876\n",
      "Iteration 725, loss = 0.45918119\n",
      "Iteration 726, loss = 0.45914048\n",
      "Iteration 727, loss = 0.45892669\n",
      "Iteration 728, loss = 0.45900532\n",
      "Iteration 729, loss = 0.45949076\n",
      "Iteration 730, loss = 0.45879240\n",
      "Iteration 731, loss = 0.45907672\n",
      "Iteration 732, loss = 0.45888638\n",
      "Iteration 733, loss = 0.45905741\n",
      "Iteration 734, loss = 0.45878315\n",
      "Iteration 735, loss = 0.45854024\n",
      "Iteration 736, loss = 0.45877244\n",
      "Iteration 737, loss = 0.45879904\n",
      "Iteration 738, loss = 0.45841888\n",
      "Iteration 739, loss = 0.45877835\n",
      "Iteration 740, loss = 0.45832403\n",
      "Iteration 741, loss = 0.45904912\n",
      "Iteration 742, loss = 0.45830001\n",
      "Iteration 743, loss = 0.45844684\n",
      "Iteration 744, loss = 0.45841990\n",
      "Iteration 745, loss = 0.45826356\n",
      "Iteration 746, loss = 0.45829070\n",
      "Iteration 747, loss = 0.45844880\n",
      "Iteration 748, loss = 0.45811854\n",
      "Iteration 749, loss = 0.45827891\n",
      "Iteration 750, loss = 0.45815414\n",
      "Iteration 751, loss = 0.45816189\n",
      "Iteration 752, loss = 0.45802868\n",
      "Iteration 753, loss = 0.45801128\n",
      "Iteration 754, loss = 0.45773139\n",
      "Iteration 755, loss = 0.45821302\n",
      "Iteration 756, loss = 0.45793629\n",
      "Iteration 757, loss = 0.45802245\n",
      "Iteration 758, loss = 0.45808970\n",
      "Iteration 759, loss = 0.45834571\n",
      "Iteration 760, loss = 0.45800009\n",
      "Iteration 761, loss = 0.45777843\n",
      "Iteration 762, loss = 0.45751354\n",
      "Iteration 763, loss = 0.45797597\n",
      "Iteration 764, loss = 0.45758795\n",
      "Iteration 765, loss = 0.45782962\n",
      "Iteration 766, loss = 0.45781354\n",
      "Iteration 767, loss = 0.45751068\n",
      "Iteration 768, loss = 0.45751027\n",
      "Iteration 769, loss = 0.45786452\n",
      "Iteration 770, loss = 0.45756437\n",
      "Iteration 771, loss = 0.45779033\n",
      "Iteration 772, loss = 0.45760887\n",
      "Iteration 773, loss = 0.45870621\n",
      "Iteration 774, loss = 0.45752457\n",
      "Iteration 775, loss = 0.45767002\n",
      "Iteration 776, loss = 0.45726561\n",
      "Iteration 777, loss = 0.45727600\n",
      "Iteration 778, loss = 0.45722960\n",
      "Iteration 779, loss = 0.45703055\n",
      "Iteration 780, loss = 0.45696918\n",
      "Iteration 781, loss = 0.45705223\n",
      "Iteration 782, loss = 0.45697088\n",
      "Iteration 783, loss = 0.45761108\n",
      "Iteration 784, loss = 0.45738751\n",
      "Iteration 785, loss = 0.45683146\n",
      "Iteration 786, loss = 0.45703217\n",
      "Iteration 787, loss = 0.45711433\n",
      "Iteration 788, loss = 0.45699070\n",
      "Iteration 789, loss = 0.45683707\n",
      "Iteration 790, loss = 0.45687399\n",
      "Iteration 791, loss = 0.45688040\n",
      "Iteration 792, loss = 0.45670221\n",
      "Iteration 793, loss = 0.45690104\n",
      "Iteration 794, loss = 0.45661695\n",
      "Iteration 795, loss = 0.45725151\n",
      "Iteration 796, loss = 0.45667954\n",
      "Iteration 797, loss = 0.45673215\n",
      "Iteration 798, loss = 0.45676953\n",
      "Iteration 799, loss = 0.45666123\n",
      "Iteration 800, loss = 0.45673603\n",
      "Iteration 801, loss = 0.45721619\n",
      "Iteration 802, loss = 0.45686802\n",
      "Iteration 803, loss = 0.45690085\n",
      "Iteration 804, loss = 0.45651110\n",
      "Iteration 805, loss = 0.45653831\n",
      "Iteration 806, loss = 0.45662818\n",
      "Iteration 807, loss = 0.45708502\n",
      "Iteration 808, loss = 0.45652271\n",
      "Iteration 809, loss = 0.45644782\n",
      "Iteration 810, loss = 0.45644316\n",
      "Iteration 811, loss = 0.45654869\n",
      "Iteration 812, loss = 0.45672909\n",
      "Iteration 813, loss = 0.45650335\n",
      "Iteration 814, loss = 0.45702927\n",
      "Iteration 815, loss = 0.45706669\n",
      "Iteration 816, loss = 0.45606001\n",
      "Iteration 817, loss = 0.45668686\n",
      "Iteration 818, loss = 0.45612677\n",
      "Iteration 819, loss = 0.45649084\n",
      "Iteration 820, loss = 0.45627493\n",
      "Iteration 821, loss = 0.45627660\n",
      "Iteration 822, loss = 0.45600909\n",
      "Iteration 823, loss = 0.45629859\n",
      "Iteration 824, loss = 0.45611669\n",
      "Iteration 825, loss = 0.45620932\n",
      "Iteration 826, loss = 0.45608940\n",
      "Iteration 827, loss = 0.45650141\n",
      "Iteration 828, loss = 0.45586877\n",
      "Iteration 829, loss = 0.45577057\n",
      "Iteration 830, loss = 0.45624503\n",
      "Iteration 831, loss = 0.45600429\n",
      "Iteration 832, loss = 0.45588342\n",
      "Iteration 833, loss = 0.45587069\n",
      "Iteration 834, loss = 0.45586361\n",
      "Iteration 835, loss = 0.45594207\n",
      "Iteration 836, loss = 0.45592678\n",
      "Iteration 837, loss = 0.45574111\n",
      "Iteration 838, loss = 0.45561395\n",
      "Iteration 839, loss = 0.45588064\n",
      "Iteration 840, loss = 0.45596111\n",
      "Iteration 841, loss = 0.45582412\n",
      "Iteration 842, loss = 0.45566565\n",
      "Iteration 843, loss = 0.45605656\n",
      "Iteration 844, loss = 0.45549150\n",
      "Iteration 845, loss = 0.45566265\n",
      "Iteration 846, loss = 0.45641489\n",
      "Iteration 847, loss = 0.45593032\n",
      "Iteration 848, loss = 0.45576257\n",
      "Iteration 849, loss = 0.45556926\n",
      "Iteration 850, loss = 0.45569859\n",
      "Iteration 851, loss = 0.45564437\n",
      "Iteration 852, loss = 0.45539674\n",
      "Iteration 853, loss = 0.45574920\n",
      "Iteration 854, loss = 0.45534761\n",
      "Iteration 855, loss = 0.45539807\n",
      "Iteration 856, loss = 0.45552298\n",
      "Iteration 857, loss = 0.45552320\n",
      "Iteration 858, loss = 0.45554615\n",
      "Iteration 859, loss = 0.45556517\n",
      "Iteration 860, loss = 0.45553993\n",
      "Iteration 861, loss = 0.45562882\n",
      "Iteration 862, loss = 0.45581247\n",
      "Iteration 863, loss = 0.45582851\n",
      "Iteration 864, loss = 0.45548175\n",
      "Iteration 865, loss = 0.45537707\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80157461\n",
      "Iteration 2, loss = 0.75511905\n",
      "Iteration 3, loss = 0.72349577\n",
      "Iteration 4, loss = 0.70224252\n",
      "Iteration 5, loss = 0.68674077\n",
      "Iteration 6, loss = 0.67308483\n",
      "Iteration 7, loss = 0.66086875\n",
      "Iteration 8, loss = 0.64915328\n",
      "Iteration 9, loss = 0.63751640\n",
      "Iteration 10, loss = 0.62581357\n",
      "Iteration 11, loss = 0.61455073\n",
      "Iteration 12, loss = 0.60356794\n",
      "Iteration 13, loss = 0.59339523\n",
      "Iteration 14, loss = 0.58446020\n",
      "Iteration 15, loss = 0.57659649\n",
      "Iteration 16, loss = 0.57013973\n",
      "Iteration 17, loss = 0.56475713\n",
      "Iteration 18, loss = 0.56039425\n",
      "Iteration 19, loss = 0.55667809\n",
      "Iteration 20, loss = 0.55331204\n",
      "Iteration 21, loss = 0.55036970\n",
      "Iteration 22, loss = 0.54808996\n",
      "Iteration 23, loss = 0.54576636\n",
      "Iteration 24, loss = 0.54364628\n",
      "Iteration 25, loss = 0.54146006\n",
      "Iteration 26, loss = 0.53964204\n",
      "Iteration 27, loss = 0.53764119\n",
      "Iteration 28, loss = 0.53573429\n",
      "Iteration 29, loss = 0.53388153\n",
      "Iteration 30, loss = 0.53199301\n",
      "Iteration 31, loss = 0.53014751\n",
      "Iteration 32, loss = 0.52835046\n",
      "Iteration 33, loss = 0.52660675\n",
      "Iteration 34, loss = 0.52483007\n",
      "Iteration 35, loss = 0.52323660\n",
      "Iteration 36, loss = 0.52175731\n",
      "Iteration 37, loss = 0.52006385\n",
      "Iteration 38, loss = 0.51854514\n",
      "Iteration 39, loss = 0.51709545\n",
      "Iteration 40, loss = 0.51575240\n",
      "Iteration 41, loss = 0.51417876\n",
      "Iteration 42, loss = 0.51299235\n",
      "Iteration 43, loss = 0.51182011\n",
      "Iteration 44, loss = 0.51050940\n",
      "Iteration 45, loss = 0.50952563\n",
      "Iteration 46, loss = 0.50829600\n",
      "Iteration 47, loss = 0.50729505\n",
      "Iteration 48, loss = 0.50620624\n",
      "Iteration 49, loss = 0.50520715\n",
      "Iteration 50, loss = 0.50448661\n",
      "Iteration 51, loss = 0.50353386\n",
      "Iteration 52, loss = 0.50307124\n",
      "Iteration 53, loss = 0.50186244\n",
      "Iteration 54, loss = 0.50091700\n",
      "Iteration 55, loss = 0.50081472\n",
      "Iteration 56, loss = 0.49974222\n",
      "Iteration 57, loss = 0.49923386\n",
      "Iteration 58, loss = 0.49864320\n",
      "Iteration 59, loss = 0.49794636\n",
      "Iteration 60, loss = 0.49748995\n",
      "Iteration 61, loss = 0.49679369\n",
      "Iteration 62, loss = 0.49616126\n",
      "Iteration 63, loss = 0.49572212\n",
      "Iteration 64, loss = 0.49531879\n",
      "Iteration 65, loss = 0.49489092\n",
      "Iteration 66, loss = 0.49448924\n",
      "Iteration 67, loss = 0.49429444\n",
      "Iteration 68, loss = 0.49380308\n",
      "Iteration 69, loss = 0.49367841\n",
      "Iteration 70, loss = 0.49356277\n",
      "Iteration 71, loss = 0.49356868\n",
      "Iteration 72, loss = 0.49271306\n",
      "Iteration 73, loss = 0.49223862\n",
      "Iteration 74, loss = 0.49195938\n",
      "Iteration 75, loss = 0.49194226\n",
      "Iteration 76, loss = 0.49179249\n",
      "Iteration 77, loss = 0.49175400\n",
      "Iteration 78, loss = 0.49097838\n",
      "Iteration 79, loss = 0.49109022\n",
      "Iteration 80, loss = 0.49072254\n",
      "Iteration 81, loss = 0.49040238\n",
      "Iteration 82, loss = 0.48999169\n",
      "Iteration 83, loss = 0.48970518\n",
      "Iteration 84, loss = 0.48969420\n",
      "Iteration 85, loss = 0.48979291\n",
      "Iteration 86, loss = 0.48906660\n",
      "Iteration 87, loss = 0.48924237\n",
      "Iteration 88, loss = 0.48914832\n",
      "Iteration 89, loss = 0.48873021\n",
      "Iteration 90, loss = 0.48848722\n",
      "Iteration 91, loss = 0.48821494\n",
      "Iteration 92, loss = 0.48801957\n",
      "Iteration 93, loss = 0.48820649\n",
      "Iteration 94, loss = 0.48820868\n",
      "Iteration 95, loss = 0.48773440\n",
      "Iteration 96, loss = 0.48760370\n",
      "Iteration 97, loss = 0.48745301\n",
      "Iteration 98, loss = 0.48733427\n",
      "Iteration 99, loss = 0.48700245\n",
      "Iteration 100, loss = 0.48734269\n",
      "Iteration 101, loss = 0.48687187\n",
      "Iteration 102, loss = 0.48661212\n",
      "Iteration 103, loss = 0.48649395\n",
      "Iteration 104, loss = 0.48628417\n",
      "Iteration 105, loss = 0.48625723\n",
      "Iteration 106, loss = 0.48610100\n",
      "Iteration 107, loss = 0.48607432\n",
      "Iteration 108, loss = 0.48571941\n",
      "Iteration 109, loss = 0.48582772\n",
      "Iteration 110, loss = 0.48584634\n",
      "Iteration 111, loss = 0.48524294\n",
      "Iteration 112, loss = 0.48523203\n",
      "Iteration 113, loss = 0.48529682\n",
      "Iteration 114, loss = 0.48515053\n",
      "Iteration 115, loss = 0.48486815\n",
      "Iteration 116, loss = 0.48471242\n",
      "Iteration 117, loss = 0.48434184\n",
      "Iteration 118, loss = 0.48425793\n",
      "Iteration 119, loss = 0.48428740\n",
      "Iteration 120, loss = 0.48381888\n",
      "Iteration 121, loss = 0.48388276\n",
      "Iteration 122, loss = 0.48364161\n",
      "Iteration 123, loss = 0.48339687\n",
      "Iteration 124, loss = 0.48334540\n",
      "Iteration 125, loss = 0.48312095\n",
      "Iteration 126, loss = 0.48312143\n",
      "Iteration 127, loss = 0.48297731\n",
      "Iteration 128, loss = 0.48271221\n",
      "Iteration 129, loss = 0.48259914\n",
      "Iteration 130, loss = 0.48236449\n",
      "Iteration 131, loss = 0.48227347\n",
      "Iteration 132, loss = 0.48255992\n",
      "Iteration 133, loss = 0.48247512\n",
      "Iteration 134, loss = 0.48198711\n",
      "Iteration 135, loss = 0.48191536\n",
      "Iteration 136, loss = 0.48215485\n",
      "Iteration 137, loss = 0.48207071\n",
      "Iteration 138, loss = 0.48129361\n",
      "Iteration 139, loss = 0.48160215\n",
      "Iteration 140, loss = 0.48132138\n",
      "Iteration 141, loss = 0.48147569\n",
      "Iteration 142, loss = 0.48088977\n",
      "Iteration 143, loss = 0.48096259\n",
      "Iteration 144, loss = 0.48095923\n",
      "Iteration 145, loss = 0.48112072\n",
      "Iteration 146, loss = 0.48066964\n",
      "Iteration 147, loss = 0.48068917\n",
      "Iteration 148, loss = 0.48024259\n",
      "Iteration 149, loss = 0.48032124\n",
      "Iteration 150, loss = 0.48026159\n",
      "Iteration 151, loss = 0.47993290\n",
      "Iteration 152, loss = 0.48029863\n",
      "Iteration 153, loss = 0.47992935\n",
      "Iteration 154, loss = 0.48008409\n",
      "Iteration 155, loss = 0.47977197\n",
      "Iteration 156, loss = 0.47967127\n",
      "Iteration 157, loss = 0.47946028\n",
      "Iteration 158, loss = 0.47943666\n",
      "Iteration 159, loss = 0.47958010\n",
      "Iteration 160, loss = 0.47927628\n",
      "Iteration 161, loss = 0.47911272\n",
      "Iteration 162, loss = 0.47975369\n",
      "Iteration 163, loss = 0.47965038\n",
      "Iteration 164, loss = 0.47903822\n",
      "Iteration 165, loss = 0.47892680\n",
      "Iteration 166, loss = 0.47849221\n",
      "Iteration 167, loss = 0.47855928\n",
      "Iteration 168, loss = 0.47849052\n",
      "Iteration 169, loss = 0.47820233\n",
      "Iteration 170, loss = 0.47815763\n",
      "Iteration 171, loss = 0.47793614\n",
      "Iteration 172, loss = 0.47785873\n",
      "Iteration 173, loss = 0.47799910\n",
      "Iteration 174, loss = 0.47781536\n",
      "Iteration 175, loss = 0.47850283\n",
      "Iteration 176, loss = 0.47769972\n",
      "Iteration 177, loss = 0.47775351\n",
      "Iteration 178, loss = 0.47737678\n",
      "Iteration 179, loss = 0.47745996\n",
      "Iteration 180, loss = 0.47719557\n",
      "Iteration 181, loss = 0.47754884\n",
      "Iteration 182, loss = 0.47722662\n",
      "Iteration 183, loss = 0.47699159\n",
      "Iteration 184, loss = 0.47689873\n",
      "Iteration 185, loss = 0.47693513\n",
      "Iteration 186, loss = 0.47705453\n",
      "Iteration 187, loss = 0.47677368\n",
      "Iteration 188, loss = 0.47661609\n",
      "Iteration 189, loss = 0.47663243\n",
      "Iteration 190, loss = 0.47666478\n",
      "Iteration 191, loss = 0.47636297\n",
      "Iteration 192, loss = 0.47630392\n",
      "Iteration 193, loss = 0.47614519\n",
      "Iteration 194, loss = 0.47597336\n",
      "Iteration 195, loss = 0.47598755\n",
      "Iteration 196, loss = 0.47594715\n",
      "Iteration 197, loss = 0.47605389\n",
      "Iteration 198, loss = 0.47580302\n",
      "Iteration 199, loss = 0.47592774\n",
      "Iteration 200, loss = 0.47569882\n",
      "Iteration 201, loss = 0.47552084\n",
      "Iteration 202, loss = 0.47555739\n",
      "Iteration 203, loss = 0.47529369\n",
      "Iteration 204, loss = 0.47524443\n",
      "Iteration 205, loss = 0.47508876\n",
      "Iteration 206, loss = 0.47519630\n",
      "Iteration 207, loss = 0.47508827\n",
      "Iteration 208, loss = 0.47504122\n",
      "Iteration 209, loss = 0.47491748\n",
      "Iteration 210, loss = 0.47475366\n",
      "Iteration 211, loss = 0.47464159\n",
      "Iteration 212, loss = 0.47494963\n",
      "Iteration 213, loss = 0.47502078\n",
      "Iteration 214, loss = 0.47508649\n",
      "Iteration 215, loss = 0.47472314\n",
      "Iteration 216, loss = 0.47424684\n",
      "Iteration 217, loss = 0.47433064\n",
      "Iteration 218, loss = 0.47469635\n",
      "Iteration 219, loss = 0.47417598\n",
      "Iteration 220, loss = 0.47399686\n",
      "Iteration 221, loss = 0.47409435\n",
      "Iteration 222, loss = 0.47385848\n",
      "Iteration 223, loss = 0.47408858\n",
      "Iteration 224, loss = 0.47438270\n",
      "Iteration 225, loss = 0.47464061\n",
      "Iteration 226, loss = 0.47424009\n",
      "Iteration 227, loss = 0.47353759\n",
      "Iteration 228, loss = 0.47350076\n",
      "Iteration 229, loss = 0.47331040\n",
      "Iteration 230, loss = 0.47348383\n",
      "Iteration 231, loss = 0.47362342\n",
      "Iteration 232, loss = 0.47305382\n",
      "Iteration 233, loss = 0.47307142\n",
      "Iteration 234, loss = 0.47297070\n",
      "Iteration 235, loss = 0.47295585\n",
      "Iteration 236, loss = 0.47284449\n",
      "Iteration 237, loss = 0.47284539\n",
      "Iteration 238, loss = 0.47292652\n",
      "Iteration 239, loss = 0.47251252\n",
      "Iteration 240, loss = 0.47246905\n",
      "Iteration 241, loss = 0.47234483\n",
      "Iteration 242, loss = 0.47257660\n",
      "Iteration 243, loss = 0.47269063\n",
      "Iteration 244, loss = 0.47220989\n",
      "Iteration 245, loss = 0.47237078\n",
      "Iteration 246, loss = 0.47223947\n",
      "Iteration 247, loss = 0.47245842\n",
      "Iteration 248, loss = 0.47187604\n",
      "Iteration 249, loss = 0.47179560\n",
      "Iteration 250, loss = 0.47195647\n",
      "Iteration 251, loss = 0.47191504\n",
      "Iteration 252, loss = 0.47173793\n",
      "Iteration 253, loss = 0.47204387\n",
      "Iteration 254, loss = 0.47155709\n",
      "Iteration 255, loss = 0.47178493\n",
      "Iteration 256, loss = 0.47154148\n",
      "Iteration 257, loss = 0.47145227\n",
      "Iteration 258, loss = 0.47140525\n",
      "Iteration 259, loss = 0.47160380\n",
      "Iteration 260, loss = 0.47151523\n",
      "Iteration 261, loss = 0.47129319\n",
      "Iteration 262, loss = 0.47141639\n",
      "Iteration 263, loss = 0.47156170\n",
      "Iteration 264, loss = 0.47174150\n",
      "Iteration 265, loss = 0.47104410\n",
      "Iteration 266, loss = 0.47128435\n",
      "Iteration 267, loss = 0.47132400\n",
      "Iteration 268, loss = 0.47083655\n",
      "Iteration 269, loss = 0.47110406\n",
      "Iteration 270, loss = 0.47111544\n",
      "Iteration 271, loss = 0.47131827\n",
      "Iteration 272, loss = 0.47072590\n",
      "Iteration 273, loss = 0.47143381\n",
      "Iteration 274, loss = 0.47132994\n",
      "Iteration 275, loss = 0.47157685\n",
      "Iteration 276, loss = 0.47097233\n",
      "Iteration 277, loss = 0.47072853\n",
      "Iteration 278, loss = 0.47061777\n",
      "Iteration 279, loss = 0.47086007\n",
      "Iteration 280, loss = 0.47057101\n",
      "Iteration 281, loss = 0.47067081\n",
      "Iteration 282, loss = 0.47047055\n",
      "Iteration 283, loss = 0.47058123\n",
      "Iteration 284, loss = 0.47111966\n",
      "Iteration 285, loss = 0.47026176\n",
      "Iteration 286, loss = 0.47030821\n",
      "Iteration 287, loss = 0.47041606\n",
      "Iteration 288, loss = 0.47052492\n",
      "Iteration 289, loss = 0.47060645\n",
      "Iteration 290, loss = 0.47034782\n",
      "Iteration 291, loss = 0.47034550\n",
      "Iteration 292, loss = 0.47043590\n",
      "Iteration 293, loss = 0.46990314\n",
      "Iteration 294, loss = 0.47053040\n",
      "Iteration 295, loss = 0.47034454\n",
      "Iteration 296, loss = 0.46984118\n",
      "Iteration 297, loss = 0.47050657\n",
      "Iteration 298, loss = 0.47028999\n",
      "Iteration 299, loss = 0.47001160\n",
      "Iteration 300, loss = 0.47026889\n",
      "Iteration 301, loss = 0.46974497\n",
      "Iteration 302, loss = 0.46997481\n",
      "Iteration 303, loss = 0.47009947\n",
      "Iteration 304, loss = 0.46979755\n",
      "Iteration 305, loss = 0.46985524\n",
      "Iteration 306, loss = 0.46970626\n",
      "Iteration 307, loss = 0.46984010\n",
      "Iteration 308, loss = 0.46952179\n",
      "Iteration 309, loss = 0.46965548\n",
      "Iteration 310, loss = 0.46988711\n",
      "Iteration 311, loss = 0.46956150\n",
      "Iteration 312, loss = 0.46966751\n",
      "Iteration 313, loss = 0.46938764\n",
      "Iteration 314, loss = 0.46954932\n",
      "Iteration 315, loss = 0.46929828\n",
      "Iteration 316, loss = 0.46944406\n",
      "Iteration 317, loss = 0.46950843\n",
      "Iteration 318, loss = 0.46922413\n",
      "Iteration 319, loss = 0.46929182\n",
      "Iteration 320, loss = 0.46950667\n",
      "Iteration 321, loss = 0.46943840\n",
      "Iteration 322, loss = 0.46954234\n",
      "Iteration 323, loss = 0.46910189\n",
      "Iteration 324, loss = 0.46947033\n",
      "Iteration 325, loss = 0.46918417\n",
      "Iteration 326, loss = 0.46950100\n",
      "Iteration 327, loss = 0.46921302\n",
      "Iteration 328, loss = 0.46914314\n",
      "Iteration 329, loss = 0.46896627\n",
      "Iteration 330, loss = 0.46913227\n",
      "Iteration 331, loss = 0.46969464\n",
      "Iteration 332, loss = 0.46933802\n",
      "Iteration 333, loss = 0.46880119\n",
      "Iteration 334, loss = 0.46901151\n",
      "Iteration 335, loss = 0.46901139\n",
      "Iteration 336, loss = 0.46863712\n",
      "Iteration 337, loss = 0.46883101\n",
      "Iteration 338, loss = 0.46889346\n",
      "Iteration 339, loss = 0.46919741\n",
      "Iteration 340, loss = 0.46972914\n",
      "Iteration 341, loss = 0.46882016\n",
      "Iteration 342, loss = 0.46849839\n",
      "Iteration 343, loss = 0.46894138\n",
      "Iteration 344, loss = 0.46879107\n",
      "Iteration 345, loss = 0.46849914\n",
      "Iteration 346, loss = 0.46837640\n",
      "Iteration 347, loss = 0.46842030\n",
      "Iteration 348, loss = 0.46877303\n",
      "Iteration 349, loss = 0.46854198\n",
      "Iteration 350, loss = 0.46878387\n",
      "Iteration 351, loss = 0.46847503\n",
      "Iteration 352, loss = 0.46849049\n",
      "Iteration 353, loss = 0.46835049\n",
      "Iteration 354, loss = 0.46835008\n",
      "Iteration 355, loss = 0.46829535\n",
      "Iteration 356, loss = 0.46824679\n",
      "Iteration 357, loss = 0.46835171\n",
      "Iteration 358, loss = 0.46786482\n",
      "Iteration 359, loss = 0.46802969\n",
      "Iteration 360, loss = 0.46831679\n",
      "Iteration 361, loss = 0.46834169\n",
      "Iteration 362, loss = 0.46822699\n",
      "Iteration 363, loss = 0.46807645\n",
      "Iteration 364, loss = 0.46813624\n",
      "Iteration 365, loss = 0.46854061\n",
      "Iteration 366, loss = 0.46815351\n",
      "Iteration 367, loss = 0.46860893\n",
      "Iteration 368, loss = 0.46852025\n",
      "Iteration 369, loss = 0.46771113\n",
      "Iteration 370, loss = 0.46821102\n",
      "Iteration 371, loss = 0.46780275\n",
      "Iteration 372, loss = 0.46784974\n",
      "Iteration 373, loss = 0.46770716\n",
      "Iteration 374, loss = 0.46792226\n",
      "Iteration 375, loss = 0.46765638\n",
      "Iteration 376, loss = 0.46771235\n",
      "Iteration 377, loss = 0.46757468\n",
      "Iteration 378, loss = 0.46761742\n",
      "Iteration 379, loss = 0.46758000\n",
      "Iteration 380, loss = 0.46748118\n",
      "Iteration 381, loss = 0.46756581\n",
      "Iteration 382, loss = 0.46753385\n",
      "Iteration 383, loss = 0.46733489\n",
      "Iteration 384, loss = 0.46740567\n",
      "Iteration 385, loss = 0.46727892\n",
      "Iteration 386, loss = 0.46727377\n",
      "Iteration 387, loss = 0.46784632\n",
      "Iteration 388, loss = 0.46757021\n",
      "Iteration 389, loss = 0.46796189\n",
      "Iteration 390, loss = 0.46751278\n",
      "Iteration 391, loss = 0.46748636\n",
      "Iteration 392, loss = 0.46794564\n",
      "Iteration 393, loss = 0.46732446\n",
      "Iteration 394, loss = 0.46868207\n",
      "Iteration 395, loss = 0.46776737\n",
      "Iteration 396, loss = 0.46715621\n",
      "Iteration 397, loss = 0.46705272\n",
      "Iteration 398, loss = 0.46746472\n",
      "Iteration 399, loss = 0.46699549\n",
      "Iteration 400, loss = 0.46702227\n",
      "Iteration 401, loss = 0.46686346\n",
      "Iteration 402, loss = 0.46687375\n",
      "Iteration 403, loss = 0.46709413\n",
      "Iteration 404, loss = 0.46706738\n",
      "Iteration 405, loss = 0.46701151\n",
      "Iteration 406, loss = 0.46678395\n",
      "Iteration 407, loss = 0.46685000\n",
      "Iteration 408, loss = 0.46694885\n",
      "Iteration 409, loss = 0.46713386\n",
      "Iteration 410, loss = 0.46661040\n",
      "Iteration 411, loss = 0.46696636\n",
      "Iteration 412, loss = 0.46653043\n",
      "Iteration 413, loss = 0.46674339\n",
      "Iteration 414, loss = 0.46661194\n",
      "Iteration 415, loss = 0.46709396\n",
      "Iteration 416, loss = 0.46694779\n",
      "Iteration 417, loss = 0.46655097\n",
      "Iteration 418, loss = 0.46661361\n",
      "Iteration 419, loss = 0.46674002\n",
      "Iteration 420, loss = 0.46772989\n",
      "Iteration 421, loss = 0.46708664\n",
      "Iteration 422, loss = 0.46621239\n",
      "Iteration 423, loss = 0.46669591\n",
      "Iteration 424, loss = 0.46669013\n",
      "Iteration 425, loss = 0.46635673\n",
      "Iteration 426, loss = 0.46679625\n",
      "Iteration 427, loss = 0.46716934\n",
      "Iteration 428, loss = 0.46696684\n",
      "Iteration 429, loss = 0.46672361\n",
      "Iteration 430, loss = 0.46633135\n",
      "Iteration 431, loss = 0.46622505\n",
      "Iteration 432, loss = 0.46613700\n",
      "Iteration 433, loss = 0.46658299\n",
      "Iteration 434, loss = 0.46625953\n",
      "Iteration 435, loss = 0.46649554\n",
      "Iteration 436, loss = 0.46607456\n",
      "Iteration 437, loss = 0.46610367\n",
      "Iteration 438, loss = 0.46647917\n",
      "Iteration 439, loss = 0.46636975\n",
      "Iteration 440, loss = 0.46620897\n",
      "Iteration 441, loss = 0.46609920\n",
      "Iteration 442, loss = 0.46628128\n",
      "Iteration 443, loss = 0.46609300\n",
      "Iteration 444, loss = 0.46630162\n",
      "Iteration 445, loss = 0.46601421\n",
      "Iteration 446, loss = 0.46616812\n",
      "Iteration 447, loss = 0.46612785\n",
      "Iteration 448, loss = 0.46571596\n",
      "Iteration 449, loss = 0.46574866\n",
      "Iteration 450, loss = 0.46607573\n",
      "Iteration 451, loss = 0.46706168\n",
      "Iteration 452, loss = 0.46607381\n",
      "Iteration 453, loss = 0.46572475\n",
      "Iteration 454, loss = 0.46591924\n",
      "Iteration 455, loss = 0.46596346\n",
      "Iteration 456, loss = 0.46599691\n",
      "Iteration 457, loss = 0.46597944\n",
      "Iteration 458, loss = 0.46598507\n",
      "Iteration 459, loss = 0.46560537\n",
      "Iteration 460, loss = 0.46566289\n",
      "Iteration 461, loss = 0.46574173\n",
      "Iteration 462, loss = 0.46558218\n",
      "Iteration 463, loss = 0.46559588\n",
      "Iteration 464, loss = 0.46597632\n",
      "Iteration 465, loss = 0.46594352\n",
      "Iteration 466, loss = 0.46620506\n",
      "Iteration 467, loss = 0.46672431\n",
      "Iteration 468, loss = 0.46576669\n",
      "Iteration 469, loss = 0.46555091\n",
      "Iteration 470, loss = 0.46577008\n",
      "Iteration 471, loss = 0.46593671\n",
      "Iteration 472, loss = 0.46581090\n",
      "Iteration 473, loss = 0.46551216\n",
      "Iteration 474, loss = 0.46525403\n",
      "Iteration 475, loss = 0.46518003\n",
      "Iteration 476, loss = 0.46516995\n",
      "Iteration 477, loss = 0.46542495\n",
      "Iteration 478, loss = 0.46512316\n",
      "Iteration 479, loss = 0.46539310\n",
      "Iteration 480, loss = 0.46537557\n",
      "Iteration 481, loss = 0.46556843\n",
      "Iteration 482, loss = 0.46576628\n",
      "Iteration 483, loss = 0.46531504\n",
      "Iteration 484, loss = 0.46500610\n",
      "Iteration 485, loss = 0.46512669\n",
      "Iteration 486, loss = 0.46528727\n",
      "Iteration 487, loss = 0.46536453\n",
      "Iteration 488, loss = 0.46486326\n",
      "Iteration 489, loss = 0.46509599\n",
      "Iteration 490, loss = 0.46500369\n",
      "Iteration 491, loss = 0.46496419\n",
      "Iteration 492, loss = 0.46495423\n",
      "Iteration 493, loss = 0.46465983\n",
      "Iteration 494, loss = 0.46507547\n",
      "Iteration 495, loss = 0.46487726\n",
      "Iteration 496, loss = 0.46553672\n",
      "Iteration 497, loss = 0.46501671\n",
      "Iteration 498, loss = 0.46536751\n",
      "Iteration 499, loss = 0.46461243\n",
      "Iteration 500, loss = 0.46464540\n",
      "Iteration 501, loss = 0.46502332\n",
      "Iteration 502, loss = 0.46502001\n",
      "Iteration 503, loss = 0.46493090\n",
      "Iteration 504, loss = 0.46492126\n",
      "Iteration 505, loss = 0.46485817\n",
      "Iteration 506, loss = 0.46490146\n",
      "Iteration 507, loss = 0.46445868\n",
      "Iteration 508, loss = 0.46452004\n",
      "Iteration 509, loss = 0.46489921\n",
      "Iteration 510, loss = 0.46471573\n",
      "Iteration 511, loss = 0.46447968\n",
      "Iteration 512, loss = 0.46461038\n",
      "Iteration 513, loss = 0.46477274\n",
      "Iteration 514, loss = 0.46430582\n",
      "Iteration 515, loss = 0.46422910\n",
      "Iteration 516, loss = 0.46453174\n",
      "Iteration 517, loss = 0.46426845\n",
      "Iteration 518, loss = 0.46470268\n",
      "Iteration 519, loss = 0.46447834\n",
      "Iteration 520, loss = 0.46439721\n",
      "Iteration 521, loss = 0.46492333\n",
      "Iteration 522, loss = 0.46410041\n",
      "Iteration 523, loss = 0.46439164\n",
      "Iteration 524, loss = 0.46444573\n",
      "Iteration 525, loss = 0.46444957\n",
      "Iteration 526, loss = 0.46414600\n",
      "Iteration 527, loss = 0.46437811\n",
      "Iteration 528, loss = 0.46393585\n",
      "Iteration 529, loss = 0.46444488\n",
      "Iteration 530, loss = 0.46445674\n",
      "Iteration 531, loss = 0.46426777\n",
      "Iteration 532, loss = 0.46394234\n",
      "Iteration 533, loss = 0.46397218\n",
      "Iteration 534, loss = 0.46418934\n",
      "Iteration 535, loss = 0.46416864\n",
      "Iteration 536, loss = 0.46416784\n",
      "Iteration 537, loss = 0.46404614\n",
      "Iteration 538, loss = 0.46398785\n",
      "Iteration 539, loss = 0.46401056\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72216563\n",
      "Iteration 2, loss = 0.69277575\n",
      "Iteration 3, loss = 0.67604628\n",
      "Iteration 4, loss = 0.66400851\n",
      "Iteration 5, loss = 0.65278303\n",
      "Iteration 6, loss = 0.64088131\n",
      "Iteration 7, loss = 0.62823720\n",
      "Iteration 8, loss = 0.61572090\n",
      "Iteration 9, loss = 0.60372900\n",
      "Iteration 10, loss = 0.59186069\n",
      "Iteration 11, loss = 0.58143232\n",
      "Iteration 12, loss = 0.57273260\n",
      "Iteration 13, loss = 0.56576671\n",
      "Iteration 14, loss = 0.55993082\n",
      "Iteration 15, loss = 0.55579452\n",
      "Iteration 16, loss = 0.55223352\n",
      "Iteration 17, loss = 0.54890565\n",
      "Iteration 18, loss = 0.54634907\n",
      "Iteration 19, loss = 0.54395328\n",
      "Iteration 20, loss = 0.54178560\n",
      "Iteration 21, loss = 0.53937182\n",
      "Iteration 22, loss = 0.53779343\n",
      "Iteration 23, loss = 0.53592779\n",
      "Iteration 24, loss = 0.53444703\n",
      "Iteration 25, loss = 0.53264095\n",
      "Iteration 26, loss = 0.53114776\n",
      "Iteration 27, loss = 0.52961949\n",
      "Iteration 28, loss = 0.52823933\n",
      "Iteration 29, loss = 0.52702670\n",
      "Iteration 30, loss = 0.52565875\n",
      "Iteration 31, loss = 0.52421820\n",
      "Iteration 32, loss = 0.52290797\n",
      "Iteration 33, loss = 0.52136241\n",
      "Iteration 34, loss = 0.52090933\n",
      "Iteration 35, loss = 0.51913198\n",
      "Iteration 36, loss = 0.51807667\n",
      "Iteration 37, loss = 0.51646847\n",
      "Iteration 38, loss = 0.51512693\n",
      "Iteration 39, loss = 0.51396674\n",
      "Iteration 40, loss = 0.51259657\n",
      "Iteration 41, loss = 0.51182318\n",
      "Iteration 42, loss = 0.51058530\n",
      "Iteration 43, loss = 0.50951738\n",
      "Iteration 44, loss = 0.50876649\n",
      "Iteration 45, loss = 0.50746692\n",
      "Iteration 46, loss = 0.50670834\n",
      "Iteration 47, loss = 0.50592252\n",
      "Iteration 48, loss = 0.50496383\n",
      "Iteration 49, loss = 0.50442936\n",
      "Iteration 50, loss = 0.50367407\n",
      "Iteration 51, loss = 0.50315707\n",
      "Iteration 52, loss = 0.50221647\n",
      "Iteration 53, loss = 0.50147681\n",
      "Iteration 54, loss = 0.50104466\n",
      "Iteration 55, loss = 0.50055679\n",
      "Iteration 56, loss = 0.49972492\n",
      "Iteration 57, loss = 0.49930577\n",
      "Iteration 58, loss = 0.49894393\n",
      "Iteration 59, loss = 0.49837728\n",
      "Iteration 60, loss = 0.49801996\n",
      "Iteration 61, loss = 0.49746476\n",
      "Iteration 62, loss = 0.49684275\n",
      "Iteration 63, loss = 0.49660128\n",
      "Iteration 64, loss = 0.49638115\n",
      "Iteration 65, loss = 0.49584352\n",
      "Iteration 66, loss = 0.49590521\n",
      "Iteration 67, loss = 0.49522714\n",
      "Iteration 68, loss = 0.49479507\n",
      "Iteration 69, loss = 0.49451580\n",
      "Iteration 70, loss = 0.49407915\n",
      "Iteration 71, loss = 0.49419386\n",
      "Iteration 72, loss = 0.49383373\n",
      "Iteration 73, loss = 0.49319635\n",
      "Iteration 74, loss = 0.49356868\n",
      "Iteration 75, loss = 0.49280924\n",
      "Iteration 76, loss = 0.49230086\n",
      "Iteration 77, loss = 0.49195281\n",
      "Iteration 78, loss = 0.49162698\n",
      "Iteration 79, loss = 0.49122277\n",
      "Iteration 80, loss = 0.49104788\n",
      "Iteration 81, loss = 0.49109078\n",
      "Iteration 82, loss = 0.49047556\n",
      "Iteration 83, loss = 0.49013747\n",
      "Iteration 84, loss = 0.49015705\n",
      "Iteration 85, loss = 0.48955917\n",
      "Iteration 86, loss = 0.48960788\n",
      "Iteration 87, loss = 0.48927300\n",
      "Iteration 88, loss = 0.48899334\n",
      "Iteration 89, loss = 0.48893113\n",
      "Iteration 90, loss = 0.48851319\n",
      "Iteration 91, loss = 0.48827102\n",
      "Iteration 92, loss = 0.48814122\n",
      "Iteration 93, loss = 0.48791730\n",
      "Iteration 94, loss = 0.48781357\n",
      "Iteration 95, loss = 0.48796712\n",
      "Iteration 96, loss = 0.48753146\n",
      "Iteration 97, loss = 0.48754977\n",
      "Iteration 98, loss = 0.48716939\n",
      "Iteration 99, loss = 0.48688102\n",
      "Iteration 100, loss = 0.48683996\n",
      "Iteration 101, loss = 0.48703074\n",
      "Iteration 102, loss = 0.48641740\n",
      "Iteration 103, loss = 0.48655521\n",
      "Iteration 104, loss = 0.48638327\n",
      "Iteration 105, loss = 0.48605836\n",
      "Iteration 106, loss = 0.48600714\n",
      "Iteration 107, loss = 0.48574626\n",
      "Iteration 108, loss = 0.48574588\n",
      "Iteration 109, loss = 0.48567307\n",
      "Iteration 110, loss = 0.48537296\n",
      "Iteration 111, loss = 0.48526095\n",
      "Iteration 112, loss = 0.48566672\n",
      "Iteration 113, loss = 0.48484247\n",
      "Iteration 114, loss = 0.48489431\n",
      "Iteration 115, loss = 0.48480800\n",
      "Iteration 116, loss = 0.48456341\n",
      "Iteration 117, loss = 0.48446102\n",
      "Iteration 118, loss = 0.48451222\n",
      "Iteration 119, loss = 0.48404617\n",
      "Iteration 120, loss = 0.48413740\n",
      "Iteration 121, loss = 0.48385220\n",
      "Iteration 122, loss = 0.48384034\n",
      "Iteration 123, loss = 0.48367219\n",
      "Iteration 124, loss = 0.48362623\n",
      "Iteration 125, loss = 0.48336099\n",
      "Iteration 126, loss = 0.48346698\n",
      "Iteration 127, loss = 0.48357387\n",
      "Iteration 128, loss = 0.48308544\n",
      "Iteration 129, loss = 0.48298122\n",
      "Iteration 130, loss = 0.48289630\n",
      "Iteration 131, loss = 0.48261937\n",
      "Iteration 132, loss = 0.48240607\n",
      "Iteration 133, loss = 0.48246635\n",
      "Iteration 134, loss = 0.48234267\n",
      "Iteration 135, loss = 0.48194707\n",
      "Iteration 136, loss = 0.48265251\n",
      "Iteration 137, loss = 0.48267787\n",
      "Iteration 138, loss = 0.48221998\n",
      "Iteration 139, loss = 0.48142449\n",
      "Iteration 140, loss = 0.48156622\n",
      "Iteration 141, loss = 0.48134333\n",
      "Iteration 142, loss = 0.48114962\n",
      "Iteration 143, loss = 0.48121501\n",
      "Iteration 144, loss = 0.48106405\n",
      "Iteration 145, loss = 0.48096380\n",
      "Iteration 146, loss = 0.48077463\n",
      "Iteration 147, loss = 0.48097879\n",
      "Iteration 148, loss = 0.48071695\n",
      "Iteration 149, loss = 0.48083081\n",
      "Iteration 150, loss = 0.48060621\n",
      "Iteration 151, loss = 0.48056299\n",
      "Iteration 152, loss = 0.48031540\n",
      "Iteration 153, loss = 0.48026403\n",
      "Iteration 154, loss = 0.48019554\n",
      "Iteration 155, loss = 0.48011497\n",
      "Iteration 156, loss = 0.47997628\n",
      "Iteration 157, loss = 0.47997975\n",
      "Iteration 158, loss = 0.48000253\n",
      "Iteration 159, loss = 0.47975154\n",
      "Iteration 160, loss = 0.47988904\n",
      "Iteration 161, loss = 0.47973682\n",
      "Iteration 162, loss = 0.47935057\n",
      "Iteration 163, loss = 0.47922671\n",
      "Iteration 164, loss = 0.47912349\n",
      "Iteration 165, loss = 0.47921488\n",
      "Iteration 166, loss = 0.47899304\n",
      "Iteration 167, loss = 0.47905667\n",
      "Iteration 168, loss = 0.47902713\n",
      "Iteration 169, loss = 0.47886984\n",
      "Iteration 170, loss = 0.47879412\n",
      "Iteration 171, loss = 0.47880252\n",
      "Iteration 172, loss = 0.47890689\n",
      "Iteration 173, loss = 0.47882224\n",
      "Iteration 174, loss = 0.47835881\n",
      "Iteration 175, loss = 0.47819571\n",
      "Iteration 176, loss = 0.47811872\n",
      "Iteration 177, loss = 0.47850310\n",
      "Iteration 178, loss = 0.47820599\n",
      "Iteration 179, loss = 0.47797818\n",
      "Iteration 180, loss = 0.47816189\n",
      "Iteration 181, loss = 0.47777699\n",
      "Iteration 182, loss = 0.47799801\n",
      "Iteration 183, loss = 0.47777371\n",
      "Iteration 184, loss = 0.47774721\n",
      "Iteration 185, loss = 0.47757220\n",
      "Iteration 186, loss = 0.47795503\n",
      "Iteration 187, loss = 0.47755159\n",
      "Iteration 188, loss = 0.47709612\n",
      "Iteration 189, loss = 0.47737969\n",
      "Iteration 190, loss = 0.47701802\n",
      "Iteration 191, loss = 0.47696656\n",
      "Iteration 192, loss = 0.47705187\n",
      "Iteration 193, loss = 0.47724807\n",
      "Iteration 194, loss = 0.47676748\n",
      "Iteration 195, loss = 0.47678362\n",
      "Iteration 196, loss = 0.47685986\n",
      "Iteration 197, loss = 0.47664145\n",
      "Iteration 198, loss = 0.47676658\n",
      "Iteration 199, loss = 0.47655286\n",
      "Iteration 200, loss = 0.47636774\n",
      "Iteration 201, loss = 0.47650227\n",
      "Iteration 202, loss = 0.47643431\n",
      "Iteration 203, loss = 0.47627580\n",
      "Iteration 204, loss = 0.47624128\n",
      "Iteration 205, loss = 0.47632144\n",
      "Iteration 206, loss = 0.47589102\n",
      "Iteration 207, loss = 0.47624403\n",
      "Iteration 208, loss = 0.47598122\n",
      "Iteration 209, loss = 0.47571057\n",
      "Iteration 210, loss = 0.47578427\n",
      "Iteration 211, loss = 0.47549861\n",
      "Iteration 212, loss = 0.47585891\n",
      "Iteration 213, loss = 0.47574668\n",
      "Iteration 214, loss = 0.47567925\n",
      "Iteration 215, loss = 0.47563766\n",
      "Iteration 216, loss = 0.47524605\n",
      "Iteration 217, loss = 0.47528404\n",
      "Iteration 218, loss = 0.47524885\n",
      "Iteration 219, loss = 0.47510455\n",
      "Iteration 220, loss = 0.47503593\n",
      "Iteration 221, loss = 0.47495266\n",
      "Iteration 222, loss = 0.47504271\n",
      "Iteration 223, loss = 0.47468953\n",
      "Iteration 224, loss = 0.47471250\n",
      "Iteration 225, loss = 0.47481762\n",
      "Iteration 226, loss = 0.47487677\n",
      "Iteration 227, loss = 0.47450535\n",
      "Iteration 228, loss = 0.47456974\n",
      "Iteration 229, loss = 0.47443106\n",
      "Iteration 230, loss = 0.47427579\n",
      "Iteration 231, loss = 0.47433294\n",
      "Iteration 232, loss = 0.47405376\n",
      "Iteration 233, loss = 0.47415189\n",
      "Iteration 234, loss = 0.47441965\n",
      "Iteration 235, loss = 0.47436830\n",
      "Iteration 236, loss = 0.47393507\n",
      "Iteration 237, loss = 0.47385483\n",
      "Iteration 238, loss = 0.47376910\n",
      "Iteration 239, loss = 0.47427693\n",
      "Iteration 240, loss = 0.47359846\n",
      "Iteration 241, loss = 0.47385478\n",
      "Iteration 242, loss = 0.47371577\n",
      "Iteration 243, loss = 0.47351292\n",
      "Iteration 244, loss = 0.47364467\n",
      "Iteration 245, loss = 0.47377152\n",
      "Iteration 246, loss = 0.47302284\n",
      "Iteration 247, loss = 0.47350664\n",
      "Iteration 248, loss = 0.47329453\n",
      "Iteration 249, loss = 0.47331246\n",
      "Iteration 250, loss = 0.47292535\n",
      "Iteration 251, loss = 0.47267120\n",
      "Iteration 252, loss = 0.47275423\n",
      "Iteration 253, loss = 0.47266942\n",
      "Iteration 254, loss = 0.47280709\n",
      "Iteration 255, loss = 0.47240315\n",
      "Iteration 256, loss = 0.47231854\n",
      "Iteration 257, loss = 0.47264130\n",
      "Iteration 258, loss = 0.47236532\n",
      "Iteration 259, loss = 0.47223465\n",
      "Iteration 260, loss = 0.47252340\n",
      "Iteration 261, loss = 0.47206942\n",
      "Iteration 262, loss = 0.47209020\n",
      "Iteration 263, loss = 0.47212872\n",
      "Iteration 264, loss = 0.47226483\n",
      "Iteration 265, loss = 0.47203450\n",
      "Iteration 266, loss = 0.47189992\n",
      "Iteration 267, loss = 0.47193043\n",
      "Iteration 268, loss = 0.47183041\n",
      "Iteration 269, loss = 0.47176657\n",
      "Iteration 270, loss = 0.47166758\n",
      "Iteration 271, loss = 0.47179603\n",
      "Iteration 272, loss = 0.47151778\n",
      "Iteration 273, loss = 0.47129466\n",
      "Iteration 274, loss = 0.47147058\n",
      "Iteration 275, loss = 0.47114820\n",
      "Iteration 276, loss = 0.47130198\n",
      "Iteration 277, loss = 0.47141308\n",
      "Iteration 278, loss = 0.47095955\n",
      "Iteration 279, loss = 0.47090868\n",
      "Iteration 280, loss = 0.47082542\n",
      "Iteration 281, loss = 0.47071358\n",
      "Iteration 282, loss = 0.47095975\n",
      "Iteration 283, loss = 0.47090540\n",
      "Iteration 284, loss = 0.47095524\n",
      "Iteration 285, loss = 0.47072448\n",
      "Iteration 286, loss = 0.47080522\n",
      "Iteration 287, loss = 0.47062391\n",
      "Iteration 288, loss = 0.47072096\n",
      "Iteration 289, loss = 0.47054260\n",
      "Iteration 290, loss = 0.47039686\n",
      "Iteration 291, loss = 0.47044281\n",
      "Iteration 292, loss = 0.47014479\n",
      "Iteration 293, loss = 0.47036922\n",
      "Iteration 294, loss = 0.47007189\n",
      "Iteration 295, loss = 0.47009904\n",
      "Iteration 296, loss = 0.46992802\n",
      "Iteration 297, loss = 0.46991915\n",
      "Iteration 298, loss = 0.46982709\n",
      "Iteration 299, loss = 0.46980298\n",
      "Iteration 300, loss = 0.46971207\n",
      "Iteration 301, loss = 0.46984142\n",
      "Iteration 302, loss = 0.46971962\n",
      "Iteration 303, loss = 0.46986394\n",
      "Iteration 304, loss = 0.46949578\n",
      "Iteration 305, loss = 0.46977550\n",
      "Iteration 306, loss = 0.46956514\n",
      "Iteration 307, loss = 0.46939878\n",
      "Iteration 308, loss = 0.46943220\n",
      "Iteration 309, loss = 0.46914911\n",
      "Iteration 310, loss = 0.46937161\n",
      "Iteration 311, loss = 0.46917797\n",
      "Iteration 312, loss = 0.46908189\n",
      "Iteration 313, loss = 0.46899771\n",
      "Iteration 314, loss = 0.46941403\n",
      "Iteration 315, loss = 0.46885437\n",
      "Iteration 316, loss = 0.46931575\n",
      "Iteration 317, loss = 0.46922422\n",
      "Iteration 318, loss = 0.46863967\n",
      "Iteration 319, loss = 0.46887531\n",
      "Iteration 320, loss = 0.46874049\n",
      "Iteration 321, loss = 0.46911907\n",
      "Iteration 322, loss = 0.46852031\n",
      "Iteration 323, loss = 0.46862194\n",
      "Iteration 324, loss = 0.46881893\n",
      "Iteration 325, loss = 0.46906401\n",
      "Iteration 326, loss = 0.46828061\n",
      "Iteration 327, loss = 0.46837921\n",
      "Iteration 328, loss = 0.46873630\n",
      "Iteration 329, loss = 0.46837320\n",
      "Iteration 330, loss = 0.46818048\n",
      "Iteration 331, loss = 0.46818523\n",
      "Iteration 332, loss = 0.46810635\n",
      "Iteration 333, loss = 0.46785149\n",
      "Iteration 334, loss = 0.46847630\n",
      "Iteration 335, loss = 0.46796234\n",
      "Iteration 336, loss = 0.46782281\n",
      "Iteration 337, loss = 0.46790244\n",
      "Iteration 338, loss = 0.46779976\n",
      "Iteration 339, loss = 0.46758032\n",
      "Iteration 340, loss = 0.46776159\n",
      "Iteration 341, loss = 0.46769507\n",
      "Iteration 342, loss = 0.46750073\n",
      "Iteration 343, loss = 0.46803091\n",
      "Iteration 344, loss = 0.46767501\n",
      "Iteration 345, loss = 0.46766936\n",
      "Iteration 346, loss = 0.46742666\n",
      "Iteration 347, loss = 0.46737463\n",
      "Iteration 348, loss = 0.46741346\n",
      "Iteration 349, loss = 0.46739687\n",
      "Iteration 350, loss = 0.46781439\n",
      "Iteration 351, loss = 0.46730624\n",
      "Iteration 352, loss = 0.46713829\n",
      "Iteration 353, loss = 0.46742223\n",
      "Iteration 354, loss = 0.46700832\n",
      "Iteration 355, loss = 0.46709294\n",
      "Iteration 356, loss = 0.46714848\n",
      "Iteration 357, loss = 0.46693258\n",
      "Iteration 358, loss = 0.46704995\n",
      "Iteration 359, loss = 0.46676474\n",
      "Iteration 360, loss = 0.46666289\n",
      "Iteration 361, loss = 0.46700457\n",
      "Iteration 362, loss = 0.46679768\n",
      "Iteration 363, loss = 0.46731771\n",
      "Iteration 364, loss = 0.46723521\n",
      "Iteration 365, loss = 0.46635289\n",
      "Iteration 366, loss = 0.46637471\n",
      "Iteration 367, loss = 0.46642485\n",
      "Iteration 368, loss = 0.46649957\n",
      "Iteration 369, loss = 0.46666185\n",
      "Iteration 370, loss = 0.46636251\n",
      "Iteration 371, loss = 0.46665783\n",
      "Iteration 372, loss = 0.46606825\n",
      "Iteration 373, loss = 0.46615681\n",
      "Iteration 374, loss = 0.46608065\n",
      "Iteration 375, loss = 0.46602227\n",
      "Iteration 376, loss = 0.46607794\n",
      "Iteration 377, loss = 0.46606268\n",
      "Iteration 378, loss = 0.46621799\n",
      "Iteration 379, loss = 0.46583759\n",
      "Iteration 380, loss = 0.46577126\n",
      "Iteration 381, loss = 0.46583246\n",
      "Iteration 382, loss = 0.46631877\n",
      "Iteration 383, loss = 0.46601251\n",
      "Iteration 384, loss = 0.46597894\n",
      "Iteration 385, loss = 0.46568802\n",
      "Iteration 386, loss = 0.46566820\n",
      "Iteration 387, loss = 0.46550092\n",
      "Iteration 388, loss = 0.46570998\n",
      "Iteration 389, loss = 0.46542739\n",
      "Iteration 390, loss = 0.46555240\n",
      "Iteration 391, loss = 0.46527928\n",
      "Iteration 392, loss = 0.46569134\n",
      "Iteration 393, loss = 0.46568439\n",
      "Iteration 394, loss = 0.46531156\n",
      "Iteration 395, loss = 0.46527253\n",
      "Iteration 396, loss = 0.46511894\n",
      "Iteration 397, loss = 0.46530938\n",
      "Iteration 398, loss = 0.46505645\n",
      "Iteration 399, loss = 0.46525756\n",
      "Iteration 400, loss = 0.46501513\n",
      "Iteration 401, loss = 0.46547133\n",
      "Iteration 402, loss = 0.46528520\n",
      "Iteration 403, loss = 0.46500827\n",
      "Iteration 404, loss = 0.46483467\n",
      "Iteration 405, loss = 0.46491484\n",
      "Iteration 406, loss = 0.46489366\n",
      "Iteration 407, loss = 0.46475039\n",
      "Iteration 408, loss = 0.46510032\n",
      "Iteration 409, loss = 0.46475136\n",
      "Iteration 410, loss = 0.46487799\n",
      "Iteration 411, loss = 0.46542204\n",
      "Iteration 412, loss = 0.46522335\n",
      "Iteration 413, loss = 0.46455053\n",
      "Iteration 414, loss = 0.46458488\n",
      "Iteration 415, loss = 0.46451614\n",
      "Iteration 416, loss = 0.46488502\n",
      "Iteration 417, loss = 0.46459601\n",
      "Iteration 418, loss = 0.46454782\n",
      "Iteration 419, loss = 0.46462033\n",
      "Iteration 420, loss = 0.46434919\n",
      "Iteration 421, loss = 0.46421949\n",
      "Iteration 422, loss = 0.46441570\n",
      "Iteration 423, loss = 0.46449120\n",
      "Iteration 424, loss = 0.46421088\n",
      "Iteration 425, loss = 0.46439731\n",
      "Iteration 426, loss = 0.46433325\n",
      "Iteration 427, loss = 0.46402726\n",
      "Iteration 428, loss = 0.46407363\n",
      "Iteration 429, loss = 0.46429110\n",
      "Iteration 430, loss = 0.46410732\n",
      "Iteration 431, loss = 0.46395863\n",
      "Iteration 432, loss = 0.46432982\n",
      "Iteration 433, loss = 0.46406976\n",
      "Iteration 434, loss = 0.46382617\n",
      "Iteration 435, loss = 0.46430504\n",
      "Iteration 436, loss = 0.46461260\n",
      "Iteration 437, loss = 0.46385738\n",
      "Iteration 438, loss = 0.46395244\n",
      "Iteration 439, loss = 0.46403221\n",
      "Iteration 440, loss = 0.46364601\n",
      "Iteration 441, loss = 0.46375304\n",
      "Iteration 442, loss = 0.46377791\n",
      "Iteration 443, loss = 0.46352704\n",
      "Iteration 444, loss = 0.46359156\n",
      "Iteration 445, loss = 0.46350173\n",
      "Iteration 446, loss = 0.46363741\n",
      "Iteration 447, loss = 0.46368819\n",
      "Iteration 448, loss = 0.46371527\n",
      "Iteration 449, loss = 0.46419863\n",
      "Iteration 450, loss = 0.46358014\n",
      "Iteration 451, loss = 0.46355193\n",
      "Iteration 452, loss = 0.46333473\n",
      "Iteration 453, loss = 0.46350603\n",
      "Iteration 454, loss = 0.46352319\n",
      "Iteration 455, loss = 0.46342369\n",
      "Iteration 456, loss = 0.46358584\n",
      "Iteration 457, loss = 0.46377183\n",
      "Iteration 458, loss = 0.46299151\n",
      "Iteration 459, loss = 0.46355556\n",
      "Iteration 460, loss = 0.46310748\n",
      "Iteration 461, loss = 0.46318103\n",
      "Iteration 462, loss = 0.46352552\n",
      "Iteration 463, loss = 0.46330005\n",
      "Iteration 464, loss = 0.46320728\n",
      "Iteration 465, loss = 0.46337389\n",
      "Iteration 466, loss = 0.46315286\n",
      "Iteration 467, loss = 0.46316132\n",
      "Iteration 468, loss = 0.46309164\n",
      "Iteration 469, loss = 0.46305979\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78768286\n",
      "Iteration 2, loss = 0.74190131\n",
      "Iteration 3, loss = 0.71227389\n",
      "Iteration 4, loss = 0.69137458\n",
      "Iteration 5, loss = 0.67529045\n",
      "Iteration 6, loss = 0.66280716\n",
      "Iteration 7, loss = 0.65178140\n",
      "Iteration 8, loss = 0.64163941\n",
      "Iteration 9, loss = 0.63253404\n",
      "Iteration 10, loss = 0.62373942\n",
      "Iteration 11, loss = 0.61506163\n",
      "Iteration 12, loss = 0.60694897\n",
      "Iteration 13, loss = 0.59898555\n",
      "Iteration 14, loss = 0.59148219\n",
      "Iteration 15, loss = 0.58467083\n",
      "Iteration 16, loss = 0.57879528\n",
      "Iteration 17, loss = 0.57354677\n",
      "Iteration 18, loss = 0.56907572\n",
      "Iteration 19, loss = 0.56523663\n",
      "Iteration 20, loss = 0.56168509\n",
      "Iteration 21, loss = 0.55860450\n",
      "Iteration 22, loss = 0.55579446\n",
      "Iteration 23, loss = 0.55324172\n",
      "Iteration 24, loss = 0.55085941\n",
      "Iteration 25, loss = 0.54863739\n",
      "Iteration 26, loss = 0.54633904\n",
      "Iteration 27, loss = 0.54432550\n",
      "Iteration 28, loss = 0.54223886\n",
      "Iteration 29, loss = 0.54022011\n",
      "Iteration 30, loss = 0.53839518\n",
      "Iteration 31, loss = 0.53694007\n",
      "Iteration 32, loss = 0.53482945\n",
      "Iteration 33, loss = 0.53362179\n",
      "Iteration 34, loss = 0.53179962\n",
      "Iteration 35, loss = 0.53035104\n",
      "Iteration 36, loss = 0.52878110\n",
      "Iteration 37, loss = 0.52758892\n",
      "Iteration 38, loss = 0.52625412\n",
      "Iteration 39, loss = 0.52490550\n",
      "Iteration 40, loss = 0.52399418\n",
      "Iteration 41, loss = 0.52271322\n",
      "Iteration 42, loss = 0.52170587\n",
      "Iteration 43, loss = 0.52047046\n",
      "Iteration 44, loss = 0.51947914\n",
      "Iteration 45, loss = 0.51857924\n",
      "Iteration 46, loss = 0.51747949\n",
      "Iteration 47, loss = 0.51642489\n",
      "Iteration 48, loss = 0.51553015\n",
      "Iteration 49, loss = 0.51456000\n",
      "Iteration 50, loss = 0.51383313\n",
      "Iteration 51, loss = 0.51316822\n",
      "Iteration 52, loss = 0.51288032\n",
      "Iteration 53, loss = 0.51202350\n",
      "Iteration 54, loss = 0.51127862\n",
      "Iteration 55, loss = 0.51057645\n",
      "Iteration 56, loss = 0.51004559\n",
      "Iteration 57, loss = 0.50957455\n",
      "Iteration 58, loss = 0.50919528\n",
      "Iteration 59, loss = 0.50853146\n",
      "Iteration 60, loss = 0.50834250\n",
      "Iteration 61, loss = 0.50810991\n",
      "Iteration 62, loss = 0.50766148\n",
      "Iteration 63, loss = 0.50728534\n",
      "Iteration 64, loss = 0.50707782\n",
      "Iteration 65, loss = 0.50707999\n",
      "Iteration 66, loss = 0.50635114\n",
      "Iteration 67, loss = 0.50634238\n",
      "Iteration 68, loss = 0.50611530\n",
      "Iteration 69, loss = 0.50574429\n",
      "Iteration 70, loss = 0.50537073\n",
      "Iteration 71, loss = 0.50532508\n",
      "Iteration 72, loss = 0.50513704\n",
      "Iteration 73, loss = 0.50510816\n",
      "Iteration 74, loss = 0.50480422\n",
      "Iteration 75, loss = 0.50477622\n",
      "Iteration 76, loss = 0.50422307\n",
      "Iteration 77, loss = 0.50386993\n",
      "Iteration 78, loss = 0.50388755\n",
      "Iteration 79, loss = 0.50356111\n",
      "Iteration 80, loss = 0.50348616\n",
      "Iteration 81, loss = 0.50325292\n",
      "Iteration 82, loss = 0.50300296\n",
      "Iteration 83, loss = 0.50266260\n",
      "Iteration 84, loss = 0.50259547\n",
      "Iteration 85, loss = 0.50250978\n",
      "Iteration 86, loss = 0.50209571\n",
      "Iteration 87, loss = 0.50223383\n",
      "Iteration 88, loss = 0.50177032\n",
      "Iteration 89, loss = 0.50161817\n",
      "Iteration 90, loss = 0.50151411\n",
      "Iteration 91, loss = 0.50141247\n",
      "Iteration 92, loss = 0.50152148\n",
      "Iteration 93, loss = 0.50140137\n",
      "Iteration 94, loss = 0.50103066\n",
      "Iteration 95, loss = 0.50074544\n",
      "Iteration 96, loss = 0.50059859\n",
      "Iteration 97, loss = 0.50051061\n",
      "Iteration 98, loss = 0.50032417\n",
      "Iteration 99, loss = 0.50015738\n",
      "Iteration 100, loss = 0.50001673\n",
      "Iteration 101, loss = 0.50006270\n",
      "Iteration 102, loss = 0.49993155\n",
      "Iteration 103, loss = 0.49961213\n",
      "Iteration 104, loss = 0.49961364\n",
      "Iteration 105, loss = 0.49954782\n",
      "Iteration 106, loss = 0.49932412\n",
      "Iteration 107, loss = 0.49932925\n",
      "Iteration 108, loss = 0.49908202\n",
      "Iteration 109, loss = 0.49902487\n",
      "Iteration 110, loss = 0.49911262\n",
      "Iteration 111, loss = 0.49912018\n",
      "Iteration 112, loss = 0.49865577\n",
      "Iteration 113, loss = 0.49892712\n",
      "Iteration 114, loss = 0.49845504\n",
      "Iteration 115, loss = 0.49853299\n",
      "Iteration 116, loss = 0.49835422\n",
      "Iteration 117, loss = 0.49836552\n",
      "Iteration 118, loss = 0.49792233\n",
      "Iteration 119, loss = 0.49816540\n",
      "Iteration 120, loss = 0.49806769\n",
      "Iteration 121, loss = 0.49780096\n",
      "Iteration 122, loss = 0.49776161\n",
      "Iteration 123, loss = 0.49752882\n",
      "Iteration 124, loss = 0.49766691\n",
      "Iteration 125, loss = 0.49767808\n",
      "Iteration 126, loss = 0.49723174\n",
      "Iteration 127, loss = 0.49729428\n",
      "Iteration 128, loss = 0.49693399\n",
      "Iteration 129, loss = 0.49684440\n",
      "Iteration 130, loss = 0.49702620\n",
      "Iteration 131, loss = 0.49669954\n",
      "Iteration 132, loss = 0.49660304\n",
      "Iteration 133, loss = 0.49637946\n",
      "Iteration 134, loss = 0.49633458\n",
      "Iteration 135, loss = 0.49658436\n",
      "Iteration 136, loss = 0.49617122\n",
      "Iteration 137, loss = 0.49622024\n",
      "Iteration 138, loss = 0.49607892\n",
      "Iteration 139, loss = 0.49603798\n",
      "Iteration 140, loss = 0.49573149\n",
      "Iteration 141, loss = 0.49569490\n",
      "Iteration 142, loss = 0.49547810\n",
      "Iteration 143, loss = 0.49555852\n",
      "Iteration 144, loss = 0.49583612\n",
      "Iteration 145, loss = 0.49540236\n",
      "Iteration 146, loss = 0.49523558\n",
      "Iteration 147, loss = 0.49531250\n",
      "Iteration 148, loss = 0.49511326\n",
      "Iteration 149, loss = 0.49486210\n",
      "Iteration 150, loss = 0.49482286\n",
      "Iteration 151, loss = 0.49532324\n",
      "Iteration 152, loss = 0.49476713\n",
      "Iteration 153, loss = 0.49466447\n",
      "Iteration 154, loss = 0.49462299\n",
      "Iteration 155, loss = 0.49436703\n",
      "Iteration 156, loss = 0.49425555\n",
      "Iteration 157, loss = 0.49438725\n",
      "Iteration 158, loss = 0.49412173\n",
      "Iteration 159, loss = 0.49459963\n",
      "Iteration 160, loss = 0.49405992\n",
      "Iteration 161, loss = 0.49406554\n",
      "Iteration 162, loss = 0.49361337\n",
      "Iteration 163, loss = 0.49381747\n",
      "Iteration 164, loss = 0.49356562\n",
      "Iteration 165, loss = 0.49346866\n",
      "Iteration 166, loss = 0.49340788\n",
      "Iteration 167, loss = 0.49321962\n",
      "Iteration 168, loss = 0.49337098\n",
      "Iteration 169, loss = 0.49317248\n",
      "Iteration 170, loss = 0.49306855\n",
      "Iteration 171, loss = 0.49291126\n",
      "Iteration 172, loss = 0.49292913\n",
      "Iteration 173, loss = 0.49295962\n",
      "Iteration 174, loss = 0.49296017\n",
      "Iteration 175, loss = 0.49273233\n",
      "Iteration 176, loss = 0.49275304\n",
      "Iteration 177, loss = 0.49244397\n",
      "Iteration 178, loss = 0.49241034\n",
      "Iteration 179, loss = 0.49219453\n",
      "Iteration 180, loss = 0.49213441\n",
      "Iteration 181, loss = 0.49214388\n",
      "Iteration 182, loss = 0.49210745\n",
      "Iteration 183, loss = 0.49205168\n",
      "Iteration 184, loss = 0.49198199\n",
      "Iteration 185, loss = 0.49191780\n",
      "Iteration 186, loss = 0.49190050\n",
      "Iteration 187, loss = 0.49162064\n",
      "Iteration 188, loss = 0.49229237\n",
      "Iteration 189, loss = 0.49160723\n",
      "Iteration 190, loss = 0.49140103\n",
      "Iteration 191, loss = 0.49175457\n",
      "Iteration 192, loss = 0.49108598\n",
      "Iteration 193, loss = 0.49132472\n",
      "Iteration 194, loss = 0.49122426\n",
      "Iteration 195, loss = 0.49109159\n",
      "Iteration 196, loss = 0.49101058\n",
      "Iteration 197, loss = 0.49113870\n",
      "Iteration 198, loss = 0.49072011\n",
      "Iteration 199, loss = 0.49080615\n",
      "Iteration 200, loss = 0.49067532\n",
      "Iteration 201, loss = 0.49083638\n",
      "Iteration 202, loss = 0.49072673\n",
      "Iteration 203, loss = 0.49049133\n",
      "Iteration 204, loss = 0.49039341\n",
      "Iteration 205, loss = 0.49047939\n",
      "Iteration 206, loss = 0.49026612\n",
      "Iteration 207, loss = 0.49020615\n",
      "Iteration 208, loss = 0.49014930\n",
      "Iteration 209, loss = 0.49001618\n",
      "Iteration 210, loss = 0.49020294\n",
      "Iteration 211, loss = 0.49060928\n",
      "Iteration 212, loss = 0.49002815\n",
      "Iteration 213, loss = 0.48965456\n",
      "Iteration 214, loss = 0.48989361\n",
      "Iteration 215, loss = 0.49001498\n",
      "Iteration 216, loss = 0.48939399\n",
      "Iteration 217, loss = 0.48950612\n",
      "Iteration 218, loss = 0.48948582\n",
      "Iteration 219, loss = 0.48951204\n",
      "Iteration 220, loss = 0.48918832\n",
      "Iteration 221, loss = 0.48929170\n",
      "Iteration 222, loss = 0.48938489\n",
      "Iteration 223, loss = 0.48914045\n",
      "Iteration 224, loss = 0.48900517\n",
      "Iteration 225, loss = 0.48930637\n",
      "Iteration 226, loss = 0.48887463\n",
      "Iteration 227, loss = 0.48873835\n",
      "Iteration 228, loss = 0.48862762\n",
      "Iteration 229, loss = 0.48857791\n",
      "Iteration 230, loss = 0.48839214\n",
      "Iteration 231, loss = 0.48821951\n",
      "Iteration 232, loss = 0.48829663\n",
      "Iteration 233, loss = 0.48822466\n",
      "Iteration 234, loss = 0.48794277\n",
      "Iteration 235, loss = 0.48826335\n",
      "Iteration 236, loss = 0.48803069\n",
      "Iteration 237, loss = 0.48799802\n",
      "Iteration 238, loss = 0.48793588\n",
      "Iteration 239, loss = 0.48787356\n",
      "Iteration 240, loss = 0.48749574\n",
      "Iteration 241, loss = 0.48745989\n",
      "Iteration 242, loss = 0.48751186\n",
      "Iteration 243, loss = 0.48741097\n",
      "Iteration 244, loss = 0.48714829\n",
      "Iteration 245, loss = 0.48735523\n",
      "Iteration 246, loss = 0.48720605\n",
      "Iteration 247, loss = 0.48699217\n",
      "Iteration 248, loss = 0.48700098\n",
      "Iteration 249, loss = 0.48703269\n",
      "Iteration 250, loss = 0.48680704\n",
      "Iteration 251, loss = 0.48674267\n",
      "Iteration 252, loss = 0.48675615\n",
      "Iteration 253, loss = 0.48669583\n",
      "Iteration 254, loss = 0.48678065\n",
      "Iteration 255, loss = 0.48637083\n",
      "Iteration 256, loss = 0.48696970\n",
      "Iteration 257, loss = 0.48719850\n",
      "Iteration 258, loss = 0.48653761\n",
      "Iteration 259, loss = 0.48619868\n",
      "Iteration 260, loss = 0.48624750\n",
      "Iteration 261, loss = 0.48585914\n",
      "Iteration 262, loss = 0.48607406\n",
      "Iteration 263, loss = 0.48606689\n",
      "Iteration 264, loss = 0.48584602\n",
      "Iteration 265, loss = 0.48594826\n",
      "Iteration 266, loss = 0.48560467\n",
      "Iteration 267, loss = 0.48570378\n",
      "Iteration 268, loss = 0.48551783\n",
      "Iteration 269, loss = 0.48525615\n",
      "Iteration 270, loss = 0.48557607\n",
      "Iteration 271, loss = 0.48536407\n",
      "Iteration 272, loss = 0.48545777\n",
      "Iteration 273, loss = 0.48535494\n",
      "Iteration 274, loss = 0.48537572\n",
      "Iteration 275, loss = 0.48515350\n",
      "Iteration 276, loss = 0.48524358\n",
      "Iteration 277, loss = 0.48480717\n",
      "Iteration 278, loss = 0.48542257\n",
      "Iteration 279, loss = 0.48488668\n",
      "Iteration 280, loss = 0.48468917\n",
      "Iteration 281, loss = 0.48474766\n",
      "Iteration 282, loss = 0.48488317\n",
      "Iteration 283, loss = 0.48526881\n",
      "Iteration 284, loss = 0.48449108\n",
      "Iteration 285, loss = 0.48475254\n",
      "Iteration 286, loss = 0.48481423\n",
      "Iteration 287, loss = 0.48462359\n",
      "Iteration 288, loss = 0.48406205\n",
      "Iteration 289, loss = 0.48446044\n",
      "Iteration 290, loss = 0.48441149\n",
      "Iteration 291, loss = 0.48428889\n",
      "Iteration 292, loss = 0.48429790\n",
      "Iteration 293, loss = 0.48431111\n",
      "Iteration 294, loss = 0.48419726\n",
      "Iteration 295, loss = 0.48386695\n",
      "Iteration 296, loss = 0.48390776\n",
      "Iteration 297, loss = 0.48381756\n",
      "Iteration 298, loss = 0.48378962\n",
      "Iteration 299, loss = 0.48378412\n",
      "Iteration 300, loss = 0.48384632\n",
      "Iteration 301, loss = 0.48409070\n",
      "Iteration 302, loss = 0.48351848\n",
      "Iteration 303, loss = 0.48361989\n",
      "Iteration 304, loss = 0.48368242\n",
      "Iteration 305, loss = 0.48422035\n",
      "Iteration 306, loss = 0.48368165\n",
      "Iteration 307, loss = 0.48340470\n",
      "Iteration 308, loss = 0.48323776\n",
      "Iteration 309, loss = 0.48330272\n",
      "Iteration 310, loss = 0.48330192\n",
      "Iteration 311, loss = 0.48353458\n",
      "Iteration 312, loss = 0.48406688\n",
      "Iteration 313, loss = 0.48324574\n",
      "Iteration 314, loss = 0.48311225\n",
      "Iteration 315, loss = 0.48346511\n",
      "Iteration 316, loss = 0.48434066\n",
      "Iteration 317, loss = 0.48321748\n",
      "Iteration 318, loss = 0.48315195\n",
      "Iteration 319, loss = 0.48283801\n",
      "Iteration 320, loss = 0.48299843\n",
      "Iteration 321, loss = 0.48290113\n",
      "Iteration 322, loss = 0.48278944\n",
      "Iteration 323, loss = 0.48293253\n",
      "Iteration 324, loss = 0.48263290\n",
      "Iteration 325, loss = 0.48296341\n",
      "Iteration 326, loss = 0.48264858\n",
      "Iteration 327, loss = 0.48245856\n",
      "Iteration 328, loss = 0.48237215\n",
      "Iteration 329, loss = 0.48297261\n",
      "Iteration 330, loss = 0.48257808\n",
      "Iteration 331, loss = 0.48239438\n",
      "Iteration 332, loss = 0.48206498\n",
      "Iteration 333, loss = 0.48268700\n",
      "Iteration 334, loss = 0.48254181\n",
      "Iteration 335, loss = 0.48232987\n",
      "Iteration 336, loss = 0.48249770\n",
      "Iteration 337, loss = 0.48201182\n",
      "Iteration 338, loss = 0.48197484\n",
      "Iteration 339, loss = 0.48195346\n",
      "Iteration 340, loss = 0.48209489\n",
      "Iteration 341, loss = 0.48189691\n",
      "Iteration 342, loss = 0.48175312\n",
      "Iteration 343, loss = 0.48172894\n",
      "Iteration 344, loss = 0.48188976\n",
      "Iteration 345, loss = 0.48174196\n",
      "Iteration 346, loss = 0.48149145\n",
      "Iteration 347, loss = 0.48175129\n",
      "Iteration 348, loss = 0.48174942\n",
      "Iteration 349, loss = 0.48163336\n",
      "Iteration 350, loss = 0.48138431\n",
      "Iteration 351, loss = 0.48170007\n",
      "Iteration 352, loss = 0.48144764\n",
      "Iteration 353, loss = 0.48136959\n",
      "Iteration 354, loss = 0.48143790\n",
      "Iteration 355, loss = 0.48151993\n",
      "Iteration 356, loss = 0.48147346\n",
      "Iteration 357, loss = 0.48102982\n",
      "Iteration 358, loss = 0.48159152\n",
      "Iteration 359, loss = 0.48139619\n",
      "Iteration 360, loss = 0.48141197\n",
      "Iteration 361, loss = 0.48092889\n",
      "Iteration 362, loss = 0.48099337\n",
      "Iteration 363, loss = 0.48084727\n",
      "Iteration 364, loss = 0.48090237\n",
      "Iteration 365, loss = 0.48094566\n",
      "Iteration 366, loss = 0.48096095\n",
      "Iteration 367, loss = 0.48094105\n",
      "Iteration 368, loss = 0.48051628\n",
      "Iteration 369, loss = 0.48099051\n",
      "Iteration 370, loss = 0.48122828\n",
      "Iteration 371, loss = 0.48099212\n",
      "Iteration 372, loss = 0.48055521\n",
      "Iteration 373, loss = 0.48061313\n",
      "Iteration 374, loss = 0.48105642\n",
      "Iteration 375, loss = 0.48078401\n",
      "Iteration 376, loss = 0.48060337\n",
      "Iteration 377, loss = 0.48058307\n",
      "Iteration 378, loss = 0.48039953\n",
      "Iteration 379, loss = 0.48057648\n",
      "Iteration 380, loss = 0.48137949\n",
      "Iteration 381, loss = 0.48071337\n",
      "Iteration 382, loss = 0.48053430\n",
      "Iteration 383, loss = 0.48049121\n",
      "Iteration 384, loss = 0.48036276\n",
      "Iteration 385, loss = 0.48042146\n",
      "Iteration 386, loss = 0.48096842\n",
      "Iteration 387, loss = 0.48038628\n",
      "Iteration 388, loss = 0.48055828\n",
      "Iteration 389, loss = 0.48057260\n",
      "Iteration 390, loss = 0.48047753\n",
      "Iteration 391, loss = 0.48061760\n",
      "Iteration 392, loss = 0.48028833\n",
      "Iteration 393, loss = 0.48008736\n",
      "Iteration 394, loss = 0.48058984\n",
      "Iteration 395, loss = 0.48035791\n",
      "Iteration 396, loss = 0.48010193\n",
      "Iteration 397, loss = 0.47994470\n",
      "Iteration 398, loss = 0.48006979\n",
      "Iteration 399, loss = 0.47999170\n",
      "Iteration 400, loss = 0.48020089\n",
      "Iteration 401, loss = 0.48008813\n",
      "Iteration 402, loss = 0.47994321\n",
      "Iteration 403, loss = 0.47996931\n",
      "Iteration 404, loss = 0.48015174\n",
      "Iteration 405, loss = 0.47968781\n",
      "Iteration 406, loss = 0.47970826\n",
      "Iteration 407, loss = 0.48018214\n",
      "Iteration 408, loss = 0.47972683\n",
      "Iteration 409, loss = 0.47973936\n",
      "Iteration 410, loss = 0.47963112\n",
      "Iteration 411, loss = 0.47964500\n",
      "Iteration 412, loss = 0.47970773\n",
      "Iteration 413, loss = 0.48020126\n",
      "Iteration 414, loss = 0.48033651\n",
      "Iteration 415, loss = 0.47972681\n",
      "Iteration 416, loss = 0.47956836\n",
      "Iteration 417, loss = 0.47976277\n",
      "Iteration 418, loss = 0.47956700\n",
      "Iteration 419, loss = 0.47986554\n",
      "Iteration 420, loss = 0.47991138\n",
      "Iteration 421, loss = 0.47946430\n",
      "Iteration 422, loss = 0.47958990\n",
      "Iteration 423, loss = 0.47980586\n",
      "Iteration 424, loss = 0.47938489\n",
      "Iteration 425, loss = 0.47936914\n",
      "Iteration 426, loss = 0.47955462\n",
      "Iteration 427, loss = 0.47924588\n",
      "Iteration 428, loss = 0.47944878\n",
      "Iteration 429, loss = 0.47993548\n",
      "Iteration 430, loss = 0.47931900\n",
      "Iteration 431, loss = 0.47919912\n",
      "Iteration 432, loss = 0.47932260\n",
      "Iteration 433, loss = 0.47942546\n",
      "Iteration 434, loss = 0.47931631\n",
      "Iteration 435, loss = 0.47901230\n",
      "Iteration 436, loss = 0.47898170\n",
      "Iteration 437, loss = 0.47906719\n",
      "Iteration 438, loss = 0.47900148\n",
      "Iteration 439, loss = 0.47907123\n",
      "Iteration 440, loss = 0.47912902\n",
      "Iteration 441, loss = 0.47902475\n",
      "Iteration 442, loss = 0.47941949\n",
      "Iteration 443, loss = 0.47909919\n",
      "Iteration 444, loss = 0.47903254\n",
      "Iteration 445, loss = 0.47914262\n",
      "Iteration 446, loss = 0.47888264\n",
      "Iteration 447, loss = 0.47883109\n",
      "Iteration 448, loss = 0.47885185\n",
      "Iteration 449, loss = 0.47894995\n",
      "Iteration 450, loss = 0.47886926\n",
      "Iteration 451, loss = 0.47889154\n",
      "Iteration 452, loss = 0.47874733\n",
      "Iteration 453, loss = 0.47898825\n",
      "Iteration 454, loss = 0.47889833\n",
      "Iteration 455, loss = 0.47904938\n",
      "Iteration 456, loss = 0.47894037\n",
      "Iteration 457, loss = 0.47864657\n",
      "Iteration 458, loss = 0.47888908\n",
      "Iteration 459, loss = 0.47890647\n",
      "Iteration 460, loss = 0.47845877\n",
      "Iteration 461, loss = 0.47869042\n",
      "Iteration 462, loss = 0.47864670\n",
      "Iteration 463, loss = 0.47886652\n",
      "Iteration 464, loss = 0.47873207\n",
      "Iteration 465, loss = 0.47889788\n",
      "Iteration 466, loss = 0.47861682\n",
      "Iteration 467, loss = 0.47860036\n",
      "Iteration 468, loss = 0.47850866\n",
      "Iteration 469, loss = 0.47855584\n",
      "Iteration 470, loss = 0.47831203\n",
      "Iteration 471, loss = 0.47836088\n",
      "Iteration 472, loss = 0.47859822\n",
      "Iteration 473, loss = 0.47845418\n",
      "Iteration 474, loss = 0.47875142\n",
      "Iteration 475, loss = 0.47814511\n",
      "Iteration 476, loss = 0.47839636\n",
      "Iteration 477, loss = 0.47841244\n",
      "Iteration 478, loss = 0.47858491\n",
      "Iteration 479, loss = 0.47824540\n",
      "Iteration 480, loss = 0.47840524\n",
      "Iteration 481, loss = 0.47813651\n",
      "Iteration 482, loss = 0.47830168\n",
      "Iteration 483, loss = 0.47830876\n",
      "Iteration 484, loss = 0.47803699\n",
      "Iteration 485, loss = 0.47858967\n",
      "Iteration 486, loss = 0.47837812\n",
      "Iteration 487, loss = 0.47815231\n",
      "Iteration 488, loss = 0.47838513\n",
      "Iteration 489, loss = 0.47798100\n",
      "Iteration 490, loss = 0.47807962\n",
      "Iteration 491, loss = 0.47818868\n",
      "Iteration 492, loss = 0.47787771\n",
      "Iteration 493, loss = 0.47802869\n",
      "Iteration 494, loss = 0.47781847\n",
      "Iteration 495, loss = 0.47802678\n",
      "Iteration 496, loss = 0.47795496\n",
      "Iteration 497, loss = 0.47839993\n",
      "Iteration 498, loss = 0.47789101\n",
      "Iteration 499, loss = 0.47786720\n",
      "Iteration 500, loss = 0.47774823\n",
      "Iteration 501, loss = 0.47777371\n",
      "Iteration 502, loss = 0.47795917\n",
      "Iteration 503, loss = 0.47811048\n",
      "Iteration 504, loss = 0.47824441\n",
      "Iteration 505, loss = 0.47799853\n",
      "Iteration 506, loss = 0.47762818\n",
      "Iteration 507, loss = 0.47790467\n",
      "Iteration 508, loss = 0.47765359\n",
      "Iteration 509, loss = 0.47752357\n",
      "Iteration 510, loss = 0.47766887\n",
      "Iteration 511, loss = 0.47736611\n",
      "Iteration 512, loss = 0.47745896\n",
      "Iteration 513, loss = 0.47752522\n",
      "Iteration 514, loss = 0.47754501\n",
      "Iteration 515, loss = 0.47789441\n",
      "Iteration 516, loss = 0.47813729\n",
      "Iteration 517, loss = 0.47736288\n",
      "Iteration 518, loss = 0.47750214\n",
      "Iteration 519, loss = 0.47728363\n",
      "Iteration 520, loss = 0.47781425\n",
      "Iteration 521, loss = 0.47763147\n",
      "Iteration 522, loss = 0.47733716\n",
      "Iteration 523, loss = 0.47726365\n",
      "Iteration 524, loss = 0.47723433\n",
      "Iteration 525, loss = 0.47748755\n",
      "Iteration 526, loss = 0.47757205\n",
      "Iteration 527, loss = 0.47708555\n",
      "Iteration 528, loss = 0.47735010\n",
      "Iteration 529, loss = 0.47731705\n",
      "Iteration 530, loss = 0.47746420\n",
      "Iteration 531, loss = 0.47761787\n",
      "Iteration 532, loss = 0.47751549\n",
      "Iteration 533, loss = 0.47711340\n",
      "Iteration 534, loss = 0.47720138\n",
      "Iteration 535, loss = 0.47729310\n",
      "Iteration 536, loss = 0.47726954\n",
      "Iteration 537, loss = 0.47731338\n",
      "Iteration 538, loss = 0.47727110\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77846912\n",
      "Iteration 2, loss = 0.70833766\n",
      "Iteration 3, loss = 0.66949500\n",
      "Iteration 4, loss = 0.64529585\n",
      "Iteration 5, loss = 0.62640053\n",
      "Iteration 6, loss = 0.61166547\n",
      "Iteration 7, loss = 0.59975229\n",
      "Iteration 8, loss = 0.59081585\n",
      "Iteration 9, loss = 0.58398908\n",
      "Iteration 10, loss = 0.57848224\n",
      "Iteration 11, loss = 0.57405674\n",
      "Iteration 12, loss = 0.57069209\n",
      "Iteration 13, loss = 0.56770991\n",
      "Iteration 14, loss = 0.56530472\n",
      "Iteration 15, loss = 0.56305595\n",
      "Iteration 16, loss = 0.56107242\n",
      "Iteration 17, loss = 0.55903740\n",
      "Iteration 18, loss = 0.55745291\n",
      "Iteration 19, loss = 0.55584105\n",
      "Iteration 20, loss = 0.55458623\n",
      "Iteration 21, loss = 0.55285239\n",
      "Iteration 22, loss = 0.55154228\n",
      "Iteration 23, loss = 0.55020173\n",
      "Iteration 24, loss = 0.54864688\n",
      "Iteration 25, loss = 0.54729334\n",
      "Iteration 26, loss = 0.54624061\n",
      "Iteration 27, loss = 0.54481941\n",
      "Iteration 28, loss = 0.54348721\n",
      "Iteration 29, loss = 0.54216345\n",
      "Iteration 30, loss = 0.54085143\n",
      "Iteration 31, loss = 0.53964935\n",
      "Iteration 32, loss = 0.53822711\n",
      "Iteration 33, loss = 0.53716062\n",
      "Iteration 34, loss = 0.53659419\n",
      "Iteration 35, loss = 0.53475309\n",
      "Iteration 36, loss = 0.53352121\n",
      "Iteration 37, loss = 0.53244399\n",
      "Iteration 38, loss = 0.53183072\n",
      "Iteration 39, loss = 0.53046417\n",
      "Iteration 40, loss = 0.52930770\n",
      "Iteration 41, loss = 0.52850078\n",
      "Iteration 42, loss = 0.52769141\n",
      "Iteration 43, loss = 0.52650256\n",
      "Iteration 44, loss = 0.52555902\n",
      "Iteration 45, loss = 0.52504876\n",
      "Iteration 46, loss = 0.52405810\n",
      "Iteration 47, loss = 0.52322045\n",
      "Iteration 48, loss = 0.52228481\n",
      "Iteration 49, loss = 0.52178714\n",
      "Iteration 50, loss = 0.52080140\n",
      "Iteration 51, loss = 0.52025124\n",
      "Iteration 52, loss = 0.51976919\n",
      "Iteration 53, loss = 0.51880291\n",
      "Iteration 54, loss = 0.51817202\n",
      "Iteration 55, loss = 0.51766542\n",
      "Iteration 56, loss = 0.51684279\n",
      "Iteration 57, loss = 0.51635840\n",
      "Iteration 58, loss = 0.51595021\n",
      "Iteration 59, loss = 0.51533918\n",
      "Iteration 60, loss = 0.51486089\n",
      "Iteration 61, loss = 0.51448584\n",
      "Iteration 62, loss = 0.51399062\n",
      "Iteration 63, loss = 0.51358520\n",
      "Iteration 64, loss = 0.51311131\n",
      "Iteration 65, loss = 0.51235394\n",
      "Iteration 66, loss = 0.51195917\n",
      "Iteration 67, loss = 0.51156927\n",
      "Iteration 68, loss = 0.51103814\n",
      "Iteration 69, loss = 0.51069716\n",
      "Iteration 70, loss = 0.50981254\n",
      "Iteration 71, loss = 0.50947964\n",
      "Iteration 72, loss = 0.50910392\n",
      "Iteration 73, loss = 0.50842515\n",
      "Iteration 74, loss = 0.50798653\n",
      "Iteration 75, loss = 0.50770976\n",
      "Iteration 76, loss = 0.50681176\n",
      "Iteration 77, loss = 0.50671075\n",
      "Iteration 78, loss = 0.50620600\n",
      "Iteration 79, loss = 0.50596822\n",
      "Iteration 80, loss = 0.50529316\n",
      "Iteration 81, loss = 0.50508444\n",
      "Iteration 82, loss = 0.50445862\n",
      "Iteration 83, loss = 0.50420890\n",
      "Iteration 84, loss = 0.50370576\n",
      "Iteration 85, loss = 0.50356729\n",
      "Iteration 86, loss = 0.50342685\n",
      "Iteration 87, loss = 0.50311217\n",
      "Iteration 88, loss = 0.50266246\n",
      "Iteration 89, loss = 0.50267484\n",
      "Iteration 90, loss = 0.50207299\n",
      "Iteration 91, loss = 0.50189394\n",
      "Iteration 92, loss = 0.50170654\n",
      "Iteration 93, loss = 0.50158864\n",
      "Iteration 94, loss = 0.50098552\n",
      "Iteration 95, loss = 0.50107308\n",
      "Iteration 96, loss = 0.50113953\n",
      "Iteration 97, loss = 0.50034211\n",
      "Iteration 98, loss = 0.50026393\n",
      "Iteration 99, loss = 0.50006217\n",
      "Iteration 100, loss = 0.49999803\n",
      "Iteration 101, loss = 0.49973503\n",
      "Iteration 102, loss = 0.49948367\n",
      "Iteration 103, loss = 0.49914837\n",
      "Iteration 104, loss = 0.49916273\n",
      "Iteration 105, loss = 0.49904970\n",
      "Iteration 106, loss = 0.49896709\n",
      "Iteration 107, loss = 0.49826956\n",
      "Iteration 108, loss = 0.49837956\n",
      "Iteration 109, loss = 0.49803333\n",
      "Iteration 110, loss = 0.49791022\n",
      "Iteration 111, loss = 0.49805935\n",
      "Iteration 112, loss = 0.49757628\n",
      "Iteration 113, loss = 0.49758364\n",
      "Iteration 114, loss = 0.49716261\n",
      "Iteration 115, loss = 0.49717465\n",
      "Iteration 116, loss = 0.49742428\n",
      "Iteration 117, loss = 0.49703033\n",
      "Iteration 118, loss = 0.49672238\n",
      "Iteration 119, loss = 0.49657512\n",
      "Iteration 120, loss = 0.49627291\n",
      "Iteration 121, loss = 0.49625345\n",
      "Iteration 122, loss = 0.49599978\n",
      "Iteration 123, loss = 0.49584801\n",
      "Iteration 124, loss = 0.49597836\n",
      "Iteration 125, loss = 0.49648953\n",
      "Iteration 126, loss = 0.49537202\n",
      "Iteration 127, loss = 0.49536441\n",
      "Iteration 128, loss = 0.49517628\n",
      "Iteration 129, loss = 0.49489956\n",
      "Iteration 130, loss = 0.49489854\n",
      "Iteration 131, loss = 0.49495808\n",
      "Iteration 132, loss = 0.49492705\n",
      "Iteration 133, loss = 0.49466034\n",
      "Iteration 134, loss = 0.49455466\n",
      "Iteration 135, loss = 0.49420364\n",
      "Iteration 136, loss = 0.49428587\n",
      "Iteration 137, loss = 0.49419173\n",
      "Iteration 138, loss = 0.49450608\n",
      "Iteration 139, loss = 0.49376270\n",
      "Iteration 140, loss = 0.49402598\n",
      "Iteration 141, loss = 0.49392771\n",
      "Iteration 142, loss = 0.49357857\n",
      "Iteration 143, loss = 0.49378862\n",
      "Iteration 144, loss = 0.49351325\n",
      "Iteration 145, loss = 0.49316158\n",
      "Iteration 146, loss = 0.49308106\n",
      "Iteration 147, loss = 0.49295892\n",
      "Iteration 148, loss = 0.49301129\n",
      "Iteration 149, loss = 0.49276664\n",
      "Iteration 150, loss = 0.49287400\n",
      "Iteration 151, loss = 0.49264818\n",
      "Iteration 152, loss = 0.49265585\n",
      "Iteration 153, loss = 0.49234714\n",
      "Iteration 154, loss = 0.49209851\n",
      "Iteration 155, loss = 0.49247779\n",
      "Iteration 156, loss = 0.49207902\n",
      "Iteration 157, loss = 0.49210054\n",
      "Iteration 158, loss = 0.49186963\n",
      "Iteration 159, loss = 0.49207796\n",
      "Iteration 160, loss = 0.49205157\n",
      "Iteration 161, loss = 0.49146846\n",
      "Iteration 162, loss = 0.49144254\n",
      "Iteration 163, loss = 0.49147173\n",
      "Iteration 164, loss = 0.49132560\n",
      "Iteration 165, loss = 0.49120649\n",
      "Iteration 166, loss = 0.49241821\n",
      "Iteration 167, loss = 0.49129902\n",
      "Iteration 168, loss = 0.49124593\n",
      "Iteration 169, loss = 0.49097497\n",
      "Iteration 170, loss = 0.49126433\n",
      "Iteration 171, loss = 0.49081316\n",
      "Iteration 172, loss = 0.49091159\n",
      "Iteration 173, loss = 0.49062472\n",
      "Iteration 174, loss = 0.49092797\n",
      "Iteration 175, loss = 0.49061093\n",
      "Iteration 176, loss = 0.49052447\n",
      "Iteration 177, loss = 0.49023192\n",
      "Iteration 178, loss = 0.49018303\n",
      "Iteration 179, loss = 0.49014361\n",
      "Iteration 180, loss = 0.49018991\n",
      "Iteration 181, loss = 0.49003746\n",
      "Iteration 182, loss = 0.48987695\n",
      "Iteration 183, loss = 0.49015788\n",
      "Iteration 184, loss = 0.49012025\n",
      "Iteration 185, loss = 0.48969119\n",
      "Iteration 186, loss = 0.48993420\n",
      "Iteration 187, loss = 0.48942946\n",
      "Iteration 188, loss = 0.48984914\n",
      "Iteration 189, loss = 0.48929065\n",
      "Iteration 190, loss = 0.48939007\n",
      "Iteration 191, loss = 0.48961102\n",
      "Iteration 192, loss = 0.48925996\n",
      "Iteration 193, loss = 0.48908986\n",
      "Iteration 194, loss = 0.48896320\n",
      "Iteration 195, loss = 0.48917773\n",
      "Iteration 196, loss = 0.48908579\n",
      "Iteration 197, loss = 0.48894779\n",
      "Iteration 198, loss = 0.48919150\n",
      "Iteration 199, loss = 0.48872577\n",
      "Iteration 200, loss = 0.48899171\n",
      "Iteration 201, loss = 0.48891852\n",
      "Iteration 202, loss = 0.48859101\n",
      "Iteration 203, loss = 0.48841707\n",
      "Iteration 204, loss = 0.48856677\n",
      "Iteration 205, loss = 0.48839824\n",
      "Iteration 206, loss = 0.48809635\n",
      "Iteration 207, loss = 0.48801231\n",
      "Iteration 208, loss = 0.48819152\n",
      "Iteration 209, loss = 0.48860878\n",
      "Iteration 210, loss = 0.48821297\n",
      "Iteration 211, loss = 0.48792932\n",
      "Iteration 212, loss = 0.48798173\n",
      "Iteration 213, loss = 0.48764049\n",
      "Iteration 214, loss = 0.48758241\n",
      "Iteration 215, loss = 0.48733155\n",
      "Iteration 216, loss = 0.48735072\n",
      "Iteration 217, loss = 0.48755613\n",
      "Iteration 218, loss = 0.48712204\n",
      "Iteration 219, loss = 0.48719661\n",
      "Iteration 220, loss = 0.48692543\n",
      "Iteration 221, loss = 0.48682970\n",
      "Iteration 222, loss = 0.48698335\n",
      "Iteration 223, loss = 0.48684739\n",
      "Iteration 224, loss = 0.48683336\n",
      "Iteration 225, loss = 0.48675842\n",
      "Iteration 226, loss = 0.48657093\n",
      "Iteration 227, loss = 0.48646204\n",
      "Iteration 228, loss = 0.48657264\n",
      "Iteration 229, loss = 0.48643613\n",
      "Iteration 230, loss = 0.48630703\n",
      "Iteration 231, loss = 0.48609898\n",
      "Iteration 232, loss = 0.48634569\n",
      "Iteration 233, loss = 0.48609442\n",
      "Iteration 234, loss = 0.48599925\n",
      "Iteration 235, loss = 0.48606803\n",
      "Iteration 236, loss = 0.48589668\n",
      "Iteration 237, loss = 0.48595058\n",
      "Iteration 238, loss = 0.48564537\n",
      "Iteration 239, loss = 0.48586193\n",
      "Iteration 240, loss = 0.48562328\n",
      "Iteration 241, loss = 0.48557659\n",
      "Iteration 242, loss = 0.48544751\n",
      "Iteration 243, loss = 0.48551889\n",
      "Iteration 244, loss = 0.48536370\n",
      "Iteration 245, loss = 0.48522947\n",
      "Iteration 246, loss = 0.48567820\n",
      "Iteration 247, loss = 0.48537987\n",
      "Iteration 248, loss = 0.48497613\n",
      "Iteration 249, loss = 0.48510020\n",
      "Iteration 250, loss = 0.48497175\n",
      "Iteration 251, loss = 0.48498486\n",
      "Iteration 252, loss = 0.48488508\n",
      "Iteration 253, loss = 0.48477443\n",
      "Iteration 254, loss = 0.48487388\n",
      "Iteration 255, loss = 0.48458053\n",
      "Iteration 256, loss = 0.48448423\n",
      "Iteration 257, loss = 0.48442202\n",
      "Iteration 258, loss = 0.48445224\n",
      "Iteration 259, loss = 0.48433236\n",
      "Iteration 260, loss = 0.48435838\n",
      "Iteration 261, loss = 0.48425621\n",
      "Iteration 262, loss = 0.48433771\n",
      "Iteration 263, loss = 0.48422822\n",
      "Iteration 264, loss = 0.48402267\n",
      "Iteration 265, loss = 0.48381449\n",
      "Iteration 266, loss = 0.48379853\n",
      "Iteration 267, loss = 0.48405149\n",
      "Iteration 268, loss = 0.48404936\n",
      "Iteration 269, loss = 0.48334278\n",
      "Iteration 270, loss = 0.48346804\n",
      "Iteration 271, loss = 0.48328133\n",
      "Iteration 272, loss = 0.48348108\n",
      "Iteration 273, loss = 0.48397407\n",
      "Iteration 274, loss = 0.48319202\n",
      "Iteration 275, loss = 0.48307959\n",
      "Iteration 276, loss = 0.48317161\n",
      "Iteration 277, loss = 0.48310094\n",
      "Iteration 278, loss = 0.48299518\n",
      "Iteration 279, loss = 0.48310206\n",
      "Iteration 280, loss = 0.48286512\n",
      "Iteration 281, loss = 0.48284637\n",
      "Iteration 282, loss = 0.48264210\n",
      "Iteration 283, loss = 0.48286155\n",
      "Iteration 284, loss = 0.48292657\n",
      "Iteration 285, loss = 0.48260565\n",
      "Iteration 286, loss = 0.48289601\n",
      "Iteration 287, loss = 0.48303987\n",
      "Iteration 288, loss = 0.48225996\n",
      "Iteration 289, loss = 0.48249995\n",
      "Iteration 290, loss = 0.48243934\n",
      "Iteration 291, loss = 0.48263850\n",
      "Iteration 292, loss = 0.48347230\n",
      "Iteration 293, loss = 0.48260514\n",
      "Iteration 294, loss = 0.48237308\n",
      "Iteration 295, loss = 0.48229567\n",
      "Iteration 296, loss = 0.48192968\n",
      "Iteration 297, loss = 0.48217130\n",
      "Iteration 298, loss = 0.48181171\n",
      "Iteration 299, loss = 0.48198263\n",
      "Iteration 300, loss = 0.48167446\n",
      "Iteration 301, loss = 0.48181438\n",
      "Iteration 302, loss = 0.48181714\n",
      "Iteration 303, loss = 0.48173067\n",
      "Iteration 304, loss = 0.48195229\n",
      "Iteration 305, loss = 0.48160975\n",
      "Iteration 306, loss = 0.48169806\n",
      "Iteration 307, loss = 0.48216066\n",
      "Iteration 308, loss = 0.48175607\n",
      "Iteration 309, loss = 0.48140677\n",
      "Iteration 310, loss = 0.48142882\n",
      "Iteration 311, loss = 0.48143369\n",
      "Iteration 312, loss = 0.48157636\n",
      "Iteration 313, loss = 0.48141768\n",
      "Iteration 314, loss = 0.48146657\n",
      "Iteration 315, loss = 0.48140611\n",
      "Iteration 316, loss = 0.48142621\n",
      "Iteration 317, loss = 0.48132664\n",
      "Iteration 318, loss = 0.48131900\n",
      "Iteration 319, loss = 0.48114846\n",
      "Iteration 320, loss = 0.48104281\n",
      "Iteration 321, loss = 0.48102672\n",
      "Iteration 322, loss = 0.48094092\n",
      "Iteration 323, loss = 0.48140823\n",
      "Iteration 324, loss = 0.48090328\n",
      "Iteration 325, loss = 0.48085431\n",
      "Iteration 326, loss = 0.48091936\n",
      "Iteration 327, loss = 0.48073635\n",
      "Iteration 328, loss = 0.48082137\n",
      "Iteration 329, loss = 0.48087139\n",
      "Iteration 330, loss = 0.48085919\n",
      "Iteration 331, loss = 0.48085062\n",
      "Iteration 332, loss = 0.48071008\n",
      "Iteration 333, loss = 0.48070455\n",
      "Iteration 334, loss = 0.48088374\n",
      "Iteration 335, loss = 0.48084696\n",
      "Iteration 336, loss = 0.48069536\n",
      "Iteration 337, loss = 0.48060156\n",
      "Iteration 338, loss = 0.48094795\n",
      "Iteration 339, loss = 0.48069436\n",
      "Iteration 340, loss = 0.48027266\n",
      "Iteration 341, loss = 0.48032743\n",
      "Iteration 342, loss = 0.48025381\n",
      "Iteration 343, loss = 0.48062343\n",
      "Iteration 344, loss = 0.48036552\n",
      "Iteration 345, loss = 0.48003040\n",
      "Iteration 346, loss = 0.48003744\n",
      "Iteration 347, loss = 0.48023678\n",
      "Iteration 348, loss = 0.48045511\n",
      "Iteration 349, loss = 0.47979043\n",
      "Iteration 350, loss = 0.48104242\n",
      "Iteration 351, loss = 0.48009785\n",
      "Iteration 352, loss = 0.47989427\n",
      "Iteration 353, loss = 0.47978912\n",
      "Iteration 354, loss = 0.47976724\n",
      "Iteration 355, loss = 0.48004343\n",
      "Iteration 356, loss = 0.47953000\n",
      "Iteration 357, loss = 0.47956433\n",
      "Iteration 358, loss = 0.47967634\n",
      "Iteration 359, loss = 0.47965094\n",
      "Iteration 360, loss = 0.47948253\n",
      "Iteration 361, loss = 0.47975897\n",
      "Iteration 362, loss = 0.47978617\n",
      "Iteration 363, loss = 0.47934005\n",
      "Iteration 364, loss = 0.47936595\n",
      "Iteration 365, loss = 0.47913001\n",
      "Iteration 366, loss = 0.47924693\n",
      "Iteration 367, loss = 0.47892280\n",
      "Iteration 368, loss = 0.47911402\n",
      "Iteration 369, loss = 0.47926356\n",
      "Iteration 370, loss = 0.47895595\n",
      "Iteration 371, loss = 0.47914482\n",
      "Iteration 372, loss = 0.47940001\n",
      "Iteration 373, loss = 0.47920693\n",
      "Iteration 374, loss = 0.47885842\n",
      "Iteration 375, loss = 0.47885931\n",
      "Iteration 376, loss = 0.47860880\n",
      "Iteration 377, loss = 0.47886003\n",
      "Iteration 378, loss = 0.47888178\n",
      "Iteration 379, loss = 0.47844222\n",
      "Iteration 380, loss = 0.47843109\n",
      "Iteration 381, loss = 0.47881721\n",
      "Iteration 382, loss = 0.47857992\n",
      "Iteration 383, loss = 0.47865142\n",
      "Iteration 384, loss = 0.47856455\n",
      "Iteration 385, loss = 0.47846453\n",
      "Iteration 386, loss = 0.47830182\n",
      "Iteration 387, loss = 0.47843906\n",
      "Iteration 388, loss = 0.47839447\n",
      "Iteration 389, loss = 0.47842601\n",
      "Iteration 390, loss = 0.47851951\n",
      "Iteration 391, loss = 0.47822094\n",
      "Iteration 392, loss = 0.47838046\n",
      "Iteration 393, loss = 0.47843368\n",
      "Iteration 394, loss = 0.47806041\n",
      "Iteration 395, loss = 0.47826809\n",
      "Iteration 396, loss = 0.47833345\n",
      "Iteration 397, loss = 0.47774427\n",
      "Iteration 398, loss = 0.47784590\n",
      "Iteration 399, loss = 0.47780225\n",
      "Iteration 400, loss = 0.47797988\n",
      "Iteration 401, loss = 0.47780404\n",
      "Iteration 402, loss = 0.47787575\n",
      "Iteration 403, loss = 0.47763774\n",
      "Iteration 404, loss = 0.47816903\n",
      "Iteration 405, loss = 0.47779819\n",
      "Iteration 406, loss = 0.47783106\n",
      "Iteration 407, loss = 0.47785026\n",
      "Iteration 408, loss = 0.47782712\n",
      "Iteration 409, loss = 0.47741053\n",
      "Iteration 410, loss = 0.47759129\n",
      "Iteration 411, loss = 0.47733783\n",
      "Iteration 412, loss = 0.47721457\n",
      "Iteration 413, loss = 0.47742703\n",
      "Iteration 414, loss = 0.47706030\n",
      "Iteration 415, loss = 0.47732409\n",
      "Iteration 416, loss = 0.47720410\n",
      "Iteration 417, loss = 0.47713675\n",
      "Iteration 418, loss = 0.47711355\n",
      "Iteration 419, loss = 0.47680467\n",
      "Iteration 420, loss = 0.47672335\n",
      "Iteration 421, loss = 0.47706603\n",
      "Iteration 422, loss = 0.47693479\n",
      "Iteration 423, loss = 0.47678657\n",
      "Iteration 424, loss = 0.47697375\n",
      "Iteration 425, loss = 0.47691868\n",
      "Iteration 426, loss = 0.47690456\n",
      "Iteration 427, loss = 0.47732717\n",
      "Iteration 428, loss = 0.47672488\n",
      "Iteration 429, loss = 0.47650103\n",
      "Iteration 430, loss = 0.47791748\n",
      "Iteration 431, loss = 0.47673596\n",
      "Iteration 432, loss = 0.47676521\n",
      "Iteration 433, loss = 0.47675418\n",
      "Iteration 434, loss = 0.47634420\n",
      "Iteration 435, loss = 0.47689829\n",
      "Iteration 436, loss = 0.47618812\n",
      "Iteration 437, loss = 0.47638860\n",
      "Iteration 438, loss = 0.47630687\n",
      "Iteration 439, loss = 0.47612616\n",
      "Iteration 440, loss = 0.47610889\n",
      "Iteration 441, loss = 0.47599005\n",
      "Iteration 442, loss = 0.47590662\n",
      "Iteration 443, loss = 0.47605403\n",
      "Iteration 444, loss = 0.47608676\n",
      "Iteration 445, loss = 0.47586165\n",
      "Iteration 446, loss = 0.47577531\n",
      "Iteration 447, loss = 0.47592103\n",
      "Iteration 448, loss = 0.47606151\n",
      "Iteration 449, loss = 0.47565860\n",
      "Iteration 450, loss = 0.47570792\n",
      "Iteration 451, loss = 0.47581504\n",
      "Iteration 452, loss = 0.47562495\n",
      "Iteration 453, loss = 0.47564112\n",
      "Iteration 454, loss = 0.47566193\n",
      "Iteration 455, loss = 0.47580510\n",
      "Iteration 456, loss = 0.47578101\n",
      "Iteration 457, loss = 0.47532219\n",
      "Iteration 458, loss = 0.47585806\n",
      "Iteration 459, loss = 0.47543821\n",
      "Iteration 460, loss = 0.47540394\n",
      "Iteration 461, loss = 0.47525218\n",
      "Iteration 462, loss = 0.47556730\n",
      "Iteration 463, loss = 0.47547601\n",
      "Iteration 464, loss = 0.47570382\n",
      "Iteration 465, loss = 0.47561809\n",
      "Iteration 466, loss = 0.47531636\n",
      "Iteration 467, loss = 0.47523342\n",
      "Iteration 468, loss = 0.47573382\n",
      "Iteration 469, loss = 0.47549986\n",
      "Iteration 470, loss = 0.47533886\n",
      "Iteration 471, loss = 0.47509777\n",
      "Iteration 472, loss = 0.47579569\n",
      "Iteration 473, loss = 0.47542060\n",
      "Iteration 474, loss = 0.47492757\n",
      "Iteration 475, loss = 0.47481308\n",
      "Iteration 476, loss = 0.47513910\n",
      "Iteration 477, loss = 0.47476483\n",
      "Iteration 478, loss = 0.47492879\n",
      "Iteration 479, loss = 0.47463047\n",
      "Iteration 480, loss = 0.47473751\n",
      "Iteration 481, loss = 0.47484856\n",
      "Iteration 482, loss = 0.47476584\n",
      "Iteration 483, loss = 0.47448334\n",
      "Iteration 484, loss = 0.47467491\n",
      "Iteration 485, loss = 0.47472904\n",
      "Iteration 486, loss = 0.47431467\n",
      "Iteration 487, loss = 0.47462227\n",
      "Iteration 488, loss = 0.47444735\n",
      "Iteration 489, loss = 0.47452063\n",
      "Iteration 490, loss = 0.47434712\n",
      "Iteration 491, loss = 0.47502218\n",
      "Iteration 492, loss = 0.47449495\n",
      "Iteration 493, loss = 0.47439098\n",
      "Iteration 494, loss = 0.47420435\n",
      "Iteration 495, loss = 0.47443227\n",
      "Iteration 496, loss = 0.47447914\n",
      "Iteration 497, loss = 0.47429523\n",
      "Iteration 498, loss = 0.47456751\n",
      "Iteration 499, loss = 0.47463019\n",
      "Iteration 500, loss = 0.47480883\n",
      "Iteration 501, loss = 0.47404611\n",
      "Iteration 502, loss = 0.47421028\n",
      "Iteration 503, loss = 0.47378191\n",
      "Iteration 504, loss = 0.47418107\n",
      "Iteration 505, loss = 0.47413657\n",
      "Iteration 506, loss = 0.47400695\n",
      "Iteration 507, loss = 0.47379394\n",
      "Iteration 508, loss = 0.47408486\n",
      "Iteration 509, loss = 0.47406462\n",
      "Iteration 510, loss = 0.47355751\n",
      "Iteration 511, loss = 0.47350863\n",
      "Iteration 512, loss = 0.47373012\n",
      "Iteration 513, loss = 0.47354284\n",
      "Iteration 514, loss = 0.47405015\n",
      "Iteration 515, loss = 0.47347606\n",
      "Iteration 516, loss = 0.47355313\n",
      "Iteration 517, loss = 0.47377481\n",
      "Iteration 518, loss = 0.47357642\n",
      "Iteration 519, loss = 0.47349674\n",
      "Iteration 520, loss = 0.47368364\n",
      "Iteration 521, loss = 0.47336454\n",
      "Iteration 522, loss = 0.47340088\n",
      "Iteration 523, loss = 0.47333678\n",
      "Iteration 524, loss = 0.47365782\n",
      "Iteration 525, loss = 0.47348947\n",
      "Iteration 526, loss = 0.47340266\n",
      "Iteration 527, loss = 0.47325477\n",
      "Iteration 528, loss = 0.47323749\n",
      "Iteration 529, loss = 0.47306736\n",
      "Iteration 530, loss = 0.47320092\n",
      "Iteration 531, loss = 0.47322584\n",
      "Iteration 532, loss = 0.47308609\n",
      "Iteration 533, loss = 0.47361411\n",
      "Iteration 534, loss = 0.47316332\n",
      "Iteration 535, loss = 0.47294086\n",
      "Iteration 536, loss = 0.47291976\n",
      "Iteration 537, loss = 0.47284348\n",
      "Iteration 538, loss = 0.47344720\n",
      "Iteration 539, loss = 0.47303321\n",
      "Iteration 540, loss = 0.47304060\n",
      "Iteration 541, loss = 0.47309588\n",
      "Iteration 542, loss = 0.47380449\n",
      "Iteration 543, loss = 0.47278478\n",
      "Iteration 544, loss = 0.47259631\n",
      "Iteration 545, loss = 0.47300916\n",
      "Iteration 546, loss = 0.47281845\n",
      "Iteration 547, loss = 0.47295018\n",
      "Iteration 548, loss = 0.47287440\n",
      "Iteration 549, loss = 0.47279150\n",
      "Iteration 550, loss = 0.47275749\n",
      "Iteration 551, loss = 0.47244928\n",
      "Iteration 552, loss = 0.47279104\n",
      "Iteration 553, loss = 0.47241269\n",
      "Iteration 554, loss = 0.47262321\n",
      "Iteration 555, loss = 0.47244133\n",
      "Iteration 556, loss = 0.47278726\n",
      "Iteration 557, loss = 0.47244284\n",
      "Iteration 558, loss = 0.47286155\n",
      "Iteration 559, loss = 0.47247135\n",
      "Iteration 560, loss = 0.47272311\n",
      "Iteration 561, loss = 0.47238096\n",
      "Iteration 562, loss = 0.47244718\n",
      "Iteration 563, loss = 0.47302644\n",
      "Iteration 564, loss = 0.47237277\n",
      "Iteration 565, loss = 0.47234760\n",
      "Iteration 566, loss = 0.47239881\n",
      "Iteration 567, loss = 0.47206494\n",
      "Iteration 568, loss = 0.47233955\n",
      "Iteration 569, loss = 0.47213129\n",
      "Iteration 570, loss = 0.47215326\n",
      "Iteration 571, loss = 0.47224149\n",
      "Iteration 572, loss = 0.47224789\n",
      "Iteration 573, loss = 0.47245390\n",
      "Iteration 574, loss = 0.47194301\n",
      "Iteration 575, loss = 0.47210254\n",
      "Iteration 576, loss = 0.47186476\n",
      "Iteration 577, loss = 0.47233211\n",
      "Iteration 578, loss = 0.47190317\n",
      "Iteration 579, loss = 0.47190700\n",
      "Iteration 580, loss = 0.47203981\n",
      "Iteration 581, loss = 0.47232481\n",
      "Iteration 582, loss = 0.47213899\n",
      "Iteration 583, loss = 0.47192904\n",
      "Iteration 584, loss = 0.47171834\n",
      "Iteration 585, loss = 0.47176952\n",
      "Iteration 586, loss = 0.47214699\n",
      "Iteration 587, loss = 0.47163024\n",
      "Iteration 588, loss = 0.47289358\n",
      "Iteration 589, loss = 0.47207177\n",
      "Iteration 590, loss = 0.47177796\n",
      "Iteration 591, loss = 0.47190506\n",
      "Iteration 592, loss = 0.47168166\n",
      "Iteration 593, loss = 0.47176066\n",
      "Iteration 594, loss = 0.47176749\n",
      "Iteration 595, loss = 0.47145190\n",
      "Iteration 596, loss = 0.47163095\n",
      "Iteration 597, loss = 0.47199196\n",
      "Iteration 598, loss = 0.47141951\n",
      "Iteration 599, loss = 0.47160759\n",
      "Iteration 600, loss = 0.47158192\n",
      "Iteration 601, loss = 0.47163685\n",
      "Iteration 602, loss = 0.47177902\n",
      "Iteration 603, loss = 0.47188098\n",
      "Iteration 604, loss = 0.47183485\n",
      "Iteration 605, loss = 0.47116083\n",
      "Iteration 606, loss = 0.47157280\n",
      "Iteration 607, loss = 0.47149725\n",
      "Iteration 608, loss = 0.47130981\n",
      "Iteration 609, loss = 0.47146192\n",
      "Iteration 610, loss = 0.47153535\n",
      "Iteration 611, loss = 0.47165384\n",
      "Iteration 612, loss = 0.47173105\n",
      "Iteration 613, loss = 0.47181302\n",
      "Iteration 614, loss = 0.47130556\n",
      "Iteration 615, loss = 0.47168178\n",
      "Iteration 616, loss = 0.47215931\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76511743\n",
      "Iteration 2, loss = 0.72857970\n",
      "Iteration 3, loss = 0.70643865\n",
      "Iteration 4, loss = 0.69232186\n",
      "Iteration 5, loss = 0.68239529\n",
      "Iteration 6, loss = 0.67425741\n",
      "Iteration 7, loss = 0.66701873\n",
      "Iteration 8, loss = 0.66013054\n",
      "Iteration 9, loss = 0.65353561\n",
      "Iteration 10, loss = 0.64718612\n",
      "Iteration 11, loss = 0.64091354\n",
      "Iteration 12, loss = 0.63394720\n",
      "Iteration 13, loss = 0.62762567\n",
      "Iteration 14, loss = 0.62029009\n",
      "Iteration 15, loss = 0.61341574\n",
      "Iteration 16, loss = 0.60710093\n",
      "Iteration 17, loss = 0.60137992\n",
      "Iteration 18, loss = 0.59581847\n",
      "Iteration 19, loss = 0.59115171\n",
      "Iteration 20, loss = 0.58634863\n",
      "Iteration 21, loss = 0.58236627\n",
      "Iteration 22, loss = 0.57878715\n",
      "Iteration 23, loss = 0.57527245\n",
      "Iteration 24, loss = 0.57166525\n",
      "Iteration 25, loss = 0.56819601\n",
      "Iteration 26, loss = 0.56489720\n",
      "Iteration 27, loss = 0.56170894\n",
      "Iteration 28, loss = 0.55841877\n",
      "Iteration 29, loss = 0.55541230\n",
      "Iteration 30, loss = 0.55246990\n",
      "Iteration 31, loss = 0.54957793\n",
      "Iteration 32, loss = 0.54733771\n",
      "Iteration 33, loss = 0.54417487\n",
      "Iteration 34, loss = 0.54172169\n",
      "Iteration 35, loss = 0.53906078\n",
      "Iteration 36, loss = 0.53650191\n",
      "Iteration 37, loss = 0.53429611\n",
      "Iteration 38, loss = 0.53183647\n",
      "Iteration 39, loss = 0.52949532\n",
      "Iteration 40, loss = 0.52740871\n",
      "Iteration 41, loss = 0.52528244\n",
      "Iteration 42, loss = 0.52351667\n",
      "Iteration 43, loss = 0.52184308\n",
      "Iteration 44, loss = 0.52047531\n",
      "Iteration 45, loss = 0.51856929\n",
      "Iteration 46, loss = 0.51702092\n",
      "Iteration 47, loss = 0.51556412\n",
      "Iteration 48, loss = 0.51427961\n",
      "Iteration 49, loss = 0.51318194\n",
      "Iteration 50, loss = 0.51210482\n",
      "Iteration 51, loss = 0.51123592\n",
      "Iteration 52, loss = 0.51026937\n",
      "Iteration 53, loss = 0.50945061\n",
      "Iteration 54, loss = 0.50862791\n",
      "Iteration 55, loss = 0.50782647\n",
      "Iteration 56, loss = 0.50715035\n",
      "Iteration 57, loss = 0.50649912\n",
      "Iteration 58, loss = 0.50585862\n",
      "Iteration 59, loss = 0.50512059\n",
      "Iteration 60, loss = 0.50486830\n",
      "Iteration 61, loss = 0.50403246\n",
      "Iteration 62, loss = 0.50349039\n",
      "Iteration 63, loss = 0.50321554\n",
      "Iteration 64, loss = 0.50270365\n",
      "Iteration 65, loss = 0.50216701\n",
      "Iteration 66, loss = 0.50192315\n",
      "Iteration 67, loss = 0.50134426\n",
      "Iteration 68, loss = 0.50102405\n",
      "Iteration 69, loss = 0.50073261\n",
      "Iteration 70, loss = 0.50019640\n",
      "Iteration 71, loss = 0.50004860\n",
      "Iteration 72, loss = 0.49972380\n",
      "Iteration 73, loss = 0.49898140\n",
      "Iteration 74, loss = 0.49878075\n",
      "Iteration 75, loss = 0.49861976\n",
      "Iteration 76, loss = 0.49787374\n",
      "Iteration 77, loss = 0.49764138\n",
      "Iteration 78, loss = 0.49735694\n",
      "Iteration 79, loss = 0.49689291\n",
      "Iteration 80, loss = 0.49658057\n",
      "Iteration 81, loss = 0.49638289\n",
      "Iteration 82, loss = 0.49597914\n",
      "Iteration 83, loss = 0.49560949\n",
      "Iteration 84, loss = 0.49541212\n",
      "Iteration 85, loss = 0.49524496\n",
      "Iteration 86, loss = 0.49484368\n",
      "Iteration 87, loss = 0.49452007\n",
      "Iteration 88, loss = 0.49437658\n",
      "Iteration 89, loss = 0.49434207\n",
      "Iteration 90, loss = 0.49395933\n",
      "Iteration 91, loss = 0.49338065\n",
      "Iteration 92, loss = 0.49325577\n",
      "Iteration 93, loss = 0.49306830\n",
      "Iteration 94, loss = 0.49269155\n",
      "Iteration 95, loss = 0.49245494\n",
      "Iteration 96, loss = 0.49219771\n",
      "Iteration 97, loss = 0.49198575\n",
      "Iteration 98, loss = 0.49202578\n",
      "Iteration 99, loss = 0.49140652\n",
      "Iteration 100, loss = 0.49140529\n",
      "Iteration 101, loss = 0.49118805\n",
      "Iteration 102, loss = 0.49121678\n",
      "Iteration 103, loss = 0.49075893\n",
      "Iteration 104, loss = 0.49043677\n",
      "Iteration 105, loss = 0.49015773\n",
      "Iteration 106, loss = 0.49003263\n",
      "Iteration 107, loss = 0.48983872\n",
      "Iteration 108, loss = 0.48965824\n",
      "Iteration 109, loss = 0.48939926\n",
      "Iteration 110, loss = 0.48956771\n",
      "Iteration 111, loss = 0.48909043\n",
      "Iteration 112, loss = 0.48914662\n",
      "Iteration 113, loss = 0.48900114\n",
      "Iteration 114, loss = 0.48944962\n",
      "Iteration 115, loss = 0.48849081\n",
      "Iteration 116, loss = 0.48844379\n",
      "Iteration 117, loss = 0.48864550\n",
      "Iteration 118, loss = 0.48823403\n",
      "Iteration 119, loss = 0.48789779\n",
      "Iteration 120, loss = 0.48781002\n",
      "Iteration 121, loss = 0.48770126\n",
      "Iteration 122, loss = 0.48738856\n",
      "Iteration 123, loss = 0.48709512\n",
      "Iteration 124, loss = 0.48722326\n",
      "Iteration 125, loss = 0.48697825\n",
      "Iteration 126, loss = 0.48687167\n",
      "Iteration 127, loss = 0.48663177\n",
      "Iteration 128, loss = 0.48646605\n",
      "Iteration 129, loss = 0.48659472\n",
      "Iteration 130, loss = 0.48610789\n",
      "Iteration 131, loss = 0.48599237\n",
      "Iteration 132, loss = 0.48585483\n",
      "Iteration 133, loss = 0.48589357\n",
      "Iteration 134, loss = 0.48564095\n",
      "Iteration 135, loss = 0.48552867\n",
      "Iteration 136, loss = 0.48531726\n",
      "Iteration 137, loss = 0.48527431\n",
      "Iteration 138, loss = 0.48504138\n",
      "Iteration 139, loss = 0.48498136\n",
      "Iteration 140, loss = 0.48486676\n",
      "Iteration 141, loss = 0.48480520\n",
      "Iteration 142, loss = 0.48467796\n",
      "Iteration 143, loss = 0.48485021\n",
      "Iteration 144, loss = 0.48461791\n",
      "Iteration 145, loss = 0.48444174\n",
      "Iteration 146, loss = 0.48441157\n",
      "Iteration 147, loss = 0.48426749\n",
      "Iteration 148, loss = 0.48408571\n",
      "Iteration 149, loss = 0.48406468\n",
      "Iteration 150, loss = 0.48396524\n",
      "Iteration 151, loss = 0.48380448\n",
      "Iteration 152, loss = 0.48369388\n",
      "Iteration 153, loss = 0.48363948\n",
      "Iteration 154, loss = 0.48333274\n",
      "Iteration 155, loss = 0.48355709\n",
      "Iteration 156, loss = 0.48318090\n",
      "Iteration 157, loss = 0.48308144\n",
      "Iteration 158, loss = 0.48304999\n",
      "Iteration 159, loss = 0.48295793\n",
      "Iteration 160, loss = 0.48287031\n",
      "Iteration 161, loss = 0.48270949\n",
      "Iteration 162, loss = 0.48266359\n",
      "Iteration 163, loss = 0.48247410\n",
      "Iteration 164, loss = 0.48233055\n",
      "Iteration 165, loss = 0.48219975\n",
      "Iteration 166, loss = 0.48200451\n",
      "Iteration 167, loss = 0.48190269\n",
      "Iteration 168, loss = 0.48170932\n",
      "Iteration 169, loss = 0.48156918\n",
      "Iteration 170, loss = 0.48140748\n",
      "Iteration 171, loss = 0.48134108\n",
      "Iteration 172, loss = 0.48121892\n",
      "Iteration 173, loss = 0.48122604\n",
      "Iteration 174, loss = 0.48108441\n",
      "Iteration 175, loss = 0.48094955\n",
      "Iteration 176, loss = 0.48086408\n",
      "Iteration 177, loss = 0.48063651\n",
      "Iteration 178, loss = 0.48047412\n",
      "Iteration 179, loss = 0.48044547\n",
      "Iteration 180, loss = 0.48048919\n",
      "Iteration 181, loss = 0.48036380\n",
      "Iteration 182, loss = 0.48044683\n",
      "Iteration 183, loss = 0.48008993\n",
      "Iteration 184, loss = 0.48005598\n",
      "Iteration 185, loss = 0.48006081\n",
      "Iteration 186, loss = 0.48001177\n",
      "Iteration 187, loss = 0.47986445\n",
      "Iteration 188, loss = 0.47968707\n",
      "Iteration 189, loss = 0.47960244\n",
      "Iteration 190, loss = 0.47949092\n",
      "Iteration 191, loss = 0.47952943\n",
      "Iteration 192, loss = 0.47943901\n",
      "Iteration 193, loss = 0.47926208\n",
      "Iteration 194, loss = 0.47932846\n",
      "Iteration 195, loss = 0.47906274\n",
      "Iteration 196, loss = 0.47903624\n",
      "Iteration 197, loss = 0.47910063\n",
      "Iteration 198, loss = 0.47881613\n",
      "Iteration 199, loss = 0.47870893\n",
      "Iteration 200, loss = 0.47870423\n",
      "Iteration 201, loss = 0.47870226\n",
      "Iteration 202, loss = 0.47865126\n",
      "Iteration 203, loss = 0.47850514\n",
      "Iteration 204, loss = 0.47851550\n",
      "Iteration 205, loss = 0.47887169\n",
      "Iteration 206, loss = 0.47820143\n",
      "Iteration 207, loss = 0.47828340\n",
      "Iteration 208, loss = 0.47823994\n",
      "Iteration 209, loss = 0.47836625\n",
      "Iteration 210, loss = 0.47835307\n",
      "Iteration 211, loss = 0.47791787\n",
      "Iteration 212, loss = 0.47782581\n",
      "Iteration 213, loss = 0.47792894\n",
      "Iteration 214, loss = 0.47772491\n",
      "Iteration 215, loss = 0.47768718\n",
      "Iteration 216, loss = 0.47757803\n",
      "Iteration 217, loss = 0.47764273\n",
      "Iteration 218, loss = 0.47749404\n",
      "Iteration 219, loss = 0.47742540\n",
      "Iteration 220, loss = 0.47763279\n",
      "Iteration 221, loss = 0.47712267\n",
      "Iteration 222, loss = 0.47727194\n",
      "Iteration 223, loss = 0.47764412\n",
      "Iteration 224, loss = 0.47687579\n",
      "Iteration 225, loss = 0.47705869\n",
      "Iteration 226, loss = 0.47698204\n",
      "Iteration 227, loss = 0.47694749\n",
      "Iteration 228, loss = 0.47687696\n",
      "Iteration 229, loss = 0.47702234\n",
      "Iteration 230, loss = 0.47673855\n",
      "Iteration 231, loss = 0.47691787\n",
      "Iteration 232, loss = 0.47661994\n",
      "Iteration 233, loss = 0.47651816\n",
      "Iteration 234, loss = 0.47670920\n",
      "Iteration 235, loss = 0.47643789\n",
      "Iteration 236, loss = 0.47686243\n",
      "Iteration 237, loss = 0.47682969\n",
      "Iteration 238, loss = 0.47654817\n",
      "Iteration 239, loss = 0.47655837\n",
      "Iteration 240, loss = 0.47607243\n",
      "Iteration 241, loss = 0.47603166\n",
      "Iteration 242, loss = 0.47631807\n",
      "Iteration 243, loss = 0.47611018\n",
      "Iteration 244, loss = 0.47592302\n",
      "Iteration 245, loss = 0.47620805\n",
      "Iteration 246, loss = 0.47577628\n",
      "Iteration 247, loss = 0.47558522\n",
      "Iteration 248, loss = 0.47592759\n",
      "Iteration 249, loss = 0.47559282\n",
      "Iteration 250, loss = 0.47548812\n",
      "Iteration 251, loss = 0.47541765\n",
      "Iteration 252, loss = 0.47530224\n",
      "Iteration 253, loss = 0.47532104\n",
      "Iteration 254, loss = 0.47544337\n",
      "Iteration 255, loss = 0.47522162\n",
      "Iteration 256, loss = 0.47502376\n",
      "Iteration 257, loss = 0.47521436\n",
      "Iteration 258, loss = 0.47510391\n",
      "Iteration 259, loss = 0.47501700\n",
      "Iteration 260, loss = 0.47503952\n",
      "Iteration 261, loss = 0.47538497\n",
      "Iteration 262, loss = 0.47551697\n",
      "Iteration 263, loss = 0.47462992\n",
      "Iteration 264, loss = 0.47473700\n",
      "Iteration 265, loss = 0.47500163\n",
      "Iteration 266, loss = 0.47499619\n",
      "Iteration 267, loss = 0.47454398\n",
      "Iteration 268, loss = 0.47465256\n",
      "Iteration 269, loss = 0.47440403\n",
      "Iteration 270, loss = 0.47440837\n",
      "Iteration 271, loss = 0.47435334\n",
      "Iteration 272, loss = 0.47450514\n",
      "Iteration 273, loss = 0.47446074\n",
      "Iteration 274, loss = 0.47459307\n",
      "Iteration 275, loss = 0.47408604\n",
      "Iteration 276, loss = 0.47429175\n",
      "Iteration 277, loss = 0.47403086\n",
      "Iteration 278, loss = 0.47419347\n",
      "Iteration 279, loss = 0.47410766\n",
      "Iteration 280, loss = 0.47420396\n",
      "Iteration 281, loss = 0.47403698\n",
      "Iteration 282, loss = 0.47379301\n",
      "Iteration 283, loss = 0.47393224\n",
      "Iteration 284, loss = 0.47374091\n",
      "Iteration 285, loss = 0.47402155\n",
      "Iteration 286, loss = 0.47381768\n",
      "Iteration 287, loss = 0.47369730\n",
      "Iteration 288, loss = 0.47375519\n",
      "Iteration 289, loss = 0.47354561\n",
      "Iteration 290, loss = 0.47375944\n",
      "Iteration 291, loss = 0.47353179\n",
      "Iteration 292, loss = 0.47355608\n",
      "Iteration 293, loss = 0.47350035\n",
      "Iteration 294, loss = 0.47342839\n",
      "Iteration 295, loss = 0.47327937\n",
      "Iteration 296, loss = 0.47326732\n",
      "Iteration 297, loss = 0.47337748\n",
      "Iteration 298, loss = 0.47319967\n",
      "Iteration 299, loss = 0.47314109\n",
      "Iteration 300, loss = 0.47292278\n",
      "Iteration 301, loss = 0.47309026\n",
      "Iteration 302, loss = 0.47297369\n",
      "Iteration 303, loss = 0.47298070\n",
      "Iteration 304, loss = 0.47299370\n",
      "Iteration 305, loss = 0.47316961\n",
      "Iteration 306, loss = 0.47280550\n",
      "Iteration 307, loss = 0.47280443\n",
      "Iteration 308, loss = 0.47291702\n",
      "Iteration 309, loss = 0.47284885\n",
      "Iteration 310, loss = 0.47291826\n",
      "Iteration 311, loss = 0.47282265\n",
      "Iteration 312, loss = 0.47270073\n",
      "Iteration 313, loss = 0.47281706\n",
      "Iteration 314, loss = 0.47256273\n",
      "Iteration 315, loss = 0.47248265\n",
      "Iteration 316, loss = 0.47268222\n",
      "Iteration 317, loss = 0.47279983\n",
      "Iteration 318, loss = 0.47250311\n",
      "Iteration 319, loss = 0.47271262\n",
      "Iteration 320, loss = 0.47241522\n",
      "Iteration 321, loss = 0.47242415\n",
      "Iteration 322, loss = 0.47241419\n",
      "Iteration 323, loss = 0.47242335\n",
      "Iteration 324, loss = 0.47227839\n",
      "Iteration 325, loss = 0.47225188\n",
      "Iteration 326, loss = 0.47219779\n",
      "Iteration 327, loss = 0.47214093\n",
      "Iteration 328, loss = 0.47211461\n",
      "Iteration 329, loss = 0.47220281\n",
      "Iteration 330, loss = 0.47224708\n",
      "Iteration 331, loss = 0.47226269\n",
      "Iteration 332, loss = 0.47200416\n",
      "Iteration 333, loss = 0.47202171\n",
      "Iteration 334, loss = 0.47208794\n",
      "Iteration 335, loss = 0.47175486\n",
      "Iteration 336, loss = 0.47211760\n",
      "Iteration 337, loss = 0.47186042\n",
      "Iteration 338, loss = 0.47187114\n",
      "Iteration 339, loss = 0.47220200\n",
      "Iteration 340, loss = 0.47181467\n",
      "Iteration 341, loss = 0.47193914\n",
      "Iteration 342, loss = 0.47192769\n",
      "Iteration 343, loss = 0.47203391\n",
      "Iteration 344, loss = 0.47179172\n",
      "Iteration 345, loss = 0.47162834\n",
      "Iteration 346, loss = 0.47187538\n",
      "Iteration 347, loss = 0.47214389\n",
      "Iteration 348, loss = 0.47162415\n",
      "Iteration 349, loss = 0.47183380\n",
      "Iteration 350, loss = 0.47201428\n",
      "Iteration 351, loss = 0.47145663\n",
      "Iteration 352, loss = 0.47138251\n",
      "Iteration 353, loss = 0.47156318\n",
      "Iteration 354, loss = 0.47159165\n",
      "Iteration 355, loss = 0.47148197\n",
      "Iteration 356, loss = 0.47133039\n",
      "Iteration 357, loss = 0.47124578\n",
      "Iteration 358, loss = 0.47147526\n",
      "Iteration 359, loss = 0.47135668\n",
      "Iteration 360, loss = 0.47134160\n",
      "Iteration 361, loss = 0.47136600\n",
      "Iteration 362, loss = 0.47157319\n",
      "Iteration 363, loss = 0.47198817\n",
      "Iteration 364, loss = 0.47111118\n",
      "Iteration 365, loss = 0.47112627\n",
      "Iteration 366, loss = 0.47119767\n",
      "Iteration 367, loss = 0.47095495\n",
      "Iteration 368, loss = 0.47118401\n",
      "Iteration 369, loss = 0.47107304\n",
      "Iteration 370, loss = 0.47118132\n",
      "Iteration 371, loss = 0.47117558\n",
      "Iteration 372, loss = 0.47088507\n",
      "Iteration 373, loss = 0.47100160\n",
      "Iteration 374, loss = 0.47069946\n",
      "Iteration 375, loss = 0.47083436\n",
      "Iteration 376, loss = 0.47087888\n",
      "Iteration 377, loss = 0.47093823\n",
      "Iteration 378, loss = 0.47099982\n",
      "Iteration 379, loss = 0.47073806\n",
      "Iteration 380, loss = 0.47068183\n",
      "Iteration 381, loss = 0.47071050\n",
      "Iteration 382, loss = 0.47090011\n",
      "Iteration 383, loss = 0.47080141\n",
      "Iteration 384, loss = 0.47050088\n",
      "Iteration 385, loss = 0.47095174\n",
      "Iteration 386, loss = 0.47076510\n",
      "Iteration 387, loss = 0.47055172\n",
      "Iteration 388, loss = 0.47072983\n",
      "Iteration 389, loss = 0.47056356\n",
      "Iteration 390, loss = 0.47071653\n",
      "Iteration 391, loss = 0.47040152\n",
      "Iteration 392, loss = 0.47061328\n",
      "Iteration 393, loss = 0.47033321\n",
      "Iteration 394, loss = 0.47041854\n",
      "Iteration 395, loss = 0.47052277\n",
      "Iteration 396, loss = 0.47046052\n",
      "Iteration 397, loss = 0.47063028\n",
      "Iteration 398, loss = 0.47067847\n",
      "Iteration 399, loss = 0.47050871\n",
      "Iteration 400, loss = 0.47033408\n",
      "Iteration 401, loss = 0.47040034\n",
      "Iteration 402, loss = 0.47050004\n",
      "Iteration 403, loss = 0.47029361\n",
      "Iteration 404, loss = 0.47052995\n",
      "Iteration 405, loss = 0.47054373\n",
      "Iteration 406, loss = 0.47037824\n",
      "Iteration 407, loss = 0.47002578\n",
      "Iteration 408, loss = 0.47028102\n",
      "Iteration 409, loss = 0.47051001\n",
      "Iteration 410, loss = 0.47034970\n",
      "Iteration 411, loss = 0.47023584\n",
      "Iteration 412, loss = 0.47014500\n",
      "Iteration 413, loss = 0.47000955\n",
      "Iteration 414, loss = 0.47015750\n",
      "Iteration 415, loss = 0.47009067\n",
      "Iteration 416, loss = 0.47027953\n",
      "Iteration 417, loss = 0.47033010\n",
      "Iteration 418, loss = 0.46989518\n",
      "Iteration 419, loss = 0.47007587\n",
      "Iteration 420, loss = 0.46994644\n",
      "Iteration 421, loss = 0.47016736\n",
      "Iteration 422, loss = 0.47017773\n",
      "Iteration 423, loss = 0.47007155\n",
      "Iteration 424, loss = 0.47014482\n",
      "Iteration 425, loss = 0.46982120\n",
      "Iteration 426, loss = 0.46980908\n",
      "Iteration 427, loss = 0.46997447\n",
      "Iteration 428, loss = 0.46979925\n",
      "Iteration 429, loss = 0.46988703\n",
      "Iteration 430, loss = 0.46980306\n",
      "Iteration 431, loss = 0.46967552\n",
      "Iteration 432, loss = 0.46948887\n",
      "Iteration 433, loss = 0.46956730\n",
      "Iteration 434, loss = 0.46972744\n",
      "Iteration 435, loss = 0.46954869\n",
      "Iteration 436, loss = 0.46948447\n",
      "Iteration 437, loss = 0.46949542\n",
      "Iteration 438, loss = 0.47001330\n",
      "Iteration 439, loss = 0.46958251\n",
      "Iteration 440, loss = 0.46952820\n",
      "Iteration 441, loss = 0.46955516\n",
      "Iteration 442, loss = 0.46955480\n",
      "Iteration 443, loss = 0.46941882\n",
      "Iteration 444, loss = 0.46979532\n",
      "Iteration 445, loss = 0.46978428\n",
      "Iteration 446, loss = 0.46952602\n",
      "Iteration 447, loss = 0.46937314\n",
      "Iteration 448, loss = 0.46936001\n",
      "Iteration 449, loss = 0.46960251\n",
      "Iteration 450, loss = 0.46946780\n",
      "Iteration 451, loss = 0.46930654\n",
      "Iteration 452, loss = 0.46929278\n",
      "Iteration 453, loss = 0.46928041\n",
      "Iteration 454, loss = 0.46920009\n",
      "Iteration 455, loss = 0.46921563\n",
      "Iteration 456, loss = 0.46936175\n",
      "Iteration 457, loss = 0.46912688\n",
      "Iteration 458, loss = 0.46917651\n",
      "Iteration 459, loss = 0.46918085\n",
      "Iteration 460, loss = 0.46918348\n",
      "Iteration 461, loss = 0.46909791\n",
      "Iteration 462, loss = 0.46889774\n",
      "Iteration 463, loss = 0.46887976\n",
      "Iteration 464, loss = 0.46917520\n",
      "Iteration 465, loss = 0.46925301\n",
      "Iteration 466, loss = 0.46889392\n",
      "Iteration 467, loss = 0.46886984\n",
      "Iteration 468, loss = 0.46886905\n",
      "Iteration 469, loss = 0.46889751\n",
      "Iteration 470, loss = 0.46930370\n",
      "Iteration 471, loss = 0.46895704\n",
      "Iteration 472, loss = 0.46893835\n",
      "Iteration 473, loss = 0.46909677\n",
      "Iteration 474, loss = 0.46922944\n",
      "Iteration 475, loss = 0.46939329\n",
      "Iteration 476, loss = 0.46863409\n",
      "Iteration 477, loss = 0.46876400\n",
      "Iteration 478, loss = 0.46868329\n",
      "Iteration 479, loss = 0.46870397\n",
      "Iteration 480, loss = 0.46882827\n",
      "Iteration 481, loss = 0.46874219\n",
      "Iteration 482, loss = 0.46853437\n",
      "Iteration 483, loss = 0.46841971\n",
      "Iteration 484, loss = 0.46855933\n",
      "Iteration 485, loss = 0.46865759\n",
      "Iteration 486, loss = 0.46868207\n",
      "Iteration 487, loss = 0.46843542\n",
      "Iteration 488, loss = 0.46844360\n",
      "Iteration 489, loss = 0.46842663\n",
      "Iteration 490, loss = 0.46857512\n",
      "Iteration 491, loss = 0.46868801\n",
      "Iteration 492, loss = 0.46896334\n",
      "Iteration 493, loss = 0.46865182\n",
      "Iteration 494, loss = 0.46882815\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94436958\n",
      "Iteration 2, loss = 0.84399187\n",
      "Iteration 3, loss = 0.77928909\n",
      "Iteration 4, loss = 0.73960438\n",
      "Iteration 5, loss = 0.71386763\n",
      "Iteration 6, loss = 0.69589868\n",
      "Iteration 7, loss = 0.68130586\n",
      "Iteration 8, loss = 0.66854154\n",
      "Iteration 9, loss = 0.65738915\n",
      "Iteration 10, loss = 0.64682090\n",
      "Iteration 11, loss = 0.63710231\n",
      "Iteration 12, loss = 0.62800254\n",
      "Iteration 13, loss = 0.61916632\n",
      "Iteration 14, loss = 0.61134439\n",
      "Iteration 15, loss = 0.60383500\n",
      "Iteration 16, loss = 0.59722868\n",
      "Iteration 17, loss = 0.59118218\n",
      "Iteration 18, loss = 0.58637617\n",
      "Iteration 19, loss = 0.58124396\n",
      "Iteration 20, loss = 0.57699150\n",
      "Iteration 21, loss = 0.57302905\n",
      "Iteration 22, loss = 0.56919270\n",
      "Iteration 23, loss = 0.56564178\n",
      "Iteration 24, loss = 0.56204133\n",
      "Iteration 25, loss = 0.55861636\n",
      "Iteration 26, loss = 0.55539577\n",
      "Iteration 27, loss = 0.55245988\n",
      "Iteration 28, loss = 0.54979473\n",
      "Iteration 29, loss = 0.54713993\n",
      "Iteration 30, loss = 0.54468738\n",
      "Iteration 31, loss = 0.54238820\n",
      "Iteration 32, loss = 0.54018955\n",
      "Iteration 33, loss = 0.53808304\n",
      "Iteration 34, loss = 0.53625439\n",
      "Iteration 35, loss = 0.53433282\n",
      "Iteration 36, loss = 0.53264191\n",
      "Iteration 37, loss = 0.53108710\n",
      "Iteration 38, loss = 0.52948817\n",
      "Iteration 39, loss = 0.52808120\n",
      "Iteration 40, loss = 0.52673124\n",
      "Iteration 41, loss = 0.52539318\n",
      "Iteration 42, loss = 0.52417741\n",
      "Iteration 43, loss = 0.52294148\n",
      "Iteration 44, loss = 0.52195835\n",
      "Iteration 45, loss = 0.52108647\n",
      "Iteration 46, loss = 0.52009349\n",
      "Iteration 47, loss = 0.51926723\n",
      "Iteration 48, loss = 0.51842358\n",
      "Iteration 49, loss = 0.51765052\n",
      "Iteration 50, loss = 0.51685062\n",
      "Iteration 51, loss = 0.51605638\n",
      "Iteration 52, loss = 0.51542411\n",
      "Iteration 53, loss = 0.51472264\n",
      "Iteration 54, loss = 0.51403711\n",
      "Iteration 55, loss = 0.51358443\n",
      "Iteration 56, loss = 0.51298813\n",
      "Iteration 57, loss = 0.51243424\n",
      "Iteration 58, loss = 0.51164423\n",
      "Iteration 59, loss = 0.51106744\n",
      "Iteration 60, loss = 0.51059154\n",
      "Iteration 61, loss = 0.51015810\n",
      "Iteration 62, loss = 0.50969799\n",
      "Iteration 63, loss = 0.50909211\n",
      "Iteration 64, loss = 0.50877626\n",
      "Iteration 65, loss = 0.50818017\n",
      "Iteration 66, loss = 0.50805132\n",
      "Iteration 67, loss = 0.50743975\n",
      "Iteration 68, loss = 0.50716649\n",
      "Iteration 69, loss = 0.50688762\n",
      "Iteration 70, loss = 0.50603338\n",
      "Iteration 71, loss = 0.50572866\n",
      "Iteration 72, loss = 0.50529041\n",
      "Iteration 73, loss = 0.50487415\n",
      "Iteration 74, loss = 0.50459034\n",
      "Iteration 75, loss = 0.50415135\n",
      "Iteration 76, loss = 0.50381867\n",
      "Iteration 77, loss = 0.50355687\n",
      "Iteration 78, loss = 0.50302450\n",
      "Iteration 79, loss = 0.50286938\n",
      "Iteration 80, loss = 0.50255970\n",
      "Iteration 81, loss = 0.50214450\n",
      "Iteration 82, loss = 0.50182122\n",
      "Iteration 83, loss = 0.50158206\n",
      "Iteration 84, loss = 0.50137327\n",
      "Iteration 85, loss = 0.50103632\n",
      "Iteration 86, loss = 0.50102346\n",
      "Iteration 87, loss = 0.50061311\n",
      "Iteration 88, loss = 0.50045687\n",
      "Iteration 89, loss = 0.50042419\n",
      "Iteration 90, loss = 0.50007553\n",
      "Iteration 91, loss = 0.50006483\n",
      "Iteration 92, loss = 0.49968202\n",
      "Iteration 93, loss = 0.49950033\n",
      "Iteration 94, loss = 0.49920449\n",
      "Iteration 95, loss = 0.49914059\n",
      "Iteration 96, loss = 0.49875315\n",
      "Iteration 97, loss = 0.49867019\n",
      "Iteration 98, loss = 0.49849108\n",
      "Iteration 99, loss = 0.49816549\n",
      "Iteration 100, loss = 0.49801793\n",
      "Iteration 101, loss = 0.49775592\n",
      "Iteration 102, loss = 0.49757381\n",
      "Iteration 103, loss = 0.49718963\n",
      "Iteration 104, loss = 0.49755638\n",
      "Iteration 105, loss = 0.49653390\n",
      "Iteration 106, loss = 0.49687840\n",
      "Iteration 107, loss = 0.49633318\n",
      "Iteration 108, loss = 0.49615178\n",
      "Iteration 109, loss = 0.49593798\n",
      "Iteration 110, loss = 0.49579663\n",
      "Iteration 111, loss = 0.49557673\n",
      "Iteration 112, loss = 0.49562265\n",
      "Iteration 113, loss = 0.49537311\n",
      "Iteration 114, loss = 0.49520758\n",
      "Iteration 115, loss = 0.49485700\n",
      "Iteration 116, loss = 0.49456799\n",
      "Iteration 117, loss = 0.49445047\n",
      "Iteration 118, loss = 0.49427083\n",
      "Iteration 119, loss = 0.49407368\n",
      "Iteration 120, loss = 0.49379802\n",
      "Iteration 121, loss = 0.49377873\n",
      "Iteration 122, loss = 0.49345850\n",
      "Iteration 123, loss = 0.49345185\n",
      "Iteration 124, loss = 0.49337411\n",
      "Iteration 125, loss = 0.49314126\n",
      "Iteration 126, loss = 0.49285054\n",
      "Iteration 127, loss = 0.49256719\n",
      "Iteration 128, loss = 0.49259763\n",
      "Iteration 129, loss = 0.49235847\n",
      "Iteration 130, loss = 0.49227902\n",
      "Iteration 131, loss = 0.49201070\n",
      "Iteration 132, loss = 0.49187335\n",
      "Iteration 133, loss = 0.49175751\n",
      "Iteration 134, loss = 0.49165617\n",
      "Iteration 135, loss = 0.49127825\n",
      "Iteration 136, loss = 0.49118609\n",
      "Iteration 137, loss = 0.49104042\n",
      "Iteration 138, loss = 0.49103070\n",
      "Iteration 139, loss = 0.49086446\n",
      "Iteration 140, loss = 0.49062744\n",
      "Iteration 141, loss = 0.49052052\n",
      "Iteration 142, loss = 0.49040795\n",
      "Iteration 143, loss = 0.49023671\n",
      "Iteration 144, loss = 0.49020027\n",
      "Iteration 145, loss = 0.49007269\n",
      "Iteration 146, loss = 0.48980763\n",
      "Iteration 147, loss = 0.49001105\n",
      "Iteration 148, loss = 0.48971201\n",
      "Iteration 149, loss = 0.48959330\n",
      "Iteration 150, loss = 0.48939892\n",
      "Iteration 151, loss = 0.48947372\n",
      "Iteration 152, loss = 0.48915185\n",
      "Iteration 153, loss = 0.48919628\n",
      "Iteration 154, loss = 0.48900368\n",
      "Iteration 155, loss = 0.48881433\n",
      "Iteration 156, loss = 0.48880406\n",
      "Iteration 157, loss = 0.48865969\n",
      "Iteration 158, loss = 0.48872620\n",
      "Iteration 159, loss = 0.48840105\n",
      "Iteration 160, loss = 0.48842604\n",
      "Iteration 161, loss = 0.48822625\n",
      "Iteration 162, loss = 0.48812876\n",
      "Iteration 163, loss = 0.48787877\n",
      "Iteration 164, loss = 0.48817267\n",
      "Iteration 165, loss = 0.48780771\n",
      "Iteration 166, loss = 0.48751598\n",
      "Iteration 167, loss = 0.48744003\n",
      "Iteration 168, loss = 0.48741013\n",
      "Iteration 169, loss = 0.48723717\n",
      "Iteration 170, loss = 0.48706500\n",
      "Iteration 171, loss = 0.48705763\n",
      "Iteration 172, loss = 0.48667900\n",
      "Iteration 173, loss = 0.48682675\n",
      "Iteration 174, loss = 0.48659373\n",
      "Iteration 175, loss = 0.48648222\n",
      "Iteration 176, loss = 0.48655437\n",
      "Iteration 177, loss = 0.48641925\n",
      "Iteration 178, loss = 0.48638094\n",
      "Iteration 179, loss = 0.48626878\n",
      "Iteration 180, loss = 0.48606662\n",
      "Iteration 181, loss = 0.48607513\n",
      "Iteration 182, loss = 0.48568318\n",
      "Iteration 183, loss = 0.48563987\n",
      "Iteration 184, loss = 0.48594688\n",
      "Iteration 185, loss = 0.48542919\n",
      "Iteration 186, loss = 0.48530361\n",
      "Iteration 187, loss = 0.48541435\n",
      "Iteration 188, loss = 0.48519241\n",
      "Iteration 189, loss = 0.48505079\n",
      "Iteration 190, loss = 0.48501405\n",
      "Iteration 191, loss = 0.48488122\n",
      "Iteration 192, loss = 0.48498387\n",
      "Iteration 193, loss = 0.48473814\n",
      "Iteration 194, loss = 0.48460375\n",
      "Iteration 195, loss = 0.48447450\n",
      "Iteration 196, loss = 0.48439299\n",
      "Iteration 197, loss = 0.48445908\n",
      "Iteration 198, loss = 0.48425692\n",
      "Iteration 199, loss = 0.48405474\n",
      "Iteration 200, loss = 0.48432066\n",
      "Iteration 201, loss = 0.48390349\n",
      "Iteration 202, loss = 0.48384118\n",
      "Iteration 203, loss = 0.48368486\n",
      "Iteration 204, loss = 0.48378261\n",
      "Iteration 205, loss = 0.48353505\n",
      "Iteration 206, loss = 0.48349133\n",
      "Iteration 207, loss = 0.48362882\n",
      "Iteration 208, loss = 0.48319552\n",
      "Iteration 209, loss = 0.48350206\n",
      "Iteration 210, loss = 0.48340781\n",
      "Iteration 211, loss = 0.48312822\n",
      "Iteration 212, loss = 0.48277951\n",
      "Iteration 213, loss = 0.48305967\n",
      "Iteration 214, loss = 0.48279378\n",
      "Iteration 215, loss = 0.48288713\n",
      "Iteration 216, loss = 0.48265168\n",
      "Iteration 217, loss = 0.48291200\n",
      "Iteration 218, loss = 0.48244059\n",
      "Iteration 219, loss = 0.48255500\n",
      "Iteration 220, loss = 0.48229762\n",
      "Iteration 221, loss = 0.48222548\n",
      "Iteration 222, loss = 0.48227738\n",
      "Iteration 223, loss = 0.48195399\n",
      "Iteration 224, loss = 0.48194257\n",
      "Iteration 225, loss = 0.48183604\n",
      "Iteration 226, loss = 0.48165097\n",
      "Iteration 227, loss = 0.48177075\n",
      "Iteration 228, loss = 0.48152277\n",
      "Iteration 229, loss = 0.48157069\n",
      "Iteration 230, loss = 0.48131845\n",
      "Iteration 231, loss = 0.48128749\n",
      "Iteration 232, loss = 0.48117997\n",
      "Iteration 233, loss = 0.48105963\n",
      "Iteration 234, loss = 0.48143015\n",
      "Iteration 235, loss = 0.48102904\n",
      "Iteration 236, loss = 0.48102200\n",
      "Iteration 237, loss = 0.48109105\n",
      "Iteration 238, loss = 0.48087479\n",
      "Iteration 239, loss = 0.48068657\n",
      "Iteration 240, loss = 0.48082435\n",
      "Iteration 241, loss = 0.48059788\n",
      "Iteration 242, loss = 0.48074968\n",
      "Iteration 243, loss = 0.48044463\n",
      "Iteration 244, loss = 0.48051006\n",
      "Iteration 245, loss = 0.48070035\n",
      "Iteration 246, loss = 0.48042647\n",
      "Iteration 247, loss = 0.48036775\n",
      "Iteration 248, loss = 0.48039386\n",
      "Iteration 249, loss = 0.48046560\n",
      "Iteration 250, loss = 0.48024236\n",
      "Iteration 251, loss = 0.48027961\n",
      "Iteration 252, loss = 0.48055891\n",
      "Iteration 253, loss = 0.48015465\n",
      "Iteration 254, loss = 0.47981383\n",
      "Iteration 255, loss = 0.47988379\n",
      "Iteration 256, loss = 0.47984161\n",
      "Iteration 257, loss = 0.47981916\n",
      "Iteration 258, loss = 0.47963832\n",
      "Iteration 259, loss = 0.47965186\n",
      "Iteration 260, loss = 0.47956946\n",
      "Iteration 261, loss = 0.47952112\n",
      "Iteration 262, loss = 0.47949946\n",
      "Iteration 263, loss = 0.47947641\n",
      "Iteration 264, loss = 0.47938781\n",
      "Iteration 265, loss = 0.47931550\n",
      "Iteration 266, loss = 0.47933693\n",
      "Iteration 267, loss = 0.47910465\n",
      "Iteration 268, loss = 0.47914402\n",
      "Iteration 269, loss = 0.47902480\n",
      "Iteration 270, loss = 0.47944805\n",
      "Iteration 271, loss = 0.47899310\n",
      "Iteration 272, loss = 0.47892708\n",
      "Iteration 273, loss = 0.47877822\n",
      "Iteration 274, loss = 0.47870735\n",
      "Iteration 275, loss = 0.47879864\n",
      "Iteration 276, loss = 0.47860980\n",
      "Iteration 277, loss = 0.47888718\n",
      "Iteration 278, loss = 0.47859706\n",
      "Iteration 279, loss = 0.47862267\n",
      "Iteration 280, loss = 0.47840418\n",
      "Iteration 281, loss = 0.47865690\n",
      "Iteration 282, loss = 0.47831048\n",
      "Iteration 283, loss = 0.47846943\n",
      "Iteration 284, loss = 0.47819062\n",
      "Iteration 285, loss = 0.47818196\n",
      "Iteration 286, loss = 0.47804962\n",
      "Iteration 287, loss = 0.47827313\n",
      "Iteration 288, loss = 0.47782181\n",
      "Iteration 289, loss = 0.47800177\n",
      "Iteration 290, loss = 0.47799761\n",
      "Iteration 291, loss = 0.47782988\n",
      "Iteration 292, loss = 0.47781512\n",
      "Iteration 293, loss = 0.47764790\n",
      "Iteration 294, loss = 0.47789929\n",
      "Iteration 295, loss = 0.47764498\n",
      "Iteration 296, loss = 0.47757989\n",
      "Iteration 297, loss = 0.47744736\n",
      "Iteration 298, loss = 0.47798584\n",
      "Iteration 299, loss = 0.47732864\n",
      "Iteration 300, loss = 0.47739586\n",
      "Iteration 301, loss = 0.47745554\n",
      "Iteration 302, loss = 0.47726217\n",
      "Iteration 303, loss = 0.47715711\n",
      "Iteration 304, loss = 0.47703097\n",
      "Iteration 305, loss = 0.47714975\n",
      "Iteration 306, loss = 0.47719733\n",
      "Iteration 307, loss = 0.47709283\n",
      "Iteration 308, loss = 0.47711703\n",
      "Iteration 309, loss = 0.47738261\n",
      "Iteration 310, loss = 0.47707299\n",
      "Iteration 311, loss = 0.47702553\n",
      "Iteration 312, loss = 0.47680959\n",
      "Iteration 313, loss = 0.47670929\n",
      "Iteration 314, loss = 0.47687667\n",
      "Iteration 315, loss = 0.47663488\n",
      "Iteration 316, loss = 0.47661484\n",
      "Iteration 317, loss = 0.47685861\n",
      "Iteration 318, loss = 0.47654644\n",
      "Iteration 319, loss = 0.47642013\n",
      "Iteration 320, loss = 0.47647198\n",
      "Iteration 321, loss = 0.47640194\n",
      "Iteration 322, loss = 0.47665220\n",
      "Iteration 323, loss = 0.47624655\n",
      "Iteration 324, loss = 0.47629295\n",
      "Iteration 325, loss = 0.47655260\n",
      "Iteration 326, loss = 0.47616681\n",
      "Iteration 327, loss = 0.47609701\n",
      "Iteration 328, loss = 0.47595509\n",
      "Iteration 329, loss = 0.47607979\n",
      "Iteration 330, loss = 0.47588151\n",
      "Iteration 331, loss = 0.47582804\n",
      "Iteration 332, loss = 0.47561523\n",
      "Iteration 333, loss = 0.47593652\n",
      "Iteration 334, loss = 0.47587003\n",
      "Iteration 335, loss = 0.47555852\n",
      "Iteration 336, loss = 0.47567821\n",
      "Iteration 337, loss = 0.47522958\n",
      "Iteration 338, loss = 0.47542333\n",
      "Iteration 339, loss = 0.47575208\n",
      "Iteration 340, loss = 0.47533813\n",
      "Iteration 341, loss = 0.47539501\n",
      "Iteration 342, loss = 0.47507185\n",
      "Iteration 343, loss = 0.47513921\n",
      "Iteration 344, loss = 0.47519780\n",
      "Iteration 345, loss = 0.47503257\n",
      "Iteration 346, loss = 0.47493451\n",
      "Iteration 347, loss = 0.47489871\n",
      "Iteration 348, loss = 0.47504974\n",
      "Iteration 349, loss = 0.47492496\n",
      "Iteration 350, loss = 0.47511518\n",
      "Iteration 351, loss = 0.47471111\n",
      "Iteration 352, loss = 0.47486089\n",
      "Iteration 353, loss = 0.47516741\n",
      "Iteration 354, loss = 0.47452064\n",
      "Iteration 355, loss = 0.47461698\n",
      "Iteration 356, loss = 0.47448948\n",
      "Iteration 357, loss = 0.47464518\n",
      "Iteration 358, loss = 0.47463735\n",
      "Iteration 359, loss = 0.47482978\n",
      "Iteration 360, loss = 0.47436645\n",
      "Iteration 361, loss = 0.47444412\n",
      "Iteration 362, loss = 0.47439060\n",
      "Iteration 363, loss = 0.47405466\n",
      "Iteration 364, loss = 0.47422165\n",
      "Iteration 365, loss = 0.47398365\n",
      "Iteration 366, loss = 0.47422561\n",
      "Iteration 367, loss = 0.47403559\n",
      "Iteration 368, loss = 0.47395348\n",
      "Iteration 369, loss = 0.47383130\n",
      "Iteration 370, loss = 0.47383313\n",
      "Iteration 371, loss = 0.47378857\n",
      "Iteration 372, loss = 0.47397908\n",
      "Iteration 373, loss = 0.47391041\n",
      "Iteration 374, loss = 0.47386879\n",
      "Iteration 375, loss = 0.47359932\n",
      "Iteration 376, loss = 0.47382827\n",
      "Iteration 377, loss = 0.47356361\n",
      "Iteration 378, loss = 0.47359032\n",
      "Iteration 379, loss = 0.47347713\n",
      "Iteration 380, loss = 0.47336217\n",
      "Iteration 381, loss = 0.47350082\n",
      "Iteration 382, loss = 0.47330158\n",
      "Iteration 383, loss = 0.47333030\n",
      "Iteration 384, loss = 0.47322390\n",
      "Iteration 385, loss = 0.47307645\n",
      "Iteration 386, loss = 0.47357182\n",
      "Iteration 387, loss = 0.47303192\n",
      "Iteration 388, loss = 0.47307567\n",
      "Iteration 389, loss = 0.47313663\n",
      "Iteration 390, loss = 0.47286617\n",
      "Iteration 391, loss = 0.47314468\n",
      "Iteration 392, loss = 0.47256116\n",
      "Iteration 393, loss = 0.47270738\n",
      "Iteration 394, loss = 0.47266729\n",
      "Iteration 395, loss = 0.47274039\n",
      "Iteration 396, loss = 0.47282712\n",
      "Iteration 397, loss = 0.47273671\n",
      "Iteration 398, loss = 0.47262433\n",
      "Iteration 399, loss = 0.47246497\n",
      "Iteration 400, loss = 0.47272211\n",
      "Iteration 401, loss = 0.47277505\n",
      "Iteration 402, loss = 0.47290662\n",
      "Iteration 403, loss = 0.47230251\n",
      "Iteration 404, loss = 0.47274301\n",
      "Iteration 405, loss = 0.47250505\n",
      "Iteration 406, loss = 0.47229753\n",
      "Iteration 407, loss = 0.47245463\n",
      "Iteration 408, loss = 0.47214376\n",
      "Iteration 409, loss = 0.47210782\n",
      "Iteration 410, loss = 0.47212081\n",
      "Iteration 411, loss = 0.47215346\n",
      "Iteration 412, loss = 0.47200045\n",
      "Iteration 413, loss = 0.47209908\n",
      "Iteration 414, loss = 0.47216216\n",
      "Iteration 415, loss = 0.47195337\n",
      "Iteration 416, loss = 0.47202093\n",
      "Iteration 417, loss = 0.47206229\n",
      "Iteration 418, loss = 0.47183177\n",
      "Iteration 419, loss = 0.47167305\n",
      "Iteration 420, loss = 0.47167922\n",
      "Iteration 421, loss = 0.47172797\n",
      "Iteration 422, loss = 0.47173352\n",
      "Iteration 423, loss = 0.47218242\n",
      "Iteration 424, loss = 0.47181815\n",
      "Iteration 425, loss = 0.47196517\n",
      "Iteration 426, loss = 0.47181439\n",
      "Iteration 427, loss = 0.47178380\n",
      "Iteration 428, loss = 0.47189902\n",
      "Iteration 429, loss = 0.47144997\n",
      "Iteration 430, loss = 0.47150822\n",
      "Iteration 431, loss = 0.47142875\n",
      "Iteration 432, loss = 0.47167354\n",
      "Iteration 433, loss = 0.47129423\n",
      "Iteration 434, loss = 0.47152354\n",
      "Iteration 435, loss = 0.47142552\n",
      "Iteration 436, loss = 0.47117834\n",
      "Iteration 437, loss = 0.47101525\n",
      "Iteration 438, loss = 0.47110620\n",
      "Iteration 439, loss = 0.47109311\n",
      "Iteration 440, loss = 0.47119434\n",
      "Iteration 441, loss = 0.47105331\n",
      "Iteration 442, loss = 0.47096815\n",
      "Iteration 443, loss = 0.47084065\n",
      "Iteration 444, loss = 0.47089264\n",
      "Iteration 445, loss = 0.47071316\n",
      "Iteration 446, loss = 0.47105958\n",
      "Iteration 447, loss = 0.47066116\n",
      "Iteration 448, loss = 0.47067764\n",
      "Iteration 449, loss = 0.47039218\n",
      "Iteration 450, loss = 0.47083507\n",
      "Iteration 451, loss = 0.47064478\n",
      "Iteration 452, loss = 0.47023785\n",
      "Iteration 453, loss = 0.47060442\n",
      "Iteration 454, loss = 0.47021594\n",
      "Iteration 455, loss = 0.47034848\n",
      "Iteration 456, loss = 0.47033284\n",
      "Iteration 457, loss = 0.47020782\n",
      "Iteration 458, loss = 0.47016284\n",
      "Iteration 459, loss = 0.47002589\n",
      "Iteration 460, loss = 0.47014956\n",
      "Iteration 461, loss = 0.46989739\n",
      "Iteration 462, loss = 0.46979597\n",
      "Iteration 463, loss = 0.46986042\n",
      "Iteration 464, loss = 0.46974214\n",
      "Iteration 465, loss = 0.46958370\n",
      "Iteration 466, loss = 0.47001973\n",
      "Iteration 467, loss = 0.46966286\n",
      "Iteration 468, loss = 0.46940337\n",
      "Iteration 469, loss = 0.46937900\n",
      "Iteration 470, loss = 0.46935094\n",
      "Iteration 471, loss = 0.46915760\n",
      "Iteration 472, loss = 0.46931535\n",
      "Iteration 473, loss = 0.46967738\n",
      "Iteration 474, loss = 0.46987234\n",
      "Iteration 475, loss = 0.46947700\n",
      "Iteration 476, loss = 0.46928030\n",
      "Iteration 477, loss = 0.46894714\n",
      "Iteration 478, loss = 0.46885036\n",
      "Iteration 479, loss = 0.46900500\n",
      "Iteration 480, loss = 0.46901112\n",
      "Iteration 481, loss = 0.46892947\n",
      "Iteration 482, loss = 0.46880456\n",
      "Iteration 483, loss = 0.46881155\n",
      "Iteration 484, loss = 0.46912499\n",
      "Iteration 485, loss = 0.46863275\n",
      "Iteration 486, loss = 0.46874422\n",
      "Iteration 487, loss = 0.46859912\n",
      "Iteration 488, loss = 0.46875314\n",
      "Iteration 489, loss = 0.46850980\n",
      "Iteration 490, loss = 0.46848116\n",
      "Iteration 491, loss = 0.46851210\n",
      "Iteration 492, loss = 0.46834794\n",
      "Iteration 493, loss = 0.46837985\n",
      "Iteration 494, loss = 0.46830691\n",
      "Iteration 495, loss = 0.46827677\n",
      "Iteration 496, loss = 0.46805767\n",
      "Iteration 497, loss = 0.46817577\n",
      "Iteration 498, loss = 0.46823694\n",
      "Iteration 499, loss = 0.46804378\n",
      "Iteration 500, loss = 0.46824995\n",
      "Iteration 501, loss = 0.46804413\n",
      "Iteration 502, loss = 0.46825083\n",
      "Iteration 503, loss = 0.46794314\n",
      "Iteration 504, loss = 0.46814470\n",
      "Iteration 505, loss = 0.46817293\n",
      "Iteration 506, loss = 0.46789531\n",
      "Iteration 507, loss = 0.46770774\n",
      "Iteration 508, loss = 0.46832028\n",
      "Iteration 509, loss = 0.46783646\n",
      "Iteration 510, loss = 0.46775290\n",
      "Iteration 511, loss = 0.46772154\n",
      "Iteration 512, loss = 0.46770524\n",
      "Iteration 513, loss = 0.46749491\n",
      "Iteration 514, loss = 0.46749400\n",
      "Iteration 515, loss = 0.46759716\n",
      "Iteration 516, loss = 0.46753484\n",
      "Iteration 517, loss = 0.46740402\n",
      "Iteration 518, loss = 0.46737224\n",
      "Iteration 519, loss = 0.46767374\n",
      "Iteration 520, loss = 0.46774114\n",
      "Iteration 521, loss = 0.46732196\n",
      "Iteration 522, loss = 0.46715970\n",
      "Iteration 523, loss = 0.46715565\n",
      "Iteration 524, loss = 0.46702702\n",
      "Iteration 525, loss = 0.46722753\n",
      "Iteration 526, loss = 0.46718128\n",
      "Iteration 527, loss = 0.46725849\n",
      "Iteration 528, loss = 0.46698910\n",
      "Iteration 529, loss = 0.46687981\n",
      "Iteration 530, loss = 0.46698179\n",
      "Iteration 531, loss = 0.46691697\n",
      "Iteration 532, loss = 0.46689406\n",
      "Iteration 533, loss = 0.46683252\n",
      "Iteration 534, loss = 0.46696743\n",
      "Iteration 535, loss = 0.46682870\n",
      "Iteration 536, loss = 0.46675029\n",
      "Iteration 537, loss = 0.46665472\n",
      "Iteration 538, loss = 0.46665048\n",
      "Iteration 539, loss = 0.46685630\n",
      "Iteration 540, loss = 0.46649753\n",
      "Iteration 541, loss = 0.46682390\n",
      "Iteration 542, loss = 0.46716551\n",
      "Iteration 543, loss = 0.46647094\n",
      "Iteration 544, loss = 0.46633568\n",
      "Iteration 545, loss = 0.46648074\n",
      "Iteration 546, loss = 0.46630383\n",
      "Iteration 547, loss = 0.46629489\n",
      "Iteration 548, loss = 0.46635492\n",
      "Iteration 549, loss = 0.46637641\n",
      "Iteration 550, loss = 0.46673072\n",
      "Iteration 551, loss = 0.46649756\n",
      "Iteration 552, loss = 0.46604765\n",
      "Iteration 553, loss = 0.46618441\n",
      "Iteration 554, loss = 0.46605615\n",
      "Iteration 555, loss = 0.46603672\n",
      "Iteration 556, loss = 0.46607648\n",
      "Iteration 557, loss = 0.46618799\n",
      "Iteration 558, loss = 0.46588195\n",
      "Iteration 559, loss = 0.46596859\n",
      "Iteration 560, loss = 0.46606647\n",
      "Iteration 561, loss = 0.46601823\n",
      "Iteration 562, loss = 0.46578021\n",
      "Iteration 563, loss = 0.46577635\n",
      "Iteration 564, loss = 0.46578840\n",
      "Iteration 565, loss = 0.46558054\n",
      "Iteration 566, loss = 0.46560410\n",
      "Iteration 567, loss = 0.46567652\n",
      "Iteration 568, loss = 0.46547749\n",
      "Iteration 569, loss = 0.46548086\n",
      "Iteration 570, loss = 0.46548144\n",
      "Iteration 571, loss = 0.46538989\n",
      "Iteration 572, loss = 0.46546628\n",
      "Iteration 573, loss = 0.46534582\n",
      "Iteration 574, loss = 0.46539941\n",
      "Iteration 575, loss = 0.46528367\n",
      "Iteration 576, loss = 0.46552989\n",
      "Iteration 577, loss = 0.46602664\n",
      "Iteration 578, loss = 0.46520458\n",
      "Iteration 579, loss = 0.46534237\n",
      "Iteration 580, loss = 0.46507022\n",
      "Iteration 581, loss = 0.46530670\n",
      "Iteration 582, loss = 0.46535216\n",
      "Iteration 583, loss = 0.46496377\n",
      "Iteration 584, loss = 0.46486041\n",
      "Iteration 585, loss = 0.46518404\n",
      "Iteration 586, loss = 0.46524889\n",
      "Iteration 587, loss = 0.46503330\n",
      "Iteration 588, loss = 0.46486718\n",
      "Iteration 589, loss = 0.46473968\n",
      "Iteration 590, loss = 0.46486633\n",
      "Iteration 591, loss = 0.46473377\n",
      "Iteration 592, loss = 0.46470122\n",
      "Iteration 593, loss = 0.46504016\n",
      "Iteration 594, loss = 0.46464474\n",
      "Iteration 595, loss = 0.46465947\n",
      "Iteration 596, loss = 0.46455934\n",
      "Iteration 597, loss = 0.46453315\n",
      "Iteration 598, loss = 0.46481852\n",
      "Iteration 599, loss = 0.46525695\n",
      "Iteration 600, loss = 0.46422884\n",
      "Iteration 601, loss = 0.46439467\n",
      "Iteration 602, loss = 0.46435796\n",
      "Iteration 603, loss = 0.46427257\n",
      "Iteration 604, loss = 0.46416954\n",
      "Iteration 605, loss = 0.46412724\n",
      "Iteration 606, loss = 0.46423006\n",
      "Iteration 607, loss = 0.46449999\n",
      "Iteration 608, loss = 0.46436011\n",
      "Iteration 609, loss = 0.46469680\n",
      "Iteration 610, loss = 0.46401575\n",
      "Iteration 611, loss = 0.46453197\n",
      "Iteration 612, loss = 0.46416566\n",
      "Iteration 613, loss = 0.46419920\n",
      "Iteration 614, loss = 0.46379081\n",
      "Iteration 615, loss = 0.46394210\n",
      "Iteration 616, loss = 0.46398412\n",
      "Iteration 617, loss = 0.46408015\n",
      "Iteration 618, loss = 0.46400004\n",
      "Iteration 619, loss = 0.46376130\n",
      "Iteration 620, loss = 0.46392400\n",
      "Iteration 621, loss = 0.46384001\n",
      "Iteration 622, loss = 0.46372476\n",
      "Iteration 623, loss = 0.46345320\n",
      "Iteration 624, loss = 0.46350793\n",
      "Iteration 625, loss = 0.46347381\n",
      "Iteration 626, loss = 0.46356976\n",
      "Iteration 627, loss = 0.46349951\n",
      "Iteration 628, loss = 0.46342264\n",
      "Iteration 629, loss = 0.46324370\n",
      "Iteration 630, loss = 0.46310264\n",
      "Iteration 631, loss = 0.46342732\n",
      "Iteration 632, loss = 0.46324425\n",
      "Iteration 633, loss = 0.46329970\n",
      "Iteration 634, loss = 0.46326866\n",
      "Iteration 635, loss = 0.46345962\n",
      "Iteration 636, loss = 0.46309211\n",
      "Iteration 637, loss = 0.46339990\n",
      "Iteration 638, loss = 0.46308792\n",
      "Iteration 639, loss = 0.46334379\n",
      "Iteration 640, loss = 0.46289088\n",
      "Iteration 641, loss = 0.46292332\n",
      "Iteration 642, loss = 0.46281904\n",
      "Iteration 643, loss = 0.46287219\n",
      "Iteration 644, loss = 0.46306140\n",
      "Iteration 645, loss = 0.46287082\n",
      "Iteration 646, loss = 0.46276043\n",
      "Iteration 647, loss = 0.46281871\n",
      "Iteration 648, loss = 0.46260687\n",
      "Iteration 649, loss = 0.46273854\n",
      "Iteration 650, loss = 0.46259306\n",
      "Iteration 651, loss = 0.46278714\n",
      "Iteration 652, loss = 0.46248398\n",
      "Iteration 653, loss = 0.46244908\n",
      "Iteration 654, loss = 0.46256789\n",
      "Iteration 655, loss = 0.46259212\n",
      "Iteration 656, loss = 0.46255292\n",
      "Iteration 657, loss = 0.46234283\n",
      "Iteration 658, loss = 0.46247169\n",
      "Iteration 659, loss = 0.46226352\n",
      "Iteration 660, loss = 0.46244467\n",
      "Iteration 661, loss = 0.46234955\n",
      "Iteration 662, loss = 0.46220824\n",
      "Iteration 663, loss = 0.46229942\n",
      "Iteration 664, loss = 0.46223468\n",
      "Iteration 665, loss = 0.46219845\n",
      "Iteration 666, loss = 0.46230034\n",
      "Iteration 667, loss = 0.46243766\n",
      "Iteration 668, loss = 0.46217811\n",
      "Iteration 669, loss = 0.46212718\n",
      "Iteration 670, loss = 0.46223573\n",
      "Iteration 671, loss = 0.46204566\n",
      "Iteration 672, loss = 0.46188659\n",
      "Iteration 673, loss = 0.46192678\n",
      "Iteration 674, loss = 0.46253375\n",
      "Iteration 675, loss = 0.46216194\n",
      "Iteration 676, loss = 0.46195420\n",
      "Iteration 677, loss = 0.46187376\n",
      "Iteration 678, loss = 0.46196629\n",
      "Iteration 679, loss = 0.46188941\n",
      "Iteration 680, loss = 0.46188018\n",
      "Iteration 681, loss = 0.46166290\n",
      "Iteration 682, loss = 0.46184305\n",
      "Iteration 683, loss = 0.46162399\n",
      "Iteration 684, loss = 0.46166698\n",
      "Iteration 685, loss = 0.46203464\n",
      "Iteration 686, loss = 0.46174068\n",
      "Iteration 687, loss = 0.46148902\n",
      "Iteration 688, loss = 0.46178071\n",
      "Iteration 689, loss = 0.46130378\n",
      "Iteration 690, loss = 0.46214093\n",
      "Iteration 691, loss = 0.46154602\n",
      "Iteration 692, loss = 0.46203063\n",
      "Iteration 693, loss = 0.46155706\n",
      "Iteration 694, loss = 0.46144161\n",
      "Iteration 695, loss = 0.46145909\n",
      "Iteration 696, loss = 0.46129859\n",
      "Iteration 697, loss = 0.46117973\n",
      "Iteration 698, loss = 0.46136598\n",
      "Iteration 699, loss = 0.46138183\n",
      "Iteration 700, loss = 0.46130728\n",
      "Iteration 701, loss = 0.46097679\n",
      "Iteration 702, loss = 0.46128346\n",
      "Iteration 703, loss = 0.46111604\n",
      "Iteration 704, loss = 0.46112334\n",
      "Iteration 705, loss = 0.46135513\n",
      "Iteration 706, loss = 0.46126693\n",
      "Iteration 707, loss = 0.46125052\n",
      "Iteration 708, loss = 0.46081163\n",
      "Iteration 709, loss = 0.46099628\n",
      "Iteration 710, loss = 0.46072650\n",
      "Iteration 711, loss = 0.46113915\n",
      "Iteration 712, loss = 0.46083617\n",
      "Iteration 713, loss = 0.46086448\n",
      "Iteration 714, loss = 0.46071447\n",
      "Iteration 715, loss = 0.46069477\n",
      "Iteration 716, loss = 0.46093701\n",
      "Iteration 717, loss = 0.46081261\n",
      "Iteration 718, loss = 0.46045716\n",
      "Iteration 719, loss = 0.46078455\n",
      "Iteration 720, loss = 0.46042693\n",
      "Iteration 721, loss = 0.46036901\n",
      "Iteration 722, loss = 0.46042878\n",
      "Iteration 723, loss = 0.46022452\n",
      "Iteration 724, loss = 0.46016978\n",
      "Iteration 725, loss = 0.46028534\n",
      "Iteration 726, loss = 0.46018035\n",
      "Iteration 727, loss = 0.46013807\n",
      "Iteration 728, loss = 0.46030622\n",
      "Iteration 729, loss = 0.46010592\n",
      "Iteration 730, loss = 0.46012676\n",
      "Iteration 731, loss = 0.46014924\n",
      "Iteration 732, loss = 0.46023096\n",
      "Iteration 733, loss = 0.45986992\n",
      "Iteration 734, loss = 0.46001780\n",
      "Iteration 735, loss = 0.45991786\n",
      "Iteration 736, loss = 0.45991067\n",
      "Iteration 737, loss = 0.45990234\n",
      "Iteration 738, loss = 0.45971944\n",
      "Iteration 739, loss = 0.45982988\n",
      "Iteration 740, loss = 0.46009978\n",
      "Iteration 741, loss = 0.45986340\n",
      "Iteration 742, loss = 0.45991466\n",
      "Iteration 743, loss = 0.45975604\n",
      "Iteration 744, loss = 0.45967406\n",
      "Iteration 745, loss = 0.45977091\n",
      "Iteration 746, loss = 0.45984829\n",
      "Iteration 747, loss = 0.45979903\n",
      "Iteration 748, loss = 0.45977243\n",
      "Iteration 749, loss = 0.45965572\n",
      "Iteration 750, loss = 0.45967857\n",
      "Iteration 751, loss = 0.45951828\n",
      "Iteration 752, loss = 0.45972168\n",
      "Iteration 753, loss = 0.45989404\n",
      "Iteration 754, loss = 0.45973692\n",
      "Iteration 755, loss = 0.45954698\n",
      "Iteration 756, loss = 0.45946170\n",
      "Iteration 757, loss = 0.45940133\n",
      "Iteration 758, loss = 0.45990784\n",
      "Iteration 759, loss = 0.45939977\n",
      "Iteration 760, loss = 0.45916867\n",
      "Iteration 761, loss = 0.45997408\n",
      "Iteration 762, loss = 0.45947781\n",
      "Iteration 763, loss = 0.45955349\n",
      "Iteration 764, loss = 0.45916250\n",
      "Iteration 765, loss = 0.45921903\n",
      "Iteration 766, loss = 0.45917874\n",
      "Iteration 767, loss = 0.45936029\n",
      "Iteration 768, loss = 0.45935306\n",
      "Iteration 769, loss = 0.45936265\n",
      "Iteration 770, loss = 0.45905037\n",
      "Iteration 771, loss = 0.45907734\n",
      "Iteration 772, loss = 0.45901081\n",
      "Iteration 773, loss = 0.45903748\n",
      "Iteration 774, loss = 0.45875777\n",
      "Iteration 775, loss = 0.45906511\n",
      "Iteration 776, loss = 0.45919721\n",
      "Iteration 777, loss = 0.45865618\n",
      "Iteration 778, loss = 0.45889516\n",
      "Iteration 779, loss = 0.45900448\n",
      "Iteration 780, loss = 0.45868644\n",
      "Iteration 781, loss = 0.45881658\n",
      "Iteration 782, loss = 0.45872692\n",
      "Iteration 783, loss = 0.45895602\n",
      "Iteration 784, loss = 0.45888819\n",
      "Iteration 785, loss = 0.45848424\n",
      "Iteration 786, loss = 0.45855550\n",
      "Iteration 787, loss = 0.45865477\n",
      "Iteration 788, loss = 0.45853305\n",
      "Iteration 789, loss = 0.45860319\n",
      "Iteration 790, loss = 0.45842570\n",
      "Iteration 791, loss = 0.45868900\n",
      "Iteration 792, loss = 0.45872787\n",
      "Iteration 793, loss = 0.45861208\n",
      "Iteration 794, loss = 0.45847999\n",
      "Iteration 795, loss = 0.45832540\n",
      "Iteration 796, loss = 0.45837931\n",
      "Iteration 797, loss = 0.45831758\n",
      "Iteration 798, loss = 0.45890409\n",
      "Iteration 799, loss = 0.45826165\n",
      "Iteration 800, loss = 0.45843710\n",
      "Iteration 801, loss = 0.45852299\n",
      "Iteration 802, loss = 0.45824403\n",
      "Iteration 803, loss = 0.45826395\n",
      "Iteration 804, loss = 0.45802528\n",
      "Iteration 805, loss = 0.45841004\n",
      "Iteration 806, loss = 0.45814598\n",
      "Iteration 807, loss = 0.45811699\n",
      "Iteration 808, loss = 0.45815862\n",
      "Iteration 809, loss = 0.45811779\n",
      "Iteration 810, loss = 0.45803770\n",
      "Iteration 811, loss = 0.45813271\n",
      "Iteration 812, loss = 0.45787600\n",
      "Iteration 813, loss = 0.45795041\n",
      "Iteration 814, loss = 0.45791132\n",
      "Iteration 815, loss = 0.45784823\n",
      "Iteration 816, loss = 0.45774347\n",
      "Iteration 817, loss = 0.45794866\n",
      "Iteration 818, loss = 0.45766696\n",
      "Iteration 819, loss = 0.45768403\n",
      "Iteration 820, loss = 0.45807111\n",
      "Iteration 821, loss = 0.45787689\n",
      "Iteration 822, loss = 0.45785828\n",
      "Iteration 823, loss = 0.45767894\n",
      "Iteration 824, loss = 0.45789181\n",
      "Iteration 825, loss = 0.45785996\n",
      "Iteration 826, loss = 0.45750477\n",
      "Iteration 827, loss = 0.45756413\n",
      "Iteration 828, loss = 0.45743499\n",
      "Iteration 829, loss = 0.45752935\n",
      "Iteration 830, loss = 0.45749615\n",
      "Iteration 831, loss = 0.45731782\n",
      "Iteration 832, loss = 0.45741389\n",
      "Iteration 833, loss = 0.45763851\n",
      "Iteration 834, loss = 0.45757864\n",
      "Iteration 835, loss = 0.45719766\n",
      "Iteration 836, loss = 0.45745539\n",
      "Iteration 837, loss = 0.45730601\n",
      "Iteration 838, loss = 0.45735429\n",
      "Iteration 839, loss = 0.45739552\n",
      "Iteration 840, loss = 0.45706650\n",
      "Iteration 841, loss = 0.45745679\n",
      "Iteration 842, loss = 0.45733165\n",
      "Iteration 843, loss = 0.45749375\n",
      "Iteration 844, loss = 0.45759454\n",
      "Iteration 845, loss = 0.45716705\n",
      "Iteration 846, loss = 0.45730186\n",
      "Iteration 847, loss = 0.45715062\n",
      "Iteration 848, loss = 0.45705102\n",
      "Iteration 849, loss = 0.45709848\n",
      "Iteration 850, loss = 0.45704495\n",
      "Iteration 851, loss = 0.45717322\n",
      "Iteration 852, loss = 0.45690629\n",
      "Iteration 853, loss = 0.45744758\n",
      "Iteration 854, loss = 0.45702492\n",
      "Iteration 855, loss = 0.45694322\n",
      "Iteration 856, loss = 0.45670535\n",
      "Iteration 857, loss = 0.45682694\n",
      "Iteration 858, loss = 0.45676626\n",
      "Iteration 859, loss = 0.45695072\n",
      "Iteration 860, loss = 0.45683455\n",
      "Iteration 861, loss = 0.45664227\n",
      "Iteration 862, loss = 0.45672523\n",
      "Iteration 863, loss = 0.45664222\n",
      "Iteration 864, loss = 0.45652807\n",
      "Iteration 865, loss = 0.45651849\n",
      "Iteration 866, loss = 0.45635047\n",
      "Iteration 867, loss = 0.45629800\n",
      "Iteration 868, loss = 0.45647843\n",
      "Iteration 869, loss = 0.45680032\n",
      "Iteration 870, loss = 0.45619704\n",
      "Iteration 871, loss = 0.45628481\n",
      "Iteration 872, loss = 0.45635821\n",
      "Iteration 873, loss = 0.45633322\n",
      "Iteration 874, loss = 0.45660504\n",
      "Iteration 875, loss = 0.45602809\n",
      "Iteration 876, loss = 0.45613785\n",
      "Iteration 877, loss = 0.45627434\n",
      "Iteration 878, loss = 0.45616192\n",
      "Iteration 879, loss = 0.45616665\n",
      "Iteration 880, loss = 0.45627705\n",
      "Iteration 881, loss = 0.45613202\n",
      "Iteration 882, loss = 0.45600690\n",
      "Iteration 883, loss = 0.45598927\n",
      "Iteration 884, loss = 0.45603936\n",
      "Iteration 885, loss = 0.45575761\n",
      "Iteration 886, loss = 0.45643079\n",
      "Iteration 887, loss = 0.45595853\n",
      "Iteration 888, loss = 0.45575693\n",
      "Iteration 889, loss = 0.45607325\n",
      "Iteration 890, loss = 0.45582969\n",
      "Iteration 891, loss = 0.45561223\n",
      "Iteration 892, loss = 0.45605210\n",
      "Iteration 893, loss = 0.45584828\n",
      "Iteration 894, loss = 0.45579266\n",
      "Iteration 895, loss = 0.45538661\n",
      "Iteration 896, loss = 0.45558761\n",
      "Iteration 897, loss = 0.45541152\n",
      "Iteration 898, loss = 0.45527050\n",
      "Iteration 899, loss = 0.45558233\n",
      "Iteration 900, loss = 0.45544878\n",
      "Iteration 901, loss = 0.45542259\n",
      "Iteration 902, loss = 0.45541928\n",
      "Iteration 903, loss = 0.45548845\n",
      "Iteration 904, loss = 0.45539252\n",
      "Iteration 905, loss = 0.45542770\n",
      "Iteration 906, loss = 0.45548041\n",
      "Iteration 907, loss = 0.45586290\n",
      "Iteration 908, loss = 0.45537125\n",
      "Iteration 909, loss = 0.45532541\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "scores_ann = cross_val_score(model_ann, X_customer_balanced, Y_customer_balanced, cv=kf_ann, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_ann_boosted = cross_val_score(model_ann_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_ann, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.71938776 0.71683673 0.76530612 0.7372449  0.70663265 0.70408163\n",
      " 0.73469388 0.78061224 0.73913043 0.77493606]\n",
      "Score médio: 0.7378862414531029\n",
      "Desvio padrão: 0.026204990025326523\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_ann}\")\n",
    "print(f\"Score médio: {np.mean(scores_ann)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_ann)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exibindo os resultados\n",
    "# print(f\"Scores de cada fold: {scores_ann_boosted}\")\n",
    "# print(f\"Score médio: {np.mean(scores_ann_boosted)}\")\n",
    "# print(f\"Desvio padrão: {np.std(scores_ann_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros = {\n",
    "#     'activation': ['relu', 'logistic', 'tanh'],  # Funções de ativação\n",
    "#     'solver': ['adam', 'sgd'],  # Otimizadores\n",
    "#     'batch_size': [32, 64],  # Tamanhos de lote comuns\n",
    "#     'learning_rate': ['constant', 'adaptive'],  # Taxa de aprendizado\n",
    "#     'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Tamanho das camadas ocultas\n",
    "#     'alpha': [0.0001, 0.001]  # Parâmetro de regularização L2\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(estimator=MLPClassifier(), param_grid=parametros)\n",
    "# grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "# melhores_parametros = grid_search.best_params_\n",
    "# melhor_resultado = grid_search.best_score_\n",
    "# print(melhores_parametros)\n",
    "# print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - 66%(Normal) 72%(Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "model_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted\n",
    "model_tree_boosted = DecisionTreeClassifier(criterion='gini', splitter='random', min_samples_leaf=10, min_samples_split=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_tree = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree = cross_val_score(model_tree, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.6505102  0.65306122 0.66071429 0.67091837 0.62755102 0.63010204\n",
      " 0.68367347 0.69642857 0.69309463 0.68030691]\n",
      "Score médio: 0.6646360718200324\n",
      "Desvio padrão: 0.023205420177937815\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tree_boosted = cross_val_score(model_tree_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_tree, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70918367 0.70153061 0.73979592 0.73469388 0.72193878 0.69897959\n",
      " 0.70918367 0.7627551  0.73401535 0.73657289]\n",
      "Score médio: 0.7248649459783915\n",
      "Desvio padrão: 0.019211770606043753\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_tree_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_tree_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_tree_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "parametros = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'min_samples_leaf': 10, 'min_samples_split': 5, 'splitter': 'random'}\n",
      "0.7309846352334037\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculando desvio padrão\n",
    "# std_dev = np.std(scores_tree_boosted)\n",
    "\n",
    "# # Plotando o desvio padrão\n",
    "# plt.bar('Desvio Padrão', std_dev)\n",
    "# plt.ylabel('Valor')\n",
    "# plt.title('Desvio Padrão dos Scores')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - 70% Boosted(72%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn_boosted = KNeighborsClassifier(n_neighbors=25, metric='minkowski', p = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_knn = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn = cross_val_score(model_knn, X_customer_balanced, Y_customer_balanced, cv=kf_knn, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_knn_boosted = cross_val_score(model_knn_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_knn, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.70153061 0.70408163 0.72704082 0.69132653 0.68877551 0.66071429\n",
      " 0.68367347 0.75510204 0.70588235 0.71611253]\n",
      "Score médio: 0.703423978286967\n",
      "Desvio padrão: 0.02444292091659416\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_knn}\")\n",
    "print(f\"Score médio: {np.mean(scores_knn)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_knn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.74489796 0.72704082 0.75255102 0.74234694 0.68877551 0.70918367\n",
      " 0.71428571 0.75765306 0.72122762 0.72890026]\n",
      "Score médio: 0.7286862571115402\n",
      "Desvio padrão: 0.020241083994342945\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_knn_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_knn_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_knn_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "parametros = {\n",
    "    'n_neighbors': range(1, 31),  # Test a range of neighbor values\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan'],  # Different distance metrics\n",
    "    'p': [1, 2]  # Minkowski parameter (1 for manhattan, 2 for euclidean)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'minkowski', 'n_neighbors': 25, 'p': 1}\n",
      "0.7302258451273229\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(random_state=42, max_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_boosted = LogisticRegression(random_state=0, max_iter=100, C=1.0, solver='newton-cg', tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_logistic = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic = cross_val_score(model_logistic, X_customer_balanced, Y_customer_balanced, cv=kf_logistic, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_logistic_boosted = cross_val_score(model_logistic_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_logistic, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.67602041 0.70663265 0.71938776 0.71683673 0.66326531 0.67091837\n",
      " 0.72959184 0.75255102 0.71355499 0.74680307]\n",
      "Score médio: 0.7095562137898638\n",
      "Desvio padrão: 0.029277793572685756\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_logistic}\")\n",
    "print(f\"Score médio: {np.mean(scores_logistic)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_logistic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.67602041 0.70663265 0.71938776 0.71683673 0.66326531 0.67091837\n",
      " 0.72959184 0.75255102 0.71355499 0.74680307]\n",
      "Score médio: 0.7095562137898638\n",
      "Desvio padrão: 0.029277793572685756\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_logistic_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_logistic_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_logistic_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "parametros = {\n",
    "    'C': [1.0, 1.5, 2.0],  # Regularization strength\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],  # Solvers\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'tol': [0.0001, 0.00001, 0.000001]# Number of iterations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'max_iter': 100, 'solver': 'newton-cg', 'tol': 0.0001}\n",
      "0.7064893007011234\n"
     ]
    }
   ],
   "source": [
    "# Finding Best Params\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive = GaussianNB()\n",
    "kf_naive = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = cross_val_score(model_naive, X_customer_balanced, Y_customer_balanced, cv=kf_naive, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.68877551 0.7244898  0.70918367 0.73214286 0.68877551 0.68112245\n",
      " 0.72193878 0.77295918 0.72634271 0.75959079]\n",
      "Score médio: 0.7205321258938358\n",
      "Desvio padrão: 0.028564427059807447\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_naive}\")\n",
    "print(f\"Score médio: {np.mean(scores_naive)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_naive)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - 75% - 76% Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(n_estimators=80, criterion='entropy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest_boosted = RandomForestClassifier(n_estimators=150, criterion='gini', random_state=42, min_samples_leaf=10, min_samples_split=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_forest = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_forest_boosted = cross_val_score(model_forest_boosted, X_customer_balanced, Y_customer_balanced, cv=kf_forest, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_forest = cross_val_score(model_forest, X_customer_balanced, Y_customer_balanced, cv=kf_forest, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.75       0.75765306 0.7627551  0.75       0.7372449  0.72193878\n",
      " 0.73469388 0.78316327 0.76470588 0.76982097]\n",
      "Score médio: 0.7531975833811785\n",
      "Desvio padrão: 0.01735613718267566\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_forest}\")\n",
    "print(f\"Score médio: {np.mean(scores_forest)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_forest)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de cada fold: [0.74234694 0.78571429 0.75510204 0.76020408 0.7372449  0.70153061\n",
      " 0.76020408 0.78571429 0.76726343 0.78772379]\n",
      "Score médio: 0.7583048436766011\n",
      "Desvio padrão: 0.025273003212994954\n"
     ]
    }
   ],
   "source": [
    "# Exibindo os resultados\n",
    "print(f\"Scores de cada fold: {scores_forest_boosted}\")\n",
    "print(f\"Score médio: {np.mean(scores_forest_boosted)}\")\n",
    "print(f\"Desvio padrão: {np.std(scores_forest_boosted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [10, 40, 100, 150],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'min_samples_leaf': 10, 'min_samples_split': 5, 'n_estimators': 150}\n",
      "0.7611049208955613\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parametros)\n",
    "grid_search.fit(X_customer_balanced, Y_customer_balanced)\n",
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_resultado = grid_search.best_score_\n",
    "print(melhores_parametros)\n",
    "print(melhor_resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
